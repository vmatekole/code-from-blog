00:00:00,066 --> 00:00:07,422
SPEAKER_0:  You could buy literally whatever else you wanted. You could throw things, drugs. You could buy heroin right from Afghanistan, the good stuff.

00:00:07,906 --> 00:00:11,262
SPEAKER_0:  hacking tools, you could hack for hire, you could buy murders for hire.

00:00:13,794 --> 00:00:14,919
SPEAKER_0:  The following is a conversation.

00:00:14,919 --> 00:00:15,902
SPEAKER_1:  with Chris Tarbell.

00:00:16,322 --> 00:00:19,678
SPEAKER_1:  former FBI Special Agent and cybercrime specialist.

00:00:19,906 --> 00:00:20,702
SPEAKER_1:  who track down.

00:00:20,962 --> 00:00:21,694
SPEAKER_1:  and arrested.

00:00:21,922 --> 00:00:22,942
SPEAKER_1:  Russ Albert

00:00:23,266 --> 00:00:24,606
SPEAKER_1:  the leader of Silk Road.

00:00:25,026 --> 00:00:26,942
SPEAKER_1:  the billion dollar drug marketplace.

00:00:27,266 --> 00:00:27,710
SPEAKER_1:  and

00:00:27,970 --> 00:00:30,526
SPEAKER_1:  he tracked down and arrested Hector Massaguer.

00:00:30,882 --> 00:00:32,030
SPEAKER_1:  AKA SaBlu.

00:00:32,386 --> 00:00:34,302
SPEAKER_1:  of LulzSec and Anonymous.

00:00:34,626 --> 00:00:37,438
SPEAKER_1:  which are some of the most influential hacker groups in history.

00:00:38,146 --> 00:00:42,622
SPEAKER_1:  He is co-founder of Naxo, a complex cybercrime investigation firm.

00:00:43,010 --> 00:00:43,422
SPEAKER_1:  and

00:00:43,650 --> 00:00:45,694
SPEAKER_1:  is a co-host of a podcast called

00:00:45,954 --> 00:00:47,198
SPEAKER_1:  The hacker and the fed.

00:00:47,938 --> 00:00:49,982
SPEAKER_1:  This conversation gives the perspective.

00:00:50,210 --> 00:00:52,542
SPEAKER_1:  of the FBI cybercrime investigator.

00:00:52,898 --> 00:00:55,166
SPEAKER_1:  both the technical and human story.

00:00:55,778 --> 00:00:58,366
SPEAKER_1:  I would also like to interview people on the other side.

00:00:58,978 --> 00:01:00,798
SPEAKER_1:  the cyber criminals who have been caught.

00:01:01,314 --> 00:01:03,038
SPEAKER_1:  and perhaps the cyber criminals.

00:01:03,394 --> 00:01:04,446
SPEAKER_1:  who have not been caught.

00:01:04,866 --> 00:01:05,470
SPEAKER_1:  And are still.

00:01:05,698 --> 00:01:06,206
SPEAKER_1:  out there.

00:01:07,330 --> 00:01:09,022
SPEAKER_1:  This is the Lux Freedman Podcast.

00:01:09,250 --> 00:01:09,822
SPEAKER_1:  to support it.

00:01:10,050 --> 00:01:12,094
SPEAKER_1:  Please check out our sponsors in the description.

00:01:12,418 --> 00:01:13,662
SPEAKER_1:  And now, dear friends.

00:01:13,986 --> 00:01:14,782
SPEAKER_1:  Here's Chris.

00:01:15,042 --> 00:01:15,550
SPEAKER_1:  Tarbell.

00:01:16,610 --> 00:01:23,326
SPEAKER_1:  You are one of the most successful cybersecurity law enforcement agents of all time. Retract and brought down.

00:01:23,650 --> 00:01:24,766
SPEAKER_1:  Russ, all break.

00:01:25,026 --> 00:01:28,158
SPEAKER_1:  aka DreadPirateRobbers who ran Silk Road and

00:01:28,802 --> 00:01:30,366
SPEAKER_1:  uh... sabu of

00:01:30,850 --> 00:01:35,198
SPEAKER_1:  lolsec and anonymous who was one of the most influential hackers in the world

00:01:35,778 --> 00:01:36,734
SPEAKER_1:  So first...

00:01:37,442 --> 00:01:39,774
SPEAKER_1:  Can you tell me the story of Jack and Don, Russ Albrecht?

00:01:40,226 --> 00:01:44,862
SPEAKER_1:  and Silk Road. Let's start from the very beginning. And maybe let's start by explaining what is the Silk Road.

00:01:45,282 --> 00:01:46,782
SPEAKER_0:  It was really the first.

00:01:47,074 --> 00:01:48,798
SPEAKER_0:  dark market website.

00:01:49,090 --> 00:01:51,870
SPEAKER_0:  You literally could buy anything there.

00:01:52,546 --> 00:01:58,142
SPEAKER_0:  Take that back. There's two things you couldn't buy there. You couldn't buy guns because that was a different website.

00:01:58,402 --> 00:02:02,014
SPEAKER_0:  and you couldn't buy fake degrees. So no one could become a doctor.

00:02:02,402 --> 00:02:10,526
SPEAKER_0:  Um, but you could buy literally whatever else you wanted. You could draw things, drugs. You could buy heroin right from Afghanistan. The good stuff.

00:02:10,978 --> 00:02:14,366
SPEAKER_0:  hacking tools you could hack for hire you could buy murders for hire

00:02:14,786 --> 00:02:16,894
SPEAKER_0:  if you wanted someone killed. Now,

00:02:17,634 --> 00:02:24,350
SPEAKER_0:  When I was an FBI agent, I had to kind of sell some of these cases and this was a big drug case, you know, that's the way people saw Silk Road. So

00:02:24,642 --> 00:02:27,262
SPEAKER_0:  Internally to the FBI how I had to sell it I did.

00:02:27,586 --> 00:02:33,822
SPEAKER_0:  find the worst thing on there that you could possibly find. I think one time I saw a posting for baby parts.

00:02:34,434 --> 00:02:37,918
SPEAKER_0:  So let's say that you had a young child that needed a liver.

00:02:38,274 --> 00:02:41,406
SPEAKER_0:  You could literally go on there and ask for a six-month-old liver.

00:02:41,986 --> 00:02:43,861
SPEAKER_0:  if you wanted to.

00:02:43,861 --> 00:02:45,918
SPEAKER_1:  surgical operations versus something darker.

00:02:46,402 --> 00:02:55,422
SPEAKER_0:  Yeah, I never saw anything that dark as far as people wanted to eat body parts. Um, I did interview a cannibal once when I was in the FBI. That's another crazy story, but, uh,

00:02:55,714 --> 00:02:57,589
SPEAKER_0:  That that one actually weirded me out. Sorry

00:02:57,589 --> 00:03:07,518
SPEAKER_1:  I just watched Jeffrey Dahmer document on Netflix and it just changed the way I see human beings because it's a portrayal of a normal looking person.

00:03:08,290 --> 00:03:10,974
SPEAKER_1:  doing really dark things.

00:03:11,298 --> 00:03:11,870
SPEAKER_1:  and doing.

00:03:12,098 --> 00:03:14,846
SPEAKER_1:  So not out of a place of insanity, seemingly.

00:03:15,298 --> 00:03:16,062
SPEAKER_1:  but just...

00:03:16,450 --> 00:03:18,654
SPEAKER_1:  because he has almost like a fetish for that kind of

00:03:19,426 --> 00:03:21,534
SPEAKER_1:  It's disturbing that people like that are out there.

00:03:22,146 --> 00:03:24,446
SPEAKER_1:  So people like that.

00:03:24,930 --> 00:03:31,358
SPEAKER_1:  would then be using Silk Road. Not like that necessarily, but people of different walks of life would be using Silk Road too.

00:03:31,618 --> 00:03:34,142
SPEAKER_1:  Primarily, what was the primary thing? Drugs.

00:03:34,274 --> 00:03:40,926
SPEAKER_0:  It was primarily drugs and that's where it started. It started off with Ross Ulbrich growing mushrooms out in the wilderness of California and selling them.

00:03:41,314 --> 00:03:49,758
SPEAKER_0:  But really his was more of a libertarian viewpoint. I mean, it was like, you choose what you wanna do for yourself and do it. And the way Silk Road kind of.

00:03:50,018 --> 00:03:56,446
SPEAKER_0:  had the anonymity is it used what's called Tor, the onion router, which is anonymizing.

00:03:56,706 --> 00:04:00,894
SPEAKER_0:  function on the deep web. It was actually invented by the US Navy.

00:04:01,282 --> 00:04:03,166
SPEAKER_0:  back in the mid 90s or so.

00:04:03,586 --> 00:04:12,094
SPEAKER_0:  But it also used cryptocurrency. So it was the first time that we saw this birth on the internet of mixing cryptocurrency and an IP.

00:04:12,322 --> 00:04:17,982
SPEAKER_0:  blocking software. So, you know, in cyber crime, you go after one, the IP address and trace it through the network.

00:04:18,242 --> 00:04:21,617
SPEAKER_0:  or two you go after the cache and this one kind of blocked both.

00:04:21,617 --> 00:04:23,070
SPEAKER_1:  cash meaning the flow of money.

00:04:23,522 --> 00:04:24,798
SPEAKER_1:  physical or digital.

00:04:25,442 --> 00:04:26,814
SPEAKER_1:  And then...

00:04:27,106 --> 00:04:27,838
SPEAKER_1:  I P

00:04:28,290 --> 00:04:30,942
SPEAKER_1:  is some kind of identifying thing of the computer.

00:04:31,042 --> 00:04:38,334
SPEAKER_0:  It's your telephone number on your computer. So yeah, all computers have a unique four octet.

00:04:38,562 --> 00:04:44,798
SPEAKER_0:  numbers, you know, it's a 123.123.123.123 and you know,

00:04:45,026 --> 00:04:56,606
SPEAKER_0:  The computer uses DNS or domain name services to render that name. So if you were looking for, you know, CNN.com, your computer then translates that to that IP address or that telephone number where it can find that information.

00:04:56,834 --> 00:04:58,878
SPEAKER_1:  Didn't Silcroad used to have guns in the beginning?

00:04:59,234 --> 00:05:01,982
SPEAKER_1:  Or was that considered to have guns?

00:05:02,402 --> 00:05:07,006
SPEAKER_1:  was, did it naturally emerge and then Russ realized like this is not good.

00:05:07,394 --> 00:05:20,766
SPEAKER_0:  It went back and forth. Uh, I think there were guns on there and he tried to police it. Um, you know, he, uh, he told himself that the captain of the boat, so you had to follow his rules. So, you know, I think you took off those posts eventually and moved guns elsewhere.

00:05:21,090 --> 00:05:23,934
SPEAKER_1:  What was the system of censorship that he used?

00:05:24,738 --> 00:05:27,230
SPEAKER_1:  by off selecting what is okay and not okay.

00:05:27,554 --> 00:05:28,679
SPEAKER_1:  I mean, it's alone.

00:05:28,679 --> 00:05:29,566
SPEAKER_0:  the captain of the boat.

00:05:30,018 --> 00:05:30,782
SPEAKER_1:  Do you know?

00:05:31,010 --> 00:05:32,606
SPEAKER_1:  by chance if there is a...

00:05:32,962 --> 00:05:35,710
SPEAKER_1:  a lot of debates and criticisms internally.

00:05:36,162 --> 00:05:38,206
SPEAKER_1:  amongst the criminals of what is and isn't allowed.

00:05:38,498 --> 00:05:39,966
SPEAKER_1:  I mean, it's interesting to see.

00:05:40,482 --> 00:05:42,590
SPEAKER_1:  totally different moral code emerge.

00:05:42,818 --> 00:05:43,806
SPEAKER_1:  That's outside.

00:05:44,514 --> 00:05:46,110
SPEAKER_1:  the legal code of society.

00:05:46,530 --> 00:05:52,190
SPEAKER_0:  we did get the server and was able to read all of the chat logs that happened. All the records were there.

00:05:52,482 --> 00:05:55,870
SPEAKER_0:  I don't remember big debates. I mean, there was a clear leadership.

00:05:56,386 --> 00:05:59,934
SPEAKER_0:  That was the final decision. I was the CEO of Silk Road.

00:06:00,386 --> 00:06:05,182
SPEAKER_1:  And so primarily it was drugs and primarily out of an ideology of

00:06:05,858 --> 00:06:06,334
SPEAKER_1:  freedom.

00:06:06,818 --> 00:06:08,158
SPEAKER_1:  which is...

00:06:08,770 --> 00:06:12,222
SPEAKER_1:  If you want to use drugs, you should be able to use drugs.

00:06:12,482 --> 00:06:14,846
SPEAKER_0:  You should put into your body what you want to put in your body.

00:06:15,170 --> 00:06:18,654
SPEAKER_1:  And when you are presenting a case of why this should be investigated.

00:06:19,106 --> 00:06:24,350
SPEAKER_1:  You're trying to find, as you mentioned, the worst possible things on there. Is that what you were saying?

00:06:24,706 --> 00:06:32,574
SPEAKER_0:  So we had arrested a guy named Jeremy Hammond and he hit himself. He was a hacker and he this would be arrested. It was the second time he had been arrested for hacking.

00:06:32,834 --> 00:06:33,726
SPEAKER_0:  He used Tor.

00:06:34,242 --> 00:06:37,054
SPEAKER_0:  Um, and so that kind of brought us to a point.

00:06:37,602 --> 00:06:45,886
SPEAKER_0:  The FBI has a computer system where you look up things, you know, you look up anything. I could look up your name or whatever if you're associated with my case.

00:06:46,498 --> 00:06:52,702
SPEAKER_0:  And we were finding at the time a lot of things in, you need to look it up. The case would end be like, Oh, this is Tor.

00:06:53,122 --> 00:06:53,662
SPEAKER_0:  and just stop.

00:06:54,306 --> 00:06:56,318
SPEAKER_0:  We can get any further. Yeah. So.

00:06:56,706 --> 00:07:03,422
SPEAKER_0:  You know, we had just had this bigger rest of a sabu and took down anonymous and sometimes in the FBI.

00:07:03,810 --> 00:07:11,358
SPEAKER_0:  the way it used to the old school FBI. When you had a big case and you're working seven days a week and 14 hours, 15 hours a day.

00:07:12,002 --> 00:07:17,790
SPEAKER_0:  You sort of take a break. The boss kind of said, yeah, I'll see you in a few months. Go, go get to know your family a little bit, you know, and come back. But.

00:07:18,786 --> 00:07:20,286
SPEAKER_0:  The group of guys I was with was like.

00:07:20,514 --> 00:07:35,646
SPEAKER_0:  let's find the next big challenge. And that's when we were finding, you know, case closed, it was Tor, case closed, it was Tor. So said, let's take a look at Tor and let's see what we can do. Maybe we'll take a different approach. And Silk Road was being looked at by other law enforcement, but it was taking like a drug approach where I'm going to...

00:07:35,970 --> 00:07:39,518
SPEAKER_0:  find a drug buyer who got drugs sent to them in the mail.

00:07:39,810 --> 00:07:41,982
SPEAKER_0:  and let's arrest up, let's go up the chain.

00:07:42,658 --> 00:07:44,446
SPEAKER_0:  The buyers didn't know their dealers, they never met them.

00:07:45,730 --> 00:07:47,774
SPEAKER_0:  And so you were taking a cyber security approach.

00:07:48,226 --> 00:07:53,630
SPEAKER_0:  Yeah, we said, let's try to look at this from a cyber approach and see if we can gleam anything out of it.

00:07:54,402 --> 00:07:55,326
SPEAKER_1:  So I'm actually...

00:07:56,066 --> 00:07:57,470
SPEAKER_1:  Indirectly connected.

00:07:57,794 --> 00:07:58,110
SPEAKER_1:  Uh oh.

00:07:58,690 --> 00:08:02,815
SPEAKER_1:  to, I'm sure I'm not admitting anything that's not already on my FBI

00:08:02,815 --> 00:08:05,182
SPEAKER_0:  Oh, I can already tell you what you're gonna tell me though.

00:08:05,506 --> 00:08:05,982
SPEAKER_0:  What's that?

00:08:06,210 --> 00:08:10,558
SPEAKER_0:  that when you were at college, you wrote a paper and you're connected to the person that started.

00:08:10,850 --> 00:08:11,870
SPEAKER_0:  You son of a bitch.

00:08:12,226 --> 00:08:21,601
SPEAKER_0:  You clever son of a bitch. I'm an FBI agent or a former FBI agent. How would I not have known that? I could have told you other stuff. No, that's exactly what you were about to tell me. I was looking up his name.

00:08:21,601 --> 00:08:28,574
SPEAKER_1:  because I forgot it so one of my advisors for my PhD was Rachel Greenstadt and she is married to Roger

00:08:28,930 --> 00:08:37,630
SPEAKER_1:  Dingle Dine, which is the co-founder of the Tor Project and actually reached out to him last night to do a podcast together. I don't know.

00:08:38,402 --> 00:08:38,718
SPEAKER_1:

00:08:39,138 --> 00:08:39,966
SPEAKER_0:

00:08:40,738 --> 00:08:44,113
SPEAKER_0:  No, it was a good party trick. I mean, it was cool.

00:08:44,113 --> 00:08:44,894
SPEAKER_1:  that you

00:08:45,506 --> 00:08:46,526
SPEAKER_1:  Know this.

00:08:46,850 --> 00:08:48,862
SPEAKER_1:  and the timing of it, it was just like.

00:08:49,602 --> 00:08:50,206
SPEAKER_1:  beautiful.

00:08:50,498 --> 00:08:51,358
SPEAKER_1:  but, um...

00:08:52,002 --> 00:08:54,206
SPEAKER_1:  just to link her on the, on the tour project.

00:08:55,682 --> 00:08:56,958
SPEAKER_1:  so understand.

00:08:57,410 --> 00:08:59,454
SPEAKER_1:  to tour is this.

00:08:59,746 --> 00:09:01,790
SPEAKER_1:  black box that people disappear in.

00:09:02,498 --> 00:09:04,862
SPEAKER_1:  in terms of like the when you're tracking people.

00:09:06,082 --> 00:09:08,734
SPEAKER_1:  Can you paint a picture of what tours used in general?

00:09:09,218 --> 00:09:09,790
SPEAKER_1:  other

00:09:10,370 --> 00:09:13,726
SPEAKER_1:  It's like when you talk about Bitcoin, for example, cryptocurrency.

00:09:13,954 --> 00:09:20,318
SPEAKER_1:  especially today, much more people use it for legal activity versus illegal activity. What about Tor?

00:09:21,058 --> 00:09:27,454
SPEAKER_0:  The tour was originally invented by the US Navy so that spies inside countries could talk to spies and no one could find them.

00:09:27,906 --> 00:09:29,374
SPEAKER_0:  There was no way of tracing them.

00:09:29,730 --> 00:09:37,822
SPEAKER_0:  And then they released that information free to the world. So Tor has two different versions of versions, two different ways it can be utilized.

00:09:38,114 --> 00:09:48,158
SPEAKER_0:  There's dot onion sites, which is like a normal website, a dot com, but it's only found within the Tor browser. You can only get there if you know the whole address and get there. The other way Tor is used is to...

00:09:48,546 --> 00:09:55,198
SPEAKER_0:  go through the internet and then come out the other side if you want a different IP address if you're trying to hide your identity so

00:09:55,714 --> 00:10:02,238
SPEAKER_0:  If you were doing like, say cyber crime, I would have the victim computer and I would trace it back out to a Tor relay and then

00:10:02,786 --> 00:10:13,214
SPEAKER_0:  Because you don't have an active connection or what's called a circuit at the time, I wouldn't be able to trace it back. But even if you had an active circuit, I would have to go to each machine physically live and try to rebuild that, which...

00:10:13,794 --> 00:10:15,134
SPEAKER_0:  is literally impossible.

00:10:15,554 --> 00:10:16,990
SPEAKER_1:  So what do you feel about Tor?

00:10:17,282 --> 00:10:18,110
SPEAKER_1:  Ethically.

00:10:18,562 --> 00:10:20,734
SPEAKER_1:  philosophically as a human being.

00:10:21,154 --> 00:10:24,030
SPEAKER_1:  on this world that spent quite a few years.

00:10:24,386 --> 00:10:25,694
SPEAKER_1:  of your life and still.

00:10:25,986 --> 00:10:26,974
SPEAKER_1:  trying to protect people.

00:10:27,746 --> 00:10:31,294
SPEAKER_0:  So part of my time in the FBI was working on child exploitation.

00:10:31,714 --> 00:10:37,438
SPEAKER_0:  as they call it, that really changed my life in a way. And so anything that helps facilitate.

00:10:37,666 --> 00:10:39,006
SPEAKER_0:  the exploitation of children.

00:10:39,714 --> 00:10:40,862
SPEAKER_0:  fucking pisses me off.

00:10:41,186 --> 00:10:43,166
SPEAKER_0:  And that

00:10:43,522 --> 00:10:45,662
SPEAKER_0:  sort of jaded my opinion towards.

00:10:46,082 --> 00:10:50,207
SPEAKER_0:  towards Tor because that because it helps facilitate those sites.

00:10:50,207 --> 00:10:53,022
SPEAKER_1:  the ideal of freedom that Russell Albrecht.

00:10:53,282 --> 00:10:53,950
SPEAKER_1:  For example,

00:10:54,882 --> 00:10:56,350
SPEAKER_1:  I tried to embody.

00:10:56,770 --> 00:10:57,630
SPEAKER_1:  is something that

00:10:58,242 --> 00:10:59,294
SPEAKER_1:  You are...

00:10:59,522 --> 00:11:01,470
SPEAKER_1:  don't connect with anymore because

00:11:02,946 --> 00:11:05,854
SPEAKER_1:  of what you've seen that ideal being used for.

00:11:06,914 --> 00:11:18,398
SPEAKER_0:  I mean, the child exploitation is the specific example for it. You know, and it's easy for me to sit here and say child exploitation, child porn, because no one listening to this is ever going to say that I'm wrong and that we should allow child porn.

00:11:18,658 --> 00:11:19,742
SPEAKER_0:  Um, should.

00:11:20,002 --> 00:11:24,126
SPEAKER_0:  because some people utilize it in a bad way, should it go away.

00:11:24,674 --> 00:11:26,366
SPEAKER_0:  No, I mean, I'm a technologist.

00:11:26,690 --> 00:11:28,542
SPEAKER_0:  I want technology to move forward.

00:11:28,834 --> 00:11:29,630
SPEAKER_0:  Um, you know.

00:11:30,402 --> 00:11:34,910
SPEAKER_0:  People are going to do bad things and they're going to use technology to help them do bad things.

00:11:35,746 --> 00:11:36,926
SPEAKER_1:  Well let me ask you then...

00:11:37,218 --> 00:11:40,958
SPEAKER_1:  Oh, we'll jump around a little bit, but the things you were able to do.

00:11:41,570 --> 00:11:43,102
SPEAKER_1:  in tracking down information.

00:11:43,458 --> 00:11:44,190
SPEAKER_1:  and we'll get to it.

00:11:44,866 --> 00:11:45,534
SPEAKER_1:  There's...

00:11:45,922 --> 00:11:46,462
SPEAKER_1:  some.

00:11:46,818 --> 00:11:49,214
SPEAKER_1:  suspicion that this was only possible.

00:11:49,826 --> 00:11:51,582
SPEAKER_1:  with mass surveillance.

00:11:51,842 --> 00:11:53,758
SPEAKER_1:  like with NSA for example.

00:11:54,498 --> 00:11:58,430
SPEAKER_1:  First of all, is there any truth to that? And second of all, what do you feel?

00:11:59,042 --> 00:12:01,310
SPEAKER_1:  are the pros and cons of mass surveillance.

00:12:02,786 --> 00:12:04,062
SPEAKER_0:  There is no truth to that.

00:12:04,418 --> 00:12:07,043
SPEAKER_0:  And then my feelings on mass surveillance.

00:12:07,043 --> 00:12:09,822
SPEAKER_1:  If there was, would you tell me? Probably not.

00:12:10,466 --> 00:12:15,102
SPEAKER_1:  I love this conversation so much. What do you feel about?

00:12:15,906 --> 00:12:18,270
SPEAKER_1:  the given that you said child porn.

00:12:19,522 --> 00:12:22,590
SPEAKER_1:  What are the pros and cons of surveillance at a society level?

00:12:24,034 --> 00:12:35,870
SPEAKER_0:  I mean, nobody wants to give up their privacy. I say that I say no one wants to give up their privacy, but I mean, I used to have to get a search warrant to look inside your house or I can just log on to your Facebook and you've got pictures of all inside your house and what's going on.

00:12:36,322 --> 00:12:38,014
SPEAKER_0:  I mean, it's not, so people.

00:12:38,242 --> 00:12:40,766
SPEAKER_0:  like the idea of not giving up their privacy.

00:12:41,154 --> 00:12:41,598
SPEAKER_0:  Um...

00:12:42,594 --> 00:12:45,598
SPEAKER_0:  They do it anyways, they're giving away their freedoms all the time.

00:12:45,890 --> 00:12:49,950
SPEAKER_0:  hearing watches that gives out their heartbeat to a weight of companies that are storing it.

00:12:50,178 --> 00:12:51,358
SPEAKER_0:  What's more personal than your-

00:12:51,842 --> 00:12:52,318
SPEAKER_0:  heartbeat.

00:12:52,450 --> 00:12:53,854
SPEAKER_1:  So I think people...

00:12:54,658 --> 00:13:03,294
SPEAKER_1:  on mass really want to protect their privacy. And I would say most people don't really need to protect their privacy. But the case against mass surveillance.

00:13:03,714 --> 00:13:04,478
SPEAKER_1:  is that

00:13:04,834 --> 00:13:07,198
SPEAKER_1:  if you want to criticize the government.

00:13:08,066 --> 00:13:09,470
SPEAKER_1:  in a very difficult time.

00:13:10,146 --> 00:13:11,518
SPEAKER_1:  You should be able to do it.

00:13:12,418 --> 00:13:14,206
SPEAKER_1:  So when you need the freedom.

00:13:14,690 --> 00:13:15,422
SPEAKER_1:  You should have it.

00:13:15,906 --> 00:13:20,734
SPEAKER_1:  So when you wake up one day and realize there's something going wrong with the country I love.

00:13:21,250 --> 00:13:22,110
SPEAKER_1:  I want to be able to

00:13:22,338 --> 00:13:23,070
SPEAKER_1:  uh, to help.

00:13:23,746 --> 00:13:25,790
SPEAKER_1:  That's one of the great things about

00:13:26,498 --> 00:13:30,878
SPEAKER_1:  the United States of America is there's that individual revolutionary spirit.

00:13:31,490 --> 00:13:34,718
SPEAKER_1:  like so that the government doesn't become too powerful.

00:13:35,426 --> 00:13:36,958
SPEAKER_1:  You can always protest.

00:13:37,186 --> 00:13:37,982
SPEAKER_1:  There's always

00:13:38,466 --> 00:13:42,526
SPEAKER_1:  the best of the ideal of freedom of speech, you can always say fuck you to the man.

00:13:43,202 --> 00:13:44,254
SPEAKER_1:  And I think...

00:13:44,578 --> 00:13:49,406
SPEAKER_1:  There's a concern of direct or indirect suppression of that through mass surveillance.

00:13:49,890 --> 00:13:50,526
SPEAKER_1:  You might not.

00:13:51,330 --> 00:13:53,854
SPEAKER_1:  is that little subtle fear.

00:13:54,178 --> 00:13:55,326
SPEAKER_1:  that grows with time.

00:13:55,970 --> 00:14:01,342
SPEAKER_1:  that why bother criticizing the government? that's gonna be a headache i'm gonna get a ticket

00:14:01,858 --> 00:14:04,030
SPEAKER_1:  Every time I say something bad, that kind of thing.

00:14:04,290 --> 00:14:07,198
SPEAKER_1:  So it gets out, it can get out of hand. The bureaucracy grows.

00:14:07,714 --> 00:14:11,390
SPEAKER_1:  and the freedom slip away. That's the criticism.

00:14:12,674 --> 00:14:27,646
SPEAKER_0:  I completely see your point and I agree with it. I mean, but I mean, on the other side, people criticize the government of these freedoms, but I mean, tech companies are talking about destroying your privacy and controlling what you can say. I realize they're private platforms and you, they can decide what's on their platform.

00:14:28,546 --> 00:14:40,414
SPEAKER_0:  But you know, they're taking away your freedoms of what you can say. And we've heard some things where maybe government officials were in line with the, with tech companies to take away some of that freedom. And that's, I agree with you, that gets scary.

00:14:40,578 --> 00:14:43,422
SPEAKER_1:  Yeah, there's something about government that feels.

00:14:44,642 --> 00:14:49,790
SPEAKER_1:  Maybe because of the history of human civilization, maybe because tech companies are a new thing.

00:14:50,082 --> 00:14:53,118
SPEAKER_1:  but just knowing the history of abuses of government.

00:14:54,466 --> 00:14:56,382
SPEAKER_1:  There's something about...

00:14:56,738 --> 00:15:01,278
SPEAKER_1:  government that enables the corrupting nature of power to take hold at scale.

00:15:01,634 --> 00:15:04,030
SPEAKER_1:  more than tech companies, at least what we've seen so far.

00:15:04,930 --> 00:15:09,278
SPEAKER_0:  Yeah, I agree. I agree. But I mean, we haven't had a voice like we've had.

00:15:09,666 --> 00:15:15,166
SPEAKER_0:  until recently. I mean, anyone that has a Twitter account now can speak and become a news article.

00:15:15,426 --> 00:15:24,446
SPEAKER_0:  Um, you know, my parents didn't have that, didn't have that voice. If they wanted to speak out against the government or do something, they had to go to a protestor, organize a protestor.

00:15:24,674 --> 00:15:26,590
SPEAKER_0:  you know, do something along those lines. So

00:15:27,074 --> 00:15:29,854
SPEAKER_0:  You know, we have more of a place to put our voice out now.

00:15:30,274 --> 00:15:31,399
SPEAKER_0:  Yeah, it's incredible, but that's...

00:15:31,399 --> 00:15:33,886
SPEAKER_1:  That's why it hurts and that's why you notice it.

00:15:34,178 --> 00:15:35,774
SPEAKER_1:  when certain voices get removed.

00:15:36,322 --> 00:15:37,374
SPEAKER_1:  the president.

00:15:38,018 --> 00:15:43,134
SPEAKER_1:  of the United States of America was removed from one such or all such platforms.

00:15:44,226 --> 00:15:45,118
SPEAKER_1:  And that hurts.

00:15:45,410 --> 00:15:47,102
SPEAKER_0:  Yeah, that's crazy to me. That's insane.

00:15:47,746 --> 00:15:49,470
SPEAKER_0:  That's insane that we took that away.

00:15:51,490 --> 00:15:52,615
SPEAKER_0:  Let's return to, uh...

00:15:52,615 --> 00:15:56,926
SPEAKER_1:  to still grow in my subject. So how did your path?

00:15:57,218 --> 00:16:00,638
SPEAKER_1:  with this very difficult, very fascinating case.

00:16:01,378 --> 00:16:01,822
SPEAKER_1:  cross.

00:16:02,690 --> 00:16:03,678
SPEAKER_0:  We were.

00:16:03,970 --> 00:16:07,614
SPEAKER_0:  looking to open a case against Tory because it was a problem. All the cases were closing.

00:16:07,874 --> 00:16:15,518
SPEAKER_0:  because Tor. So we went on Tor and we came up with 26 different onion, dot onions that

00:16:15,810 --> 00:16:23,614
SPEAKER_0:  We targeted we were looking for nexuses to hacking because I was on a squad called CY2 and we were like the premier.

00:16:24,226 --> 00:16:26,526
SPEAKER_0:  squad in New York that was working.

00:16:27,010 --> 00:16:28,702
SPEAKER_0:  criminal cyber intrusions.

00:16:29,378 --> 00:16:30,654
SPEAKER_0:  And so, you know.

00:16:30,914 --> 00:16:34,206
SPEAKER_0:  any website that was offering hackers for hire or

00:16:34,818 --> 00:16:35,390
SPEAKER_0:  Um...

00:16:36,130 --> 00:16:38,462
SPEAKER_0:  hacking tools for free in our

00:16:38,722 --> 00:16:44,222
SPEAKER_0:  Paid services, you know, like now we're seeing ransomware as a paid service and phishing as a paid service.

00:16:44,482 --> 00:16:47,774
SPEAKER_0:  Anything that offered that. So we open this case on.

00:16:48,066 --> 00:16:53,278
SPEAKER_0:  I think we called it, we say you have to name cases. One of the fun things in the FBI is when you start a case, you get to name it.

00:16:53,794 --> 00:16:56,990
SPEAKER_0:  you would not believe how much time is spent in coming up with the name.

00:16:57,410 --> 00:17:00,318
SPEAKER_0:  Yeah. I think we called this onion peeler.

00:17:00,610 --> 00:17:01,630
SPEAKER_0:  because of the...

00:17:02,306 --> 00:17:04,990
SPEAKER_1:  So a little bit of humor, a little bit of wit.

00:17:05,442 --> 00:17:10,819
SPEAKER_1:  and some profundity to the language. Yeah, yeah. Cause you're gonna have to work with this one for quite a lot.

00:17:10,819 --> 00:17:16,318
SPEAKER_0:  Yeah, this one had the potential of being a big one, you know, because I think I think Silk Road was like the sixth on the list.

00:17:16,674 --> 00:17:21,598
SPEAKER_0:  uh, for that case, but we all knew that was sort of the golden ring. If you could make the splash.

00:17:21,986 --> 00:17:30,910
SPEAKER_0:  that that onion site was going down, then it would probably get some publicity. And that's part of law enforcement, is getting some publicity out of it that, you know, that.

00:17:31,362 --> 00:17:33,086
SPEAKER_0:  makes others think not to do it.

00:17:33,730 --> 00:17:34,494
SPEAKER_1:  I wish to say that.

00:17:34,754 --> 00:17:35,390
SPEAKER_1:  whore.

00:17:35,842 --> 00:17:37,758
SPEAKER_1:  is the name of the project, the browser.

00:17:38,242 --> 00:17:40,094
SPEAKER_1:  What is the onion technology behind Tor?

00:17:40,578 --> 00:17:42,366
SPEAKER_0:  Let's say you want to go to a Dagenian site.

00:17:42,946 --> 00:17:50,782
SPEAKER_0:  You'll put in the dot you want to go to, and your computer will build communications with a Tor relay, which are all publicly available out there.

00:17:51,266 --> 00:17:57,566
SPEAKER_0:  But you'll encrypt it. You'll put a package around your data. And so it's encrypted.

00:17:58,178 --> 00:18:01,054
SPEAKER_0:  can't read it. It goes to that first relay.

00:18:01,506 --> 00:18:05,886
SPEAKER_0:  That first relay knows about you, and then knows about the next relay down the chain.

00:18:06,434 --> 00:18:10,846
SPEAKER_0:  And so it takes your data and then encrypts that on the outside and sends it to relay number two.

00:18:11,394 --> 00:18:24,350
SPEAKER_0:  Now, relay number two only knows about relay number one. It doesn't know who you are asking for this. And it goes through there, adding those layers on top, layers of encryption till it gets to where it is. And then even the onion service doesn't know, except for the

00:18:24,802 --> 00:18:26,942
SPEAKER_0:  the relay it came from, who it's talking to.

00:18:27,362 --> 00:18:28,606
SPEAKER_0:  And so it peels back that.

00:18:28,930 --> 00:18:31,102
SPEAKER_0:  gives the information, puts another layer back on.

00:18:31,426 --> 00:18:34,462
SPEAKER_0:  And so it's layers like you're peeling an onion back.

00:18:34,722 --> 00:18:38,814
SPEAKER_0:  of the different relays and that encryption protects.

00:18:39,234 --> 00:18:42,609
SPEAKER_0:  who the sender is and what information they're sending. more layers there are at the

00:18:42,609 --> 00:18:43,038
SPEAKER_1:  Uh...

00:18:43,426 --> 00:18:46,110
SPEAKER_1:  more exponentially difficult it is to decrypt it.

00:18:47,010 --> 00:18:56,286
SPEAKER_0:  I mean, you get to a place where you don't have to have so many layers because it doesn't matter anymore. It's mathematically impossible to decrypt it. But, you know...

00:18:56,546 --> 00:19:02,398
SPEAKER_0:  The more relays you have, the slower it is. I mean, that's one of the big drawbacks on TOR is how slow it operates.

00:19:03,298 --> 00:19:04,798
SPEAKER_1:  So how do you peel the onion?

00:19:06,050 --> 00:19:08,766
SPEAKER_1:  So what are the different methodologies for trying to get?

00:19:09,346 --> 00:19:10,398
SPEAKER_1:  some information.

00:19:10,978 --> 00:19:14,590
SPEAKER_1:  from a cybersecurity perspective on these operations lets turn on the speed counter, and turn on the

00:19:15,394 --> 00:19:22,494
SPEAKER_0:  It's very difficult. Um, people have come up with different techniques. There, um, there's been techniques to put out in the, in the news media.

00:19:22,754 --> 00:19:30,206
SPEAKER_0:  about how they do it, running like massive amounts of relays and you're controlling those relays I think. I think somebody tried that once.

00:19:30,338 --> 00:19:32,254
SPEAKER_1:  So this is a technical solution and.

00:19:32,706 --> 00:19:33,950
SPEAKER_1:  What about social engineering?

00:19:34,594 --> 00:19:35,198
SPEAKER_1:  What about...

00:19:35,586 --> 00:19:37,118
SPEAKER_1:  trying to infiltrate.

00:19:38,338 --> 00:19:40,510
SPEAKER_1:  the actual humans they're using.

00:19:41,314 --> 00:19:42,526
SPEAKER_1:  Nah, this'll crowed.

00:19:42,850 --> 00:19:43,966
SPEAKER_1:  trying to get in that way.

00:19:45,090 --> 00:19:59,294
SPEAKER_0:  Yeah, I mean, I definitely could see the way of doing that. And in this case, in our takedown, we used that. There was one of my partners, Jared Derrigan, he was an HSI investigator, and he had worked his way up to be a system admin on the site.

00:19:59,842 --> 00:20:05,470
SPEAKER_0:  Um, so that did glean quite a bit of information because he was, he was inside and talking to.

00:20:05,762 --> 00:20:13,470
SPEAKER_0:  You know at that time a wheel I know it is DPR or Dreadpire Roberts We didn't know who that was yet, but but we had that open communication.

00:20:14,050 --> 00:20:14,462
SPEAKER_0:  Um...

00:20:14,818 --> 00:20:21,086
SPEAKER_0:  You know, and one of the things, you know, the technical aspects on that is there was a jabber server. And there wasâ€”

00:20:21,410 --> 00:20:24,222
SPEAKER_0:  That's a communication type of communication server.

00:20:24,482 --> 00:20:31,230
SPEAKER_0:  Um, that was being used and we knew that Ross had his jabber set to Pacific time.

00:20:32,066 --> 00:20:34,622
SPEAKER_0:  So we had a pretty good idea what part of the...

00:20:35,522 --> 00:20:35,838
SPEAKER_0:  We'll...

00:20:36,258 --> 00:20:37,342
SPEAKER_0:  part of the country he was in.

00:20:38,530 --> 00:20:39,422
SPEAKER_1:  I mean, isn't that...

00:20:40,130 --> 00:20:43,422
SPEAKER_1:  from DPR's perspective, from Russ's perspective.

00:20:44,034 --> 00:20:45,086
SPEAKER_1:  Isn't that clumsy?

00:20:45,314 --> 00:20:47,486
SPEAKER_0:  He wasn't a big computer guy.

00:20:47,714 --> 00:20:52,158
SPEAKER_1:  Do you notice that aspect of like the technical savvy of some of these guys doesn't seem to be quite?

00:20:52,930 --> 00:20:55,527
SPEAKER_1:  Why weren't they good at this?

00:20:55,527 --> 00:20:59,582
SPEAKER_0:  the real techy savvy ones we don't arrest. We don't get to them, we don't find them. We don't get to them.

00:21:02,530 --> 00:21:04,766
SPEAKER_1:  Shout out to the techie criminals.

00:21:04,994 --> 00:21:06,494
SPEAKER_1:  They're probably watching this.

00:21:06,914 --> 00:21:17,790
SPEAKER_0:  I mean, yeah, I mean, you were getting the low hanging fruit. I mean, you're getting the ones that can be caught. I mean, they, you know, I'm sure we'll talk about it, but the anonymous case, there was a guy named AV Unit. He's still, I lose sleep over him because we didn't catch him.

00:21:18,146 --> 00:21:19,774
SPEAKER_0:  We caught everybody else, we didn't catch him.

00:21:21,026 --> 00:21:21,726
SPEAKER_0:  He's good though.

00:21:22,754 --> 00:21:29,566
SPEAKER_0:  He pops up too once in a while on the internet and it pisses me off. Yeah. What's his name again? AV unit. That's all I know is his AV unit.

00:21:29,826 --> 00:21:32,382
SPEAKER_0:  AV unit. I have a funny story about him.

00:21:33,026 --> 00:21:33,950
SPEAKER_0:  people think he is.

00:21:34,338 --> 00:21:36,213
SPEAKER_1:  Can I actually, can we go on that brief t-

00:21:36,213 --> 00:21:37,694
SPEAKER_0:  Sure, I love tangents.

00:21:38,914 --> 00:21:39,262
SPEAKER_0:  I'll see you guys next time.

00:21:39,618 --> 00:21:41,054
SPEAKER_1:  Well let me ask you, uh...

00:21:42,018 --> 00:21:43,614
SPEAKER_1:  since he's probably he or she.

00:21:43,746 --> 00:21:45,822
SPEAKER_0:  Do we know what's a he? We have no idea.

00:21:46,178 --> 00:21:49,438
SPEAKER_0:  That's another funny story about hackers, the he-she issue.

00:21:49,954 --> 00:21:55,038
SPEAKER_0:  What's the funny story there? Well, one of the guys in LulzSec was a she, was a 17 year old girl.

00:21:55,522 --> 00:22:00,958
SPEAKER_0:  Uh, and, uh, my source in the case, the guy, Sabu that I arrested and part of it.

00:22:01,282 --> 00:22:01,918
SPEAKER_0:  Yeah, we've set.

00:22:02,274 --> 00:22:03,934
SPEAKER_0:  Side by side for 9 months and...

00:22:04,738 --> 00:22:06,750
SPEAKER_0:  then took down the case and all that.

00:22:07,042 --> 00:22:09,950
SPEAKER_0:  He was convinced she was a girl and we said, you know.

00:22:10,210 --> 00:22:15,710
SPEAKER_0:  And he was in love with her almost at one point. It turns out to be a 35 year old guy living in England.

00:22:16,130 --> 00:22:18,750
SPEAKER_1:  Also, he was convinced it was a...

00:22:18,946 --> 00:22:20,821
SPEAKER_0:  Yes, he was absolutely convinced.

00:22:20,821 --> 00:22:22,782
SPEAKER_1:  what exactly by linguistic like.

00:22:23,266 --> 00:22:25,950
SPEAKER_1:  human-based linguistic analysis or what?

00:22:26,114 --> 00:22:32,350
SPEAKER_0:  she, he, uh, whatever, you know, Kayla, his way when it was ended up being like a modification of his sister's name.

00:22:32,578 --> 00:22:34,494
SPEAKER_0:  The real guy's sister's name.

00:22:34,786 --> 00:22:37,086
SPEAKER_0:  was so good at building the backstory.

00:22:37,634 --> 00:22:42,558
SPEAKER_0:  All these guys, and it's funny, these guys are part of a hacking crew, they social engineer the shit out of each other.

00:22:43,554 --> 00:22:45,726
SPEAKER_0:  just to build if one of them ever gets caught.

00:22:46,050 --> 00:22:50,206
SPEAKER_0:  They'll convince everybody else that they're a Brazilian.

00:22:50,690 --> 00:22:53,406
SPEAKER_0:  ISP owner or something like that. That's how I'm so powerful.

00:22:53,762 --> 00:23:00,542
SPEAKER_1:  Well, yeah, that social engineering aspect is part of living a life of cyber crime or cybersecurity and offensive or defensive.

00:23:00,770 --> 00:23:01,918
SPEAKER_1:  So AV unit.

00:23:02,562 --> 00:23:05,470
SPEAKER_1:  Can I also get attention of attention first?

00:23:05,698 --> 00:23:07,486
SPEAKER_0:  That's my favorite tangent. OK,

00:23:08,450 --> 00:23:09,118
SPEAKER_1:  Um...

00:23:09,410 --> 00:23:10,782
SPEAKER_1:  Is it possible?

00:23:11,298 --> 00:23:13,694
SPEAKER_1:  for me to have a podcast conversation.

00:23:14,626 --> 00:23:16,766
SPEAKER_1:  with somebody who hasn't been caught yet.

00:23:18,370 --> 00:23:21,374
SPEAKER_1:  And because they have the conversation, they still won't be caught.

00:23:22,274 --> 00:23:23,710
SPEAKER_1:  And is that a good idea?

00:23:24,418 --> 00:23:27,870
SPEAKER_1:  Meaning, is there a safe way for a criminal to talk to a Miata podcast?

00:23:29,474 --> 00:23:30,366
SPEAKER_0:  I would think so.

00:23:31,490 --> 00:23:39,358
SPEAKER_0:  I would think that someone could, I mean, someone who has been living a double life for long enough where you think they're not a criminal.

00:23:39,906 --> 00:23:40,446
SPEAKER_0:  Um...

00:23:40,578 --> 00:23:42,398
SPEAKER_1:  I know they would have to admit.

00:23:42,818 --> 00:23:44,693
SPEAKER_1:  that they would say I am a

00:23:44,693 --> 00:23:47,646
SPEAKER_0:  Oh, you would want to have a conversation with AV Unit. Yes.

00:23:49,570 --> 00:23:56,958
SPEAKER_1:  Is there a way? I'm just speaking from an FBI perspective, technically speaking, because I, let me explain my motivation. For more information, visit www.fema.gov.au

00:23:57,378 --> 00:23:57,886
SPEAKER_1:  I think.

00:24:00,514 --> 00:24:02,270
SPEAKER_1:  I would like to be able to talk.

00:24:03,362 --> 00:24:06,782
SPEAKER_1:  people from all walks of life and understanding criminals.

00:24:07,714 --> 00:24:10,782
SPEAKER_1:  understanding their mind, I think is very important.

00:24:12,642 --> 00:24:16,734
SPEAKER_1:  And I think there's fundamentally something different between a criminal who's still active.

00:24:17,090 --> 00:24:18,334
SPEAKER_1:  versus one that's been caught.

00:24:18,786 --> 00:24:21,630
SPEAKER_1:  the mind just from observing it changes.

00:24:22,530 --> 00:24:23,870
SPEAKER_1:  once you're caught.

00:24:24,322 --> 00:24:27,486
SPEAKER_1:  You have a big shift in your understanding of the world.

00:24:28,130 --> 00:24:28,510
SPEAKER_1:  Um...

00:24:29,122 --> 00:24:36,606
SPEAKER_1:  I mean, I do have a question about the ethics of having such conversations, but first, technically, is that possible?

00:24:37,890 --> 00:24:45,470
SPEAKER_0:  If I was technically advising you, I would say first off, don't advertise it. Don't, the fewer people that you're going to tell that you're having this conversation with.

00:24:45,922 --> 00:24:46,430
SPEAKER_0:  the better.

00:24:47,106 --> 00:24:48,798
SPEAKER_0:  Um, and yeah, you could.

00:24:49,122 --> 00:24:50,997
SPEAKER_0:  Are you doing it in person? Are you doing it in person?

00:24:50,997 --> 00:24:53,854
SPEAKER_1:  The person would be amazing, yeah, but their face would not be shown.

00:24:54,082 --> 00:24:58,974
SPEAKER_0:  face would not be shy. Yeah, I mean, you couldn't publish the show for a while, they'd have to put a lot of trust in you.

00:24:59,234 --> 00:25:01,918
SPEAKER_0:  that you're not going to, you're gonna have to.

00:25:02,178 --> 00:25:03,230
SPEAKER_0:  Alter those tapes.

00:25:03,586 --> 00:25:12,749
SPEAKER_0:  Uh, I say tapes cause it's old school, the opto, you know, exactly. I'm sure a lot of people just said that like, oh shit, this old guy just didn't tape. I heard it.

00:25:12,749 --> 00:25:14,430
SPEAKER_1:  I guess it was in the 1800s, I think.

00:25:14,914 --> 00:25:15,454
SPEAKER_1:  Um...

00:25:16,514 --> 00:25:21,630
SPEAKER_0:  Yeah, yeah, you could do it. They'd have to have complete faith and trust in you that you'd

00:25:21,986 --> 00:25:24,611
SPEAKER_0:  destroy the originals after you've altered it.

00:25:24,611 --> 00:25:27,166
SPEAKER_1:  What about if they don't have faith? Is there a way for them?

00:25:27,682 --> 00:25:28,798
SPEAKER_1:  to attain security.

00:25:29,378 --> 00:25:30,302
SPEAKER_1:  uh... so

00:25:30,978 --> 00:25:31,358
SPEAKER_1:  Uh...

00:25:31,618 --> 00:25:35,678
SPEAKER_1:  like for me to go through some kind of process where I meet them somewhere where

00:25:36,098 --> 00:25:40,126
SPEAKER_0:  I mean, you're not gonna do it without a bag over your head. I don't know if that's the life you want to live.

00:25:40,290 --> 00:25:41,886
SPEAKER_1:  I'm fine with a bag over my head.

00:25:42,338 --> 00:25:46,942
SPEAKER_1:  That's going to get taken out of context. I just, I think it's a worthy effort.

00:25:47,586 --> 00:25:52,126
SPEAKER_1:  It's worthy to go through the hardship of that to understand the mind of somebody.

00:25:52,450 --> 00:25:54,174
SPEAKER_1:  I think fundamentally.

00:25:54,530 --> 00:25:56,414
SPEAKER_1:  Conversations are a different thing.

00:25:56,866 --> 00:25:58,430
SPEAKER_1:  in the operation of law enforcement.

00:25:58,850 --> 00:26:01,086
SPEAKER_1:  Understanding the mind of a criminal I think is really important.

00:26:01,698 --> 00:26:12,030
SPEAKER_0:  I don't know if you're going to have the honest conversation that you're looking for. I mean, it may sound honest, but it may not be the truth. I found most times when I was talking to criminals, it's lies mixed with half-truths.

00:26:12,386 --> 00:26:13,246
SPEAKER_0:  uh... and you

00:26:13,922 --> 00:26:17,374
SPEAKER_0:  If they're good, they can keep that story going for long enough.

00:26:17,826 --> 00:26:23,198
SPEAKER_0:  If they're not, you kind of see the relief in them when you finally break that wall down.

00:26:24,162 --> 00:26:24,958
SPEAKER_1:  That's the job.

00:26:25,602 --> 00:26:26,430
SPEAKER_1:  of an interviewer.

00:26:26,754 --> 00:26:28,062
SPEAKER_1:  if the interview is good.

00:26:28,546 --> 00:26:30,974
SPEAKER_1:  then perhaps not directly, but.

00:26:31,682 --> 00:26:32,702
SPEAKER_1:  through the gaps.

00:26:33,250 --> 00:26:35,614
SPEAKER_1:  seeps out the truth of the human being.

00:26:36,066 --> 00:26:39,518
SPEAKER_1:  for not necessarily the details of how they do the operations and so on.

00:26:39,842 --> 00:26:43,774
SPEAKER_1:  but just who they are as a human being, what their motivations are, what their ethics are.

00:26:44,354 --> 00:26:49,406
SPEAKER_1:  How they see the world, what is good, what is evil? Do they see themselves as good? Where do they see their motivation?

00:26:49,794 --> 00:26:51,550
SPEAKER_1:  as do they have a resentment?

00:26:51,970 --> 00:27:02,526
SPEAKER_1:  What did they think about love for the people within their small community? Do they have resentment for the government or for other nations or for other people? Did they have childhood issues that led to...

00:27:03,042 --> 00:27:05,566
SPEAKER_1:  a different view of the world than others perhaps have.

00:27:05,858 --> 00:27:07,198
SPEAKER_1:  Do they have certain fetishes?

00:27:07,586 --> 00:27:11,166
SPEAKER_1:  like sexual and otherwise, that led to the construction of the world.

00:27:11,618 --> 00:27:13,534
SPEAKER_1:  They might be able to review.

00:27:14,082 --> 00:27:14,526
SPEAKER_1:  some.

00:27:14,946 --> 00:27:17,278
SPEAKER_1:  deep flaws to the cybersecurity.

00:27:17,730 --> 00:27:19,646
SPEAKER_1:  infrastructure of our world.

00:27:20,162 --> 00:27:23,358
SPEAKER_1:  not in detail, but like philosophically speaking.

00:27:24,418 --> 00:27:25,758
SPEAKER_1:  they might have.

00:27:27,138 --> 00:27:29,342
SPEAKER_1:  I know you might say it's just a narrative.

00:27:29,602 --> 00:27:31,134
SPEAKER_1:  but they might have a kind of...

00:27:32,194 --> 00:27:35,102
SPEAKER_1:  ethical concern for the well-being of the world.

00:27:36,322 --> 00:27:41,438
SPEAKER_1:  that they're essentially attacking the weakness of the cybersecurity infrastructure because they believe

00:27:41,890 --> 00:27:43,998
SPEAKER_1:  Ultimately that would lead to a safer world.

00:27:44,962 --> 00:27:47,230
SPEAKER_1:  so the attacks will reveal the weaknesses.

00:27:47,970 --> 00:27:49,694
SPEAKER_1:  and if they're stealing a bunch of money.

00:27:50,402 --> 00:27:54,526
SPEAKER_1:  That's okay because that's gonna enforce you to invest a lot more money in defending

00:27:55,202 --> 00:27:55,646
SPEAKER_1:  Um.

00:27:55,970 --> 00:27:57,982
SPEAKER_1:  yeah, defending things that actually matter.

00:27:58,434 --> 00:28:00,990
SPEAKER_1:  nuclear warheads and all those kinds of things.

00:28:01,378 --> 00:28:04,126
SPEAKER_1:  I mean, I could see, you know, it's.

00:28:04,386 --> 00:28:07,806
SPEAKER_1:  Fascinating to explore the mind of a human being like that because

00:28:08,546 --> 00:28:08,894
SPEAKER_1:  Um.

00:28:09,570 --> 00:28:11,326
SPEAKER_1:  I think it will help people understand.

00:28:11,746 --> 00:28:12,510
SPEAKER_1:  Of course.

00:28:14,466 --> 00:28:15,230
SPEAKER_1:  Uh...

00:28:16,002 --> 00:28:17,726
SPEAKER_1:  It's still a person that's.

00:28:18,146 --> 00:28:20,446
SPEAKER_1:  creating a lot of suffering in the world, which is a problem.

00:28:20,866 --> 00:28:23,102
SPEAKER_1:  So do you think ethically is a good thing to do?

00:28:23,874 --> 00:28:27,198
SPEAKER_0:  I don't, I mean I feel like I have a fairly high...

00:28:28,002 --> 00:28:30,334
SPEAKER_0:  ethical bar that I have to put myself on.

00:28:30,882 --> 00:28:33,662
SPEAKER_0:  and I don't think I have a problem with it. I would love to listen to it.

00:28:34,338 --> 00:28:34,750
SPEAKER_1:  Okay.

00:28:35,042 --> 00:28:36,167
SPEAKER_1:  Great.

00:28:36,167 --> 00:28:38,417
SPEAKER_0:  I mean, not that I'm your ethical coacher here.

00:28:38,417 --> 00:28:42,558
SPEAKER_1:  Yeah, well, that's interesting. I mean, because I thought you would have...

00:28:43,042 --> 00:28:43,934
SPEAKER_1:  become jaded.

00:28:44,450 --> 00:28:45,118
SPEAKER_1:  and

00:28:45,474 --> 00:28:47,838
SPEAKER_1:  exhausted by the

00:28:48,194 --> 00:28:48,734
SPEAKER_1:  criminal.

00:28:50,338 --> 00:28:50,686
SPEAKER_1:  mine.

00:28:51,746 --> 00:28:53,182
SPEAKER_0:  It's funny, um...

00:28:53,442 --> 00:29:00,382
SPEAKER_0:  You know, I'm, you know, fast forward in our story. I'm very good friends with Hector Montserrat, the guy I arrested.

00:29:00,674 --> 00:29:01,150
SPEAKER_0:  Um...

00:29:01,730 --> 00:29:04,158
SPEAKER_0:  and he tells stories of what he did in his past.

00:29:04,546 --> 00:29:05,886
SPEAKER_0:  And I'm like, I'm that Hector.

00:29:06,210 --> 00:29:06,654
SPEAKER_0:  You know.

00:29:07,106 --> 00:29:07,422
SPEAKER_0:  Heh.

00:29:07,682 --> 00:29:10,750
SPEAKER_0:  But then I listened to your episode with Brett Johnson.

00:29:11,266 --> 00:29:12,094
SPEAKER_0:  And I was like...

00:29:13,442 --> 00:29:21,406
SPEAKER_0:  this guy stealing money from the US government and welfare fraud and all this sort of things. It just pissed me off. And I don't know why I have that.

00:29:22,178 --> 00:29:23,742
SPEAKER_0:  differentiation in my head.

00:29:24,098 --> 00:29:29,854
SPEAKER_0:  I don't know why I think one's just, oh, Hector will be Hector, and then this guy just pissed me off.

00:29:30,146 --> 00:29:33,086
SPEAKER_1:  Well, you didn't feel that way about Hector until you probably met him.

00:29:33,602 --> 00:29:34,046
SPEAKER_1:  Wow.

00:29:34,434 --> 00:29:36,030
SPEAKER_0:  I didn't know Hector I knew Sabu.

00:29:36,450 --> 00:29:37,950
SPEAKER_0:  So I hunted down Sabu.

00:29:38,434 --> 00:29:40,958
SPEAKER_0:  And I learned about Hector over those nine months.

00:29:41,378 --> 00:29:46,366
SPEAKER_1:  We'll talk about it. Let's finish with, let's return tangent to back to attention. Oh,

00:29:47,970 --> 00:29:49,822
SPEAKER_1:  Grunt tangent up, who's AV unit?

00:29:50,082 --> 00:29:52,830
SPEAKER_1:  I don't know. That's interesting. He's at the core.

00:29:53,218 --> 00:29:53,854
SPEAKER_1:  of

00:29:54,082 --> 00:29:57,598
SPEAKER_1:  Anonymous, he's one of the critical people in an honest. The what is known about him.

00:29:58,050 --> 00:30:02,110
SPEAKER_0:  There's what's known in public and what was known because I sat with Hector and, um.

00:30:02,882 --> 00:30:05,118
SPEAKER_0:  He was sort of like the set things up guy.

00:30:05,506 --> 00:30:06,430
SPEAKER_0:  uh... so if

00:30:07,842 --> 00:30:14,046
SPEAKER_0:  Lil Sec had like their hackers, which was Sabu and Kayla, and they had their media guy.

00:30:14,274 --> 00:30:15,230
SPEAKER_0:  Skytopiary.

00:30:15,618 --> 00:30:20,382
SPEAKER_0:  up, he lived up in the Northern end of England and they had a few other guys, but.

00:30:21,122 --> 00:30:26,046
SPEAKER_0:  AVUnit was the guy that set up infrastructure. So if you need a VPN in Brazil or something like that to pop through.

00:30:26,434 --> 00:30:26,846
SPEAKER_0:  Um...

00:30:27,842 --> 00:30:32,702
SPEAKER_0:  One of the first things Hector told me after we arrested him is that Havy Unit was a secret service agent.

00:30:33,634 --> 00:30:35,070
SPEAKER_0:  And I was like, oh shit.

00:30:35,394 --> 00:30:41,534
SPEAKER_0:  Just because he kind of lived that lifestyle. He'd be around for a bunch of days and then all of a sudden gone for the rest of the week.

00:30:42,434 --> 00:30:55,198
SPEAKER_0:  Um, and I, I tried to get more out of Hector and that early on in that relationship. Um, you know, I'm sure he was a little bit guarded, uh, maybe trying to social engineer me, maybe he wanted that, uh, that, oh shit, there's law enforcement involved in this.

00:30:55,586 --> 00:30:56,222
SPEAKER_0:  Um...

00:30:56,834 --> 00:30:58,462
SPEAKER_0:  And not to say, I mean...

00:30:58,914 --> 00:31:04,222
SPEAKER_0:  I was in over my head with that case, just the amount of work that was going on. Um, so to track them all down.

00:31:04,482 --> 00:31:05,566
SPEAKER_0:  Plus the

00:31:05,858 --> 00:31:09,854
SPEAKER_0:  350 hacks that came in about just military institutions.

00:31:10,082 --> 00:31:16,030
SPEAKER_0:  You know, it was swimming in the deep end. So it was just at the end of the case, I looked back and I was like,

00:31:16,610 --> 00:31:17,182
SPEAKER_0:  Heavy unit.

00:31:17,538 --> 00:31:18,462
SPEAKER_0:  I coulda had them all.

00:31:18,914 --> 00:31:20,894
SPEAKER_0:  Uh, you know, maybe that's the perfectionist in me.

00:31:21,698 --> 00:31:22,750
SPEAKER_1:  Ah man.

00:31:23,074 --> 00:31:26,211
SPEAKER_1:  Well, reach out somehow. I can't, I won't say how, right?

00:31:26,211 --> 00:31:32,862
SPEAKER_0:  We'll have to figure out. Would you have them on? Yeah. Oh my God, if you just let me know. And just talk shit about you the whole time. That's perfect.

00:31:33,090 --> 00:31:36,158
SPEAKER_0:  He probably doesn't even care about me. Well now he will.

00:31:36,322 --> 00:31:37,790
SPEAKER_1:  because there's a certain pleasure.

00:31:38,114 --> 00:31:40,734
SPEAKER_1:  of a guy who's extremely good at his job.

00:31:41,314 --> 00:31:42,270
SPEAKER_1:  Not catching.

00:31:42,498 --> 00:31:44,373
SPEAKER_1:  another guy who's extremely good at his job.

00:31:44,373 --> 00:31:46,046
SPEAKER_0:  Obviously better. He got away.

00:31:46,530 --> 00:31:47,655
SPEAKER_0:  There you go, he's still here.

00:31:47,655 --> 00:31:48,405
SPEAKER_1:

00:31:48,405 --> 00:31:54,590
SPEAKER_0:  I love it. You or she. If I can meet that guy one day, he or she, that'd be great. I have no power.

00:31:55,618 --> 00:31:59,870
SPEAKER_1:  So yes, Silk Road, can you speak to the scale of this thing? at what ë§ï¿½ theres

00:32:00,386 --> 00:32:02,238
SPEAKER_1:  just for people who are not familiar.

00:32:02,786 --> 00:32:04,126
SPEAKER_1:  How big was it?

00:32:04,642 --> 00:32:04,990
SPEAKER_1:  uh

00:32:05,314 --> 00:32:09,278
SPEAKER_1:  and any other interesting things you understand about this operation.

00:32:09,634 --> 00:32:10,334
SPEAKER_1:  when it was active.

00:32:11,074 --> 00:32:12,382
SPEAKER_0:  So it was...

00:32:12,674 --> 00:32:15,326
SPEAKER_0:  when we finally got looking through the books and...

00:32:15,970 --> 00:32:16,414
SPEAKER_0:  You know.

00:32:16,642 --> 00:32:26,334
SPEAKER_0:  The numbers came out as about $1.2 billion in sales. It's kind of hard with the fluctuation value of Bitcoin at the time to come up with a real number. So you kind of pick a daily average.

00:32:26,562 --> 00:32:28,437
SPEAKER_0:  you know, go across, so...

00:32:28,437 --> 00:32:28,862
SPEAKER_1:  I was done.

00:32:28,962 --> 00:32:30,398
SPEAKER_0:  and Bitcoin. Hey, I'm cake supplies in Bitcoin.

00:32:30,690 --> 00:32:32,606
SPEAKER_0:  You couldn't. You had escrow accounts.

00:32:32,962 --> 00:32:43,134
SPEAKER_0:  on, you came in and you put money in an escrow account and the transaction wasn't done until the client got the drugs or whatever they had bought.

00:32:43,490 --> 00:32:45,918
SPEAKER_0:  And then the drug dealers had sent it in.

00:32:46,626 --> 00:32:50,430
SPEAKER_0:  There was some talk at the time that the cartel was starting to sell on there.

00:32:50,658 --> 00:32:53,214
SPEAKER_0:  So that started getting a little hairy there at the end.

00:32:53,314 --> 00:32:55,454
SPEAKER_1:  What was the understanding of the relationship between?

00:32:55,778 --> 00:32:58,398
SPEAKER_1:  organized crime like the cartels and

00:32:58,818 --> 00:33:00,350
SPEAKER_1:  this kind of more ad hoc.

00:33:01,442 --> 00:33:02,782
SPEAKER_1:  New Age.

00:33:03,778 --> 00:33:04,382
SPEAKER_1:  Uhhhh...

00:33:04,770 --> 00:33:06,366
SPEAKER_1:  market that is the Silk Road.

00:33:06,882 --> 00:33:13,310
SPEAKER_0:  I mean, it was all just chatter. It was just, you know, cause like I said, Jared was in the inside. So we saw some of it from the admin sides.

00:33:13,602 --> 00:33:18,078
SPEAKER_0:  And Ross had a lot of private conversations with different people that he had advised him.

00:33:18,626 --> 00:33:21,694
SPEAKER_0:  Um, but no one knew each other. I mean, the only thing

00:33:22,082 --> 00:33:30,526
SPEAKER_0:  The only thing that they knew were the admins had to send an ID to Ross, had to send a picture of their driver's license or passport.

00:33:30,754 --> 00:33:32,958
SPEAKER_0:  Which I always found very strange, because if you-

00:33:33,250 --> 00:33:36,190
SPEAKER_0:  or an admin on a site that sells fake IDs.

00:33:36,450 --> 00:33:40,382
SPEAKER_0:  Why would you send your real ID? And then why would the guy running the site who f-

00:33:40,802 --> 00:33:41,566
SPEAKER_0:  profits from

00:33:41,890 --> 00:33:43,070
SPEAKER_0:  Selling fake IDs.

00:33:43,458 --> 00:33:44,446
SPEAKER_0:  Believe that it was.

00:33:45,794 --> 00:33:47,038
SPEAKER_0:  Fast forward.

00:33:47,298 --> 00:33:52,382
SPEAKER_0:  they were all real IDs. All the IDs that we found on Ross's computer as the admins were the real people's IDs.

00:33:52,642 --> 00:33:54,942
SPEAKER_1:  What do you make of that? Other clumsiness?

00:33:55,234 --> 00:33:55,646
SPEAKER_1:  Yeah!

00:33:55,746 --> 00:33:58,238
SPEAKER_0:  Low hanging fruit, I guess. I guess that's what it is. I mean.

00:33:58,466 --> 00:34:00,734
SPEAKER_0:  I mean, I would have bought, I mean, even Ross bought.

00:34:01,730 --> 00:34:04,318
SPEAKER_0:  fake IDs off the site. He had federal agents knock on his door.

00:34:05,250 --> 00:34:07,710
SPEAKER_0:  Um, you know, and then he got a little cocky about it.

00:34:08,130 --> 00:34:11,230
SPEAKER_1:  The landscape, the dynamics of trust is fascinating here.

00:34:11,682 --> 00:34:13,182
SPEAKER_1:  to trust certain ideas or

00:34:13,794 --> 00:34:15,518
SPEAKER_1:  Like who do you trust in that kind of market?

00:34:15,778 --> 00:34:18,206
SPEAKER_1:  What was your understanding of the network of trust?

00:34:19,234 --> 00:34:23,870
SPEAKER_0:  I don't think anyone trusts anybody, you know? I mean, I think Ross had his advisors of trust, but outside of that...

00:34:24,450 --> 00:34:24,958
SPEAKER_0:  I mean.

00:34:25,250 --> 00:34:29,566
SPEAKER_0:  He required people to send their ID for their trust. He, you know, people...

00:34:29,826 --> 00:34:30,622
SPEAKER_0:  stole from him.

00:34:31,106 --> 00:34:33,246
SPEAKER_0:  Uh, there was, there's open cases of that.

00:34:33,570 --> 00:34:34,206
SPEAKER_0:  Um...

00:34:35,394 --> 00:34:37,694
SPEAKER_0:  It's a criminal world. You can't trust anybody.

00:34:38,882 --> 00:34:40,638
SPEAKER_1:  What was his life like you think?

00:34:41,730 --> 00:34:42,302
SPEAKER_0:  lonely.

00:34:43,490 --> 00:34:49,246
SPEAKER_0:  Can you imagine being trapped in something like that where your whole world focused on that and you can't tell people what you do all day?

00:34:50,050 --> 00:34:51,070
SPEAKER_0:  Could he have walked away?

00:34:53,730 --> 00:34:55,710
SPEAKER_0:  someone else take over the site just shut down.

00:34:56,194 --> 00:34:57,598
SPEAKER_1:  Either one. It's you.

00:34:57,890 --> 00:35:00,670
SPEAKER_1:  putting yourself in his shoes, the loneliness, the...

00:35:01,058 --> 00:35:02,654
SPEAKER_1:  The anxiety, the...

00:35:02,946 --> 00:35:04,926
SPEAKER_1:  just the growing immensity of it.

00:35:05,794 --> 00:35:08,094
SPEAKER_1:  walk away with some kind of financial stability.

00:35:08,418 --> 00:35:10,046
SPEAKER_0:  I couldn't have made it past two days.

00:35:10,370 --> 00:35:12,414
SPEAKER_0:  I don't like loneliness.

00:35:12,866 --> 00:35:13,502
SPEAKER_0:  I mean

00:35:13,762 --> 00:35:16,158
SPEAKER_0:  My- I- If my wife's away, I'd probably call her-

00:35:16,514 --> 00:35:17,566
SPEAKER_0:  10-12 times a day.

00:35:18,018 --> 00:35:22,206
SPEAKER_0:  We just talk about things, you know, I just, you know, something crossed my mind. I want to talk about it. I'm sure she.

00:35:22,946 --> 00:35:24,071
SPEAKER_0:  and you'd like to talk to her.

00:35:24,071 --> 00:35:25,374
SPEAKER_1:  like honestly about everything.

00:35:25,666 --> 00:35:28,414
SPEAKER_1:  So if you were running so crud, you would...

00:35:28,738 --> 00:35:29,863
SPEAKER_1:  You wouldn't be able to, like...

00:35:29,863 --> 00:35:30,782
SPEAKER_0:  Take care.

00:35:31,234 --> 00:35:34,174
SPEAKER_0:  Hopefully I'd have a little protection, I'd only mention to her when we were in bed.

00:35:34,594 --> 00:35:43,870
SPEAKER_0:  Um, to have that marital, uh, connection, but, but who knows? I mean, she's going to question why the Ferrari is outside and things like that.

00:35:44,002 --> 00:35:44,446
SPEAKER_1:  Hehehe

00:35:44,834 --> 00:35:47,518
SPEAKER_1:  I'm sure you can come up with something. Why didn't you walk away?

00:35:47,938 --> 00:35:51,550
SPEAKER_1:  It's another question of why don't criminals walk away in these situations?

00:35:51,906 --> 00:35:58,911
SPEAKER_0:  Well, I mean, I don't know every criminal mind and some do. I mean, AV unit walked away. I mean, not to go back to that son of a bitch.

00:35:58,911 --> 00:36:00,411
SPEAKER_1:  There's a theme.

00:36:00,411 --> 00:36:14,366
SPEAKER_0:  But Ross started counting his dollars. I mean, he really kept track of how much money he was making and it started getting exponentially growth. I mean, if he would have stayed at it, he would have probably been one of the richest people in the world.

00:36:14,946 --> 00:36:15,582
SPEAKER_1:  and

00:36:15,970 --> 00:36:18,270
SPEAKER_1:  Do you think you like the actual money or the?

00:36:18,498 --> 00:36:20,062
SPEAKER_1:  Fact of the number growing.

00:36:20,258 --> 00:36:21,822
SPEAKER_0:  I mean, have you ever held a Bitcoin?

00:36:22,562 --> 00:36:30,270
SPEAKER_0:  Oh, you have? Well, he never did. It wouldn't even hold the Bitcoin. You can't hold it. It's not real. Oh, oh, yeah. It's not like I can give you a briefcase of Bitcoin. Right. Or something like that.

00:36:30,498 --> 00:36:30,814
SPEAKER_0:  Eek

00:36:31,138 --> 00:36:38,270
SPEAKER_0:  He liked the idea of it growing. He liked the idea. I mean, I think it started off as sharing this idea, but then he really did turn to like, I am the

00:36:38,530 --> 00:36:40,862
SPEAKER_0:  Captain of the ship, that's what goes.

00:36:41,122 --> 00:36:42,654
SPEAKER_0:  He was making a lot of money.

00:36:43,746 --> 00:36:44,318
SPEAKER_0:  And again.

00:36:45,122 --> 00:36:46,814
SPEAKER_0:  My interaction with Ross was about...

00:36:48,610 --> 00:36:50,494
SPEAKER_0:  Maybe five or six hours over a.

00:36:50,882 --> 00:36:52,990
SPEAKER_0:  over a two day period.

00:36:53,602 --> 00:36:57,630
SPEAKER_0:  I knew DPR because I read his words and all that. I didn't really know Ross.

00:36:58,178 --> 00:36:58,494
SPEAKER_0:  Um.

00:36:59,010 --> 00:37:02,878
SPEAKER_0:  There was a journal found on his computer and so it sort of kind of gave me a little inside.

00:37:03,266 --> 00:37:06,046
SPEAKER_0:  Um, so I don't like to do a playbook for criminals.

00:37:06,530 --> 00:37:07,550
SPEAKER_0:  But I'll tell you right now.

00:37:08,034 --> 00:37:09,054
SPEAKER_0:  Don't write things down.

00:37:09,666 --> 00:37:14,142
SPEAKER_0:  There was a big fad about people like remember kids going around shooting people with paintballs and filming it

00:37:14,978 --> 00:37:18,142
SPEAKER_0:  I don't know why you would do that. Why would you videotape yourself committing crime?

00:37:18,466 --> 00:37:19,614
SPEAKER_0:  and then publish it.

00:37:19,970 --> 00:37:21,566
SPEAKER_0:  if there's one thing I've taught my children.

00:37:22,018 --> 00:37:24,643
SPEAKER_0:  Don't record yourself doing bad things. Never goes back to a-

00:37:24,643 --> 00:37:29,950
SPEAKER_1:  as well. And you actually give advice on the other end of logs being very useful for the defense perspective.

00:37:30,882 --> 00:37:31,678
SPEAKER_1:  uh... for

00:37:33,090 --> 00:37:35,134
SPEAKER_1:  you know, information.

00:37:35,490 --> 00:37:38,334
SPEAKER_1:  is useful for being able to figure out what the attacks were.

00:37:38,562 --> 00:37:38,942
SPEAKER_1:  about.

00:37:39,266 --> 00:37:46,366
SPEAKER_0:  Logs are the only reason I found Hector Montsegur. I mean, the one time his VPN dropped during a Fox hack.

00:37:46,786 --> 00:37:49,822
SPEAKER_0:  And he says he did it wasn't even hacking, he just was sent a link and he clicked on it.

00:37:50,178 --> 00:37:54,302
SPEAKER_0:  And in 10 million lines of logs, there was one IP address that stuck out.

00:37:55,874 --> 00:37:59,326
SPEAKER_1:  This is fascinating. We'll explore several angles of that. So, um...

00:38:00,578 --> 00:38:03,294
SPEAKER_1:  What was the process of bringing down?

00:38:04,066 --> 00:38:06,238
SPEAKER_1:  Ross and the Silk Road.

00:38:06,626 --> 00:38:10,622
SPEAKER_0:  All right, so that's a long story. You want the whole thing or you want to break it up? Let's start at the beginning.

00:38:11,650 --> 00:38:13,886
SPEAKER_0:  once we had the information.

00:38:14,146 --> 00:38:14,750
SPEAKER_0:  of

00:38:15,074 --> 00:38:16,990
SPEAKER_0:  the chat logs and all that from the server.

00:38:17,698 --> 00:38:19,678
SPEAKER_0:  We found the server with the chat log.

00:38:19,938 --> 00:38:26,238
SPEAKER_0:  So the dot onion was running the website, the Silk Road, was running on a server in Iceland.

00:38:27,042 --> 00:38:27,934
SPEAKER_1:  How did you figure that out?

00:38:28,450 --> 00:38:29,758
SPEAKER_1:  That's one of the...

00:38:30,050 --> 00:38:31,486
SPEAKER_1:  claims that the NSA.

00:38:31,650 --> 00:38:38,878
SPEAKER_0:  Yeah, that's the one that we said that, yeah, I wouldn't tell you if it was. It's on the internet. I mean, the internet has their conspiracy theories and all that, so.

00:38:39,426 --> 00:38:40,551
SPEAKER_0:  But you figure out that's...

00:38:40,551 --> 00:38:48,318
SPEAKER_1:  the part of the thing you do, it's puzzle pieces and you have to put them together and look for different pieces of information and figure out, okay, so you figure out the servers in Iceland.

00:38:48,418 --> 00:38:58,430
SPEAKER_0:  We get a copy of it and so we start getting clues off of that. With a physical copy of the server? Yeah, you fly over there. So you go. If you've never been to Iceland, you should definitely go.

00:38:58,690 --> 00:39:02,014
SPEAKER_0:  Is it beautiful or? I love it, I love it. So I'll tell you this.

00:39:02,530 --> 00:39:13,022
SPEAKER_0:  Sorry, tangents. You know what I mean? I love this. Yeah. So I went to Iceland for the anonymous case. Then I went to Iceland for the Silk Road case. And I was like, oh shit, all cybercrime goes through Iceland.

00:39:13,378 --> 00:39:31,806
SPEAKER_0:  Um, it was just my sort of thing. And I was over there for like the third time. And I said, if I ever can bring my family here, like so there's a place called thing of our, and I'm sure I'm fucking up the name that Icelandics are pissed right now, but it's where the, the North American continental plate and the European condom plate are pulling apart and it's being filled in with a volcanic.

00:39:32,034 --> 00:39:34,014
SPEAKER_0:  material in the middle.

00:39:34,658 --> 00:39:35,966
SPEAKER_0:  And it's so cool.

00:39:36,354 --> 00:39:37,214
SPEAKER_0:  Like, I was like...

00:39:37,762 --> 00:39:40,990
SPEAKER_0:  One day, I'll be able to afford to bring my family here.

00:39:41,410 --> 00:39:43,285
SPEAKER_0:  uh... and what i left a second

00:39:43,285 --> 00:39:44,958
SPEAKER_1:  humbling and the beauty of nature.

00:39:45,122 --> 00:39:52,894
SPEAKER_0:  Just everything, man, it was a different world. It was insane how great Iceland is. And so we went back and we rented a van and we took friends.

00:39:53,378 --> 00:39:56,382
SPEAKER_0:  And we drove around the entire country.

00:39:57,058 --> 00:39:57,406
SPEAKER_0:  Uh...

00:39:57,858 --> 00:40:00,446
SPEAKER_0:  Absolutely, like a beautiful place.

00:40:01,314 --> 00:40:05,854
SPEAKER_0:  Like Reykjavik's nice, but get out of Reykjavik as quick as you can and see the countryside.

00:40:06,082 --> 00:40:07,262
SPEAKER_0:  How is this place even real?

00:40:07,586 --> 00:40:08,478
SPEAKER_0:  What's so new?

00:40:08,834 --> 00:40:09,246
SPEAKER_0:  I mean that's.

00:40:09,602 --> 00:40:16,798
SPEAKER_0:  So you know, our rivers have been going through here for millions of years and flattened everything out and all that. These are new land being carved by these rivers.

00:40:17,058 --> 00:40:19,166
SPEAKER_0:  You can walk behind a waterfall in one place.

00:40:19,618 --> 00:40:22,750
SPEAKER_0:  Um, it's, it's, it's the most beautiful place I've ever been.

00:40:23,138 --> 00:40:26,462
SPEAKER_1:  You understand why this is a place where a lot of hacking is being done?

00:40:26,882 --> 00:40:32,830
SPEAKER_0:  because the energy is free and it's cool. So you have a lot of servers going on there. Server farms, you know, they're.

00:40:33,058 --> 00:40:36,542
SPEAKER_0:  The energy has come up out of the ground, geothermal.

00:40:37,026 --> 00:40:40,446
SPEAKER_0:  And so, and then it keeps all the servers nice and cool. So....

00:40:40,930 --> 00:40:43,134
SPEAKER_0:  Why not keep your computers there at a cheap rate?

00:40:43,554 --> 00:40:44,679
SPEAKER_0:  uh... and i'll do it

00:40:44,679 --> 00:40:50,046
SPEAKER_1:  me visit for several reasons including to talk to AV unit. yeah

00:40:50,338 --> 00:40:52,894
SPEAKER_1:  Well, the servers are there, but they don't probably live there.

00:40:53,250 --> 00:40:55,070
SPEAKER_1:  I mean, that's the interesting, I mean, the Pacific.

00:40:55,330 --> 00:41:00,955
SPEAKER_1:  the PSD of the time zones. There's so many fascinating things to explore here.

00:41:00,955 --> 00:41:13,182
SPEAKER_0:  Sorry, to add to that, the European internet cable goes through there, so across the Greenland and down through Canada and all that, so they have backbone access with cheap energy and free cold weather.

00:41:13,634 --> 00:41:15,422
SPEAKER_0:  And beautiful. Oh, and beautiful, yes.

00:41:16,354 --> 00:41:17,479
SPEAKER_0:  So check

00:41:17,479 --> 00:41:20,094
SPEAKER_1:  logs on that server what did what did the

00:41:20,930 --> 00:41:22,805
SPEAKER_1:  what was in the chat logs.

00:41:22,805 --> 00:41:26,910
SPEAKER_0:  Everything. He kept them all. That's another issue. If you keep writing a criminal enterprise.

00:41:27,138 --> 00:41:30,878
SPEAKER_0:  Please don't keep, again, I'm not making a guidebook on how to commit your perfect crime.

00:41:31,138 --> 00:41:36,094
SPEAKER_0:  But you know, every chat he ever had and everyone's chat, it was like going into Facebook.

00:41:36,546 --> 00:41:37,598
SPEAKER_0:  of criminal activity.

00:41:38,242 --> 00:41:41,758
SPEAKER_1:  Yeah, I'm just looking at texts with Elon Musk.

00:41:42,690 --> 00:41:47,614
SPEAKER_1:  being part of the conversations. I don't know if you're familiar, but they've been made public.

00:41:48,418 --> 00:41:50,046
SPEAKER_1:  before the court case is going through.

00:41:50,306 --> 00:41:53,310
SPEAKER_1:  What's going through? Is going through? What's going through? Twitter.

00:41:54,338 --> 00:41:54,942
SPEAKER_1:  Um

00:41:55,394 --> 00:41:58,334
SPEAKER_1:  but it made me realize that, oh, okay.

00:41:59,362 --> 00:42:04,926
SPEAKER_1:  I'm generally, that's my philosophy on life, is like anything I text or email or say.

00:42:05,378 --> 00:42:07,582
SPEAKER_1:  publicly or privately I should be proud of.

00:42:08,674 --> 00:42:13,086
SPEAKER_1:  So I tried to kind of do that because you basically you say don't keep chat logs.

00:42:13,826 --> 00:42:15,358
SPEAKER_1:  but it's very difficult.

00:42:16,034 --> 00:42:17,502
SPEAKER_1:  to erase chat logs.

00:42:17,730 --> 00:42:19,070
SPEAKER_1:  from this world.

00:42:19,330 --> 00:42:21,310
SPEAKER_1:  I guess if you're a criminal that should be a-

00:42:22,178 --> 00:42:27,125
SPEAKER_1:  Like you have to be exceptionally competent at that kind of thing. To erase your footprints is very, very difficult.

00:42:27,125 --> 00:42:29,534
SPEAKER_0:  Can't make one mistake all it takes is one mistake

00:42:30,082 --> 00:42:32,702
SPEAKER_0:  of keeping it. But yeah, I mean, not only do you have to

00:42:33,666 --> 00:42:34,238
SPEAKER_0:  Be-

00:42:34,658 --> 00:42:41,950
SPEAKER_0:  Whatever you put in a chat log or whatever you put in an email, it has to hold up and you have to stand behind it publicly when it comes out, but if it comes out 10 years from now.

00:42:42,498 --> 00:42:43,454
SPEAKER_0:  You have to stand behind it.

00:42:43,682 --> 00:42:45,694
SPEAKER_0:  I mean, we're seeing that now in today's society.

00:42:46,338 --> 00:42:47,463
SPEAKER_0:  but that's a response.

00:42:47,463 --> 00:42:51,710
SPEAKER_1:  you have to take it really, really seriously. If I was a parent and advising-

00:42:52,034 --> 00:42:53,886
SPEAKER_1:  like you kind of have to teach them that.

00:42:54,562 --> 00:42:56,350
SPEAKER_1:  I know there's a sense like...

00:42:56,866 --> 00:43:00,510
SPEAKER_1:  No, we'll become more accustomed to that kind of thing, but in reality, nope.

00:43:00,834 --> 00:43:03,486
SPEAKER_1:  I think in the future we'll still be held responsible.

00:43:03,842 --> 00:43:04,926
SPEAKER_0:  What a weird shit we do.

00:43:05,218 --> 00:43:09,790
SPEAKER_0:  Yeah, a friend of mine, his daughter got kicked out of college because of something she posted in high school.

00:43:10,146 --> 00:43:11,422
SPEAKER_0:  The shittiest thing for him.

00:43:12,034 --> 00:43:15,998
SPEAKER_0:  Great for my kids. Great lesson. Look over there and you don't want that to happen to you.

00:43:16,386 --> 00:43:21,182
SPEAKER_1:  Yeah. Okay. So in the chat logs was useful information.

00:43:21,410 --> 00:43:22,782
SPEAKER_1:  Like, uh...

00:43:23,106 --> 00:43:26,481
SPEAKER_1:  breadcrumbs of information that you can then pull.

00:43:26,481 --> 00:43:30,494
SPEAKER_0:  Yeah, great evidence and stuff, you know, I mean, obviously, yeah, a lot of evidence.

00:43:31,202 --> 00:43:34,142
SPEAKER_0:  I here's a sale of this much heroin because

00:43:34,466 --> 00:43:41,214
SPEAKER_0:  You know, Ross ended up getting charged with czar status on certain things. And that's, it's a certain weight in each type of drug.

00:43:41,602 --> 00:43:56,286
SPEAKER_0:  that you had like, I think it's four or five employees of your empire and that you made more than $10 million. And so it's, it's, it's, you know, it's just like the, the narco track fitters get charged with or, you know, uh, anybody out of Columbia, you know, so.

00:43:56,898 --> 00:43:58,910
SPEAKER_1:  And that was primarily what he was.

00:43:59,234 --> 00:44:02,366
SPEAKER_1:  charged with during when he was arrested is the drug.

00:44:03,010 --> 00:44:05,278
SPEAKER_0:  Yeah, and you gotta charge for some of the hacking tools too.

00:44:06,562 --> 00:44:08,606
SPEAKER_1:  Like, because he's in prison, what, for?

00:44:08,866 --> 00:44:11,134
SPEAKER_0:  Two life sentences plus 40 years.

00:44:11,490 --> 00:44:13,310
SPEAKER_1:  and no possibility of parole.

00:44:13,762 --> 00:44:16,798
SPEAKER_0:  In the federal system, there's no possibility of parole when you have life.

00:44:17,026 --> 00:44:19,486
SPEAKER_0:  The only way you get out is if the president pardons you.

00:44:22,914 --> 00:44:24,030
SPEAKER_0:  There's always a chance.

00:44:24,674 --> 00:44:26,878
SPEAKER_0:  There is, I think it was close.

00:44:27,266 --> 00:44:29,141
SPEAKER_0:  I heard rumors there was clothes.

00:44:29,141 --> 00:44:31,070
SPEAKER_1:  Well, right, so it depends.

00:44:31,362 --> 00:44:35,006
SPEAKER_1:  given its fascinating, but given the political, the ideological.

00:44:35,362 --> 00:44:36,926
SPEAKER_1:  ideas that he represented.

00:44:37,474 --> 00:44:38,814
SPEAKER_1:  and espoused.

00:44:39,170 --> 00:44:41,726
SPEAKER_1:  It's not out of the realm of possibility.

00:44:42,690 --> 00:44:47,774
SPEAKER_0:  Yeah, I mean, I've been asked before, who, you know, who does he get out of prison first or does Snowden come back into America?

00:44:48,162 --> 00:44:48,702
SPEAKER_0:  I don't know.

00:44:49,090 --> 00:44:54,398
SPEAKER_0:  I have no idea. Someone just became a Russian citizen. I saw that and that's, yeah, I've heard a lot of weird theories about that one.

00:44:54,850 --> 00:44:55,975
SPEAKER_0:  will actually

00:44:55,975 --> 00:44:59,006
SPEAKER_1:  On another tangent, let me ask you, do you think Snowden...

00:45:00,066 --> 00:45:00,862
SPEAKER_1:  is uh...

00:45:02,498 --> 00:45:04,478
SPEAKER_1:  Good or bad person? Good or bad person?

00:45:05,666 --> 00:45:07,742
SPEAKER_1:  Can you make the case that he's a bad person?

00:45:08,034 --> 00:45:09,822
SPEAKER_0:  There's ways of being a whistleblower.

00:45:10,050 --> 00:45:11,774
SPEAKER_0:  And there's...

00:45:12,226 --> 00:45:14,078
SPEAKER_0:  rules set up on how to do that.

00:45:14,338 --> 00:45:14,910
SPEAKER_0:  Um...

00:45:15,618 --> 00:45:18,526
SPEAKER_0:  He didn't follow those rules. I mean, they...

00:45:19,650 --> 00:45:24,062
SPEAKER_0:  You know, I'm red, white and blue. So I'm pretty, you know, I think his actions were anti-American.

00:45:24,514 --> 00:45:29,389
SPEAKER_0:  I think the results of his actions were anti-American. I don't know if his actions were anti-American.

00:45:29,389 --> 00:45:33,118
SPEAKER_1:  he could have anticipated the negative consequences of his action

00:45:33,474 --> 00:45:36,670
SPEAKER_1:  Should we judge him by the consequences or the ideals?

00:45:37,346 --> 00:45:39,358
SPEAKER_1:  of the intent of his actions.

00:45:39,554 --> 00:45:43,646
SPEAKER_0:  I think we all get to judge him by best our own beliefs, but I believe what he did.

00:45:44,002 --> 00:45:44,382
SPEAKER_0:  was wrong.

00:45:44,578 --> 00:45:47,230
SPEAKER_1:  Can you still man the case that he is actually...

00:45:48,194 --> 00:45:51,998
SPEAKER_1:  a good person and good for this country, for the United States of America.

00:45:52,738 --> 00:45:54,654
SPEAKER_1:  as a flag bearer for

00:45:54,946 --> 00:45:55,262
SPEAKER_1:  of.

00:45:55,874 --> 00:45:59,230
SPEAKER_1:  whistleblowers, the check on the power of government.

00:46:00,546 --> 00:46:01,854
SPEAKER_0:  Yeah, I mean, I'm not-

00:46:02,466 --> 00:46:08,350
SPEAKER_0:  big government type guy. You know, so, you know, even that sounds weird coming from a government guy for so many years.

00:46:08,642 --> 00:46:09,118
SPEAKER_0:  Um...

00:46:10,242 --> 00:46:14,302
SPEAKER_0:  There's rules in place for a reason. I mean, he put, you know, some of our

00:46:14,530 --> 00:46:17,886
SPEAKER_0:  best capabilities, he made them publicly available.

00:46:18,338 --> 00:46:18,814
SPEAKER_0:  Um...

00:46:19,042 --> 00:46:21,022
SPEAKER_0:  It really kind of set us back in the.

00:46:21,538 --> 00:46:25,663
SPEAKER_0:  And this isn't my world at all, but the offensive side of cybersecurity.

00:46:25,663 --> 00:46:29,118
SPEAKER_1:  So he revealed stuff that he didn't need to reveal in order to make the point.

00:46:30,306 --> 00:46:31,230
SPEAKER_1:  Uh... so..."

00:46:31,810 --> 00:46:32,670
SPEAKER_1:  So you, if.

00:46:32,962 --> 00:46:37,310
SPEAKER_1:  You can imagine a world where he leaked stuff that revealed the

00:46:37,762 --> 00:46:39,422
SPEAKER_1:  Mass surveillance efforts.

00:46:40,514 --> 00:46:41,726
SPEAKER_1:  and not reveal...

00:46:42,242 --> 00:46:42,878
SPEAKER_1:  other stuff.

00:46:44,770 --> 00:46:53,150
SPEAKER_1:  Like, is the mass surveillance, I mean, that's the thing that, of course, in the interpretation of that, there's fear mongering, but at the core.

00:46:53,666 --> 00:46:56,478
SPEAKER_1:  that was a real shock to people that...

00:46:57,154 --> 00:46:57,598
SPEAKER_1:  Um...

00:46:57,922 --> 00:47:00,702
SPEAKER_1:  it's possible for government to collect data at scale.

00:47:03,394 --> 00:47:06,238
SPEAKER_0:  It's surprising to me that people are that shocked by it.

00:47:06,914 --> 00:47:09,246
SPEAKER_1:  Well, there's conspiracies and then there's like...

00:47:09,858 --> 00:47:10,686
SPEAKER_1:  actual

00:47:11,074 --> 00:47:11,422
SPEAKER_1:  Uh...

00:47:11,714 --> 00:47:13,278
SPEAKER_1:  evidence that that is happening.

00:47:13,922 --> 00:47:16,414
SPEAKER_1:  I mean, it's a, it's a real, there's a lot of reality.

00:47:16,994 --> 00:47:17,918
SPEAKER_1:  that people...

00:47:18,306 --> 00:47:19,038
SPEAKER_1:  Ignore.

00:47:19,458 --> 00:47:23,198
SPEAKER_1:  But when it hits you in the face, you realize, holy shit, we're living in a new world. This is-

00:47:23,810 --> 00:47:26,526
SPEAKER_1:  this is the new reality and we have to deal with that reality just like

00:47:26,850 --> 00:47:28,542
SPEAKER_1:  You work in cyber security, I think.

00:47:29,058 --> 00:47:31,102
SPEAKER_1:  It really hasn't hit most people.

00:47:32,098 --> 00:47:32,734
SPEAKER_1:  How?

00:47:33,506 --> 00:47:34,654
SPEAKER_1:  Fucked we all are.

00:47:35,330 --> 00:47:41,662
SPEAKER_1:  in terms of cyber security. Okay, let me rephrase that. How many dangers there are in the digital world?

00:47:42,050 --> 00:47:45,150
SPEAKER_1:  how much under attack we all are and how more.

00:47:45,666 --> 00:47:47,550
SPEAKER_1:  intensity attacks are getting

00:47:47,778 --> 00:47:50,526
SPEAKER_1:  and how difficult the defense is and how important it is.

00:47:50,882 --> 00:47:57,598
SPEAKER_1:  and how much we should value it and all the different things we should do at the small and large scale to defend. Like most people really haven't woken up.

00:47:58,242 --> 00:47:59,678
SPEAKER_1:  they think about privacy.

00:48:00,066 --> 00:48:02,494
SPEAKER_1:  from tech companies. They don't think about attacks.

00:48:02,786 --> 00:48:03,911
SPEAKER_1:  cyber attacks.

00:48:03,911 --> 00:48:08,318
SPEAKER_0:  People don't think they're a target and that message definitely has to get out there.

00:48:08,578 --> 00:48:22,197
SPEAKER_0:  You know, if you have a voice, you're a target. If the place you work, you might be a target, you know, so your husband might work at someplace, you know, because now people are working from home. So they're gonna target, you know, target you to get access to his network in order to get in. on Audible.

00:48:22,197 --> 00:48:27,646
SPEAKER_1:  same way the idea that the US government or any government could be doing mass surveillance.

00:48:27,874 --> 00:48:28,830
SPEAKER_1:  on its citizens.

00:48:29,474 --> 00:48:30,462
SPEAKER_1:  is, um...

00:48:30,882 --> 00:48:32,606
SPEAKER_1:  is one that was a wake up call.

00:48:32,898 --> 00:48:34,142
SPEAKER_1:  because you could.

00:48:34,434 --> 00:48:35,230
SPEAKER_1:  Imagine.

00:48:35,554 --> 00:48:37,598
SPEAKER_1:  the ways in which that could, um...

00:48:39,042 --> 00:48:41,662
SPEAKER_1:  like you could abuse the power of that.

00:48:42,050 --> 00:48:45,406
SPEAKER_1:  to control the citizenry for political reasons and purposes.

00:48:45,954 --> 00:48:53,566
SPEAKER_0:  Absolutely, you know, you could abuse it. I think during the part of the Snowden League saw the two NSA guys were

00:48:53,794 --> 00:48:55,006
SPEAKER_0:  Modern, I think they're girlfriends.

00:48:55,330 --> 00:48:58,654
SPEAKER_0:  And there's rules in place for that. Those people should be punished for abusing that.

00:48:58,882 --> 00:48:59,294
SPEAKER_0:  but

00:48:59,554 --> 00:49:03,998
SPEAKER_0:  How else are we going to hear about, you know, terrorists that are in the country?

00:49:04,258 --> 00:49:05,566
SPEAKER_0:  talking about birthday cakes.

00:49:06,210 --> 00:49:11,646
SPEAKER_0:  And you know, that was a case where that was the trip word that, you know, we're going to go bomb New York City's.

00:49:11,938 --> 00:49:12,478
SPEAKER_0:  Subway.

00:49:12,674 --> 00:49:16,766
SPEAKER_1:  Yeah, it's complicated, but it just feels like there should be some balance of transparency.

00:49:17,282 --> 00:49:18,718
SPEAKER_1:  There should be a check in that power.

00:49:19,458 --> 00:49:22,462
SPEAKER_1:  Because like you, you know, in the name of the war on terror.

00:49:24,418 --> 00:49:28,734
SPEAKER_1:  you can sort of sacrifice, that there is a trade-off between security and freedom.

00:49:29,378 --> 00:49:32,446
SPEAKER_1:  but it just feels like there's a giant slippery slope.

00:49:33,058 --> 00:49:35,390
SPEAKER_1:  on the sacrificing of freedom in the name of security.

00:49:35,682 --> 00:49:37,150
SPEAKER_1:  I hear you and-

00:49:37,474 --> 00:49:39,934
SPEAKER_0:  You know, we live in a world where...

00:49:40,162 --> 00:49:47,102
SPEAKER_0:  Well, I live in a world where I had to tell you exactly how, when I arrested someone, I had to write a 50 page document of how I arrested you.

00:49:47,554 --> 00:49:49,790
SPEAKER_0:  uh... and all the probable cause i have against you know that

00:49:50,562 --> 00:49:55,934
SPEAKER_0:  You know, bad guys are reading that. They're reading how I caught you and they're changing their way they're doing things. They're changing their MO.

00:49:56,386 --> 00:49:59,006
SPEAKER_0:  You know, they're doing it to be more secure

00:49:59,298 --> 00:50:03,550
SPEAKER_0:  If we tell people how we're monitoring, what we're surveilling.

00:50:04,226 --> 00:50:19,934
SPEAKER_0:  we're going to lose that. I mean, the terrorists are just going to go a different way. And I'm not trying to, again, I'm not big government. I'm not trying to say that, you know, it's cool that we're monitoring the US government's monitoring everything, you know, big techs monitoring everything. They're just monetizing it versus.

00:50:20,226 --> 00:50:22,101
SPEAKER_0:  uh... possibly using it against you but

00:50:22,101 --> 00:50:24,126
SPEAKER_1:  There is a balance in those 50 pages.

00:50:24,386 --> 00:50:25,790
SPEAKER_1:  They have a lot of value.

00:50:26,562 --> 00:50:26,910
SPEAKER_1:  uh

00:50:27,170 --> 00:50:28,670
SPEAKER_1:  if they make your job harder.

00:50:30,018 --> 00:50:32,958
SPEAKER_1:  but they prevent you from abusing the power of the job.

00:50:34,082 --> 00:50:34,750
SPEAKER_1:  as a balance.

00:50:35,170 --> 00:50:36,190
SPEAKER_1:  That's the tricky balance.

00:50:37,602 --> 00:50:38,974
SPEAKER_1:  of the chat logs.

00:50:39,490 --> 00:50:40,286
SPEAKER_1:  in Iceland.

00:50:42,242 --> 00:50:43,646
SPEAKER_1:  give you evidence.

00:50:44,290 --> 00:50:49,310
SPEAKER_1:  of the heroin and all the large-scale czar level.

00:50:49,762 --> 00:50:51,038
SPEAKER_1:  uh... drug trading

00:50:51,490 --> 00:50:53,854
SPEAKER_1:  What else did it give you in terms of?

00:50:54,114 --> 00:50:55,326
SPEAKER_1:  the hard to catch.

00:50:55,682 --> 00:50:57,342
SPEAKER_0:  It gave us infrastructure. So

00:50:57,602 --> 00:51:01,854
SPEAKER_0:  the onion name was actually running on a server in France. So if you like

00:51:02,402 --> 00:51:05,470
SPEAKER_0:  and it only can be done through a back channel of VPN.

00:51:06,018 --> 00:51:07,582
SPEAKER_0:  to connect to the Iceland server.

00:51:07,874 --> 00:51:10,238
SPEAKER_0:  Um, there was a, um,

00:51:10,690 --> 00:51:14,270
SPEAKER_0:  Bitcoin like kind of vault server that was also in Iceland.

00:51:14,690 --> 00:51:19,262
SPEAKER_0:  I think that was so that the admins couldn't get into the bitcoins.

00:51:19,618 --> 00:51:24,062
SPEAKER_0:  the other admins that were hired to work on the site. So you could get into the site, but you couldn't touch the money.

00:51:24,546 --> 00:51:25,822
SPEAKER_0:  Only Ross had access to that.

00:51:26,434 --> 00:51:31,518
SPEAKER_0:  And then, you know, another, another big mistake on Ross's part is he had the backups for everything.

00:51:31,746 --> 00:51:33,886
SPEAKER_0:  at a data center in Philadelphia.

00:51:35,266 --> 00:51:36,862
SPEAKER_0:  Don't put your infrastructure in the United States.

00:51:38,210 --> 00:51:38,846
SPEAKER_0:  I mean, again.

00:51:39,490 --> 00:51:41,278
SPEAKER_0:  Let's not make a playbook, but.

00:51:41,474 --> 00:51:47,998
SPEAKER_1:  You know, I think these are low hanging fruit. The people of competence would know already. Uh, but it's interesting that.

00:51:48,642 --> 00:51:50,334
SPEAKER_1:  he wasn't competent enough to make.

00:51:50,850 --> 00:51:52,638
SPEAKER_1:  So he was incompetent in certain ways.

00:51:53,218 --> 00:51:59,582
SPEAKER_0:  Yeah, I don't think he was a mastermind of setting up an infrastructure that would protect his.

00:51:59,938 --> 00:52:01,886
SPEAKER_0:  uh, his- his-

00:52:02,242 --> 00:52:06,046
SPEAKER_0:  online business because you know, keeping chat logs, keeping a diary.

00:52:06,274 --> 00:52:07,966
SPEAKER_0:  infrastructure where it shouldn't be.

00:52:08,482 --> 00:52:08,862
SPEAKER_0:  um

00:52:11,170 --> 00:52:11,870
SPEAKER_0:  Bad decisions.

00:52:12,930 --> 00:52:15,998
SPEAKER_1:  How did you figure out that he's in San Francisco?

00:52:17,314 --> 00:52:21,726
SPEAKER_0:  So we had that part with Jared that he was on the West coast and then... And we're Guinness Jared.

00:52:22,178 --> 00:52:27,582
SPEAKER_0:  Jared Day Egan was a partner in, he was a DHS agent.

00:52:27,810 --> 00:52:31,710
SPEAKER_0:  um... worked for HSI Homeland Security Investigations in Chicago.

00:52:32,002 --> 00:52:37,118
SPEAKER_0:  Uh, he started his Silk Road investigation because he was working at O'Hare and a weird package came in.

00:52:37,378 --> 00:52:44,190
SPEAKER_0:  I'm coming to find out he traced it back to Silk Road. So he started working at a Silk Road investigation long before I started my case.

00:52:44,578 --> 00:52:48,670
SPEAKER_0:  And he made his way up undercover all the way to be an admin on Silk Road.

00:52:49,314 --> 00:52:54,686
SPEAKER_0:  Um, so he was talking to Ross on a Jabber server, the private Jabber server, private.

00:52:55,330 --> 00:52:56,414
SPEAKER_0:  communication server.

00:52:56,674 --> 00:53:00,286
SPEAKER_0:  And we noticed that Ross's.

00:53:00,610 --> 00:53:03,550
SPEAKER_0:  timezone on that Jabber server was set to the west coast.

00:53:04,034 --> 00:53:10,782
SPEAKER_0:  So we had Pacific time on there. So we had a region, 1 24th of the world was covered of where we thought we might be.

00:53:11,330 --> 00:53:11,646
SPEAKER_1:

00:53:11,938 --> 00:53:14,270
SPEAKER_1:  And from there, how do you get to San Francisco?

00:53:14,498 --> 00:53:23,582
SPEAKER_0:  There was another guy, an IRS agent that was part of the team and he used a powerful tool to find his clue.

00:53:23,842 --> 00:53:25,534
SPEAKER_0:  uh... he used the world of google

00:53:26,082 --> 00:53:31,294
SPEAKER_0:  He simply just went back and googled around for Silk Road at the time it was coming up.

00:53:31,874 --> 00:53:35,742
SPEAKER_0:  and found some posts on like some help forums.

00:53:36,098 --> 00:53:40,638
SPEAKER_0:  that this guy was starting an onion website and wanted some cryptocurrency help.

00:53:41,122 --> 00:53:46,398
SPEAKER_0:  And if you could help him, please reach out to Ross.Aubrick at gmail.com.

00:53:46,722 --> 00:53:49,214
SPEAKER_0:  in my world, that's a clue.

00:53:50,914 --> 00:53:52,798
SPEAKER_1:  Okay, so that's as simple as that.

00:53:53,442 --> 00:53:55,934
SPEAKER_0:  And the name he used on that post was Frosty.

00:53:57,058 --> 00:53:58,398
SPEAKER_1:  So he had to connect.

00:53:58,818 --> 00:54:01,662
SPEAKER_1:  frosty and other uses in frosty and

00:54:02,018 --> 00:54:03,070
SPEAKER_1:  Here's a Gmail.

00:54:03,362 --> 00:54:04,862
SPEAKER_1:  and the Gmail has the name.

00:54:05,218 --> 00:54:09,662
SPEAKER_0:  The Gmail posted that I need help under the name Frosty on this forum.

00:54:09,890 --> 00:54:11,614
SPEAKER_0:  So what's the connection of frosty elsewhere?

00:54:12,674 --> 00:54:17,566
SPEAKER_0:  person logging into the Philadelphia backup server, the name of the computer was Frosty.

00:54:19,426 --> 00:54:20,638
SPEAKER_0:  Another clue in my world.

00:54:21,314 --> 00:54:22,206
SPEAKER_1:  And that's it.

00:54:22,722 --> 00:54:23,710
SPEAKER_1:  The name is there.

00:54:24,290 --> 00:54:27,550
SPEAKER_1:  The connection to the Philadelphia server and then to Iceland is there.

00:54:28,066 --> 00:54:31,550
SPEAKER_1:  And so the rest is small details in terms of...

00:54:31,874 --> 00:54:32,999
SPEAKER_1:  or is there interesting?

00:54:32,999 --> 00:54:41,566
SPEAKER_0:  No, I mean there's some electronic surveillance that find Ross Albrecht living in a house and is there, you know, is a computer at his house attaching to...

00:54:41,922 --> 00:54:42,558
SPEAKER_0:  uh uh

00:54:42,786 --> 00:54:46,462
SPEAKER_0:  you know, does it have Tor traffic at the same time that DPR is on?

00:54:47,042 --> 00:54:48,254
SPEAKER_0:  Another big clue.

00:54:49,410 --> 00:54:50,942
SPEAKER_0:  matching up timeframes.

00:54:51,938 --> 00:54:52,542
SPEAKER_1:  again.

00:54:52,866 --> 00:54:56,702
SPEAKER_1:  Just putting your email out there, putting your name out there like that.

00:54:57,954 --> 00:54:59,614
SPEAKER_1:  Like what I see from that.

00:55:00,226 --> 00:55:07,838
SPEAKER_1:  just at the scale of that market. What just makes me wonder how many criminals are out there not making these law hanging fruit mistakes.

00:55:08,514 --> 00:55:10,142
SPEAKER_1:  and are still successfully operating.

00:55:10,370 --> 00:55:13,758
SPEAKER_1:  To me, it seems like you could be a criminal.

00:55:14,914 --> 00:55:17,342
SPEAKER_1:  It's much easier to be a criminal on the internet.

00:55:19,042 --> 00:55:23,486
SPEAKER_1:  What else to you is interesting to understand about that case of Russ and...

00:55:24,738 --> 00:55:28,990
SPEAKER_1:  and Silk Road and just the history of it from your own.

00:55:29,826 --> 00:55:30,942
SPEAKER_1:  relationship with it.

00:55:31,426 --> 00:55:37,694
SPEAKER_1:  from a cybersecurity perspective, from an ethical perspective, all that kind of stuff. When you look back, what's interesting to you about?

00:55:38,306 --> 00:55:38,910
SPEAKER_1:  that case.

00:55:39,650 --> 00:55:43,966
SPEAKER_0:  I think my views on the case have changed over time. I mean, it was my job back then.

00:55:44,226 --> 00:55:46,142
SPEAKER_0:  So I just looked at it as of.

00:55:46,594 --> 00:55:49,406
SPEAKER_0:  You know, I'm going after this. I sort of.

00:55:49,954 --> 00:56:00,222
SPEAKER_0:  made a name for myself in the bureau for the anonymous case. And then this one was just, I mean, this was a bigger deal. I mean, they flew me down to DC to meet with the director about this case.

00:56:00,610 --> 00:56:06,046
SPEAKER_0:  The President of the United States was going to announce this case, the arrest. Unfortunately, the government shut down two days before.

00:56:06,338 --> 00:56:12,350
SPEAKER_0:  So it was just us and that's really the only reason I had any publicity out of it is because the government shut down

00:56:12,738 --> 00:56:16,446
SPEAKER_0:  And the only thing that went public was that affidavit with my signature at the end.

00:56:17,058 --> 00:56:22,622
SPEAKER_0:  Otherwise it would have just been the attorney general and the president announcing the rest of this big thing.

00:56:23,394 --> 00:56:24,519
SPEAKER_0:  You wouldn't have seen me.

00:56:24,519 --> 00:56:26,019
SPEAKER_1:  Do you understand that this is a big case?

00:56:26,019 --> 00:56:28,030
SPEAKER_0:  Yeah, I knew it the mo- yeah, I knew it the time.

00:56:28,610 --> 00:56:30,942
SPEAKER_1:  Was it because of the scale of it or?

00:56:31,298 --> 00:56:32,190
SPEAKER_1:  what it stood for.

00:56:33,442 --> 00:56:46,430
SPEAKER_0:  I just knew that the public was going to react in a big way. Like the media was not, did I think that it was going to be on the front page of every newspaper the day after the arrest? No, but I could sense it. Like I went like three or four days without sleep.

00:56:46,658 --> 00:56:51,614
SPEAKER_0:  Um, when I was out in San Francisco to arrest Ross, I had sent three guys to Iceland to, um,

00:56:52,354 --> 00:56:55,646
SPEAKER_0:  So it was a three-prong approach for the takedown. It was get Ross.

00:56:55,938 --> 00:57:06,430
SPEAKER_0:  get the bitcoins and seize the site. Like we didn't want someone else taking control of the site. And we wanted that big splash of that banner. Like, look, the government found this site. Like you might not want to think about doing this again.

00:57:07,778 --> 00:57:14,622
SPEAKER_0:  So, and you were able to pull off all three? Maybe that's my superpower. I'm really good about putting smarter people on than I am.

00:57:14,946 --> 00:57:16,542
SPEAKER_0:  together on the right things.

00:57:16,770 --> 00:57:26,366
SPEAKER_0:  You know, I've done the only way to do it in the business I formed. That's what I did. I hired only smarter people than me. And I, you know, I'm not that smart, but you know, smart enough to know who the smart people are.

00:57:26,754 --> 00:57:28,222
SPEAKER_0:  The team was able to do all three.

00:57:28,578 --> 00:57:30,238
SPEAKER_0:  Yeah, we were able to get all three done.

00:57:30,530 --> 00:57:39,294
SPEAKER_0:  Um, yeah, and the one guy, one of the guys, the main guys I sent to Iceland, man, he was so smart. Like I sent another guy from the FBI to, uh,

00:57:40,418 --> 00:57:42,046
SPEAKER_0:  France to get that part.

00:57:42,562 --> 00:57:45,118
SPEAKER_0:  and he couldn't do it, so the guy in Iceland did it from.

00:57:45,346 --> 00:57:46,110
SPEAKER_0:  from Iceland.

00:57:46,370 --> 00:57:48,382
SPEAKER_0:  They had to pull some stuff out of memory on a computer.

00:57:48,802 --> 00:57:52,638
SPEAKER_0:  Um, you know, it's live process stuff. I'm sure you've done that before, but...

00:57:53,826 --> 00:58:01,310
SPEAKER_1:  Uh, I'm sure you did. Look, look what you're doing. You're, this is like a multi layer interrogation going on.

00:58:01,570 --> 00:58:02,142
SPEAKER_1:  Uh...

00:58:03,074 --> 00:58:06,449
SPEAKER_1:  Was there a concern that somebody else would step in and control the site?

00:58:06,449 --> 00:58:09,854
SPEAKER_0:  Absolutely. We didn't have insight on who exactly had control.

00:58:10,210 --> 00:58:13,374
SPEAKER_1:  So it turns out that Russ had like dictatorial control.

00:58:13,858 --> 00:58:17,054
SPEAKER_1:  So it wasn't easy to delegate to somebody else.

00:58:17,378 --> 00:58:23,934
SPEAKER_0:  He hadn't. I think he had some sort of ideas. I mean, his diary talked about walking away and giving it to somebody else, but he didn't.

00:58:24,354 --> 00:58:26,558
SPEAKER_0:  Uh, he couldn't give up that control to anybody, apparently.

00:58:28,098 --> 00:58:33,950
SPEAKER_1:  which makes you think that power corrupts and his ideals were not as strong as he espoused.

00:58:34,306 --> 00:58:38,366
SPEAKER_1:  about because if it was about the freedom of being able to buy drugs.

00:58:39,810 --> 00:58:40,926
SPEAKER_1:  if you want to.

00:58:41,602 --> 00:58:44,862
SPEAKER_1:  then he surely should have found ways to delegate that power.

00:58:45,346 --> 00:58:47,870
SPEAKER_0:  Well he changed over time, you can see it in his writings.

00:58:48,194 --> 00:58:49,854
SPEAKER_0:  Um, that he changed, like.

00:58:50,466 --> 00:58:50,846
SPEAKER_0:  So.

00:58:51,970 --> 00:58:55,614
SPEAKER_0:  People argue back and forth that there was never murders on Silk Road.

00:58:55,970 --> 00:58:58,750
SPEAKER_0:  When we were doing the investigation, to us there were six murders.

00:58:59,234 --> 00:59:00,318
SPEAKER_0:  Um, so.

00:59:01,122 --> 00:59:06,238
SPEAKER_0:  There was, the way we saw him at the time was, Ross ordered people to be murdered.

00:59:06,754 --> 00:59:11,806
SPEAKER_0:  Um, you know, somebody, people stole from him and all that. It was sort of an evolution from.

00:59:12,290 --> 00:59:16,830
SPEAKER_0:  Oh man, I can't deal with this. I can't do it. It's too much to the last one was like.

00:59:17,282 --> 00:59:20,158
SPEAKER_0:  The guy said, well, he's got three roommates.

00:59:20,514 --> 00:59:22,750
SPEAKER_0:  uh... it's like i will kill them too

00:59:23,010 --> 00:59:24,542
SPEAKER_1:  Was that ever proven in court?

00:59:24,770 --> 00:59:32,830
SPEAKER_0:  No, the murders never went forward because there was some, uh, some stuff problems in that case. So there was a separate case in Baltimore.

00:59:33,282 --> 00:59:41,566
SPEAKER_0:  that they had been working on for a lot longer. And so, you know, during the investigation, that caused a bunch of problems because now we have multiple federal agencies.

00:59:41,954 --> 00:59:43,454
SPEAKER_0:  case against the same thing.

00:59:43,778 --> 00:59:46,654
SPEAKER_1:  How do you decide not to push forward the...

00:59:46,978 --> 00:59:48,853
SPEAKER_1:  the murder investigations.

00:59:48,853 --> 00:59:52,734
SPEAKER_0:  There was a deconfliction meeting that happened in DC.

00:59:52,962 --> 00:59:56,094
SPEAKER_0:  I didn't happen to go to that meeting, but Jared went.

00:59:56,354 --> 00:59:57,758
SPEAKER_0:  This is Before I Ever Knew Jared.

00:59:58,050 --> 00:59:59,262
SPEAKER_0:  And, um...

00:59:59,714 --> 01:00:00,926
SPEAKER_0:  We have like, uh...

01:00:01,730 --> 01:00:10,430
SPEAKER_0:  where we can just sit in a room and sit in on the meeting. But it's all, you know, secure network and all that. So we can talk openly about secure things.

01:00:10,818 --> 01:00:11,998
SPEAKER_0:  Um, and.

01:00:12,258 --> 01:00:14,462
SPEAKER_0:  We sat in on the meeting and...

01:00:15,106 --> 01:00:17,982
SPEAKER_0:  People just kept saying the term sweat equity. I've got sweat equity.

01:00:18,210 --> 01:00:21,406
SPEAKER_0:  meaning that they had worked on the case for so long.

01:00:21,890 --> 01:00:22,910
SPEAKER_0:  that they deserve.

01:00:23,426 --> 01:00:24,414
SPEAKER_0:  to take them down.

01:00:25,122 --> 01:00:25,534
SPEAKER_0:  uh...

01:00:25,762 --> 01:00:26,078
SPEAKER_0:  and

01:00:26,434 --> 01:00:34,110
SPEAKER_0:  By this time, no one knew about us, but we told them at the meeting that we had found the server, and we have a copy of it, and we have the infrastructure.

01:00:34,626 --> 01:00:38,334
SPEAKER_0:  Um, and, and these guys had just had communications under covers.

01:00:38,562 --> 01:00:40,414
SPEAKER_0:  They didn't really know what was going on.

01:00:40,770 --> 01:00:44,734
SPEAKER_0:  And this wasn't my first deconfliction meeting. We had a huge deconfliction meeting during, um,

01:00:45,090 --> 01:00:46,215
SPEAKER_0:  during the anonymous case.

01:00:46,215 --> 01:00:47,646
SPEAKER_1:  What's the deconflection mean?

01:00:47,842 --> 01:00:51,998
SPEAKER_0:  agents within your agency or other, other federal agencies.

01:00:52,290 --> 01:00:57,854
SPEAKER_0:  have an investigation that if you exposed your case or took down your case would hurt their case.

01:00:58,082 --> 01:00:59,207
SPEAKER_0:  or the other day. we

01:00:59,207 --> 01:00:59,998
SPEAKER_1:  Have a...

01:01:00,450 --> 01:01:03,998
SPEAKER_1:  It's like the rival gangs meet at the table in a smoke filled room.

01:01:04,418 --> 01:01:05,543
SPEAKER_1:  And uh...

01:01:05,543 --> 01:01:07,230
SPEAKER_0:  bullets at the end, but yes.

01:01:08,098 --> 01:01:13,598
SPEAKER_1:  Oh boy, with the sweat equity. Yeah. I hate sweat. I mean, there's careers at stake, right? Yeah.

01:01:14,146 --> 01:01:15,271
SPEAKER_1:  You hate that, I...

01:01:15,271 --> 01:01:19,614
SPEAKER_0:  Yeah. I mean, why would you, why is that a stake just because you've worked on it long enough?

01:01:20,002 --> 01:01:27,422
SPEAKER_0:  longer than I have, that means you did better. Yeah. That's insane to me. That's rewarding bad behavior.

01:01:28,098 --> 01:01:32,158
SPEAKER_1:  And so that one of the part of the threat equity discussion was about murder and this was

01:01:32,450 --> 01:01:36,734
SPEAKER_1:  Here's a chance to actually bust them, given the data you have from Iceland and all that kind of stuff.

01:01:37,154 --> 01:01:37,982
SPEAKER_1:  So why...

01:01:38,626 --> 01:01:40,798
SPEAKER_0:  they wanted us just to turn the data over to them.

01:01:41,090 --> 01:01:47,966
SPEAKER_0:  to them. Yeah, thanks. Thanks for getting us this far. Here it is. I mean, it came to the point where they sent us like they

01:01:48,322 --> 01:01:50,398
SPEAKER_0:  had a picture of what they thought Ross was.

01:01:50,914 --> 01:01:51,902
SPEAKER_0:  And it was an internet meme.

01:01:52,514 --> 01:01:55,230
SPEAKER_0:  It really was a meme, it was a photo that we could look up.

01:01:55,938 --> 01:01:57,502
SPEAKER_0:  Like, it was insane.

01:01:58,018 --> 01:02:04,094
SPEAKER_1:  All right, so there's different degrees of competence all across the world between different people, yes.

01:02:04,578 --> 01:02:05,086
SPEAKER_1:  Okay.

01:02:05,474 --> 01:02:06,014
SPEAKER_1:  Uh...

01:02:06,338 --> 01:02:07,934
SPEAKER_1:  Does part of you regret?

01:02:08,322 --> 01:02:09,150
SPEAKER_1:  because.

01:02:09,474 --> 01:02:11,349
SPEAKER_1:  You push forward the hair.

01:02:11,349 --> 01:02:12,099
SPEAKER_0:  when the drug

01:02:12,099 --> 01:02:12,638
SPEAKER_1:  trade.

01:02:12,994 --> 01:02:14,654
SPEAKER_1:  We never got to the murder.

01:02:14,978 --> 01:02:15,582
SPEAKER_0:  discussion.

01:02:15,810 --> 01:02:35,998
SPEAKER_0:  I mean, the only regret in is that the internet doesn't seem to understand, like they just kind of blow that part off, that he literally paid people to have people murdered. It didn't result in a murder. And I thank God no one resulted in a murder. But that's where his mind was. His mind and where he wrote in his diary was that I had people killed and here's the money, he paid it. He paid a large amount of bitcoins.

01:02:36,482 --> 01:02:37,086
SPEAKER_0:  Uh...

01:02:37,314 --> 01:02:38,270
SPEAKER_0:  for that murder.

01:02:38,466 --> 01:02:39,870
SPEAKER_1:  So he didn't just even...

01:02:40,226 --> 01:02:42,142
SPEAKER_1:  think about it he actually took action.

01:02:42,562 --> 01:02:44,437
SPEAKER_1:  But the murders never happened. He took action by-

01:02:44,437 --> 01:02:45,150
SPEAKER_0:  paying the money.

01:02:45,378 --> 01:02:48,638
SPEAKER_0:  and the people came back with results. He thought they were murdered.

01:02:50,018 --> 01:02:50,686
SPEAKER_1:  That said...

01:02:51,426 --> 01:02:51,934
SPEAKER_1:  Can you?

01:02:52,290 --> 01:02:54,174
SPEAKER_1:  Understand the steel man the case

01:02:54,434 --> 01:02:55,966
SPEAKER_1:  for the drug trade on Silk Road.

01:02:56,770 --> 01:03:00,094
SPEAKER_1:  Like, can you make the case that it's a net positive for society?

01:03:01,634 --> 01:03:05,278
SPEAKER_0:  So there was a time period of when we found out the infrastructure.

01:03:05,890 --> 01:03:07,806
SPEAKER_0:  and when we built the case against Ross.

01:03:08,546 --> 01:03:09,598
SPEAKER_0:  I don't remember exactly.

01:03:10,722 --> 01:03:13,566
SPEAKER_0:  six weeks, a month, two months, I don't know, somewhere in there.

01:03:14,178 --> 01:03:17,982
SPEAKER_0:  Um, but then at Ross's sentencing, there was a father that stood up.

01:03:18,530 --> 01:03:20,734
SPEAKER_0:  and talked about his son dying.

01:03:21,218 --> 01:03:24,894
SPEAKER_0:  And I went back and kind of did the math and it was between those time periods of when we knew

01:03:25,314 --> 01:03:27,358
SPEAKER_0:  We could shut it down. We could have pulled the plug on the server and gone.

01:03:27,938 --> 01:03:29,278
SPEAKER_0:  and when Ross was arrested.

01:03:29,954 --> 01:03:32,030
SPEAKER_0:  His son died from buying drugs on Silk Road.

01:03:32,674 --> 01:03:33,438
SPEAKER_0:  and

01:03:34,722 --> 01:03:36,638
SPEAKER_0:  I still think about that father a lot.

01:03:37,410 --> 01:03:40,286
SPEAKER_1:  but if you look at scale at the war on drugs.

01:03:41,058 --> 01:03:43,198
SPEAKER_1:  Let's just even outside of Silk Road.

01:03:43,714 --> 01:03:45,214
SPEAKER_1:  Do you think the war on drugs?

01:03:45,954 --> 01:03:47,358
SPEAKER_1:  by the United States.

01:03:48,066 --> 01:03:49,214
SPEAKER_1:  has caused.

01:03:50,338 --> 01:03:53,534
SPEAKER_1:  has alleviated more suffering or caused more suffering in the world.

01:03:56,002 --> 01:04:10,302
SPEAKER_0:  That might be above my pay scale. I mean, I understand the other side of the argument. I mean, people said that I don't have to go down to the corner to buy drugs. I'm not going to get shot on the corner buying drugs or something. I can just have them sent to my house. People are going to do drugs anyways. I understand that argument.

01:04:11,106 --> 01:04:11,646
SPEAKER_0:  Um...

01:04:12,162 --> 01:04:17,787
SPEAKER_0:  from my personal standpoint, if I made it more difficult for my children to get drugs, that I'm satisfied.

01:04:17,787 --> 01:04:19,198
SPEAKER_1:  personal philosophy.

01:04:19,714 --> 01:04:23,102
SPEAKER_1:  is that if we legalize all drugs, including heroin and cocaine...

01:04:23,874 --> 01:04:25,566
SPEAKER_1:  that would not make for a better world.

01:04:28,258 --> 01:04:32,871
SPEAKER_0:  I don't know personally I don't believe legalizing all drugs would make make for a better world.

01:04:32,871 --> 01:04:33,374
SPEAKER_1:  Imagine.

01:04:35,362 --> 01:04:36,094
SPEAKER_1:  that it would.

01:04:36,866 --> 01:04:37,991
SPEAKER_1:  Do you understand that?

01:04:37,991 --> 01:04:45,886
SPEAKER_0:  I mean, as I've gotten older, I've started to, I like to see both sides of an argument. And when I can't see the other side, that's when I really like to dive into it.

01:04:46,146 --> 01:04:49,278
SPEAKER_0:  And I can see the other side. I can see why people would say that.

01:04:49,858 --> 01:04:50,366
SPEAKER_0:  Um...

01:04:51,330 --> 01:04:52,350
SPEAKER_0:  I don't wanna be a-

01:04:52,642 --> 01:04:57,374
SPEAKER_0:  my group raised children in a world where drugs are just free for use.

01:04:57,506 --> 01:05:00,446
SPEAKER_1:  Well, and then the other side of it is, Was Still Crude!!

01:05:01,474 --> 01:05:02,430
SPEAKER_1:  Did, uh...

01:05:03,650 --> 01:05:10,782
SPEAKER_1:  you know, taking down Silk Road, did that increase or decrease the number of drug trading criminals in the world?

01:05:11,330 --> 01:05:11,966
SPEAKER_1:  unclear.

01:05:12,258 --> 01:05:14,110
SPEAKER_0:  Online I think it increased.

01:05:14,402 --> 01:05:19,518
SPEAKER_0:  I think, you know, that is one of the things I think about a lot with Silk Road was that.

01:05:19,778 --> 01:05:23,550
SPEAKER_0:  No one really knew. I mean, there was thousands of users.

01:05:24,098 --> 01:05:39,294
SPEAKER_0:  But then after that, it was on the front page of the paper and there was millions of people that knew about Tor and Onion Sites. It was an advertisement. I thought crypto was gonna crash right after that. Like, oh, well people will now see that bad people are doing bad things with crypto. That'll crash more.

01:05:39,618 --> 01:05:42,270
SPEAKER_0:  I'm obviously wrong on that one. Uh, and I thought.

01:05:42,978 --> 01:05:47,518
SPEAKER_0:  You know, Ross was sentenced to two life sentences plus 40 years. No one's going to start up these b-

01:05:48,194 --> 01:05:49,886
SPEAKER_0:  stock markets exploded after that.

01:05:50,690 --> 01:05:58,910
SPEAKER_0:  You know, some of them started as, you know, opportunistic. I'm going to, you know, take those escrow accounts and I'm going to steal all the money that came in. You know, they were over that.

01:05:59,138 --> 01:06:04,126
SPEAKER_0:  But there were a lot of dark markets that popped up after that. Now we put the playbook out there.

01:06:05,250 --> 01:06:05,790
SPEAKER_1:  Yeah.

01:06:06,594 --> 01:06:08,926
SPEAKER_1:  Yeah, but and also there's a case for.

01:06:09,538 --> 01:06:11,070
SPEAKER_1:  Do you ever think about?

01:06:11,682 --> 01:06:12,830
SPEAKER_1:  not taking down.

01:06:13,538 --> 01:06:14,430
SPEAKER_1:  If you've not...

01:06:15,074 --> 01:06:18,238
SPEAKER_1:  taking dollar silk road, you could use it because it's a market.

01:06:19,170 --> 01:06:23,198
SPEAKER_1:  It itself is not necessarily the primary criminal organization.

01:06:23,522 --> 01:06:24,894
SPEAKER_1:  It's a market for criminals.

01:06:25,282 --> 01:06:26,398
SPEAKER_1:  so it could be used.

01:06:26,626 --> 01:06:27,518
SPEAKER_1:  to track down.

01:06:27,778 --> 01:06:29,438
SPEAKER_1:  criminals in the physical world.

01:06:30,114 --> 01:06:31,326
SPEAKER_1:  So if you don't take it down...

01:06:31,842 --> 01:06:33,054
SPEAKER_1:  given that it was.

01:06:33,570 --> 01:06:34,622
SPEAKER_1:  You know, the central.

01:06:35,106 --> 01:06:36,510
SPEAKER_1:  how centralized it was.

01:06:36,738 --> 01:06:39,646
SPEAKER_1:  It could be used as a place to find criminals.

01:06:40,066 --> 01:06:42,691
SPEAKER_0:  Right? So the dealers, the drug dealers, take you down to the drug dealers. US.

01:06:42,691 --> 01:06:43,102
SPEAKER_1:  Yeah.

01:06:43,490 --> 01:06:47,646
SPEAKER_1:  So if you have the cartel start getting involved, you go after the dealers.

01:06:48,226 --> 01:06:49,566
SPEAKER_0:  It would have been very difficult.

01:06:50,274 --> 01:06:53,182
SPEAKER_0:  Because of Tor and all that. Because of all the production's anonymity.

01:06:53,698 --> 01:06:55,262
SPEAKER_0:  Decloking all that would have been.

01:06:56,194 --> 01:06:57,214
SPEAKER_0:  drastically.

01:06:57,538 --> 01:07:02,686
SPEAKER_0:  more difficult and a lot of people in upper management of the FBI didn't have the appetite of running.

01:07:03,490 --> 01:07:06,430
SPEAKER_0:  Something like that. That would have been the FBI running a drug market.

01:07:06,946 --> 01:07:12,542
SPEAKER_0:  How many kids, how many fathers would have to come in and said, my kid bought while the FBI was running a site?

01:07:12,866 --> 01:07:14,558
SPEAKER_0:  A drug site. My kid died.

01:07:14,850 --> 01:07:19,742
SPEAKER_0:  So I didn't know anybody in the FBI in management would have the appetite to let us run.

01:07:20,386 --> 01:07:21,566
SPEAKER_0:  what was happening on Silk Road.

01:07:22,018 --> 01:07:25,822
SPEAKER_0:  Um, you know, cause remember that time we're still believing six people are dead.

01:07:26,306 --> 01:07:28,862
SPEAKER_0:  We're still investigating, you know, where are all these bodies?

01:07:29,250 --> 01:07:30,366
SPEAKER_0:  Um, you know.

01:07:30,658 --> 01:07:35,454
SPEAKER_0:  That's pretty much why we took down Ross when we did. I mean, we had to jump on it fast.

01:07:37,666 --> 01:07:41,886
SPEAKER_1:  What else can you say about this complicated world that has grown of the dark web?

01:07:44,450 --> 01:07:49,790
SPEAKER_0:  I don't understand it. I like it would have been a something for me. I thought I thought it was going to collapse, but.

01:07:50,626 --> 01:07:58,501
SPEAKER_0:  I mean, it's just gotten bigger in what's going out there. Now, I'm really surprised that it hasn't grown into other networks or people haven't developed other networks.

01:07:58,501 --> 01:08:00,001
SPEAKER_1:  But but- stocking

01:08:00,001 --> 01:08:06,622
SPEAKER_0:  Yeah, Tor's still the main one out there. I mean, there's a few others, and I'm not going to put an advertisement out for them.

01:08:06,882 --> 01:08:10,366
SPEAKER_0:  But, you know, I thought that market would have grown.

01:08:10,466 --> 01:08:15,006
SPEAKER_1:  Yeah. My sense was when I interacted with Tori, it was that there's huge usability issues.

01:08:15,330 --> 01:08:17,022
SPEAKER_1:  But that's for like legal activity.

01:08:17,314 --> 01:08:19,486
SPEAKER_1:  Yeah. Cause like if you care about privacy, it's just.

01:08:20,162 --> 01:08:22,014
SPEAKER_1:  not as good of a browser.

01:08:22,466 --> 01:08:22,974
SPEAKER_1:  like a

01:08:23,234 --> 01:08:25,726
SPEAKER_1:  to look at stuff.

01:08:25,890 --> 01:08:29,534
SPEAKER_0:  No, it's way too slow. It's way too slow. I mean, you can't even like.

01:08:29,954 --> 01:08:37,054
SPEAKER_0:  I know some people would use it to like view movies like Netflix, you can only view certain movies in certain countries, you can use it for that, but it's too slow even for that, so unfortunately no...

01:08:37,922 --> 01:08:42,526
SPEAKER_1:  Were you ever able to hold in your mind the landscape of the dark web? Like what?

01:08:42,818 --> 01:08:43,966
SPEAKER_1:  What's going on out there?

01:08:44,354 --> 01:08:46,238
SPEAKER_1:  It's such a, to me as a human being.

01:08:46,754 --> 01:08:47,966
SPEAKER_1:  It's just difficult.

01:08:48,866 --> 01:08:51,006
SPEAKER_1:  to understand the digital world.

01:08:51,330 --> 01:08:53,342
SPEAKER_1:  like these anonymous usernames.

01:08:54,626 --> 01:08:57,918
SPEAKER_1:  like doing anonymous activity. It's hard to, um.

01:08:58,978 --> 01:09:04,222
SPEAKER_1:  What am I trying to say? It's hard to visualize it in the way I can visualize it. I've been reading a lot about Hitler.

01:09:04,674 --> 01:09:06,910
SPEAKER_1:  I can visualize meetings between people.

01:09:07,170 --> 01:09:08,670
SPEAKER_1:  military strategy.

01:09:08,898 --> 01:09:16,382
SPEAKER_1:  uh... deciding on in the uh... certain evil atrocities all that kind of stuff i can visualize the people there's agreements

01:09:16,706 --> 01:09:17,470
SPEAKER_1:  hands.

01:09:17,986 --> 01:09:18,878
SPEAKER_1:  handshakes.

01:09:19,490 --> 01:09:20,670
SPEAKER_1:  P- stuff signed.

01:09:21,026 --> 01:09:28,222
SPEAKER_1:  groups built like in the digital space, like with bots, with anonymity, anyone human can be multiple people.

01:09:28,450 --> 01:09:29,470
SPEAKER_1:  Uh, it's just-

01:09:29,634 --> 01:09:40,190
SPEAKER_0:  Yeah, it's all lies. It's all lies. Like, yeah, it feels like I can't trust anything. No, you can't. You honestly can't. And like, you can talk to two different people and it's the same person. Like, there's so many different, you know.

01:09:40,418 --> 01:09:42,398
SPEAKER_0:  Hector had so many different identities on life.

01:09:42,658 --> 01:09:43,358
SPEAKER_0:  That, you know.

01:09:43,746 --> 01:09:48,894
SPEAKER_0:  of things that, you know, the lies to each other. I mean, he lied to people inside his group.

01:09:49,186 --> 01:09:54,782
SPEAKER_0:  Uh, just to use another name to spy on, make sure what they're, you know, we're talking shit behind his back or weren't doing anything.

01:09:55,266 --> 01:09:58,974
SPEAKER_0:  Um, it's all lies and people that can keep all those lies straight.

01:09:59,266 --> 01:10:00,158
SPEAKER_0:  It's unbelievable to me.

01:10:00,898 --> 01:10:05,374
SPEAKER_1:  Ross Albrecht represents the very early days of that. That's why the competence wasn't there.

01:10:06,754 --> 01:10:08,542
SPEAKER_1:  Imagine how good the people are now.

01:10:09,058 --> 01:10:09,982
SPEAKER_1:  the kids that grow up.

01:10:10,530 --> 01:10:12,478
SPEAKER_0:  Oh, they've learned from his mistakes.

01:10:14,402 --> 01:10:18,590
SPEAKER_1:  just the extreme competence. You just see how good people are at video games, Break the level..

01:10:19,074 --> 01:10:19,518
SPEAKER_1:  of.

01:10:19,810 --> 01:10:20,158
SPEAKER_1:  Uh

01:10:20,386 --> 01:10:20,862
SPEAKER_1:  play.

01:10:21,218 --> 01:10:22,366
SPEAKER_1:  In terms of video games.

01:10:22,754 --> 01:10:24,574
SPEAKER_1:  Okay, I used to think I sucked.

01:10:24,898 --> 01:10:26,270
SPEAKER_1:  And now I'm not even.

01:10:26,914 --> 01:10:27,326
SPEAKER_1:  like.

01:10:27,714 --> 01:10:28,798
SPEAKER_1:  I'm not even...

01:10:29,314 --> 01:10:36,606
SPEAKER_1:  in the consideration of calling myself shitty at video games. I'm not even, I'm like non-existent. I'm like, uh...

01:10:36,770 --> 01:10:45,395
SPEAKER_0:  The mold. Yeah, I stopped playing because it's so embarrassing. It's embarrassing. It's like wrestling with your kid and he finally beats you. He's like, well, fuck that. I'm not wrestling with my kid any ever again.

01:10:45,395 --> 01:10:46,942
SPEAKER_1:  and in some sense hacking.

01:10:47,330 --> 01:10:49,886
SPEAKER_1:  uh... at its best and its worst is a kind of game

01:10:50,370 --> 01:10:52,478
SPEAKER_1:  and you can get exceptionally good at that kind of game.

01:10:53,442 --> 01:10:54,686
SPEAKER_1:  and you get the...

01:10:54,786 --> 01:10:59,902
SPEAKER_0:  I mean, there's power that comes along if you have success.

01:11:00,162 --> 01:11:06,238
SPEAKER_0:  Look at the kid that was hacking into Uber and Rockstar Games. He put it out there that he was doing it. I mean, he used the name.

01:11:06,498 --> 01:11:06,910
SPEAKER_0:  Um...

01:11:07,170 --> 01:11:10,910
SPEAKER_0:  whatever hacked into Uber was his screen name. He's very proud of it.

01:11:11,202 --> 01:11:12,862
SPEAKER_0:  one building evidence against himself.

01:11:13,186 --> 01:11:18,174
SPEAKER_0:  But you know, they wanted that slap on the back. Like, look what a great hacker you are.

01:11:19,522 --> 01:11:21,086
SPEAKER_1:  What do you think is in the mind of that?

01:11:21,858 --> 01:11:26,558
SPEAKER_1:  Guy, what do you think is in the mind of Russ? Do you think they see themselves as good people?

01:11:27,650 --> 01:11:32,423
SPEAKER_1:  Do you think they acknowledge the bad they're doing on?

01:11:32,423 --> 01:11:32,926
SPEAKER_0:  the world.

01:11:33,826 --> 01:11:39,134
SPEAKER_0:  So that Uber hacker, I think that's just you not realizing what consequences are based on his actions.

01:11:39,490 --> 01:11:43,262
SPEAKER_0:  Ross was a little bit older. Um, I think I'd Ross truly.

01:11:44,066 --> 01:11:49,566
SPEAKER_0:  is a libertarian. He truly had his beliefs that he could provide.

01:11:50,402 --> 01:11:54,846
SPEAKER_0:  the gateway for other people to live that libertarian lifestyle and put in their body what they want.

01:11:55,074 --> 01:11:57,726
SPEAKER_0:  I don't think that was a front or a lie.

01:11:58,306 --> 01:12:01,758
SPEAKER_1:  What's the difference between DPR and Rossi said like.

01:12:01,986 --> 01:12:02,686
SPEAKER_1:  I have never...

01:12:03,106 --> 01:12:10,558
SPEAKER_1:  Matt Ross until I have only had those two two days of worth of interaction. Yeah, it's just interesting given how long you've chased him.

01:12:10,946 --> 01:12:13,566
SPEAKER_1:  and then having met him, what was the difference to you as a human being?

01:12:14,082 --> 01:12:19,486
SPEAKER_0:  He was a human being. He was an actual person. He was nervous when we arrested him.

01:12:19,906 --> 01:12:22,046
SPEAKER_0:  Um, so one of the things that, that.

01:12:22,370 --> 01:12:30,526
SPEAKER_0:  I learned through my law enforcement career is if I'm going to be the case agent, I'm going to be the one in charge of, you know, dealing with this person. I'm not putting handcuffs on him.

01:12:30,914 --> 01:12:31,806
SPEAKER_0:  Something else is gonna do that.

01:12:32,290 --> 01:12:33,502
SPEAKER_0:  Like I'm going to be there to help him.

01:12:34,018 --> 01:12:43,006
SPEAKER_0:  uh, you know, I'm your conduit to help. And so, you know, right after someone's arrested, you obviously have had them down for weapons to make sure for everybody's safety, but then I just put my hand on their chest.

01:12:43,522 --> 01:12:49,406
SPEAKER_0:  just feel their heart, feel their breathing. I'm sure it's the scariest day, but then to have that.

01:12:50,338 --> 01:12:51,358
SPEAKER_0:  human contact.

01:12:51,778 --> 01:12:58,174
SPEAKER_0:  kind of settles people down and you kind of like, let's start thinking about this. I'm gonna tell you, you know, I'm gonna be open and honest with you.

01:12:58,434 --> 01:13:04,030
SPEAKER_0:  You know, there's a lot of cops out there and federal agents cops that just go to the hard ass tactic.

01:13:04,802 --> 01:13:05,950
SPEAKER_0:  You don't get very far with that.

01:13:06,370 --> 01:13:09,278
SPEAKER_0:  You don't get very far being a mean asshole to somebody, you know?

01:13:09,890 --> 01:13:13,310
SPEAKER_0:  be compassionate, be human, and it's going to go a lot further.

01:13:13,794 --> 01:13:16,222
SPEAKER_1:  So given everything he's done, you're still able to have.

01:13:16,770 --> 01:13:17,438
SPEAKER_1:  passion for him.

01:13:18,274 --> 01:13:27,582
SPEAKER_0:  We took him to the jail and we, so he, it was after hours. So he didn't get to see a judge that day. So you stick, we stuck him in the San Francisco jail. Um.

01:13:28,002 --> 01:13:30,814
SPEAKER_0:  I hadn't slept for about four days because I was dealing with the-

01:13:31,650 --> 01:13:32,446
SPEAKER_0:  people in Iceland.

01:13:32,802 --> 01:13:36,926
SPEAKER_0:  bosses in DC, bosses in New York. So I, and I was in.

01:13:37,218 --> 01:13:38,270
SPEAKER_0:  San Francisco so.

01:13:38,722 --> 01:13:42,302
SPEAKER_0:  timeframe, like the Iceland people were calling me when I was supposed to be sleeping. It was insane.

01:13:43,426 --> 01:13:55,006
SPEAKER_0:  I still went out that night while Ross sat in jail and bought him breakfast. I said, what do you want for breakfast? I'll have a nice breakfast for you. Cause we picked him up in the morning and took him over to the FBI to do the, the FBI booking, the fingerprints and all that.

01:13:55,266 --> 01:14:01,599
SPEAKER_0:  And I got him breakfast. I mean, and you don't get paid back for that sort of thing. I'm not looking, but out of my own-%.

01:14:01,599 --> 01:14:03,099
SPEAKER_1:  special request for breakfast. yeah.

01:14:03,099 --> 01:14:04,599
SPEAKER_0:  He asked for certain things. it is

01:14:04,599 --> 01:14:06,849
SPEAKER_1:  Can you imagine there's that top secret FBI?

01:14:06,849 --> 01:14:11,166
SPEAKER_0:  top secret. I think he wanted some granola bars like and, and, and, you know.

01:14:11,906 --> 01:14:18,270
SPEAKER_0:  But he already had lawyered up, so we, you know, which is his right. He can do that. So I knew we were going to work together.

01:14:18,626 --> 01:14:20,062
SPEAKER_0:  you know, like I did with Hector.

01:14:20,354 --> 01:14:20,862
SPEAKER_0:  Um...

01:14:21,474 --> 01:14:24,807
SPEAKER_1:  I mean, this is the last day. Most of the conversations have to be.

01:14:24,807 --> 01:14:25,886
SPEAKER_0:  to be them with lawyers.

01:14:26,562 --> 01:14:28,222
SPEAKER_0:  From that point on, I can't question him.

01:14:28,514 --> 01:14:29,662
SPEAKER_0:  Yeah. When he asked for a lawyer.

01:14:30,018 --> 01:14:35,358
SPEAKER_0:  Um, or if I did, it couldn't be used against him. Um, so we just had conversation where I talked to him.

01:14:36,002 --> 01:14:44,798
SPEAKER_0:  You know, he could, you know, could say things to me, but then I would remind him that he asked for a lawyer and he'd have to waive that and all that. But we didn't talk about his case so much. We just talked about like human beings.

01:14:45,090 --> 01:14:46,206
SPEAKER_1:  Did he, uh...

01:14:47,042 --> 01:14:48,894
SPEAKER_1:  with his eyes, with his words.

01:14:49,250 --> 01:14:49,726
SPEAKER_1:  Um...

01:14:49,954 --> 01:14:51,038
SPEAKER_1:  Reveal any kind of

01:14:51,586 --> 01:14:52,286
SPEAKER_1:  Regret.

01:14:52,706 --> 01:14:53,150
SPEAKER_1:  or.

01:14:53,474 --> 01:14:55,070
SPEAKER_1:  Did you see a human being changing?

01:14:55,458 --> 01:14:58,142
SPEAKER_1:  understanding something about themselves and the process of being caught.

01:14:59,362 --> 01:15:10,878
SPEAKER_0:  No, I don't think that. I mean, he did offer me $20 million to let him go when we were driving to the jail. And I asked him what I was going to, we were going to do with the agent that sat in the front seat.

01:15:11,202 --> 01:15:12,542
SPEAKER_0:  The money really broke him, huh?

01:15:13,090 --> 01:15:17,054
SPEAKER_0:  I think so. I think he kind of got caught up in how much money it was.

01:15:17,602 --> 01:15:18,814
SPEAKER_0:  and how, you know.

01:15:19,266 --> 01:15:27,326
SPEAKER_0:  When crypto started, it was pennies. And by the time he got arrested, it was $120. And you know, 177,000 bitcoins, even today.

01:15:27,586 --> 01:15:29,854
SPEAKER_0:  You know, that's a lot of bitcoins.

01:15:29,954 --> 01:15:34,046
SPEAKER_1:  So you really could have been, if you continued to be one of the richest people in the world.

01:15:34,658 --> 01:15:41,086
SPEAKER_0:  I possibly could have been if I took that 20 million then. I could have been living, we could have this conversation in Venezuela.

01:15:41,538 --> 01:15:42,366
SPEAKER_1:  Yeah.

01:15:42,786 --> 01:15:43,911
SPEAKER_1:  in a castle in a

01:15:43,911 --> 01:15:44,606
SPEAKER_0:  Palace.

01:15:44,994 --> 01:15:48,222
SPEAKER_0:  Yeah, until it runs out and then the government storms the castle.

01:15:48,578 --> 01:15:49,022
SPEAKER_1:  Yeah.

01:15:50,658 --> 01:15:51,934
SPEAKER_0:  Have you talked to us since?

01:15:53,058 --> 01:15:56,574
SPEAKER_0:  No, I would, I'd be open to it. I don't think he probably wants to hear from me.

01:15:57,090 --> 01:15:59,262
SPEAKER_1:  Do you know where and in which prison he is?

01:16:00,482 --> 01:16:13,342
SPEAKER_0:  I think he's somewhere out in Arizona. I know he was in the one next to Supermax for a little while, like the high security one that shares the fence with Supermax, but I don't think he's there anymore. I think he's out in Arizona. I haven't seen in a while.

01:16:13,890 --> 01:16:15,486
SPEAKER_1:  I wonder if you can do interviews in prison.

01:16:15,874 --> 01:16:16,478
SPEAKER_1:  That'll be nice.

01:16:16,866 --> 01:16:23,838
SPEAKER_0:  Some people are allowed to, so I've not seen an interview with him. I know people have wanted to interview him about books and that sort of thing.

01:16:24,354 --> 01:16:28,158
SPEAKER_1:  Right, because the story really blew up. Did it surprise you how much the story?

01:16:28,866 --> 01:16:30,558
SPEAKER_1:  and many elements of it.

01:16:31,010 --> 01:16:31,518
SPEAKER_1:  BLUAT

01:16:31,746 --> 01:16:39,582
SPEAKER_0:  movies. It did surprise me. Like my wife's uncle who I didn't, I've been married my wife for 22 years now. I don't think he knew my name.

01:16:40,162 --> 01:16:44,222
SPEAKER_0:  And he was excited about that. He reached out when Silk Road came out. So...

01:16:44,610 --> 01:16:46,485
SPEAKER_0:  He, you know, that was surprising to see.

01:16:46,485 --> 01:16:49,278
SPEAKER_1:  I think the movie on the topic was good.

01:16:49,826 --> 01:16:59,747
SPEAKER_0:  I didn't have anything to do with that movie. I've watched it once. It was kind of cool that Jimmy Simpson, you know, was my name in the movie. But outside of that, I thought it sort of missed the mark on some things.

01:16:59,747 --> 01:17:03,774
SPEAKER_1:  would I don't think they understand what's interesting about

01:17:04,546 --> 01:17:09,438
SPEAKER_1:  these kinds of stories and there's a lot of things that are interesting and they missed all of them

01:17:09,794 --> 01:17:12,478
SPEAKER_1:  So for example, I recently talked to John Carmack.

01:17:13,026 --> 01:17:15,710
SPEAKER_1:  who's a world-class developer and so on.

01:17:16,098 --> 01:17:17,054
SPEAKER_1:  So Hollywood.

01:17:17,538 --> 01:17:21,118
SPEAKER_1:  would think that the interesting thing about John Carmack is some kind of like

01:17:21,826 --> 01:17:23,166
SPEAKER_1:  shitty

01:17:23,778 --> 01:17:24,126
SPEAKER_1:  like

01:17:24,482 --> 01:17:29,246
SPEAKER_1:  a parody of a hacker something like that they would show like really uh...

01:17:29,602 --> 01:17:30,686
SPEAKER_1:  Crappy.

01:17:30,978 --> 01:17:40,894
SPEAKER_1:  like emulation of some kind of Linux terminal thing. The reality is like the technical details for five hours with him, for 10 hours with him, is what people actually wanna see, even people that don't program.

01:17:41,282 --> 01:17:42,814
SPEAKER_1:  They want to see a brilliant mind.

01:17:43,362 --> 01:17:51,614
SPEAKER_1:  the details that they're not, even if they don't understand all the details, they want to have an inkling of the genius there. There's just one way I'm saying like.

01:17:52,354 --> 01:17:53,950
SPEAKER_1:  that you want to reveal.

01:17:54,690 --> 01:17:58,302
SPEAKER_1:  the genius, the complexity of that world in interesting ways.

01:17:58,562 --> 01:17:59,678
SPEAKER_1:  and to make a Hollywood.

01:18:00,002 --> 01:18:01,822
SPEAKER_1:  almost parody caricature of it.

01:18:02,082 --> 01:18:05,694
SPEAKER_1:  It just destroys the spirit of the thing. So one.

01:18:06,946 --> 01:18:09,022
SPEAKER_1:  The operation of BI is fascinating.

01:18:09,954 --> 01:18:13,534
SPEAKER_1:  just tracking down these people on the cyber security front. fascinating.

01:18:13,954 --> 01:18:16,030
SPEAKER_1:  The other is just how you've run.

01:18:16,770 --> 01:18:17,694
SPEAKER_1:  missing you.

01:18:17,986 --> 01:18:24,542
SPEAKER_1:  how you run this kind of organization, the trust issues of the different criminal entities involved, the anonymity.

01:18:24,866 --> 01:18:26,174
SPEAKER_1:  Uh, the...

01:18:26,786 --> 01:18:29,982
SPEAKER_1:  uh... the law hanging fruit of being shitty at certain parts

01:18:30,306 --> 01:18:33,214
SPEAKER_1:  on the technical front. All those are fascinating things.

01:18:33,442 --> 01:18:35,390
SPEAKER_1:  You know, that's that's what a movie should reveal.

01:18:35,618 --> 01:18:38,174
SPEAKER_1:  Should probably be a series, honestly, a Netflix series.

01:18:38,786 --> 01:18:42,142
SPEAKER_0:  Yeah, maybe one of those FX shows or something like that, kind of gritty, you know.

01:18:42,306 --> 01:18:50,686
SPEAKER_1:  Yeah, yeah, gritty, exactly, gritty. I mean, shows like Chernobyl from HBO made me realize, okay, you can do a good job of a difficult story.

01:18:51,138 --> 01:18:57,950
SPEAKER_1:  and reveal the human side, but also reveal the technical side, and have some deep, profound understanding on that case.

01:18:58,338 --> 01:19:00,190
SPEAKER_1:  and the bureaucracy of a...

01:19:00,674 --> 01:19:01,758
SPEAKER_1:  of a Soviet regime.

01:19:02,242 --> 01:19:05,662
SPEAKER_1:  In this case, you could review the bureaucracy, the chaos.

01:19:05,922 --> 01:19:07,454
SPEAKER_1:  of a criminal organization.

01:19:07,714 --> 01:19:11,839
SPEAKER_1:  of law enforcement organization. I mean, there's so much to explore. It's fascinating.

01:19:11,839 --> 01:19:12,222
SPEAKER_0:  Yeah.

01:19:12,546 --> 01:19:18,110
SPEAKER_0:  Yeah, I like Chernobyl. When I re-watch it, I can't watch episode 3 though. The, the animals sent, theâ€”

01:19:18,530 --> 01:19:19,006
SPEAKER_0:  episode.

01:19:19,298 --> 01:19:20,926
SPEAKER_0:  They go around shooting all the dogs and all that.

01:19:21,154 --> 01:19:26,398
SPEAKER_0:  I got to skip that part. You're a big softie, aren't you? I really am. Yeah. Sure, I'll probably cry at some point.

01:19:26,754 --> 01:19:33,182
SPEAKER_0:  I love it. I love it. Listen, don't get me talking about that episode you made about your grandmother. Oh my god.

01:19:33,506 --> 01:19:34,078
SPEAKER_0:  That was rough.

01:19:34,658 --> 01:19:36,030
SPEAKER_1:  just to linger on this.

01:19:36,482 --> 01:19:43,134
SPEAKER_1:  ethical versus legal question. What do you think about people like Aaron Schwartz? I don't know if you're familiar with him, but he was somebody.

01:19:44,738 --> 01:19:48,574
SPEAKER_1:  Uh, who broke the law in the name of an ethical ideal.

01:19:48,994 --> 01:19:51,262
SPEAKER_1:  downloaded and released.

01:19:53,346 --> 01:19:56,446
SPEAKER_1:  academic publications that were behind a paywall.

01:19:57,634 --> 01:19:58,910
SPEAKER_1:  and um...

01:19:59,298 --> 01:20:00,254
SPEAKER_1:  He was...

01:20:00,674 --> 01:20:03,358
SPEAKER_1:  arrested for that and then committed suicide.

01:20:03,682 --> 01:20:07,006
SPEAKER_1:  And a lot of people see him, certainly in the MIT community. But...

01:20:07,554 --> 01:20:09,374
SPEAKER_1:  Throughout the world as a hero

01:20:10,530 --> 01:20:11,390
SPEAKER_1:  because...

01:20:12,386 --> 01:20:14,110
SPEAKER_1:  You look at the way.

01:20:14,658 --> 01:20:15,646
SPEAKER_1:  Knowledge.

01:20:16,034 --> 01:20:20,542
SPEAKER_1:  scientific knowledge is being put behind paywalls, it does seem somehow unethical.

01:20:21,602 --> 01:20:23,486
SPEAKER_1:  and he basically...

01:20:24,098 --> 01:20:25,598
SPEAKER_1:  broke the law.

01:20:26,498 --> 01:20:28,478
SPEAKER_1:  to do the ethical thing.

01:20:29,378 --> 01:20:34,654
SPEAKER_1:  Now you could challenge it maybe it is unethical, but there's a gray area and

01:20:35,170 --> 01:20:36,126
SPEAKER_1:  To me at least.

01:20:36,642 --> 01:20:39,102
SPEAKER_1:  It is ethical to me at least. He is a hero.

01:20:39,458 --> 01:20:40,190
SPEAKER_1:  Because I'm...

01:20:40,514 --> 01:20:41,694
SPEAKER_1:  familiar with.

01:20:42,210 --> 01:20:43,038
SPEAKER_1:  the paywall.

01:20:43,618 --> 01:20:44,478
SPEAKER_1:  created by...

01:20:46,050 --> 01:20:47,646
SPEAKER_1:  the institutions that hold.

01:20:47,970 --> 01:20:48,990
SPEAKER_1:  these publications.

01:20:49,314 --> 01:20:51,262
SPEAKER_1:  they're adding very little value.

01:20:51,874 --> 01:20:52,862
SPEAKER_1:  So it is.

01:20:53,154 --> 01:20:53,950
SPEAKER_1:  basically.

01:20:54,210 --> 01:20:57,790
SPEAKER_1:  holding hostage the work of millions of brilliant scientists.

01:20:58,370 --> 01:20:58,910
SPEAKER_1:  Um...

01:20:59,778 --> 01:21:01,246
SPEAKER_1:  for some kind of

01:21:01,762 --> 01:21:02,398
SPEAKER_1:  Honestly, I-

01:21:03,106 --> 01:21:09,342
SPEAKER_1:  a crappy capitalist institution. Like they're not actually making that much money. It doesn't make any sense to me. It should.

01:21:09,570 --> 01:21:11,902
SPEAKER_1:  And to me, it should all be open.

01:21:12,354 --> 01:21:13,534
SPEAKER_1:  public access.

01:21:14,082 --> 01:21:18,590
SPEAKER_1:  There's no reason it shouldn't be. All publications should be. So he stood for that ideal.

01:21:19,298 --> 01:21:20,478
SPEAKER_1:  and um

01:21:21,282 --> 01:21:22,782
SPEAKER_1:  and was punished harshly for it.

01:21:23,074 --> 01:21:24,958
SPEAKER_1:  That's the other criticism, it's too harshly.

01:21:25,442 --> 01:21:26,558
SPEAKER_1:  And of course...

01:21:27,490 --> 01:21:31,358
SPEAKER_1:  uh... deeply unfortunately that also led to suicide

01:21:31,682 --> 01:21:39,710
SPEAKER_1:  because he was also tormented on many levels. I mean, are you familiar with him? What do you think about that line between what is legal and what is ethical?

01:21:41,634 --> 01:21:45,854
SPEAKER_0:  So it's tough, it's a tough case. I mean, the outcome was tragic, obviously.

01:21:46,370 --> 01:21:50,558
SPEAKER_0:  Unfortunately, when you're in law enforcement.

01:21:51,074 --> 01:22:02,206
SPEAKER_0:  You have to, your job is to enforce the laws. I mean, it's not, if you're told that you have to do a certain case, you know, and there is a violation of at the time, you know, 18 USC 10, 10 30.

01:22:02,914 --> 01:22:03,646
SPEAKER_0:  hacking.

01:22:04,130 --> 01:22:04,606
SPEAKER_0:  Um...

01:22:06,018 --> 01:22:14,846
SPEAKER_0:  You have to press forward with that. I mean, you have to charge, you bring the case to the U.S. Attorney's Office and whether they're gonna press charges or not, you know. You can't.

01:22:15,554 --> 01:22:17,086
SPEAKER_0:  You can't really pick and choose what you-

01:22:17,378 --> 01:22:25,822
SPEAKER_0:  press and don't press for it. I never felt that at least that flexibility and on the FBI. I mean, maybe when you're a street cop and you pull somebody over, you can let them go with a warning.

01:22:26,466 --> 01:22:28,958
SPEAKER_1:  So in FBI you're sitting in a room, but you're also...

01:22:29,410 --> 01:22:32,382
SPEAKER_1:  You're also a human being, you have compassion, you're arrested.

01:22:33,026 --> 01:22:33,726
SPEAKER_1:  Ross

01:22:34,178 --> 01:22:37,022
SPEAKER_1:  the hand on the chest, I mean, that's a human thing.

01:22:38,082 --> 01:22:39,134
SPEAKER_1:  So there's a...

01:22:39,490 --> 01:22:43,390
SPEAKER_0:  But I can't be the jury for whether it was a good hack or a bad hack.

01:22:44,482 --> 01:22:53,150
SPEAKER_0:  It's all someone, a victim has come forward and said, we're the victim of this. And I agree with you, because again, the basis of the internet was to share academic thought.

01:22:53,378 --> 01:22:55,198
SPEAKER_0:  I mean that's where the internet was born.

01:22:55,778 --> 01:22:57,758
SPEAKER_1:  But it's not, it's not up to you.

01:22:58,306 --> 01:23:00,094
SPEAKER_1:  to the role of the FBI.

01:23:00,802 --> 01:23:01,758
SPEAKER_1:  It's enforced the law.

01:23:02,082 --> 01:23:02,494
SPEAKER_0:  Correct.

01:23:05,410 --> 01:23:08,254
SPEAKER_0:  And there's a limited number of tools.

01:23:08,578 --> 01:23:10,718
SPEAKER_0:  on our Batman belt that we can use.

01:23:11,458 --> 01:23:17,118
SPEAKER_0:  Um, you know, not to get into all the aspects of the Trump case and Mar-a-Lago and the documents there, I mean...

01:23:17,602 --> 01:23:28,478
SPEAKER_0:  The FBI has so many tools they can use and a search warrant is the only way they could get in there. I mean, that's it. No other legal document or legal way to enter and get those documents.

01:23:28,674 --> 01:23:30,078
SPEAKER_1:  What do you think about the...

01:23:31,522 --> 01:23:35,390
SPEAKER_1:  the FBI and Mar-a-Lago and the FBI taking the documents for Donald Trump.

01:23:36,194 --> 01:23:41,086
SPEAKER_0:  You know, it's a tough spot. I it's a really tough spot. The FBI has gotten a lot of black eyes.

01:23:41,442 --> 01:23:42,462
SPEAKER_0:  you know, recently.

01:23:42,850 --> 01:23:47,230
SPEAKER_0:  Um, and I don't know if it's the same FBI that I remember when I was there.

01:23:47,810 --> 01:23:49,374
SPEAKER_1:  Do you think they deserve it in part?

01:23:51,106 --> 01:23:56,414
SPEAKER_1:  Was it done clumsily? The rating of the former president's residence?

01:23:57,218 --> 01:23:58,526
SPEAKER_1:  Yeah

01:23:59,970 --> 01:24:07,806
SPEAKER_0:  It's tough. It's tough, you know, because again, they're only limited to what they're allowed, what they're legally allowed to do. And a search warrant is the only legal way of doing it.

01:24:08,482 --> 01:24:12,382
SPEAKER_0:  I have my personal and political views on certain things.

01:24:12,610 --> 01:24:16,894
SPEAKER_0:  you know, and I think it might be surprising to some where those political

01:24:18,530 --> 01:24:19,486
SPEAKER_0:  point stand.

01:24:19,650 --> 01:24:24,525
SPEAKER_1:  But you told me offline that you're a hardcore communist. That was very strange, very surprising.

01:24:24,525 --> 01:24:29,251
SPEAKER_0:  to me. Well that's only you who tried to bring me into the Communist Party. Exactly, I was trying to recruit.

01:24:29,251 --> 01:24:31,102
SPEAKER_1:  It's giving you all kinds of flyers.

01:24:31,746 --> 01:24:33,630
SPEAKER_1:  Um...

01:24:34,146 --> 01:24:35,742
SPEAKER_1:  Okay, but...

01:24:36,130 --> 01:24:43,390
SPEAKER_1:  You said like, you know, people in FBI just following the law, but there's a chain of command and so on. What do you think about the conspiracy theories that people...

01:24:43,842 --> 01:24:46,494
SPEAKER_1:  some small number of people.

01:24:46,786 --> 01:24:49,886
SPEAKER_1:  inside the FBI conspired to undermine the presidency.

01:24:50,306 --> 01:24:51,358
SPEAKER_1:  of Donald Trump.

01:24:52,386 --> 01:24:56,062
SPEAKER_0:  If you would have asked me when I was inside and before all this happened, I would say it could never happen.

01:24:56,290 --> 01:25:01,886
SPEAKER_0:  I don't believe in conspiracies. You know, there's too many people involved. Something's going to come out with some sort of information.

01:25:02,530 --> 01:25:02,878
SPEAKER_0:  I mean...

01:25:03,234 --> 01:25:09,886
SPEAKER_0:  from the more of the stuff that comes out, it's surprising that agents are being fired because of certain actions they're taking inside.

01:25:10,210 --> 01:25:10,654
SPEAKER_0:  Um...

01:25:10,946 --> 01:25:14,654
SPEAKER_0:  and being dismissed because of politically motivated actions.

01:25:16,194 --> 01:25:17,319
SPEAKER_0:  So do you think it's explicit?

01:25:17,319 --> 01:25:19,134
SPEAKER_1:  it or just pressure.

01:25:19,554 --> 01:25:22,910
SPEAKER_1:  Do you think there could exist pressure at the higher ups?

01:25:23,586 --> 01:25:25,886
SPEAKER_1:  of that that has a political leaning and you kind of.

01:25:26,498 --> 01:25:35,422
SPEAKER_1:  Maybe don't explicitly order any kind of thing, but just kind of pressure people to lean one way or the other and then create a culture that leans one way or the other based on political leanings.

01:25:35,810 --> 01:25:40,382
SPEAKER_0:  You would really, really hope not, but I mean, that seems to be the narrative that's being written.

01:25:41,378 --> 01:25:44,286
SPEAKER_1:  But when you were operating, you didn't feel that pressure.

01:25:45,122 --> 01:25:52,247
SPEAKER_0:  Man, I was still at such a low level, you know, I had no aspirations of being a boss. I wanted to be a case agent my entire life. So you love the puzzle of it.

01:25:52,247 --> 01:25:52,830
SPEAKER_1:  BYE

01:25:53,058 --> 01:25:59,006
SPEAKER_0:  the chase. I love solving things. Yeah. Yeah. To be management and manage people and all that. Nick.

01:25:59,522 --> 01:26:00,830
SPEAKER_0:  No desire whatsoever.

01:26:01,986 --> 01:26:02,750
SPEAKER_1:  What do you think?

01:26:03,234 --> 01:26:03,934
SPEAKER_1:  abouts.

01:26:04,514 --> 01:26:07,422
SPEAKER_1:  Mark Zuckerberg on Joe Rogan's podcast.

01:26:07,874 --> 01:26:11,262
SPEAKER_1:  saying that the FBI warned Facebook about potential foreign interference.

01:26:12,674 --> 01:26:13,214
SPEAKER_1:  Uh...

01:26:13,506 --> 01:26:14,782
SPEAKER_1:  and then Facebook.

01:26:15,682 --> 01:26:20,414
SPEAKER_1:  inferred from that, that they're talking about Hunter Biden laptop story.

01:26:20,930 --> 01:26:22,206
SPEAKER_1:  thereby censored it.

01:26:22,466 --> 01:26:23,678
SPEAKER_1:  What do you think about that whole story?

01:26:25,218 --> 01:26:35,614
SPEAKER_0:  Again, you asked me when I was in the FBI, I wouldn't believed it from being on the inside and I wouldn't believe these things, but there's a certain narrative being written that is surprising to me that the FBI is involved in these stories.

01:26:36,226 --> 01:26:38,110
SPEAKER_1:  So but the interesting thing there is...

01:26:38,850 --> 01:26:43,102
SPEAKER_1:  The FBI is saying that they didn't really make that implication. They're saying that there's...

01:26:43,490 --> 01:26:45,246
SPEAKER_1:  Interference activity happening.

01:26:45,538 --> 01:26:46,462
SPEAKER_1:  Just watch out.

01:26:46,914 --> 01:26:49,566
SPEAKER_1:  It's a weird relationship between FBI and Facebook.

01:26:50,306 --> 01:26:51,358
SPEAKER_1:  You could see.

01:26:51,810 --> 01:26:55,422
SPEAKER_1:  from the best possible interpretation that the FBI just wants Facebook to be aware.

01:26:55,874 --> 01:26:57,310
SPEAKER_1:  because it is a powerful platform.

01:26:57,698 --> 01:26:59,678
SPEAKER_1:  A platform for viral spread.

01:27:00,130 --> 01:27:01,438
SPEAKER_1:  of misinformation.

01:27:01,794 --> 01:27:02,302
SPEAKER_1:  So.

01:27:03,042 --> 01:27:09,182
SPEAKER_1:  in the best possible interpretation of it, it makes sense for API to send some information saying, like we were seeing some.

01:27:09,634 --> 01:27:11,166
SPEAKER_1:  shady activity. Absolutely.

01:27:11,394 --> 01:27:15,486
SPEAKER_1:  but it seems like all of that somehow escalated to a political interpretation.

01:27:15,906 --> 01:27:19,070
SPEAKER_0:  I mean, yeah, it sounded like there was a wink wink with it.

01:27:19,330 --> 01:27:21,790
SPEAKER_0:  that I don't know if Mark.

01:27:22,594 --> 01:27:25,534
SPEAKER_0:  meant for that to be that way, you know, I was like, again.

01:27:25,794 --> 01:27:31,419
SPEAKER_0:  Are we being social engineered or was that a true, uh, you know, expression that Mark had?

01:27:31,419 --> 01:27:35,070
SPEAKER_1:  I wonder if the wink-wink is direct or just culture.

01:27:35,330 --> 01:27:36,830
SPEAKER_1:  You know, maybe...

01:27:37,090 --> 01:27:40,862
SPEAKER_1:  certain people responsible on the Facebook side have a certain political lean.

01:27:41,250 --> 01:27:44,190
SPEAKER_1:  and then certain people on the FBI side have a political lean.

01:27:44,514 --> 01:27:47,454
SPEAKER_1:  when they're interacting together. And it's like literally has nothing.

01:27:47,874 --> 01:27:53,310
SPEAKER_1:  to do with a giant conspiracy theory, or just with a culture that has a particular-

01:27:53,730 --> 01:27:55,006
SPEAKER_1:  a political lean during a

01:27:55,586 --> 01:27:58,782
SPEAKER_1:  a particular time in history. And so like,

01:27:59,042 --> 01:28:03,774
SPEAKER_1:  Maybe it could be a Hunter Biden laptop one time and then it could be

01:28:04,162 --> 01:28:07,742
SPEAKER_1:  uh... whoever dot from juniors laptop

01:28:08,290 --> 01:28:09,310
SPEAKER_1:  uh... another time

01:28:09,602 --> 01:28:14,206
SPEAKER_0:  It's a tough job. I mean, if you're the liaison, if you're the FBI's liaison to Facebook,

01:28:14,786 --> 01:28:16,862
SPEAKER_0:  You know, they're certain.

01:28:17,154 --> 01:28:23,422
SPEAKER_0:  people that I'm sure they were offered a position at some point. It seems we, you know, there's FBI agents that go. I know, I know a couple.

01:28:23,714 --> 01:28:24,958
SPEAKER_0:  that's gone to Facebook.

01:28:25,474 --> 01:28:29,182
SPEAKER_0:  This is a really good agent that now leads up their child exploitation stuff.

01:28:29,762 --> 01:28:30,206
SPEAKER_0:  Um...

01:28:31,074 --> 01:28:43,294
SPEAKER_0:  Another squad mate runs their internal investigations, both great investigators. So, you know, there's good money, especially when you're an FBI agent that's capped out at a, you know, a 1310 or whatever pay scale you're capped out at.

01:28:43,650 --> 01:28:48,830
SPEAKER_0:  It's alluring to be, you know, maybe want to please them.

01:28:49,090 --> 01:28:50,110
SPEAKER_0:  and uh and

01:28:50,722 --> 01:28:52,350
SPEAKER_0:  be asked to join them.

01:28:52,738 --> 01:28:53,470
SPEAKER_1:  Yeah.

01:28:54,338 --> 01:28:55,902
SPEAKER_1:  and over time that corrupts.

01:28:57,154 --> 01:29:01,374
SPEAKER_1:  i think there has to be an introspection in tech companies about the culture that they develop

01:29:01,730 --> 01:29:02,814
SPEAKER_1:  about the...

01:29:03,074 --> 01:29:04,382
SPEAKER_1:  the political ideology.

01:29:04,738 --> 01:29:05,342
SPEAKER_1:  the bubble.

01:29:05,890 --> 01:29:07,230
SPEAKER_1:  It's interesting to see that bubble.

01:29:08,002 --> 01:29:09,214
SPEAKER_1:  Like I've, uh...

01:29:10,018 --> 01:29:10,814
SPEAKER_1:  Ask myself.

01:29:12,706 --> 01:29:16,254
SPEAKER_1:  A lot of questions I've interviewed the Pfizer CEO.

01:29:16,962 --> 01:29:17,342
SPEAKER_1:  Uh...

01:29:17,922 --> 01:29:19,742
SPEAKER_1:  What seems now a long time ago.

01:29:20,450 --> 01:29:22,206
SPEAKER_1:  and I've gotten a lot of criticism.

01:29:22,530 --> 01:29:25,950
SPEAKER_1:  Positive comments, but also criticism from that conversation

01:29:26,306 --> 01:29:27,710
SPEAKER_1:  I did a lot of soul searching.

01:29:28,322 --> 01:29:30,206
SPEAKER_1:  about the kind of bubbles we have in this world.

01:29:30,690 --> 01:29:31,710
SPEAKER_1:  It makes me wonder.

01:29:32,482 --> 01:29:33,790
SPEAKER_1:  pharmaceutical companies.

01:29:34,882 --> 01:29:36,414
SPEAKER_1:  They all believe.

01:29:37,410 --> 01:29:38,334
SPEAKER_1:  They're doing good.

01:29:39,810 --> 01:29:40,126
SPEAKER_1:  Dosikfre mÃªme teve Ð² Ð¿Ð¾Ð¼Ð¾Ñ‰Ð¸ ÑÑ‚Ð¾Ð¹people

01:29:40,418 --> 01:29:41,694
SPEAKER_1:  and I wonder...

01:29:42,946 --> 01:29:44,926
SPEAKER_1:  because the ideal they have is to...

01:29:45,282 --> 01:29:46,270
SPEAKER_1:  create drugs.

01:29:46,594 --> 01:29:47,454
SPEAKER_1:  that help people.

01:29:47,842 --> 01:29:49,086
SPEAKER_1:  and do so at scale.

01:29:51,394 --> 01:29:52,670
SPEAKER_1:  and it's hard to know.

01:29:53,186 --> 01:29:56,702
SPEAKER_1:  at which point that can be corrupted and it's hard to know when it was corrupted.

01:29:57,218 --> 01:30:01,278
SPEAKER_1:  and if it was corrupted and where, which drugs and which companies and so on.

01:30:02,754 --> 01:30:03,518
SPEAKER_1:  And I don't know.

01:30:04,066 --> 01:30:05,342
SPEAKER_1:  I don't know that complicated.

01:30:05,634 --> 01:30:08,990
SPEAKER_1:  It seems like inside a bubble you can convince yourself if anything is good.

01:30:09,538 --> 01:30:12,414
SPEAKER_1:  people inside the Third Reich regime.

01:30:13,122 --> 01:30:15,710
SPEAKER_1:  were able to convince themselves, I'm sure, many.

01:30:16,226 --> 01:30:16,958
SPEAKER_1:  Just...

01:30:17,346 --> 01:30:23,102
SPEAKER_1:  Bloodlands is another book I've been recently reading about it and the ability of humans to convince their

01:30:23,426 --> 01:30:25,438
SPEAKER_1:  doing good when they're clearly

01:30:25,954 --> 01:30:28,926
SPEAKER_1:  murdering and torturing people in front of their eyes.

01:30:29,538 --> 01:30:30,622
SPEAKER_1:  is fascinating.

01:30:31,458 --> 01:30:34,398
SPEAKER_1:  They're able to convince themselves they're doing good. It's crazy.

01:30:35,170 --> 01:30:37,182
SPEAKER_1:  Like there's not even a inkling of doubt.

01:30:37,762 --> 01:30:43,390
SPEAKER_1:  I don't know what to make of that. It has taught me to be a little bit more careful.

01:30:43,746 --> 01:30:45,438
SPEAKER_1:  when I enter into different bubbles.

01:30:46,818 --> 01:30:47,710
SPEAKER_1:  to, uh...

01:30:48,642 --> 01:30:50,430
SPEAKER_1:  be skeptical about.

01:30:50,818 --> 01:30:52,958
SPEAKER_1:  what's taken as an assumption of truth.

01:30:53,602 --> 01:30:54,654
SPEAKER_1:  Like you always have to.

01:30:54,978 --> 01:30:59,422
SPEAKER_1:  be skeptical about like what's assumed as true is it possible it's not true

01:31:00,546 --> 01:31:01,726
SPEAKER_1:  You know, if you're doing...

01:31:01,954 --> 01:31:04,126
SPEAKER_1:  If you're talking about America.

01:31:04,930 --> 01:31:06,238
SPEAKER_1:  uh... it's assumed

01:31:06,626 --> 01:31:09,630
SPEAKER_1:  that uh... you know certain places that surveillance is good

01:31:10,114 --> 01:31:13,278
SPEAKER_1:  Well, let's question that assumption.

01:31:14,050 --> 01:31:14,366
SPEAKER_1:  Um.

01:31:14,594 --> 01:31:14,910
SPEAKER_1:  Yeah.

01:31:15,522 --> 01:31:20,222
SPEAKER_1:  And also it inspired me to question my own assumptions that I hold as true.

01:31:21,090 --> 01:31:21,886
SPEAKER_1:  constantly.

01:31:22,914 --> 01:31:24,126
SPEAKER_1:  constantly. It's tough.

01:31:24,578 --> 01:31:31,742
SPEAKER_0:  It's tough. But you don't grow. I mean, did you want to be just static and not grow? You have to question yourself on some of these things. If you want to grow as a person.

01:31:32,834 --> 01:31:33,470
SPEAKER_1:  Yeah, for sure.

01:31:33,762 --> 01:31:37,758
SPEAKER_1:  now one of the tough things actually being a public personality when you speak publicly

01:31:38,370 --> 01:31:39,646
SPEAKER_1:  as you get attacked.

01:31:40,386 --> 01:31:42,366
SPEAKER_1:  all along the way as you're growing.

01:31:42,978 --> 01:31:46,750
SPEAKER_1:  And I'm in part a big softie.

01:31:47,202 --> 01:31:48,734
SPEAKER_1:  as well if I may say in.

01:31:49,282 --> 01:31:50,407
SPEAKER_1:  Those heart, it hurts.

01:31:50,407 --> 01:31:51,102
SPEAKER_0:  It hurts.

01:31:51,394 --> 01:31:52,926
SPEAKER_0:  It hurts. Do you pay attention to it?

01:31:54,146 --> 01:31:57,374
SPEAKER_1:  Yeah, yeah, yeah, yeah, it's very hard.

01:31:58,658 --> 01:31:59,742
SPEAKER_1:  Like I have to...

01:32:00,226 --> 01:32:00,990
SPEAKER_1:  two choices.

01:32:01,218 --> 01:32:03,646
SPEAKER_1:  when you can uh... shut yourself off from the world

01:32:04,290 --> 01:32:09,182
SPEAKER_1:  and ignore it. I never found that compelling, this kind of idea of like haters gonna hate.

01:32:09,602 --> 01:32:09,950
SPEAKER_1:  Yeah.

01:32:10,210 --> 01:32:11,294
SPEAKER_1:  Like, uh...

01:32:12,066 --> 01:32:14,206
SPEAKER_1:  this idea that anyone

01:32:14,530 --> 01:32:17,310
SPEAKER_1:  with a big platform or anyone's ever done anything.

01:32:17,570 --> 01:32:19,774
SPEAKER_1:  was always gotten hate. Okay, maybe.

01:32:20,386 --> 01:32:21,054
SPEAKER_1:  But like...

01:32:21,346 --> 01:32:27,774
SPEAKER_1:  I still want to be vulnerable, wear my heart on my sleeve, really show myself, like open myself to the world, really listen to people.

01:32:28,066 --> 01:32:28,446
SPEAKER_1:  and

01:32:28,834 --> 01:32:33,566
SPEAKER_1:  That means every once in a while somebody will say something that touches me in a way that's like

01:32:34,754 --> 01:32:35,582
SPEAKER_1:  What if they're right?

01:32:35,874 --> 01:32:37,854
SPEAKER_0:  Did you let that hate influence you? I mean...

01:32:38,114 --> 01:32:42,989
SPEAKER_0:  Can you be bullied into a different opinion than you think you really are just because of that?

01:32:42,989 --> 01:32:43,774
SPEAKER_1:  Did that hate? No.

01:32:44,226 --> 01:32:45,662
SPEAKER_1:  I believe not, but...

01:32:46,434 --> 01:32:49,950
SPEAKER_1:  it hurts in a way that's hard to explain. like...

01:32:51,330 --> 01:32:53,726
SPEAKER_1:  Yeah, it just, it gets to like a...

01:32:54,722 --> 01:32:58,974
SPEAKER_1:  It shakes your faith in humanity actually, is probably why it hurts.

01:32:59,618 --> 01:33:00,222
SPEAKER_1:  Like, um...

01:33:01,346 --> 01:33:02,622
SPEAKER_1:  people that, um...

01:33:03,522 --> 01:33:06,622
SPEAKER_1:  Call me a Putin apologist or a Zelensky apologist.

01:33:06,978 --> 01:33:09,630
SPEAKER_1:  which I'm currently getting almost an equal amount of.

01:33:11,682 --> 01:33:12,446
SPEAKER_1:  But it hurts.

01:33:13,122 --> 01:33:14,206
SPEAKER_1:  It hurts.

01:33:14,722 --> 01:33:15,550
SPEAKER_1:  because I...

01:33:16,706 --> 01:33:17,374
SPEAKER_1:  Um...

01:33:17,954 --> 01:33:19,326
SPEAKER_1:  It hurts because it

01:33:19,682 --> 01:33:20,158
SPEAKER_1:  It's like.

01:33:21,026 --> 01:33:24,798
SPEAKER_1:  damages slightly my faith in humanity to be able to see.

01:33:25,794 --> 01:33:26,334
SPEAKER_1:  Um,

01:33:27,522 --> 01:33:30,942
SPEAKER_1:  the love that connects us and then to see that I'm trying to.

01:33:31,202 --> 01:33:31,710
SPEAKER_1:  Find that.

01:33:32,706 --> 01:33:34,078
SPEAKER_1:  And that's, I'm doing my best.

01:33:34,434 --> 01:33:37,118
SPEAKER_1:  and the limited capabilities I have to find that.

01:33:37,474 --> 01:33:39,262
SPEAKER_1:  and so to call me something.

01:33:39,778 --> 01:33:42,942
SPEAKER_1:  uh... like a bad actor essentially from whatever perspective

01:33:43,330 --> 01:33:44,734
SPEAKER_1:  It just makes me realize, well...

01:33:45,570 --> 01:33:46,014
SPEAKER_1:  Um...

01:33:46,498 --> 01:33:48,670
SPEAKER_1:  people don't have empathy and compassion for each other.

01:33:48,994 --> 01:33:50,846
SPEAKER_1:  And it makes me question that for a brief moment.

01:33:51,330 --> 01:33:53,566
SPEAKER_1:  And that's like a crack and it hurts.

01:33:53,922 --> 01:33:55,294
SPEAKER_0:  How many people do this to your face?

01:33:57,506 --> 01:34:01,631
SPEAKER_0:  Uh, very few. Yeah. It's all it's online. E muscles, man. They're just flexing.

01:34:01,631 --> 01:34:02,174
SPEAKER_1:  Be honest.

01:34:02,562 --> 01:34:02,910
SPEAKER_1:  that

01:34:03,170 --> 01:34:06,206
SPEAKER_1:  uh it's it happens yeah because i've

01:34:06,690 --> 01:34:08,830
SPEAKER_1:  hung around with Rogan enough.

01:34:09,570 --> 01:34:11,006
SPEAKER_1:  when your platform grows.

01:34:11,490 --> 01:34:14,686
SPEAKER_1:  There's people that will come up to Joe and say stuff to his face.

01:34:15,586 --> 01:34:17,598
SPEAKER_1:  that they forget, they still.

01:34:17,986 --> 01:34:20,830
SPEAKER_1:  uh... they forget he's actually a real human being.

01:34:21,410 --> 01:34:23,582
SPEAKER_1:  they'll make accusations about him.

01:34:23,682 --> 01:34:25,598
SPEAKER_0:  So does that cause him to wall himself off more?

01:34:26,338 --> 01:34:29,758
SPEAKER_1:  No, he's pretty gangster on that. But, uh...

01:34:29,986 --> 01:34:33,438
SPEAKER_1:  Yeah, it still hurts if you're human, if you've really...

01:34:34,530 --> 01:34:38,238
SPEAKER_1:  others. I think that's also the difference with Joe and me.

01:34:39,074 --> 01:34:39,998
SPEAKER_1:  Here's a...

01:34:41,026 --> 01:34:43,134
SPEAKER_1:  is a family that he deeply loves.

01:34:43,394 --> 01:34:44,958
SPEAKER_1:  and that's an escape from the world for him.

01:34:46,466 --> 01:34:47,006
SPEAKER_1:  Um...

01:34:47,682 --> 01:34:48,542
SPEAKER_1:  There's a...

01:34:48,930 --> 01:34:52,190
SPEAKER_1:  loneliness in me that I'm always longing to connect with people.

01:34:52,514 --> 01:34:53,982
SPEAKER_1:  and with like regular people.

01:34:54,306 --> 01:34:57,470
SPEAKER_1:  and just to learn their stories and so on.

01:34:57,730 --> 01:35:01,374
SPEAKER_1:  And so if you open yourself up that way, the things they tell you can really hurt.

01:35:01,986 --> 01:35:05,054
SPEAKER_1:  in every way, like just me going to Ukraine.

01:35:05,506 --> 01:35:07,550
SPEAKER_1:  Just seeing so much loss.

01:35:08,130 --> 01:35:08,830
SPEAKER_1:  and death.

01:35:10,210 --> 01:35:11,390
SPEAKER_1:  Some of it is like...

01:35:12,386 --> 01:35:12,798
SPEAKER_1:

01:35:13,250 --> 01:35:13,566
SPEAKER_1:  is

01:35:13,954 --> 01:35:15,806
SPEAKER_1:  I mean, unforgettably haunting.

01:35:16,162 --> 01:35:19,038
SPEAKER_1:  not in some kind of political way, activist way or...

01:35:19,650 --> 01:35:22,046
SPEAKER_1:  uh... who's right who's wrong way but just like

01:35:23,266 --> 01:35:23,742
SPEAKER_1:  Man.

01:35:24,482 --> 01:35:27,166
SPEAKER_1:  like so much paint and you see it and it just stays with you.

01:35:27,426 --> 01:35:30,110
SPEAKER_0:  When you see a human being bad to another human, you can't...

01:35:30,530 --> 01:35:31,422
SPEAKER_0:  get rid of that in your head.

01:35:32,866 --> 01:35:36,670
SPEAKER_0:  You can't imagine that we can treat each other like that.

01:35:37,186 --> 01:35:40,222
SPEAKER_0:  That's the hard part, I think. I mean, for me it is.

01:35:41,218 --> 01:35:43,070
SPEAKER_0:  when I saw parents like.

01:35:43,586 --> 01:35:46,430
SPEAKER_0:  when I did the child exploitation stuff, when they rented their children out.

01:35:46,658 --> 01:35:50,526
SPEAKER_0:  They literally rented infant children out to others for sexual gratification.

01:35:51,394 --> 01:35:54,334
SPEAKER_0:  Like, I don't know how a human being could do that to another human being.

01:35:55,522 --> 01:35:57,982
SPEAKER_0:  And that sounds like the kind of thing you're going through.

01:35:58,306 --> 01:36:00,894
SPEAKER_0:  I mean, I went through a huge funk when I did those cases.

01:36:01,122 --> 01:36:06,814
SPEAKER_0:  Afterwards, I should have talked to somebody, but in the FBI you have to keep that machismo Cincinnati Service and it was a separ malfunction.

01:36:07,234 --> 01:36:08,350
SPEAKER_0:  They're going to take your gun away from you.

01:36:08,450 --> 01:36:10,942
SPEAKER_1:  Well, I think that's examples of evil.

01:36:12,226 --> 01:36:12,670
SPEAKER_1:  Um...

01:36:13,890 --> 01:36:16,926
SPEAKER_1:  that that's like the worst of human nature, but.

01:36:17,378 --> 01:36:19,253
SPEAKER_1:  I- just because I have more-

01:36:19,253 --> 01:36:20,990
SPEAKER_0:  is just as bad. I mean...

01:36:21,634 --> 01:36:22,942
SPEAKER_1:  somehow war.

01:36:25,090 --> 01:36:26,494
SPEAKER_1:  It's somehow understandable.

01:36:26,850 --> 01:36:29,918
SPEAKER_1:  given all the very intense propaganda that's happening.

01:36:30,786 --> 01:36:32,862
SPEAKER_1:  So you can understand.

01:36:33,218 --> 01:36:33,758
SPEAKER_1:  Um...

01:36:34,978 --> 01:36:37,054
SPEAKER_1:  that there is love in the heart.

01:36:37,282 --> 01:36:40,318
SPEAKER_1:  of the soldiers on each side given the information they're given.

01:36:40,866 --> 01:36:43,486
SPEAKER_1:  as a lot of people on the Russian side believe they're saving.

01:36:43,874 --> 01:36:46,270
SPEAKER_1:  these Ukrainian cities from Nazi occupation.

01:36:48,258 --> 01:36:50,718
SPEAKER_1:  Now, there is stories.

01:36:51,618 --> 01:36:56,670
SPEAKER_1:  There is a lot of evidence of people for fun murdering civilians

01:36:57,506 --> 01:36:58,302
SPEAKER_1:  of that.

01:36:59,010 --> 01:37:01,310
SPEAKER_1:  That is closer to the things you've experienced.

01:37:01,890 --> 01:37:03,166
SPEAKER_1:  of like evil.

01:37:04,002 --> 01:37:04,478
SPEAKER_1:  Uh...

01:37:05,314 --> 01:37:06,430
SPEAKER_1:  of evil.

01:37:06,658 --> 01:37:11,518
SPEAKER_1:  embodied and I haven't interacted with that directly with people who

01:37:11,842 --> 01:37:12,967
SPEAKER_1:  for fun murder civilians.

01:37:12,967 --> 01:37:15,934
SPEAKER_0:  But you know it's there in the world. You're not naive to it.

01:37:16,226 --> 01:37:18,494
SPEAKER_1:  Yes, but if you experience that directly.

01:37:18,978 --> 01:37:21,726
SPEAKER_1:  if somebody shouts somebody for fun in front of me.

01:37:22,178 --> 01:37:23,614
SPEAKER_1:  That would probably break me.

01:37:25,378 --> 01:37:29,342
SPEAKER_1:  Like seeing it yourself, knowing that it exists is different than seeing it yourself.

01:37:30,018 --> 01:37:32,190
SPEAKER_1:  Now I've interacted with the victims of that.

01:37:33,410 --> 01:37:35,006
SPEAKER_1:  and they tell me stories.

01:37:35,298 --> 01:37:37,022
SPEAKER_1:  and you see their homes destroyed.

01:37:37,506 --> 01:37:42,078
SPEAKER_1:  destroyed for no good military reason, its civilians with civilian homes being destroyed.

01:37:42,530 --> 01:37:44,030
SPEAKER_1:  that really lingers with you.

01:37:44,674 --> 01:37:45,598
SPEAKER_1:  It's, um...

01:37:46,434 --> 01:37:48,126
SPEAKER_1:  Yeah, the people that are capable of that.

01:37:48,514 --> 01:37:54,430
SPEAKER_0:  that goes with the propaganda. I mean, if you were to build a story, you have to, you know, you have to have on the other side, you know.

01:37:54,882 --> 01:37:58,910
SPEAKER_0:  the homes are going to be destroyed, the non-military targets are going to be destroyed.

01:37:59,618 --> 01:38:04,382
SPEAKER_1:  To put it in perspective, I'm not sure a lot of people understand the deep human side.

01:38:04,802 --> 01:38:07,358
SPEAKER_1:  or even the military strategy side of this war.

01:38:07,714 --> 01:38:15,422
SPEAKER_1:  There's a lot of experts outside of the situation that are commenting on it with certainty. And that kind of hurts me because I feel like there's a lot of uncertainty.

01:38:15,938 --> 01:38:18,686
SPEAKER_1:  There's so much propaganda, it's very difficult to know what is true.

01:38:19,522 --> 01:38:19,838
SPEAKER_1:  time.

01:38:22,562 --> 01:38:24,894
SPEAKER_1:  Yeah, so my whole

01:38:25,218 --> 01:38:26,014
SPEAKER_1:  hope was...

01:38:27,650 --> 01:38:32,574
SPEAKER_1:  to travel to Ukraine, to travel to Russia, to talk to soldiers, to talk to leaders, to talk to real people.

01:38:32,802 --> 01:38:35,294
SPEAKER_1:  that have lost homes, that have lost family members.

01:38:35,906 --> 01:38:38,558
SPEAKER_1:  that who this war has divided.

01:38:39,170 --> 01:38:41,566
SPEAKER_1:  who this war changed completely how they see the world.

01:38:42,146 --> 01:38:46,078
SPEAKER_1:  whether they have love or hate in their heart to understand their stories.

01:38:46,466 --> 01:38:50,238
SPEAKER_1:  I've learned a lot on the human side of things by having talked to a lot of people there.

01:38:50,818 --> 01:38:53,822
SPEAKER_1:  But it has been on the Ukrainian side for me currently.

01:38:54,370 --> 01:38:56,350
SPEAKER_1:  traveling to the Russian side is more difficult.

01:38:59,138 --> 01:39:00,158
SPEAKER_1:  Let me ask you.

01:39:00,578 --> 01:39:02,494
SPEAKER_1:  about your now friend?

01:39:02,722 --> 01:39:07,390
SPEAKER_1:  Can we go as far as to say his friend in Asabu, Hector Massaguer.

01:39:08,578 --> 01:39:13,278
SPEAKER_1:  What's the story? What's your long story with him?

01:39:13,794 --> 01:39:14,686
SPEAKER_1:  Can you tell me about?

01:39:15,490 --> 01:39:16,606
SPEAKER_1:  What is LALSEC?

01:39:17,922 --> 01:39:18,846
SPEAKER_1:  Who is Sabu?

01:39:20,130 --> 01:39:22,366
SPEAKER_1:  and who's anonymous, what is anonymous.

01:39:22,594 --> 01:39:23,719
SPEAKER_1:  Where's the right place to start?

01:39:23,719 --> 01:39:25,470
SPEAKER_0:  that story. Probably anonymous.

01:39:25,954 --> 01:39:30,654
SPEAKER_0:  Anonymous is a was a decent. I still is, I guess a decentralized organization.

01:39:31,138 --> 01:39:35,678
SPEAKER_0:  They call themselves headless, but once you look into them a little ways, they're not really headless.

01:39:36,098 --> 01:39:36,510
SPEAKER_0:  Um...

01:39:36,834 --> 01:39:37,598
SPEAKER_0:  The, the, the...

01:39:38,370 --> 01:39:39,806
SPEAKER_0:  power struggle comes with.

01:39:40,130 --> 01:39:42,046
SPEAKER_0:  whoever has a hacking ability.

01:39:42,594 --> 01:39:48,286
SPEAKER_0:  Um, that might be your good hacker or you have a giant botnet use for DDoS. Um.

01:39:48,866 --> 01:39:51,934
SPEAKER_0:  So you're going to wield more power if you can control where it goes.

01:39:53,122 --> 01:40:00,990
SPEAKER_0:  Anonymous started doing their like hacktivism stuff in 2010 or so The word hack was in the media all the time then

01:40:01,634 --> 01:40:06,238
SPEAKER_0:  And then right around then there was a federal contractor named HB Gary Federal.

01:40:06,466 --> 01:40:08,414
SPEAKER_0:  The CEO is Aaron Barr.

01:40:08,930 --> 01:40:12,382
SPEAKER_0:  And Aaron Barr said he was gonna come out and de-anonymize anonymous.

01:40:13,026 --> 01:40:13,758
SPEAKER_0:  He's gonna come out and...

01:40:13,986 --> 01:40:20,702
SPEAKER_0:  at Black Hat or Def Con or one of those and say, you know, who they are. He figured it out or so he figured it out by based on a

01:40:21,122 --> 01:40:31,646
SPEAKER_0:  when people were online, when people were in IRC, when tweets came out, there was no scientific proof behind it or anything. So he was just gonna falsely name people that were in anonymous.

01:40:32,866 --> 01:40:39,998
SPEAKER_0:  So Anonymous went on the attack. They went and hacked in HPA federal and they turned his life upside down. They took over his Twitter account and all that stuff.

01:40:40,578 --> 01:40:41,758
SPEAKER_0:  pretty quickly.

01:40:42,178 --> 01:40:45,374
SPEAKER_1:  I have very mixed feelings about all of this. Okay...

01:40:45,666 --> 01:40:46,110
SPEAKER_1:  I get it.

01:40:48,002 --> 01:40:48,958
SPEAKER_1:  Like part of me.

01:40:50,946 --> 01:40:51,966
SPEAKER_1:  admires.

01:40:52,642 --> 01:40:54,302
SPEAKER_1:  the positive side of the hack.

01:40:55,074 --> 01:40:55,454
SPEAKER_1:  Okay.

01:40:56,450 --> 01:40:58,750
SPEAKER_1:  Is there no room for admiration there?

01:40:59,426 --> 01:41:00,862
SPEAKER_1:  of the fuck you to the man.

01:41:01,090 --> 01:41:03,326
SPEAKER_0:  Not at the time. Again, it was a violation.

01:41:03,810 --> 01:41:09,054
SPEAKER_0:  The 18 USC 1030. So it was my job. It's what I, you know, so at the time, no, in retrospect, sure.

01:41:10,626 --> 01:41:13,214
SPEAKER_1:  But what was the philosophy of the hacktivism?

01:41:13,826 --> 01:41:18,782
SPEAKER_1:  Was it, philosophically, were they at least expressing it for the good of humanity?

01:41:19,970 --> 01:41:26,782
SPEAKER_0:  The Outwardly said that they were going to go after people that they thought were corrupt. So they were judging Jerry on corruption. And they were to go after it.

01:41:27,554 --> 01:41:35,134
SPEAKER_0:  once you get inside and realize what they were doing, they were going after people that they had an opportunity to go after.

01:41:35,778 --> 01:41:43,038
SPEAKER_0:  So maybe someone had a zero day and then they searched for servers running that zero day and then from there Let's find a target.

01:41:43,906 --> 01:41:46,270
SPEAKER_0:  One time they went after a toilet paper company.

01:41:46,722 --> 01:41:51,646
SPEAKER_0:  I still don't understand what that toilet paper company did, but it was an opportunity to make a splash.

01:41:51,810 --> 01:41:54,270
SPEAKER_1:  Is there some- some way for the joke? For the lolz?

01:41:55,234 --> 01:42:12,606
SPEAKER_0:  It developed into that. So I think the hacktivism and the anonymous stuff wasn't so much for the lulz, but from that HP Gary federal hack, then there were six guys that worked well together and they formed a crew, a hacking crew, and they kind of split off into their own private channels and that was lulz sec or laughing at your security was their motto.

01:42:14,338 --> 01:42:19,966
SPEAKER_1:  So that's L-U-L-Z-S-E-C, lulzsec. Of course it is. LULZ.

01:42:20,322 --> 01:42:20,894
SPEAKER_1:  Suck.

01:42:21,506 --> 01:42:22,558
SPEAKER_1:  And uh...

01:42:23,682 --> 01:42:25,022
SPEAKER_1:  Who founded that?

01:42:25,282 --> 01:42:26,046
SPEAKER_1:  organization.

01:42:26,530 --> 01:42:27,006
SPEAKER_0:  So.

01:42:27,266 --> 01:42:32,766
SPEAKER_0:  Kayla and Sabu were the hackers of the group. And so they really did all the work on HP Gary.

01:42:33,154 --> 01:42:34,279
SPEAKER_0:  So there's.

01:42:34,279 --> 01:42:35,029
SPEAKER_1:  code names.

01:42:35,029 --> 01:42:36,926
SPEAKER_0:  Yeah, these are online names there.

01:42:37,186 --> 01:42:38,206
SPEAKER_0:  They're nicks.

01:42:38,850 --> 01:42:40,286
SPEAKER_0:  Um, and so.

01:42:40,706 --> 01:42:43,646
SPEAKER_0:  You know, they, they, they, that's all they knew each other as.

01:42:44,418 --> 01:42:46,142
SPEAKER_0:  You know, they talked as those names.

01:42:46,562 --> 01:42:51,550
SPEAKER_0:  Um, and they worked well together and so they, they formed a hacking crew and that's when they started the, the.

01:42:51,906 --> 01:42:56,030
SPEAKER_0:  At first they didn't name it this, but it was the 50 Days of Lulz, where they would just release.

01:42:56,674 --> 01:42:58,334
SPEAKER_0:  major, major breaches.

01:42:58,818 --> 01:43:03,102
SPEAKER_0:  Um, and it stirred up the media. I mean, it put hacking in the media every day.

01:43:03,522 --> 01:43:06,014
SPEAKER_0:  They had 400 or 500 thousand.

01:43:06,274 --> 01:43:07,678
SPEAKER_0:  Twitter followers.

01:43:08,290 --> 01:43:09,534
SPEAKER_0:  You know, and it was kind of.

01:43:10,082 --> 01:43:10,942
SPEAKER_0:  Interesting.

01:43:11,522 --> 01:43:17,566
SPEAKER_0:  Um, but then they started swinging at the beehive and they, they, they took out some FBI affiliated sites.

01:43:18,146 --> 01:43:20,766
SPEAKER_0:  And then they started, uh, fuck FBI Fridays.

01:43:21,250 --> 01:43:27,038
SPEAKER_0:  where every Friday they would release something. And we waited it with bated breath. I mean, they had us, Hucklind and Stinker, pissed.

01:43:27,522 --> 01:43:30,878
SPEAKER_0:  Uh, we were waiting to see what was going to be dropped every Friday. I don't know if you'll be able to hear me. Of course I could.

01:43:31,650 --> 01:43:33,525
SPEAKER_0:  It's a little embarrassing looking back on it now.

01:43:33,525 --> 01:43:34,846
SPEAKER_1:  in their early 20's.

01:43:35,810 --> 01:43:38,142
SPEAKER_0:  This was 2010, 2011, around there.

01:43:38,498 --> 01:43:40,190
SPEAKER_1:  Can I actually linger on...

01:43:41,250 --> 01:43:42,078
SPEAKER_1:  Anonymous.

01:43:43,234 --> 01:43:46,023
SPEAKER_1:  Do you still understand what the heck is anonymous?

01:43:46,023 --> 01:43:53,406
SPEAKER_0:  It's just a place where you hang out. I mean, it started on 4chan, went to 8chan, and then it's really just anyone. You could be an anonymous right now if you wanted to.

01:43:53,634 --> 01:44:00,030
SPEAKER_0:  just you're in there hanging out in the channel. Now you're probably not gonna get much cred until you work your way up and prove who you are if someone vouches for you.

01:44:00,322 --> 01:44:03,697
SPEAKER_0:  Uh, but anybody can be an anonymous, and we can leave anonymous.

01:44:03,697 --> 01:44:06,430
SPEAKER_1:  leadership of Anonymous. Do you have a sense that there is a leadership?

01:44:07,010 --> 01:44:07,358
SPEAKER_0:  There's a-

01:44:07,778 --> 01:44:12,653
SPEAKER_0:  Power play. No, there's not someone that you know that says this is what we're doing. No, we're doing

01:44:12,653 --> 01:44:13,886
SPEAKER_1:  I love the...

01:44:14,626 --> 01:44:18,334
SPEAKER_1:  the philosophical and the technical aspect of all of this.

01:44:18,786 --> 01:44:20,446
SPEAKER_1:  But I think there is a slippery slope.

01:44:20,898 --> 01:44:22,878
SPEAKER_1:  to where for the lulls you can actually...

01:44:24,066 --> 01:44:24,990
SPEAKER_1:  really hurt people.

01:44:26,050 --> 01:44:27,742
SPEAKER_1:  That's the terrifying thing.

01:44:28,098 --> 01:44:28,926
SPEAKER_1:  When you touch.

01:44:29,954 --> 01:44:33,246
SPEAKER_1:  I'm actually really terrified of the power of the lull.

01:44:34,466 --> 01:44:41,694
SPEAKER_1:  It's the fun thing somehow becomes a slippery slope. I haven't quite understood the dynamics of that, but even in myself.

01:44:42,210 --> 01:44:43,070
SPEAKER_1:  If you just...

01:44:43,586 --> 01:44:45,118
SPEAKER_1:  Have fun with the thing.

01:44:45,410 --> 01:44:48,094
SPEAKER_1:  you lose track of the ethical grounding of the thing.

01:44:48,642 --> 01:44:53,470
SPEAKER_1:  And so like, it feels like hacking for fun can just turn it, like literally lead to nuclear war.

01:44:54,338 --> 01:44:55,463
SPEAKER_1:  Like literally just.

01:44:55,463 --> 01:44:56,158
SPEAKER_0:

01:44:56,610 --> 01:44:59,934
SPEAKER_0:  Yeah, yada yada yada nuclear war. I could see it. Yeah.

01:45:00,802 --> 01:45:04,414
SPEAKER_1:  So I've been more careful with the law.

01:45:04,802 --> 01:45:10,206
SPEAKER_1:  I've been more careful about that and I wonder about it because in internet speak.

01:45:10,722 --> 01:45:12,702
SPEAKER_1:  Somehow ethics can be put aside.

01:45:13,698 --> 01:45:14,654
SPEAKER_1:  are through.

01:45:15,106 --> 01:45:16,798
SPEAKER_1:  the slippery slope of language.

01:45:17,922 --> 01:45:22,398
SPEAKER_1:  I don't know, everything becomes a joke. If everything's a joke, then everything's a loud, everything's a loud.

01:45:22,722 --> 01:45:26,110
SPEAKER_1:  then you don't have a sense of what is right and wrong. You lose sense of what is right and wrong.

01:45:26,210 --> 01:45:30,014
SPEAKER_0:  You still have victims. I mean, you're laughing at someone. Someone's the butt of this joke.

01:45:30,978 --> 01:45:39,550
SPEAKER_0:  you know, whether it's major corporations or the individuals, I mean, some of the stuff they did was just, you know, releasing people's PII and their personal identifying information and stuff like that, I mean...

01:45:40,034 --> 01:45:46,206
SPEAKER_0:  Is it a big deal? I don't know. Maybe, maybe not, but you know, if you could choose to not have your information put out there.

01:45:46,498 --> 01:45:47,038
SPEAKER_0:  Probably wouldn't.

01:45:48,866 --> 01:45:50,878
SPEAKER_1:  We do have a sense of what anonymous is today.

01:45:51,330 --> 01:45:56,414
SPEAKER_1:  Has it ever been one stable organization or is it a collection of hackers that kind of

01:45:57,026 --> 01:45:57,886
SPEAKER_1:  Emerge.

01:45:59,074 --> 01:46:02,142
SPEAKER_1:  for particular tasks, for particular...

01:46:03,266 --> 01:46:05,141
SPEAKER_1:  a hacktivism task and that kind of stuff.

01:46:05,141 --> 01:46:07,774
SPEAKER_0:  It's a collection of people that has some hackers in it.

01:46:08,162 --> 01:46:09,566
SPEAKER_0:  There's not a lot of

01:46:10,146 --> 01:46:13,950
SPEAKER_0:  big hackers in it. I mean, there's something that'll come bounce in bounce out.

01:46:14,178 --> 01:46:17,342
SPEAKER_0:  Even back then, there was probably just as many.

01:46:17,762 --> 01:46:18,718
SPEAKER_0:  reporters in it.

01:46:19,042 --> 01:46:20,958
SPEAKER_0:  people the media and it with the

01:46:21,442 --> 01:46:24,798
SPEAKER_0:  than hackers at the time, just trying to get the inside scoop on things.

01:46:25,506 --> 01:46:32,286
SPEAKER_0:  You know, some giving the inside scoop. You know, we arrested a reporter that gave over the username and password to his newspaper.

01:46:32,706 --> 01:46:35,038
SPEAKER_0:  and just so he could break the story.

01:46:37,122 --> 01:46:37,758
SPEAKER_0:  He trusted him.

01:46:39,618 --> 01:46:40,606
SPEAKER_0:  Speaking of trusts...

01:46:40,802 --> 01:46:41,630
SPEAKER_1:  reporters.

01:46:42,114 --> 01:46:43,710
SPEAKER_1:  Boy, there's good ones.

01:46:43,938 --> 01:46:45,374
SPEAKER_1:  There's good ones. Sorry.

01:46:45,666 --> 01:46:46,110
SPEAKER_1:  there are.

01:46:47,618 --> 01:46:49,493
SPEAKER_1:  boy do I have a complicated relationship with

01:46:49,493 --> 01:46:52,062
SPEAKER_0:  How many stories about you are completely true?

01:46:53,058 --> 01:46:55,006
SPEAKER_1:  You can just make stuff up on the internet.

01:46:55,554 --> 01:46:56,638
SPEAKER_1:  And one of the things that

01:46:57,634 --> 01:47:02,206
SPEAKER_1:  I mean, there's so many fascinating psychological, sociological elements of the internet.

01:47:02,818 --> 01:47:03,678
SPEAKER_1:  to me.

01:47:04,002 --> 01:47:05,470
SPEAKER_1:  One of them is that...

01:47:05,922 --> 01:47:06,974
SPEAKER_1:  You can say that.

01:47:07,522 --> 01:47:08,094
SPEAKER_1:  uh...

01:47:08,514 --> 01:47:10,302
SPEAKER_1:  Lex is a lizard.

01:47:11,522 --> 01:47:15,806
SPEAKER_1:  And if it's not funny, so Liz is kind of funny, what should we say?

01:47:16,034 --> 01:47:16,670
SPEAKER_1:  Um...

01:47:17,346 --> 01:47:22,238
SPEAKER_1:  Lex has admitted to being an agent of the FBI.

01:47:22,498 --> 01:47:27,486
SPEAKER_1:  Okay, you can just say that, right? And then the response that the internet will be like, Oh, is that true?

01:47:28,002 --> 01:47:29,022
SPEAKER_1:  I didn't realize that.

01:47:29,346 --> 01:47:29,822
SPEAKER_1:  They won't.

01:47:30,114 --> 01:47:30,878
SPEAKER_1:  Go like...

01:47:31,746 --> 01:47:33,246
SPEAKER_1:  Provide evidence please.

01:47:33,602 --> 01:47:41,438
SPEAKER_1:  Right. They'll just say like, oh, that's weird. I didn't. I kind of thought he might be kind of weird. And then it piles on. It's like, hey, hey, hey, guys.

01:47:42,178 --> 01:47:42,558
SPEAKER_1:  Like.

01:47:42,882 --> 01:47:47,102
SPEAKER_1:  Here's a random dude on the internet just said a random thing. You can't just like pile up as.

01:47:47,266 --> 01:47:50,814
SPEAKER_0:  And then Johnny 6969 is now a source that says.

01:47:51,138 --> 01:47:51,806
SPEAKER_1:  And then like

01:47:52,450 --> 01:47:55,806
SPEAKER_1:  The thing is, I'm a tiny guy, but when it grows...

01:47:56,258 --> 01:47:58,878
SPEAKER_1:  uh... if your sister like have a big platform

01:47:59,394 --> 01:48:01,438
SPEAKER_1:  I feel like newspapers will pick that up.

01:48:01,986 --> 01:48:09,758
SPEAKER_1:  and then they'll like start to build on a story and you never know where that story really started. It's so cool. I mean, to me, actually, honestly, it's kind of cool that-

01:48:10,050 --> 01:48:22,302
SPEAKER_1:  There's a viral nature of the internet that can just fabricate truth completely. I think we have to accept that new reality and try to deal with it somehow. You can't just complain that Johnny 69 can start a random thing.

01:48:22,690 --> 01:48:23,038
SPEAKER_1:  I-

01:48:23,522 --> 01:48:24,126
SPEAKER_1:  I think.

01:48:24,642 --> 01:48:30,110
SPEAKER_1:  in the best possible world, it is the role of the journalist to be the adult in the room.

01:48:31,042 --> 01:48:31,934
SPEAKER_1:  put a stop to it.

01:48:32,354 --> 01:48:33,246
SPEAKER_1:  versus look.

01:48:33,762 --> 01:48:37,726
SPEAKER_1:  for the sexiest story so that there could be clickbait that can generate money.

01:48:38,658 --> 01:48:39,998
SPEAKER_1:  journalism should be about.

01:48:41,250 --> 01:48:41,822
SPEAKER_1:  sort of.

01:48:42,178 --> 01:48:43,550
SPEAKER_1:  slowing things down.

01:48:43,906 --> 01:48:48,926
SPEAKER_1:  thinking deeply through what is true and not and showing that to the world. I think there's a lot of hunger for that.

01:48:49,314 --> 01:48:51,939
SPEAKER_1:  I think that would actually get the most clicks in the end.

01:48:51,939 --> 01:48:57,566
SPEAKER_0:  I mean, it's that same pressure I think we're talking about with the FBI and with the tech companies about controllers. I mean.

01:48:57,890 --> 01:49:01,150
SPEAKER_0:  The editors have to please and get those clicks. I mean, they're measured.

01:49:01,666 --> 01:49:07,902
SPEAKER_0:  buy those clicks. So, you know, I'm sure the journalists, the true journalists, the good ones out there want that.

01:49:09,026 --> 01:49:10,174
SPEAKER_0:  They want to stay employed too.

01:49:10,562 --> 01:49:16,414
SPEAKER_1:  Can I actually ask you, really as another tangent, the Jared and others, they're doing undercover?

01:49:16,706 --> 01:49:19,358
SPEAKER_1:  Yep. In terms of the tools you have.

01:49:20,066 --> 01:49:23,326
SPEAKER_1:  for catching cyber security criminals, how much of his undercover.

01:49:23,874 --> 01:49:29,182
SPEAKER_0:  Undercover is a high bar to jump over. You have to do a lot to start an undercover.

01:49:29,410 --> 01:49:31,678
SPEAKER_0:  In the FBI, there's a lot of thresholds.

01:49:31,938 --> 01:49:32,478
SPEAKER_0:  Um...

01:49:33,218 --> 01:49:35,998
SPEAKER_0:  So it's not your first investigative tool step.

01:49:36,322 --> 01:49:39,422
SPEAKER_0:  You have to identify a problem and then show that.

01:49:39,650 --> 01:49:41,598
SPEAKER_0:  The lower steps can't get you there.

01:49:42,114 --> 01:49:44,574
SPEAKER_0:  Um, but I mean, I think we.

01:49:44,898 --> 01:49:48,286
SPEAKER_0:  We had an undercover going on the squad about all times when one.

01:49:48,738 --> 01:49:50,430
SPEAKER_0:  was being shut down or taken down.

01:49:51,010 --> 01:49:52,094
SPEAKER_0:  spinning up another one.

01:49:52,514 --> 01:49:54,846
SPEAKER_0:  So it's a good tool to have.

01:49:55,106 --> 01:49:56,510
SPEAKER_0:  you know, and utilize.

01:49:56,994 --> 01:49:58,110
SPEAKER_0:  They're a lot of work.

01:49:58,658 --> 01:50:01,598
SPEAKER_0:  I don't think if you run one, you'll never run another one in your life.

01:50:02,626 --> 01:50:03,038
SPEAKER_0:  Oh.

01:50:03,202 --> 01:50:08,827
SPEAKER_1:  So it's like psychologically, there's a lot of work just technically, but also psychologically.

01:50:08,827 --> 01:50:18,430
SPEAKER_0:  You have to really, it's 24 seven, you're inside that world. Like you have to know what's going on and what's happening. You have to remember who you are when you're...

01:50:18,722 --> 01:50:19,966
SPEAKER_0:  is you're a criminal online.

01:50:20,930 --> 01:50:22,366
SPEAKER_0:  You have to go to a special school for it too.

01:50:22,850 --> 01:50:24,510
SPEAKER_1:  Was that ever something compelling to you?

01:50:25,026 --> 01:50:26,494
SPEAKER_0:  I went through the school, but...

01:50:27,330 --> 01:50:35,198
SPEAKER_0:  I'm a pretty open and honest guy, and so it's tough for me to build that wall of lies. Maybe I'm just not smart enough to keep all the lies straight.

01:50:35,426 --> 01:50:40,414
SPEAKER_1:  Yeah, but a guy who's good at building up a wall of lies would say that exact same thing. Exactly. It's so annoying.

01:50:40,738 --> 01:50:43,390
SPEAKER_1:  the way truth works in this world. It's like a-

01:50:43,906 --> 01:50:47,070
SPEAKER_1:  People have told me that because I'm trying to be honest and transparent

01:50:47,362 --> 01:50:49,438
SPEAKER_1:  That's exactly what an agent would do.

01:50:49,666 --> 01:50:50,174
SPEAKER_1:  Right?

01:50:51,074 --> 01:50:51,582
SPEAKER_1:  Um...

01:50:51,970 --> 01:50:53,886
SPEAKER_1:  but I feel like an agent would not wear a suit and tie.

01:50:55,106 --> 01:50:56,606
SPEAKER_0:  I wear a suit and tie every day.

01:50:57,570 --> 01:51:07,614
SPEAKER_0:  I was a suit and tie guy. You were? Yeah, every day. I remember one time I wore shorts in and the SAC came in and this was when I was a rock star at the time in the bureau and I had shorts in and...

01:51:07,906 --> 01:51:13,758
SPEAKER_0:  I said, sorry ma'am, I apologize for my attire. And she goes, you could wear bike shorts in here, I wouldn't care. Oh shit.

01:51:14,210 --> 01:51:14,942
SPEAKER_0:  That sounds nice.

01:51:16,418 --> 01:51:18,846
SPEAKER_0:  I never wore the bike shorts, but. Yeah.

01:51:19,362 --> 01:51:26,487
SPEAKER_1:  But I see a suit and tie is constraining. I think it's liberating in sorts. It shows that you're taking the moment seriously.

01:51:26,487 --> 01:51:41,790
SPEAKER_0:  Well, not just that, people wanted it. I mean, people expected when you're not, you are dressed like the perfect FBI agent. When someone knocks on their door, that's what they want to see. They want to see what Hollywood built up is what an FBI agent is. You show up like my friend, Dohwan. He was dressed always in t-shirts and shorts.

01:51:42,306 --> 01:51:44,931
SPEAKER_0:  People aren't gonna take him serious, they're not gonna give him what they want. I wanna ho-

01:51:44,931 --> 01:51:50,270
SPEAKER_1:  any place I can just show up and like say I'm from the FBI and start interrogating them. I cut a bar.

01:51:50,434 --> 01:51:55,902
SPEAKER_0:  Probably. I count. Oh definitely if they've had a few drinks, you can definitely. Well, but people are gonna recognize you. That's the only problem.

01:51:56,482 --> 01:52:01,470
SPEAKER_0:  That's another thing. You start taking out big cases. You can't work cases anymore in the FBI. Your face gets out there.

01:52:01,922 --> 01:52:03,326
SPEAKER_1:  Your name too.

01:52:05,346 --> 01:52:10,302
SPEAKER_1:  Well, actually, let me ask you about that before we return to our friend Sabu. Okay.

01:52:10,914 --> 01:52:11,518
SPEAKER_1:  You f-

01:52:11,746 --> 01:52:13,502
SPEAKER_1:  You've tracked an-

01:52:14,658 --> 01:52:15,422
SPEAKER_1:  worked on.

01:52:15,842 --> 01:52:17,854
SPEAKER_1:  some of the most dangerous people in this world.

01:52:18,850 --> 01:52:19,326
SPEAKER_1:  Um...

01:52:19,874 --> 01:52:21,342
SPEAKER_0:  Have you ever feared for your life?

01:52:22,114 --> 01:52:25,726
SPEAKER_0:  So I had to make a really, really shitty phone call one time.

01:52:26,402 --> 01:52:30,494
SPEAKER_0:  Um, I was sitting in the bureau and this was right after silk road. Uh...

01:52:30,818 --> 01:52:31,454
SPEAKER_0:  End.

01:52:31,906 --> 01:52:33,726
SPEAKER_0:  Jared called me, he was back in Chicago.

01:52:34,210 --> 01:52:35,646
SPEAKER_0:  And he called me and said, hey.

01:52:36,098 --> 01:52:41,758
SPEAKER_0:  Your name and your kid's name are on a website for an assassination. They're paying to have you guys killed.

01:52:42,562 --> 01:52:46,366
SPEAKER_0:  Now these things happen on the black market. They come up, you know, and-

01:52:46,594 --> 01:52:49,246
SPEAKER_0:  when you know people debate whether they're real or not

01:52:49,890 --> 01:52:55,230
SPEAKER_0:  We have to take it serious. Someone's paying to have me killed in my... So I had to call my wife and we have a word.

01:52:55,586 --> 01:52:56,062
SPEAKER_0:  Um...

01:52:56,354 --> 01:52:59,550
SPEAKER_0:  in that if I said this word and we only said it one time to each other.

01:52:59,874 --> 01:53:03,486
SPEAKER_0:  If I said this is serious, drop what you're doing and get to the kids.

01:53:03,970 --> 01:53:04,446
SPEAKER_0:  Um...

01:53:04,674 --> 01:53:06,718
SPEAKER_0:  And so I had to drop the word to her.

01:53:07,394 --> 01:53:07,902
SPEAKER_0:  Um...

01:53:08,162 --> 01:53:08,990
SPEAKER_0:  And...

01:53:09,954 --> 01:53:15,518
SPEAKER_0:  I could feel the breath come out of her because she thought her kids were in danger. At the time they were.

01:53:16,258 --> 01:53:16,798
SPEAKER_0:  Um...

01:53:17,858 --> 01:53:25,822
SPEAKER_0:  I wasn't in a state of mind to drive myself, so an agent on the squad and a girl named Evelina, she drove me. Lights and sirens all the way to my kid's school.

01:53:26,402 --> 01:53:26,942
SPEAKER_0:  Um...

01:53:27,266 --> 01:53:28,062
SPEAKER_0:  End.

01:53:28,866 --> 01:53:31,038
SPEAKER_0:  We had locked, I called the school, we were in a lockdown.

01:53:31,490 --> 01:53:32,062
SPEAKER_0:  Um...

01:53:32,418 --> 01:53:35,038
SPEAKER_0:  Nobody should get in or out, especially someone with a gun.

01:53:35,522 --> 01:53:38,046
SPEAKER_0:  The first thing they did was let me in the building with a gun.

01:53:38,690 --> 01:53:40,158
SPEAKER_0:  So I was a little disappointed with that.

01:53:40,770 --> 01:53:47,710
SPEAKER_0:  Um, my kids were, I think kindergarten and fifth grade or somewhere around there. Maybe they're closer second year. I'm not sure where.

01:53:47,938 --> 01:53:48,350
SPEAKER_0:  Um...

01:53:48,706 --> 01:53:53,694
SPEAKER_0:  But all hell broke loose and we had to, from there, go move into a safe house.

01:53:54,146 --> 01:53:57,374
SPEAKER_0:  I live in New York City, NYPD surrounded my house.

01:53:57,730 --> 01:54:02,078
SPEAKER_0:  The FBI put cameras outside my house. You couldn't drive in my neighborhood without like a-

01:54:02,306 --> 01:54:03,582
SPEAKER_0:  your license plate being red.

01:54:03,906 --> 01:54:05,982
SPEAKER_0:  Hey, why is this person here? Why is that person there?

01:54:06,818 --> 01:54:09,310
SPEAKER_0:  I got to watch my house on an iPad while I sat at my desk.

01:54:09,858 --> 01:54:10,302
SPEAKER_0:  Um...

01:54:10,594 --> 01:54:11,806
SPEAKER_0:  But, you know, again.

01:54:12,162 --> 01:54:13,374
SPEAKER_0:  I put my family through that.

01:54:13,666 --> 01:54:15,646
SPEAKER_0:  and it scared the shit out of him.

01:54:15,938 --> 01:54:17,950
SPEAKER_0:  And that's, to be honest, I think that's.

01:54:18,274 --> 01:54:19,294
SPEAKER_0:  Sort of, uh...

01:54:19,810 --> 01:54:21,598
SPEAKER_0:  My mother-in-law's words were.

01:54:21,826 --> 01:54:22,878
SPEAKER_0:  I thought you did cyber crime.

01:54:23,906 --> 01:54:24,734
SPEAKER_0: ...

01:54:25,986 --> 01:54:34,718
SPEAKER_0:  And because during Silk Road, I didn't tell my family what I was working on. I don't know. I'll talk about this. So I want to escape that. I don't want to be there. You know, I remember that like, so when I was in the FBI, like.

01:54:35,170 --> 01:54:45,982
SPEAKER_0:  Driving in, I used to go in at 4.30 every morning because I used to go to the gym before I go to the desk. I'd be at the desk at seven, so in the gym at five, I'd go in for a couple hours and then go.

01:54:47,458 --> 01:54:47,934
SPEAKER_0:  The-

01:54:48,194 --> 01:54:54,942
SPEAKER_0:  Best time I had was that drive in in the morning where I could just be myself. I listened to a sports podcast out of DC.

01:54:55,170 --> 01:55:05,086
SPEAKER_0:  Um, and I, I, I, we talked about sports and, you know, the nationals and whatever it was, the capitals, you know, it was great to not think about Silk Road for 10 minutes.

01:55:05,858 --> 01:55:11,262
SPEAKER_0:  So, but that was my best time, but, but yet again, so yeah, I've had that move into the safe house.

01:55:11,554 --> 01:55:15,294
SPEAKER_0:  I left my MP5 at home. That's the bureau's machine gun.

01:55:15,810 --> 01:55:17,438
SPEAKER_0:  showed my wife to

01:55:18,370 --> 01:55:19,646
SPEAKER_0:  Pull and spray.

01:55:20,098 --> 01:55:21,223
SPEAKER_0:  So, uh...

01:55:21,223 --> 01:55:22,590
SPEAKER_1:  How often did you live?

01:55:23,778 --> 01:55:25,246
SPEAKER_1:  Work and live with fear.

01:55:25,698 --> 01:55:26,270
SPEAKER_1:  in your heart.

01:55:26,434 --> 01:55:33,758
SPEAKER_0:  It was only that time. I mean, in my, for actual physical security, um, then, I mean, after the anonymous stuff, I, you know, I really.

01:55:34,274 --> 01:55:36,254
SPEAKER_0:  tightened down to my cyber security.

01:55:36,834 --> 01:55:37,534
SPEAKER_0:  Umm...

01:55:37,954 --> 01:55:41,470
SPEAKER_0:  I don't have social media, I don't have pictures of me and my kids online.

01:55:42,146 --> 01:55:47,774
SPEAKER_0:  If I go to a wedding or something, I say, I don't take my picture with my kids, if you're going to post it someplace or something like that.

01:55:48,322 --> 01:55:50,110
SPEAKER_0:  So that's sort of security I have.

01:55:50,562 --> 01:55:50,942
SPEAKER_0:  Um...

01:55:51,618 --> 01:55:55,646
SPEAKER_0:  You know, just like everybody, you start to relax a little bit.

01:55:56,194 --> 01:55:57,886
SPEAKER_0:  security breaks down because it's not convenient.

01:55:59,714 --> 01:56:06,782
SPEAKER_1:  but it's also part of your job. So you're much better at, you know, you might need your job now and your job before.

01:56:07,010 --> 01:56:10,110
SPEAKER_1:  So you're probably much better taking care of the low hanging fruit at least.

01:56:11,522 --> 01:56:17,310
SPEAKER_0:  I understand the threat and I think that's what a lot of people don't understand is understanding what the threat against them is.

01:56:17,922 --> 01:56:19,934
SPEAKER_0:  So I'm aware of that.

01:56:20,290 --> 01:56:23,166
SPEAKER_0:  Possibly, and I think about it, you know, I think about things.

01:56:23,394 --> 01:56:27,870
SPEAKER_0:  I do remember, so you tripped a memory in my mind.

01:56:28,354 --> 01:56:32,574
SPEAKER_0:  I remember a lot of times that I had a gun on my hip. I still carry a gun to this day.

01:56:33,090 --> 01:56:36,030
SPEAKER_0:  opening my front door and being concerned what was on the other side.

01:56:36,866 --> 01:56:38,302
SPEAKER_0:  leave walking out of the house.

01:56:38,594 --> 01:56:39,582
SPEAKER_0:  Yeah.ï¿½ï¿½

01:56:39,906 --> 01:56:42,270
SPEAKER_0:  I remember those four o' clocks heading to the car.

01:56:43,458 --> 01:56:45,886
SPEAKER_0:  I was literally scared.

01:56:48,802 --> 01:56:52,926
SPEAKER_1:  I mean, having seen some of the things you've seen, it makes you perhaps question.

01:56:54,530 --> 01:56:56,222
SPEAKER_1:  how much evil there is out there in the world.

01:56:56,738 --> 01:56:58,590
SPEAKER_1:  how many dangerous people are there out there.

01:57:00,226 --> 01:57:01,342
SPEAKER_1:  Crazy people even.

01:57:02,594 --> 01:57:06,622
SPEAKER_0:  If there's a lot of crazy, there's a lot of evil, most people I think.

01:57:07,010 --> 01:57:09,854
SPEAKER_0:  get into like cyber crime or just opportunistic.

01:57:10,722 --> 01:57:13,950
SPEAKER_0:  Not necessarily evil. They don't really know, maybe think about the victim.

01:57:14,562 --> 01:57:17,086
SPEAKER_0:  It's a crime of opportunity.

01:57:18,018 --> 01:57:19,518
SPEAKER_0:  I don't label that as evil.

01:57:20,098 --> 01:57:21,438
SPEAKER_1:  And one of the things about.

01:57:21,794 --> 01:57:23,934
SPEAKER_1:  America that I'm also very happy about

01:57:25,122 --> 01:57:28,958
SPEAKER_1:  is that rule of law, despite everything we talk about, there is.

01:57:29,602 --> 01:57:31,678
SPEAKER_1:  It's tough to be a criminal in the United States.

01:57:32,898 --> 01:57:35,902
SPEAKER_1:  So like if you walk outside your house.

01:57:36,322 --> 01:57:39,294
SPEAKER_1:  you're much safer than you are in most other places in the world.

01:57:40,258 --> 01:57:44,062
SPEAKER_0:  you're safer and the system's tougher. I mean

01:57:44,482 --> 01:57:48,414
SPEAKER_0:  Uh, LulzSec, six guys, one guy in the United States, five guys other places.

01:57:48,706 --> 01:57:49,182
SPEAKER_0:  Um...

01:57:49,666 --> 01:57:51,678
SPEAKER_0:  Hector was faced in 125 years.

01:57:52,034 --> 01:57:54,366
SPEAKER_0:  Those guys got slapped on the wrist and went back to college.

01:57:55,650 --> 01:57:57,246
SPEAKER_0:  You know, different laws, different places.

01:57:58,402 --> 01:57:59,294
SPEAKER_1:  So who's after?

01:58:00,578 --> 01:58:01,726
SPEAKER_1:  Tell me the story of Hector.

01:58:01,986 --> 01:58:05,758
SPEAKER_1:  So this law cycle organization was started. So heck, there was before that in a

01:58:06,658 --> 01:58:12,670
SPEAKER_1:  He was, he was in part anonymous. He was all, he was doing all kinds of hacking stuff, but then he launched a lawsuit.

01:58:13,058 --> 01:58:19,134
SPEAKER_0:  He's old school hacker. I mean, he, he, he learned how to hack and I don't want to tell his story, but he, he learned to hack.

01:58:19,394 --> 01:58:22,686
SPEAKER_0:  because he grew up in the lower east side of New York and uh...

01:58:23,138 --> 01:58:23,710
SPEAKER_0:  end.

01:58:24,002 --> 01:58:28,190
SPEAKER_0:  picked up some NYPD computers that were left on the sidewalk for trash.

01:58:28,514 --> 01:58:34,398
SPEAKER_1:  taught himself how to. He doesn't exactly look like a hacker. For people who don't know, he looks, I don't know exactly what he looks like, but not.

01:58:34,850 --> 01:58:36,670
SPEAKER_1:  Not a, not like a technical...

01:58:36,962 --> 01:58:38,110
SPEAKER_1:  Not what you would imagine.

01:58:38,882 --> 01:58:42,238
SPEAKER_1:  But perhaps that's a Hollywood portrayal.

01:58:42,530 --> 01:58:44,830
SPEAKER_0:  Yeah, I think you get in trouble these days saying that, uh...

01:58:45,346 --> 01:58:46,910
SPEAKER_0:  that what a hacker looks like.

01:58:47,586 --> 01:58:54,974
SPEAKER_0:  I don't know if they have a traditional look. Just like I said, Hollywood has an idea, an FBI looks like. I don't think you can do that anymore. I don't think you can say that anymore.

01:58:55,618 --> 01:58:59,198
SPEAKER_1:  Well, he certainly has a big personality and charisma and all that kind of stuff.

01:59:00,034 --> 01:59:00,798
SPEAKER_1:  That's Sabu.

01:59:01,058 --> 01:59:04,094
SPEAKER_1:  I can see him selling me anything.

01:59:04,322 --> 01:59:06,197
SPEAKER_1:  That's taboo. That's the-

01:59:06,197 --> 01:59:22,078
SPEAKER_0:  You know, there are two different people. There's Sabu and there's Hector. Hector is a sweet guy. He likes to have intellectual conversations and that's just the thing. He'd rather just sit there and have a one-on-one conversation with you. But Sabu, that's a ruthless motherfucker.

01:59:22,626 --> 01:59:23,902
SPEAKER_0:  and you first met Sabu.

01:59:24,866 --> 01:59:27,422
SPEAKER_0:  was tracking Sabu. That's all I knew was Sabu.

01:59:27,842 --> 01:59:28,967
SPEAKER_0:  I didn't know Hector. I didn't know Hector.

01:59:28,967 --> 01:59:31,422
SPEAKER_1:  when did your paths cross in terms of tracking?

01:59:32,386 --> 01:59:34,174
SPEAKER_1:  When did you first take on the case?

01:59:34,434 --> 01:59:36,222
SPEAKER_0:  the spring of a eleven.

01:59:36,962 --> 01:59:40,670
SPEAKER_0:  So it was through anonymous. Really kind of a little sec.

01:59:41,186 --> 01:59:41,662
SPEAKER_0:  Um...

01:59:42,562 --> 01:59:52,446
SPEAKER_0:  We were, LulzSec was a big thing and it was pushed out to all the cyber, you know, 56 field offices in the FBI. Most of them have cyber squads or cyber units.

01:59:52,866 --> 01:59:53,694
SPEAKER_0:  And so.

01:59:54,114 --> 02:00:01,854
SPEAKER_0:  You know, it was being pushed out there and it was in the news every day, but it really wasn't ours. So we didn't have a lot of victims in our AOR area of responsibility.

02:00:02,146 --> 02:00:04,606
SPEAKER_0:  Um, and so we just kind of pay attention to it.

02:00:04,898 --> 02:00:09,150
SPEAKER_0:  Then I got a tip that a local hacker in New York had broken into AOL.

02:00:09,410 --> 02:00:10,142
SPEAKER_0:  And so...

02:00:10,850 --> 02:00:11,358
SPEAKER_0:  Olivia.

02:00:11,778 --> 02:00:16,414
SPEAKER_0:  Olivia Olsen and I, she's another agent who she's still in. She's a supervisor out in LA. She's a great agent.

02:00:17,538 --> 02:00:19,358
SPEAKER_0:  And we went all around New York looking for this kid.

02:00:19,778 --> 02:00:21,246
SPEAKER_0:  Just to see what we can find

02:00:21,666 --> 02:00:28,766
SPEAKER_0:  and ended up out in Staten Island at his grandmother's house. She didn't know where he was obviously. Why would she?

02:00:28,994 --> 02:00:30,270
SPEAKER_0:  But I left my card.

02:00:31,106 --> 02:00:33,758
SPEAKER_0:  Um, he gave me a call that night and started talking to me.

02:00:33,986 --> 02:00:34,974
SPEAKER_0:  And I said it.

02:00:35,330 --> 02:00:38,238
SPEAKER_0:  Let's just meet up tomorrow at the McDonald's across from 26 Fed.

02:00:38,882 --> 02:00:43,678
SPEAKER_0:  And he came in and three of us sat there and talked and you know.

02:00:44,610 --> 02:00:48,702
SPEAKER_0:  gave me a stuff, he started telling me about all the felonies he was committing those days, including that.

02:00:48,994 --> 02:00:50,014
SPEAKER_0:  Breaking day well.

02:00:50,338 --> 02:00:53,534
SPEAKER_0:  Um, and then he finally says, you know, you know, I can give you Sabu.

02:00:54,594 --> 02:00:56,574
SPEAKER_0:  Sabu to us was the Kaiser Sozei Vakon.

02:00:56,930 --> 02:00:57,662
SPEAKER_0:  He was our guy.

02:00:57,890 --> 02:01:00,734
SPEAKER_0:  uh... who's the guy that was in the news that was pissing us off

02:01:01,506 --> 02:01:04,190
SPEAKER_1:  So he was part of the FBI Fridays.

02:01:04,770 --> 02:01:08,766
SPEAKER_0:  Sabu was, yeah, oh, he led it. Yeah, he was the leader of Fuck FBI Fridays.

02:01:09,186 --> 02:01:09,886
SPEAKER_0:  So yeah.

02:01:11,490 --> 02:01:13,118
SPEAKER_1:  What was one of the more memorable?

02:01:13,570 --> 02:01:15,550
SPEAKER_1:  F, the triple F's.

02:01:17,634 --> 02:01:22,270
SPEAKER_1:  I said, how do you get, how and why do you go after the beehive?

02:01:22,818 --> 02:01:23,943
SPEAKER_1:  That's kind of intense.

02:01:23,943 --> 02:01:29,182
SPEAKER_0:  You get you on the news, it gets you, it's the, it's the, the lulz. It's, it's funnier to go after the big ones.

02:01:29,410 --> 02:01:39,774
SPEAKER_0:  And they weren't getting like real FBI, they weren't breaking into FBI mainframes or anything, but they were affiliate sites or anything that have to, a lot of law enforcement stuff.

02:01:40,162 --> 02:01:41,278
SPEAKER_0:  was coming out so.

02:01:42,050 --> 02:01:42,462
SPEAKER_0:  but.

02:01:42,882 --> 02:01:43,262
SPEAKER_0:  You know.

02:01:44,706 --> 02:01:51,198
SPEAKER_0:  We looked back and so if this kid knew that Sabu, maybe there was a chance we used him to lure Sabu out.

02:01:51,650 --> 02:01:55,774
SPEAKER_0:  But we also said, well, maybe this could no sub in real life. And so we went and looked through the IPs.

02:01:56,066 --> 02:01:58,910
SPEAKER_0:  10 million IPs, we find one and it belonged to him.

02:02:00,098 --> 02:02:01,822
SPEAKER_0:  That day, Sabu...

02:02:02,242 --> 02:02:32,254
SPEAKER_0:  Someone had doxed Sabu and we were a little afraid he was going to be on the run. We had a surveillance team and FBI surveillance teams are awesome. Like you cannot even tell they're FBI agents. It's, it's, it's, they are really that good. I mean, there's baby strollers and all, whatever you wouldn't expect an FBI agent to have. So that's a little like the movies. A little bit. Yeah. I mean, it is true, but, and, but they fit into the area. So now they're on the Lower East Side, which is, you know, you know, baby stroll, it might not fit in there as well. So, you know, somebody just laying on the ground or something like that.

02:02:32,706 --> 02:02:33,054
SPEAKER_0:  Um.

02:02:33,314 --> 02:02:35,582
SPEAKER_0:  they really get to play the character and get into it.

02:02:36,130 --> 02:02:38,005
SPEAKER_1:  So now I can never trust a big.

02:02:38,005 --> 02:02:39,038
SPEAKER_0:  baby stroller.

02:02:40,066 --> 02:02:41,767
SPEAKER_0:  Probably shouldn't. Every baby

02:02:41,767 --> 02:02:42,517
SPEAKER_1:  I was just like, look at.

02:02:42,517 --> 02:02:47,017
SPEAKER_0:  stare at them suspiciously. Especially if the moms were in cargo pants while she pushes it.

02:02:47,017 --> 02:02:48,350
SPEAKER_1:  It's like a verse.

02:02:48,866 --> 02:02:53,699
SPEAKER_1:  stereotypical mom, stereotypical baby. I'm gonna be very suspicious. I'm gonna question the baby.

02:02:53,699 --> 02:02:56,382
SPEAKER_0:  That baby's wired, be careful.

02:02:56,962 --> 02:03:06,494
SPEAKER_0:  You know, we raced out there and like our squad's not even full. There's only a few guys there. And like I said, I was a suit guy, but that day I had shorts and a T-shirt on. I had a white T-shirt on.

02:03:06,786 --> 02:03:12,350
SPEAKER_0:  And I only bring it up because Seb who makes fun of me to this day So I had a bulletproof vest and a white t-shirt on and that was it.

02:03:13,026 --> 02:03:14,462
SPEAKER_0:  shorts too and all that but

02:03:14,690 --> 02:03:19,966
SPEAKER_0:  Raced over to there. We didn't have any equipment. We brought our boss's boss's boss.

02:03:20,450 --> 02:03:25,790
SPEAKER_0:  He stopped off at NYPD, got us like a ballistic shield and a battery and RAM if we needed it.

02:03:26,594 --> 02:03:31,582
SPEAKER_0:  Um, and then we get to Hector's house, Sabu's house, and he's on the sixth floor.

02:03:32,066 --> 02:03:37,502
SPEAKER_0:  Um, and so normally, you know, we're the, the cyber dork squad. Um, we'll hop in the elevator.

02:03:37,922 --> 02:03:41,630
SPEAKER_0:  Six floors is a long ways to go up and bulletproof vest and a ballistic shield, but but

02:03:42,306 --> 02:03:45,598
SPEAKER_0:  We had been caught in an elevator before on a search, so we did.

02:03:45,890 --> 02:03:46,750
SPEAKER_0:  Check the stairs.

02:03:47,298 --> 02:03:47,966
SPEAKER_0:  of

02:03:49,122 --> 02:03:55,934
SPEAKER_0:  We get to the top a tad winded, but knocking the door and this big towering guy opens the door just slightly.

02:03:56,450 --> 02:04:00,318
SPEAKER_0:  and he sees the green vest with big yellow letters, FBI.

02:04:00,642 --> 02:04:06,142
SPEAKER_0:  And he steps outside. Can I help you? You know, tries to social engineer us.

02:04:06,626 --> 02:04:08,606
SPEAKER_0:  But eventually we get our way inside the house.

02:04:09,122 --> 02:04:09,854
SPEAKER_0:  Um...

02:04:10,626 --> 02:04:15,454
SPEAKER_0:  I noticed a few things that are out of place. There's a laptop charger.

02:04:15,778 --> 02:04:17,022
SPEAKER_0:  and a flashing modem.

02:04:17,378 --> 02:04:20,350
SPEAKER_0:  And I said, well, do you have a computer here? And he says, no, there's no computer here.

02:04:20,994 --> 02:04:21,374
SPEAKER_0:  Um.

02:04:21,698 --> 02:04:31,166
SPEAKER_0:  So we knew the truce and then the half lies and all that sort of thing. So it took us about another two hours and finally he gave up. He was Sabu, he was the guy we were looking for.

02:04:31,650 --> 02:04:35,198
SPEAKER_0:  So we sat there and we kind of showed him sort of the evidence we had against him

02:04:35,810 --> 02:04:36,382
SPEAKER_0:  Um...

02:04:36,674 --> 02:04:39,838
SPEAKER_0:  And, you know, from his words, we sat there and talked.

02:04:40,514 --> 02:04:45,950
SPEAKER_0:  talked to two grown adults and I gave him the options and he said well, let's uh...

02:04:46,626 --> 02:04:47,870
SPEAKER_0:  Let's talk about working together.

02:04:48,994 --> 02:04:49,566
SPEAKER_1:  So...

02:04:49,858 --> 02:04:51,646
SPEAKER_1:  he chose to become an informant.

02:04:52,802 --> 02:04:55,582
SPEAKER_0:  I don't think he chose that night, but that's where it kind of went to.

02:04:55,842 --> 02:04:59,518
SPEAKER_0:  Um, so the, the, we brought them down to the FBI that night.

02:04:59,970 --> 02:05:03,806
SPEAKER_0:  Um, which was, it was a funny trip because I'm sitting in the backseat of the car with him.

02:05:04,354 --> 02:05:07,678
SPEAKER_0:  Um, and I was getting calls from all over, uh,

02:05:07,970 --> 02:05:10,910
SPEAKER_0:  The US from different FBI agents saying that we arrested the wrong guy.

02:05:11,842 --> 02:05:12,350
SPEAKER_0:  I was like...

02:05:12,578 --> 02:05:15,518
SPEAKER_0:  I don't think so. And they're like, why do you think so? Because he says it's him.

02:05:16,258 --> 02:05:18,206
SPEAKER_0:  And they still said not the wrong guy

02:05:18,594 --> 02:05:20,446
SPEAKER_0:  So I said, well, we'll see how it plays out.

02:05:20,930 --> 02:05:22,055
SPEAKER_0:  That's interesting because it's...

02:05:22,055 --> 02:05:23,934
SPEAKER_1:  This is such a strange world.

02:05:24,482 --> 02:05:27,198
SPEAKER_1:  Such a strange world, because it's tough to...

02:05:27,714 --> 02:05:29,694
SPEAKER_1:  because you still have to prove it's the same guy, right?

02:05:30,722 --> 02:05:31,774
SPEAKER_1:  because the anonymity.

02:05:32,162 --> 02:05:37,150
SPEAKER_0:  Yeah, I mean we had his laptop by that point. Yeah, I know. I mean, that helped.

02:05:37,378 --> 02:05:38,718
SPEAKER_0:  I'm getting a clue in my world.

02:05:40,802 --> 02:05:41,566
SPEAKER_1:  Yeah.

02:05:41,762 --> 02:05:47,390
SPEAKER_0:  Yeah. But yeah, if he would have fought it, I mean, that definitely would have come in as evidence that ever if your agents are saying it's not him.

02:05:47,714 --> 02:05:49,589
SPEAKER_0:  You have to disclose that stuff.

02:05:49,589 --> 02:05:50,430
SPEAKER_1:  stuff on him.

02:05:51,330 --> 02:05:54,238
SPEAKER_1:  What was he facing?

02:05:54,626 --> 02:05:57,726
SPEAKER_1:  He was faced 125 years. 125 years in prison.

02:05:57,858 --> 02:06:03,006
SPEAKER_0:  That's that. Now that's if you took every charge we had against him and put them, you know, uh.

02:06:03,266 --> 02:06:04,030
SPEAKER_0:  consecutively.

02:06:04,482 --> 02:06:06,782
SPEAKER_0:  No, no one ever gets charged that, but yeah.

02:06:07,042 --> 02:06:13,406
SPEAKER_0:  Essentially, it would have been on her 25 years. You know, fast forward to the end, he got thanked by the judge for his service after nine months.

02:06:14,274 --> 02:06:15,742
SPEAKER_0:  and he walked out of the court of free bed.

02:06:17,186 --> 02:06:17,950
SPEAKER_1:  But that's...

02:06:18,178 --> 02:06:20,158
SPEAKER_1:  while being an informant.

02:06:22,338 --> 02:06:22,910
SPEAKER_0:  I don't know.

02:06:23,394 --> 02:06:23,710
SPEAKER_0:  So.

02:06:24,770 --> 02:06:26,846
SPEAKER_0:  The word informant here really isn't

02:06:27,778 --> 02:06:28,542
SPEAKER_0:  that good?

02:06:28,834 --> 02:06:30,302
SPEAKER_0:  It's not fitting that.

02:06:30,914 --> 02:06:32,766
SPEAKER_0:  Technically, I guess that's what he was.

02:06:33,890 --> 02:06:35,102
SPEAKER_0:  He didn't know the other people.

02:06:35,746 --> 02:06:37,374
SPEAKER_0:  It was all in our- he knew Knicks and all that.

02:06:38,050 --> 02:06:42,398
SPEAKER_0:  Um, he really gave us the insight of what was happening in the hacker world.

02:06:43,010 --> 02:06:50,686
SPEAKER_0:  Like I said, he was an old school hacker. He was back when hackers didn't work together with Anonymous. He was down, you know, Cult of Dead Cow and those type guys, like way back.

02:06:51,010 --> 02:06:53,950
SPEAKER_0:  uh... he was around for that he's a good cycle pd packing

02:06:55,138 --> 02:06:57,534
SPEAKER_0:  You know, we just. Kiss Prime was in the nineties.

02:06:58,466 --> 02:07:05,374
SPEAKER_0:  for terror hack, but yeah, he kind of came back when, when anonymous started going after MasterCard and PayPal and all that do the WikiLeak stuff.

02:07:06,306 --> 02:07:11,230
SPEAKER_1:  But even that little interaction, being an informant, he probably made a lot of enemies.

02:07:11,522 --> 02:07:13,342
SPEAKER_1:  How do you protect a guy like that?

02:07:13,986 --> 02:07:16,062
SPEAKER_0:  He made enemies after it was revealed? Yeah

02:07:17,506 --> 02:07:19,646
SPEAKER_0:  How does the FBI protect him? Yeah. Good luck.

02:07:22,082 --> 02:07:22,974
SPEAKER_1:  I mean...

02:07:23,202 --> 02:07:25,374
SPEAKER_1:  Perhaps I'll talk to him one day.

02:07:25,826 --> 02:07:27,934
SPEAKER_1:  Uh, is that guy afraid for his life?

02:07:28,738 --> 02:07:34,558
SPEAKER_0:  I, I, again, I think it doesn't seem like it. He has very good security for himself. Cybersecurity.

02:07:35,202 --> 02:07:35,902
SPEAKER_0:  Um...

02:07:36,226 --> 02:07:40,414
SPEAKER_0:  But, you know, he doesn't like the negative things said about him online.

02:07:40,738 --> 02:07:41,918
SPEAKER_0:  I don't think anybody does.

02:07:42,338 --> 02:07:42,846
SPEAKER_0:  Um...

02:07:43,522 --> 02:07:46,078
SPEAKER_0:  You know, I think it's so many years of the internet.

02:07:48,354 --> 02:07:51,838
SPEAKER_0:  bitching at you and all that, you get calloused to it's just internet bitching.

02:07:52,642 --> 02:07:55,422
SPEAKER_1:  and also the hacking world moves on very quickly.

02:07:55,938 --> 02:07:57,534
SPEAKER_1:  He is kind of, uh...

02:07:57,794 --> 02:08:01,118
SPEAKER_1:  They have their own wars to fight now.

02:08:01,602 --> 02:08:03,614
SPEAKER_1:  and he's not part of those wars anymore.

02:08:03,810 --> 02:08:08,510
SPEAKER_0:  There's still people out there that bitching moan about him, but yeah, I think it's less.

02:08:09,122 --> 02:08:13,022
SPEAKER_0:  I think, you know, he has a good message out there of, you know...

02:08:13,922 --> 02:08:15,070
SPEAKER_0:  EEEEEE

02:08:15,522 --> 02:08:17,790
SPEAKER_0:  trying to keep kids from making the same mistakes he made.

02:08:18,562 --> 02:08:20,126
SPEAKER_0:  He tries to really preach that.

02:08:22,978 --> 02:08:25,182
SPEAKER_1:  How do people get into this line of work?

02:08:26,146 --> 02:08:30,814
SPEAKER_1:  Is there all kinds of ways being not your line of work, his line of work?

02:08:31,426 --> 02:08:35,486
SPEAKER_1:  It's just all the stories you've seen of people that are anonymous.

02:08:36,002 --> 02:08:37,086
SPEAKER_1:  LOL SECOND

02:08:37,346 --> 02:08:40,126
SPEAKER_1:  Silk Road and all the cyber criminals you've interacted with.

02:08:40,802 --> 02:08:43,262
SPEAKER_1:  What's the profile of a cyber criminal?

02:08:43,906 --> 02:08:50,302
SPEAKER_0:  I don't think there's a profile anymore. You know, I used to be able to say, you know, the kid in your mom's basement or something like that, but it's not true anymore.

02:08:51,170 --> 02:08:54,014
SPEAKER_0:  You know, like it's, it's, it's wide. It's like.

02:08:54,242 --> 02:08:55,070
SPEAKER_0:  I've arrested.

02:08:56,258 --> 02:08:59,134
SPEAKER_0:  I've arrested people that you wouldn't expect would be cyber criminals.

02:09:00,482 --> 02:09:03,107
SPEAKER_1:  and it's in the United States, it's international.

02:09:03,107 --> 02:09:07,070
SPEAKER_0:  Everything else international. I mean we're seeing a lot of the big hack hackers now

02:09:07,554 --> 02:09:11,646
SPEAKER_0:  the big arrests for hackers in England. Surprisingly, you know, there's, you know.

02:09:12,418 --> 02:09:19,422
SPEAKER_0:  You're not going to see this a lot of good hackers like down in Brazil, but I don't think Brazil law enforcement is as good as hunting them down.

02:09:19,714 --> 02:09:21,150
SPEAKER_0:  so you're not gonna see the big arrests.

02:09:21,602 --> 02:09:22,238
SPEAKER_1:  How much?

02:09:22,786 --> 02:09:24,382
SPEAKER_1:  Stay sponsored.

02:09:25,410 --> 02:09:27,422
SPEAKER_1:  How cyber attacks are there, do you think?

02:09:28,386 --> 02:09:29,246
SPEAKER_0:  more than you can imagine.

02:09:30,562 --> 02:09:34,814
SPEAKER_0:  And what do you want to say an attack? A successful attack or just a probing?

02:09:35,298 --> 02:09:36,958
SPEAKER_1:  probing for information.

02:09:37,602 --> 02:09:38,270
SPEAKER_1:  Just like.

02:09:38,722 --> 02:09:39,518
SPEAKER_1:  feeling, you know.

02:09:40,130 --> 02:09:44,414
SPEAKER_1:  testing that there's where the attack factors are trying to collect.

02:09:44,610 --> 02:09:50,814
SPEAKER_0:  all the possible attack. Put a Windows 7 machine on the internet, forward face it and put a packet sniffer on there and look at where the driver comes from.

02:09:51,234 --> 02:10:01,886
SPEAKER_0:  I mean, in 24 hours, you are going to fill up a hard drive with packets just coming at it. Yeah. I mean, it's not hard to know. I mean, it's just constantly probing for entry points into things.

02:10:02,146 --> 02:10:03,806
SPEAKER_0:  You could go mad.

02:10:04,066 --> 02:10:04,894
SPEAKER_0:  put the honeypot.

02:10:05,602 --> 02:10:19,390
SPEAKER_0:  draws in intrusions to see what's out there. Yeah. And it doesn't go anywhere. It maybe has fake information and stuff like that. You know, it's just kind of to see what's going on and judge what's happening. You know, you know,

02:10:19,618 --> 02:10:22,993
SPEAKER_0:  lick your finger and test the wind of what's happening these days.

02:10:22,993 --> 02:10:23,518
SPEAKER_1:  but like.

02:10:23,874 --> 02:10:25,406
SPEAKER_1:  because I'm at MIT.

02:10:25,858 --> 02:10:30,462
SPEAKER_1:  That attracted even more attention, not for the lulls, but for the technical challenge.

02:10:31,042 --> 02:10:36,318
SPEAKER_1:  it seems like people enjoy hacking MIT. It's just the amount of traffic MIT was getting for that.

02:10:36,834 --> 02:10:43,038
SPEAKER_1:  in terms of just the sheer number of attacks from different places is crazy. Yeah, just like that, putting up a machine seeing what comes.

02:10:43,362 --> 02:10:50,206
SPEAKER_0:  NASA used to be the golden ring. Now everybody got NASA. That was like the early 90s. If you could hack NASA, that was the, now MIT is a big one.

02:10:50,306 --> 02:10:52,062
SPEAKER_1:  Yeah, it's fun. It's fun to see.

02:10:53,282 --> 02:10:54,526
SPEAKER_1:  uh... respect

02:10:55,202 --> 02:11:02,174
SPEAKER_1:  uh... cuz i think in that case it comes from a somewhat good place because that you know that i get in any way from it

02:11:02,466 --> 02:11:03,710
SPEAKER_1:  It's more for the challenge.

02:11:04,578 --> 02:11:07,902
SPEAKER_1:  Let me ask you about that, about this world of cyber security.

02:11:08,738 --> 02:11:09,246
SPEAKER_1:  Um...

02:11:09,826 --> 02:11:13,022
SPEAKER_1:  How big of a threat are cyber attacks for companies and for individuals?

02:11:14,274 --> 02:11:15,198
SPEAKER_1:  Let's layout.

02:11:15,746 --> 02:11:17,502
SPEAKER_1:  Where are we in this world?

02:11:17,954 --> 02:11:18,782
SPEAKER_1:  What's out there?

02:11:19,202 --> 02:11:20,638
SPEAKER_0:  It's the Wild Wild West.

02:11:20,930 --> 02:11:22,270
SPEAKER_0:  and it's...

02:11:23,106 --> 02:11:24,158
SPEAKER_0:  I mean...

02:11:25,762 --> 02:11:26,398
SPEAKER_0:  People.

02:11:27,234 --> 02:11:31,678
SPEAKER_0:  want the idea of security, but it's inconvenient, so they don't, they push back on it.

02:11:31,906 --> 02:11:39,614
SPEAKER_0:  And there are a lot of opportunistic nation state, financially motivated hackers, hackers for the lulz. You got three different tiers there.

02:11:39,906 --> 02:11:41,278
SPEAKER_0:  Um, and.

02:11:41,762 --> 02:11:46,686
SPEAKER_0:  They're on the prowl, they have tools, they have really good tools that are being used against us.

02:11:47,970 --> 02:11:49,246
SPEAKER_1:  and I would scale.

02:11:49,570 --> 02:11:51,518
SPEAKER_1:  So when you're thinking of...

02:11:52,834 --> 02:11:53,694
SPEAKER_1:  I don't know what's...

02:11:53,954 --> 02:11:55,518
SPEAKER_1:  Let's talk about companies first.

02:11:55,970 --> 02:11:59,678
SPEAKER_1:  So say you're talking to a mid-tier.

02:12:00,674 --> 02:12:03,134
SPEAKER_1:  I wonder what the most interesting business is.

02:12:03,522 --> 02:12:07,774
SPEAKER_1:  So Google, we can look at large tech companies or we can look at minute.

02:12:08,034 --> 02:12:08,894
SPEAKER_1:  medium size.

02:12:09,538 --> 02:12:11,742
SPEAKER_1:  tech companies and like you were sitting in a room.

02:12:12,098 --> 02:12:13,854
SPEAKER_1:  with a CTO, with a CEO.

02:12:14,658 --> 02:12:17,310
SPEAKER_1:  And the question is, how fucked are we?

02:12:17,794 --> 02:12:22,462
SPEAKER_1:  and what should we do? What's the low hanging fruit? What are the different strategies?

02:12:23,106 --> 02:12:24,254
SPEAKER_1:  those companies should consider.

02:12:24,418 --> 02:12:31,879
SPEAKER_0:  I mean the problem is they want a push button. They want an out of the box solution that I'm secure. They want to tell people they're secure.

02:12:31,879 --> 02:12:33,598
SPEAKER_1:  But that's very challenging to have.

02:12:33,826 --> 02:12:37,630
SPEAKER_0:  It's impossible. If someone had it, they'd be a billionaire.

02:12:38,274 --> 02:12:45,342
SPEAKER_0:  uh, you know, they'd be beyond a billionaire, you know, because that's what everybody wants. So, so you know, you can buy all the tools you want.

02:12:45,794 --> 02:12:50,302
SPEAKER_0:  it's configuring them the proper way. And there's, if anyone's trying to tell you that

02:12:50,690 --> 02:12:56,606
SPEAKER_0:  There's one solution that fits all. There's Stain Coil Salesman. And there's a lot of people in cybersecurity that are Stain Coil Salesmen.

02:12:57,218 --> 02:13:00,158
SPEAKER_1:  Yeah. And I feel like there's tools if they're not configured correctly.

02:13:01,186 --> 02:13:03,230
SPEAKER_1:  They just introduce.

02:13:04,130 --> 02:13:08,062
SPEAKER_1:  they don't increase security significantly and they introduce a lot of pain for the people

02:13:08,514 --> 02:13:11,454
SPEAKER_1:  they decrease efficiency of the actual work you have to do.

02:13:11,810 --> 02:13:12,414
SPEAKER_1:  It's like, uh...

02:13:12,706 --> 02:13:15,358
SPEAKER_1:  We had as a Google for time.

02:13:16,642 --> 02:13:17,246
SPEAKER_1:  and

02:13:18,818 --> 02:13:22,590
SPEAKER_1:  I think mostly they want to give props to their security efforts.

02:13:23,458 --> 02:13:24,030
SPEAKER_1:  But...

02:13:24,418 --> 02:13:25,566
SPEAKER_1:  User data.

02:13:26,178 --> 02:13:28,606
SPEAKER_1:  So like data that belongs to users.

02:13:28,898 --> 02:13:30,398
SPEAKER_1:  is like the holy...

02:13:31,426 --> 02:13:31,870
SPEAKER_1:  like

02:13:32,226 --> 02:13:34,206
SPEAKER_1:  the amount of security they have.

02:13:34,530 --> 02:13:37,982
SPEAKER_1:  around that is incredible. So most anytime.

02:13:38,242 --> 02:13:39,806
SPEAKER_1:  I had to work with...

02:13:40,418 --> 02:13:50,846
SPEAKER_1:  anything even resembling user data. So I never got a chance to work with actual user data. Anything resembling that. First of all, you have no access to the internet. It's impossible to even come close to accessing the internet.

02:13:51,074 --> 02:13:52,926
SPEAKER_1:  And there's so much pain.

02:13:53,250 --> 02:13:54,174
SPEAKER_1:  to actually.

02:13:54,498 --> 02:13:54,878
SPEAKER_1:  Like.

02:13:55,362 --> 02:13:56,606
SPEAKER_1:  interact with that data.

02:13:57,282 --> 02:14:00,510
SPEAKER_1:  where it, I mean, it was extremely inefficient.

02:14:00,930 --> 02:14:05,502
SPEAKER_1:  in places where I thought it didn't have to be that inefficient. The security was too much.

02:14:06,434 --> 02:14:11,454
SPEAKER_1:  I have to give respect to that because in that case, you want to err on the side of security. But that's Google.

02:14:11,714 --> 02:14:12,839
SPEAKER_1:  Yeah, but that...Ð½Ð¾ÑÑ‚Ð¸

02:14:12,839 --> 02:14:19,422
SPEAKER_0:  job of this. Reputational harm if it got out. I mean, Google, you know, why is Google drive free? You know, because they want.

02:14:19,746 --> 02:14:20,190
SPEAKER_0:  your data.

02:14:20,546 --> 02:14:22,142
SPEAKER_0:  They want you to park your data there, so.

02:14:23,106 --> 02:14:24,926
SPEAKER_0:  If they got hacked or...

02:14:25,282 --> 02:14:27,646
SPEAKER_0:  leaked information, the reputational harm would be.

02:14:28,034 --> 02:14:28,638
SPEAKER_0:  tremendous.

02:14:29,154 --> 02:14:30,846
SPEAKER_1:  but for a company that.

02:14:31,074 --> 02:14:31,774
SPEAKER_1:  It's not.

02:14:32,322 --> 02:14:37,150
SPEAKER_1:  It's really hard to do that, right? And the company is not as big as Google or not as tech savvy as Google.

02:14:37,858 --> 02:14:42,398
SPEAKER_1:  might have a lot of trouble with doing that kind of stuff. Instead of increasing security, you'll just...

02:14:42,786 --> 02:14:43,582
SPEAKER_1:  Decreasy.

02:14:44,194 --> 02:14:44,958
SPEAKER_1:  Efficiency.

02:14:45,794 --> 02:14:56,350
SPEAKER_0:  Well, yeah, so there's a big difference between IT and security. And unfortunately, these mid-side companies, they try to stack security into their IT department. Your IT department is about business continuity.

02:14:56,610 --> 02:14:57,054
SPEAKER_0:  Thereabouts.

02:14:57,282 --> 02:15:07,454
SPEAKER_0:  trying to move business forward. They want users to get the data they need to do their job so the company can grow. Security is not that. They don't want you to get the data. But there's...

02:15:08,162 --> 02:15:14,974
SPEAKER_0:  fine tuning you can do to ensure that. I mean, it's simple as having good onboarding procedures for employees.

02:15:15,234 --> 02:15:19,966
SPEAKER_0:  Like you come into my company, you don't need access to everything. Maybe you need access to something for one day.

02:15:20,418 --> 02:15:34,622
SPEAKER_0:  turn the access on, don't leave it on. I mean, I was the victim of the OPM hack, the Office of Personnel Management, because old credentials from a third party vendor were sitting there inactive. And the Chinese government found those credentials and were able to log in and steal all my information.

02:15:34,978 --> 02:15:35,838
SPEAKER_1:  So a lot.

02:15:36,098 --> 02:15:39,966
SPEAKER_1:  could be helped if you just control the credentials, the access, the access control.

02:15:40,354 --> 02:15:41,502
SPEAKER_1:  how long they last.

02:15:41,986 --> 02:15:43,166
SPEAKER_1:  and people who have.

02:15:43,618 --> 02:15:45,054
SPEAKER_1:  we need access to certain things.

02:15:45,602 --> 02:15:48,446
SPEAKER_1:  only get access to that thing and not nothing else and then

02:15:48,898 --> 02:15:50,023
SPEAKER_1:  and it just gets refreshed.

02:15:50,023 --> 02:15:52,190
SPEAKER_0:  the access control, you know, like we said, setting up.

02:15:52,450 --> 02:15:55,806
SPEAKER_0:  people leaving the company, get rid of their, they don't need control.

02:15:56,322 --> 02:16:07,582
SPEAKER_0:  two factor authentication, you know, that's a big thing. You know, it's, I mean, I sound like a broken record because this isn't anything new. This isn't rocket science. The problem is we're not implementing it. If we are, we're not doing it correctly.

02:16:08,066 --> 02:16:10,590
SPEAKER_0:  Because these guys are taking taking us

02:16:10,722 --> 02:16:13,438
SPEAKER_1:  Well, two factor authentication is a good example of something.

02:16:14,274 --> 02:16:14,654
SPEAKER_1:  that.

02:16:15,042 --> 02:16:17,790
SPEAKER_1:  I just was annoyed by it for the longest time.

02:16:18,114 --> 02:16:22,974
SPEAKER_1:  Because yes, it's very good, but like it's, it seems that it's pretty easy to implement.

02:16:23,522 --> 02:16:24,094
SPEAKER_1:  horribly.

02:16:24,706 --> 02:16:29,054
SPEAKER_1:  to where it's like, it's not convenient at all for the legitimate user to use.

02:16:29,314 --> 02:16:31,134
SPEAKER_1:  It should be trivial to do.

02:16:31,682 --> 02:16:32,158
SPEAKER_1:  uh

02:16:32,642 --> 02:16:35,518
SPEAKER_1:  Like to authenticate yourself twice should be super easy.

02:16:35,746 --> 02:16:38,878
SPEAKER_0:  if security is slightly inconvenient for you.

02:16:39,490 --> 02:16:44,031
SPEAKER_0:  It's thinking about how inconvenient it is for a hacker and how they're just gonna move on to the next person. Yes, yes, in theory-

02:16:44,031 --> 02:16:45,630
SPEAKER_1:  when implemented extremely well.

02:16:46,626 --> 02:16:47,038
SPEAKER_1:  but...

02:16:47,522 --> 02:16:48,862
SPEAKER_1:  I just don't...

02:16:49,282 --> 02:16:49,918
SPEAKER_1:  Think so.

02:16:50,210 --> 02:16:51,806
SPEAKER_1:  I think actually if it's...

02:16:52,578 --> 02:16:56,350
SPEAKER_1:  Inconvenient, it shows that system has been thought through a lot.

02:16:56,930 --> 02:16:58,686
SPEAKER_0:  Do you know why we need?

02:16:58,946 --> 02:16:59,870
SPEAKER_0:  two-factor authentication.

02:17:00,130 --> 02:17:15,879
SPEAKER_0:  people using the same password across the same site. So when one site is compromised, people just take that username and password, it's called credential stuffing and just stuff it across the internet. So if 10 years ago when we told everybody, don't use the same fucking password across the internet, across the vulnerable sites, maybe two factor wouldn't be.

02:17:15,879 --> 02:17:19,358
SPEAKER_1:  Yeah, so you wouldn't need to factor if everyone did.

02:17:19,650 --> 02:17:21,118
SPEAKER_1:  good job with passwords. yeah

02:17:22,050 --> 02:17:23,486
SPEAKER_1:  Right, but I'm saying like...

02:17:23,938 --> 02:17:29,566
SPEAKER_1:  The two factor authentication, it should be super easy to authenticate myself.

02:17:30,050 --> 02:17:32,862
SPEAKER_1:  uh... in some with some other device really quickly

02:17:33,122 --> 02:17:33,438
SPEAKER_1:  Like.

02:17:33,922 --> 02:17:35,678
SPEAKER_1:  there should be, it should be frictionless.

02:17:35,970 --> 02:17:39,230
SPEAKER_1:  Like you just hit, okay. Okay. And anything that belongs to me.

02:17:40,610 --> 02:17:41,950
SPEAKER_1:  And like I should, it should.

02:17:42,306 --> 02:17:45,726
SPEAKER_1:  very importantly, be easy to set up what belongs to me.

02:17:46,210 --> 02:17:48,478
SPEAKER_1:  I don't know the full complexity.

02:17:48,706 --> 02:17:51,422
SPEAKER_1:  of the cyber attacks these platforms are under.

02:17:51,650 --> 02:17:53,790
SPEAKER_1:  They're probably under insane

02:17:54,178 --> 02:17:56,030
SPEAKER_0:  You've got it right there.

02:17:56,450 --> 02:18:02,046
SPEAKER_0:  People have no idea these large companies how often they're attacked, you know on a per second basis

02:18:02,562 --> 02:18:05,150
SPEAKER_0:  and they have to fight all that off and pick out the good traffic.

02:18:05,506 --> 02:18:06,430
SPEAKER_0:  in there. So.

02:18:07,010 --> 02:18:08,094
SPEAKER_0:  Yeah, I wouldn't- I don't-

02:18:08,354 --> 02:18:10,366
SPEAKER_0:  There's no way I'd want to run a large tech company.

02:18:10,562 --> 02:18:10,910
SPEAKER_1:  Thanks for watching!

02:18:12,482 --> 02:18:14,974
SPEAKER_1:  What about protecting individuals for individuals?

02:18:15,650 --> 02:18:17,054
SPEAKER_1:  What's good advice?

02:18:17,314 --> 02:18:19,806
SPEAKER_1:  for to try to protect yourself.

02:18:20,226 --> 02:18:23,198
SPEAKER_1:  from this increasingly dangerous world of cyber attacks.

02:18:23,426 --> 02:18:30,814
SPEAKER_0:  Again, educate yourself that you understand that there is a threat. First, you have to realize that. Then you're going to step up and you're going to do stuff a little bit more.

02:18:31,074 --> 02:18:35,006
SPEAKER_0:  Sometimes I think I take that to a little bit extreme. I remember one time...

02:18:35,266 --> 02:18:40,094
SPEAKER_0:  My mom called me and she was screaming that

02:18:40,386 --> 02:18:45,598
SPEAKER_0:  I woke up this morning and I just clicked on a link and now my phone is making weird noises and I was like

02:18:46,466 --> 02:18:53,470
SPEAKER_0:  throw your phone in a glass of water. Just put it in a glass of water right now. And she's, I made my mom cry. It was not a pleasant thing.

02:18:53,794 --> 02:18:57,022
SPEAKER_0:  So sometimes I go to a little extremes on those ones, but.

02:18:58,114 --> 02:19:03,390
SPEAKER_0:  Understanding is a risk and making it a little bit more, a little more difficult to become a victim. I mean just

02:19:03,682 --> 02:19:05,854
SPEAKER_0:  understanding certain things.

02:19:06,402 --> 02:19:07,870
SPEAKER_0:  You know, simple things like.

02:19:08,354 --> 02:19:17,086
SPEAKER_0:  You know, as we add more Internet of the things to people's houses, I mean, how many Wi-Fi networks do people have? It's normally just one, and you're bumping your phones and giving your password to people to come to visit.

02:19:17,762 --> 02:19:22,110
SPEAKER_0:  Set up a guest network, set up something you can change every 30 days. Simple little things like that.

02:19:22,338 --> 02:19:26,750
SPEAKER_0:  I hate to remind you, but change your passwords. I mean, I feel like I'm a broken record again.

02:19:27,202 --> 02:19:27,582
SPEAKER_0:  but...

02:19:27,938 --> 02:19:30,430
SPEAKER_0:  Just make it more difficult for others to victimize you.

02:19:31,106 --> 02:19:31,614
SPEAKER_1:  And then.

02:19:31,906 --> 02:19:33,662
SPEAKER_1:  Don't use the same password everywhere.

02:19:34,242 --> 02:19:35,262
SPEAKER_0:  That that

02:19:35,778 --> 02:19:36,903
SPEAKER_0:  Yes, I mean.

02:19:36,903 --> 02:19:38,403
SPEAKER_1:  I still know people that do that.

02:19:38,403 --> 02:19:41,950
SPEAKER_0:  I mean, Ask.fm got popped last week, two weeks ago.

02:19:42,274 --> 02:19:47,486
SPEAKER_0:  And that's 350 million username and passwords with connected Twitter accounts, Google accounts.

02:19:47,970 --> 02:19:52,574
SPEAKER_0:  uh, you know, all the different social media accounts, you know, that is a treasure trove for the next.

02:19:53,218 --> 02:19:54,366
SPEAKER_0:  two and a half, three years.

02:19:54,818 --> 02:20:05,438
SPEAKER_0:  of just using those credentials everywhere. Using, you'll learn, even if it's not the right password, you'll learn people's password styles. Bad guys are making portfolios out of people.

02:20:05,730 --> 02:20:11,614
SPEAKER_0:  We're figuring out how people generate their passwords and kind of figuring that it's easier to crack their password.

02:20:12,482 --> 02:20:14,398
SPEAKER_0:  You know, we're making a dossier on each person.

02:20:15,074 --> 02:20:18,846
SPEAKER_0:  350 million dossiers just in that one hack. Yahoo, there was a half a billion.

02:20:19,682 --> 02:20:21,726
SPEAKER_1:  So the thing a hacker would do with that.

02:20:22,242 --> 02:20:24,734
SPEAKER_1:  is try to find all the like, lohing fruit.

02:20:25,218 --> 02:20:27,038
SPEAKER_1:  like have some kind of program that.

02:20:28,226 --> 02:20:30,238
SPEAKER_1:  Yeah, it values the strength of the passwords.

02:20:31,106 --> 02:20:33,278
SPEAKER_1:  and then finds the weak ones.

02:20:33,506 --> 02:20:36,881
SPEAKER_1:  That means that this person is probably the kind of person that would use the same password.

02:20:36,881 --> 02:20:37,790
SPEAKER_0:  across multiple.

02:20:38,338 --> 02:20:49,214
SPEAKER_0:  or even just write a program into that. Remember the ring hack a couple of years ago? That's all it was is credential stuffing. So ring the security system by default had two factor, but didn't turn it on. And they also had a

02:20:49,666 --> 02:21:03,422
SPEAKER_0:  try unlimited tries to log into my account. You can lock it out after 10 by default, not turned on. Cause it's not convenient for people. They, you know, ring, you know, it was like, I want people to stick these little things up and have security in their house. But you know, cybersecurity.

02:21:04,034 --> 02:21:06,270
SPEAKER_0:  Don't make it inconvenient, then people won't buy our product.

02:21:07,554 --> 02:21:14,334
SPEAKER_0:  That's how they got hacked. They wanted to say that it's insecure and got hacked into reputational harm right there for Ring, but they didn't. Credential stuffing.

02:21:14,562 --> 02:21:15,198
SPEAKER_0:  People bot.

02:21:15,554 --> 02:21:18,174
SPEAKER_0:  username and passwords on the black market.

02:21:19,298 --> 02:21:25,406
SPEAKER_0:  just wrote a bot that just went through Ring and used every one of them to maybe 1% hit, that's a big hit.

02:21:26,178 --> 02:21:27,230
SPEAKER_0:  the number of ring users.

02:21:27,394 --> 02:21:32,030
SPEAKER_1:  You can use also password managers to make the changing of the passwords easier.

02:21:32,770 --> 02:21:33,214
SPEAKER_0:  uh...

02:21:33,698 --> 02:21:39,166
SPEAKER_0:  And to make you can choose the difficulty, the number of special characters, the length of it and all that.

02:21:39,650 --> 02:21:42,174
SPEAKER_1:  My favorite thing is websites.

02:21:43,170 --> 02:21:47,774
SPEAKER_1:  yell at you for your password being too long or having too many special care or like

02:21:48,194 --> 02:21:52,319
SPEAKER_1:  Uh, or, uh, yay. You're not allowed to have this special characters.

02:21:52,319 --> 02:22:03,511
SPEAKER_0:  You can only use these three special characters. It's a, you know, do you understand how password cracking works? If you specifically tell me which pass, what special characters I can use.

02:22:03,511 --> 02:22:04,958
SPEAKER_1:  Like, I honestly...

02:22:05,186 --> 02:22:07,102
SPEAKER_1:  Just wanna have a one-on-one meeting.

02:22:07,618 --> 02:22:12,606
SPEAKER_1:  like late at night with the engineer that program that because that's that's like an intern

02:22:12,898 --> 02:22:14,302
SPEAKER_1:  I just wanna have a sit down meeting.

02:22:14,498 --> 02:22:19,902
SPEAKER_0:  Yeah, I made my parents switch banks once because the security was so poor. I was like, you just, you can't have money here.

02:22:20,546 --> 02:22:27,230
SPEAKER_1:  But then there's also like the zero day tax. I mentioned before the QNAP NOS that got hacked.

02:22:27,746 --> 02:22:30,622
SPEAKER_1:  Luckily I didn't have anything private on there.

02:22:31,234 --> 02:22:33,086
SPEAKER_1:  But it really woke me up.

02:22:33,634 --> 02:22:35,070
SPEAKER_1:  Okay, so like...

02:22:35,426 --> 02:22:36,551
SPEAKER_1:  if you dig everything extremely.

02:22:36,551 --> 02:22:37,214
SPEAKER_0:  seriously.

02:22:38,018 --> 02:22:48,670
SPEAKER_0:  Unfortunately for the end users, there's nothing you can do about zero day. It's, you know, there's this, you have no control over that. I mean, it's a, it's a, the engineers that made the software don't even know about it. Now let's talk about one days.

02:22:49,154 --> 02:22:56,606
SPEAKER_0:  There's a patch now out there for the security. If you're not updating your system for the security patches if it's just not on you.

02:22:57,378 --> 02:23:00,894
SPEAKER_0:  My father-in-law has such an old iPhone you can't security patch it anymore.

02:23:01,314 --> 02:23:07,742
SPEAKER_0:  So, you know, and I tell him, I said, you know, this is what you're missing out on. This is what you're exposing yourself to because

02:23:08,130 --> 02:23:09,086
SPEAKER_0:  uh... you know

02:23:09,346 --> 02:23:24,510
SPEAKER_0:  We talked about that powerful tool, how we found RossAlbrick at gmail.com. Well, bad guys are using that too. It used to be called Google dorking. Now I think it's named Google hacking by the community. You can go in and

02:23:24,898 --> 02:23:25,854
SPEAKER_0:  Find a vulnerability.

02:23:26,082 --> 02:23:30,686
SPEAKER_0:  read about the white paper, what's wrong with that software, and then you can go on the internet and find out.

02:23:31,138 --> 02:23:33,502
SPEAKER_0:  all of the computers that are running that outdated software.

02:23:34,050 --> 02:23:35,422
SPEAKER_0:  And there's your list, there's your target list.

02:23:35,682 --> 02:23:37,566
SPEAKER_0:  Yeah. I know the vulnerabilities that are running.

02:23:37,858 --> 02:23:39,422
SPEAKER_0:  Again, not making a playbook here.

02:23:39,810 --> 02:23:45,182
SPEAKER_0:  but that's how easy it is to find your targets and that's what the bad guys are doing.

02:23:45,698 --> 02:23:49,790
SPEAKER_1:  then the reverse is tough. It's much tougher, but still doable, which is like.

02:23:50,434 --> 02:23:52,958
SPEAKER_1:  first find the target if you have specific targets.

02:23:54,050 --> 02:23:57,150
SPEAKER_1:  to hack into a Twitter account, for example.

02:23:57,410 --> 02:23:57,886
SPEAKER_1:  much harder.

02:23:58,146 --> 02:24:00,771
SPEAKER_1:  That's probably social engineering, right? That's probably the best.

02:24:00,771 --> 02:24:05,374
SPEAKER_0:  Probably if you wanted something specific to that. I mean if you really want to go far, you know.

02:24:06,498 --> 02:24:26,974
SPEAKER_0:  If you're targeting a specific person, you know, how hard is it to get into their office and put a, you know, a little device, USB device in line with their mouse? Who checks how their mouse is plugged in? And you can, for 40 bucks on the black market, you can buy a keylogger that just USB, then the mouse plugs right into it. It looks like an extension on the mouse. If you can even find it, you can buy the...

02:24:27,202 --> 02:24:28,606
SPEAKER_0:  stuff with a mouse inside of it.

02:24:28,898 --> 02:24:35,678
SPEAKER_0:  and just plug it into somebody's computer. And there's a keylogger that lives in there and calls home, sends everything you want. So, I mean, and it's cheap.

02:24:36,226 --> 02:24:38,110
SPEAKER_1:  Yeah, in grad school.

02:24:39,362 --> 02:24:43,166
SPEAKER_1:  a program that built a bunch of keyloggers. It was fascinating tracking mouse.

02:24:43,618 --> 02:24:47,614
SPEAKER_1:  just for what I was doing as part of the research.

02:24:48,386 --> 02:24:49,022
SPEAKER_1:  to a.

02:24:49,634 --> 02:24:50,046
SPEAKER_1:  uh

02:24:50,658 --> 02:24:53,470
SPEAKER_1:  to see if by the dynamics of how you type.

02:24:54,530 --> 02:24:58,526
SPEAKER_1:  and how you move the mouse, you can tell who the person is. Oh wow. uh

02:24:58,818 --> 02:24:59,262
SPEAKER_1:  This is a test.

02:24:59,618 --> 02:25:05,118
SPEAKER_1:  it's called the active authentication or like it's basically biometrics that's not using bio.

02:25:05,922 --> 02:25:10,462
SPEAKER_1:  to see how identifiable that is. So it's fascinating to study that, but it's also fascinating.

02:25:10,754 --> 02:25:12,254
SPEAKER_1:  how damn easy it is to install.

02:25:12,674 --> 02:25:19,742
SPEAKER_1:  Keyloggers, I think is In natural what happens is you realize how many vulnerabilities there are in this world?

02:25:20,130 --> 02:25:21,214
SPEAKER_1:  You do that when you...

02:25:21,538 --> 02:25:24,542
SPEAKER_1:  Understand bacteria and viruses, you realize they're everywhere.

02:25:24,962 --> 02:25:28,414
SPEAKER_1:  in the same way with the, I'm talking about biological ones. And then.

02:25:28,834 --> 02:25:32,894
SPEAKER_1:  You realize that all the vulnerabilities that are out there. One of the things I've noticed quite a lot is.

02:25:33,794 --> 02:25:36,062
SPEAKER_1:  how many people don't log out of their computers.

02:25:37,250 --> 02:25:37,566
SPEAKER_1:  Just.

02:25:37,986 --> 02:25:40,734
SPEAKER_1:  how easy physical access the system actually is.

02:25:41,922 --> 02:25:44,286
SPEAKER_1:  uh... like in a lot of places in this world

02:25:45,346 --> 02:25:48,862
SPEAKER_1:  I'm not talking about private homes, I'm talking about companies, especially large companies.

02:25:49,602 --> 02:25:54,654
SPEAKER_1:  seems quite trivial in certain places that I've been to to walk in and have physical access to a system.

02:25:55,010 --> 02:25:56,062
SPEAKER_1:  And that's depressing to me.

02:25:56,322 --> 02:26:03,134
SPEAKER_0:  It is. It just, I laugh because one of my partners at Naxo that I work at now, he worked at a

02:26:03,458 --> 02:26:19,934
SPEAKER_0:  big company, like you would know the name as soon as I told you, I'm not going to say it. But the guy who owned the company and the company has his name on it, didn't want to ever log into a computer just to know the shit out of him. So they hired a person that stands next to his computer when he's not there. And that's his physical security.

02:26:20,738 --> 02:26:25,246
SPEAKER_0:  That's good. That's pretty good. Yeah, I mean, I guess if you could afford to do that

02:26:25,506 --> 02:26:31,166
SPEAKER_1:  At least you're taking your security seriously. I feel like there's a lot of people in that case who just not have a login.

02:26:31,522 --> 02:26:32,030
SPEAKER_0:  lol

02:26:32,290 --> 02:26:36,094
SPEAKER_0:  No, the security team there had to really work around to make that work.

02:26:36,386 --> 02:26:38,782
SPEAKER_0:  non-compliant with company policy.

02:26:39,426 --> 02:26:43,422
SPEAKER_1:  But that's interesting, the keylog is, there's a lot of.

02:26:44,034 --> 02:26:45,278
SPEAKER_1:  There's just a lot of threads.

02:26:45,730 --> 02:26:47,166
SPEAKER_1:  Yeah, I mean. Let's get in.

02:26:47,266 --> 02:26:53,246
SPEAKER_0:  Yeah, I mean, so you can't sit around and worry about someone physically gaining access to your computer with keylogger and stuff like that.

02:26:53,474 --> 02:27:03,806
SPEAKER_0:  If you're traveling to a foreign country and you work for the FBI, then yeah, you do. Sometimes some countries you would bring a fake laptop just to see if they stole it or accessed it.

02:27:04,258 --> 02:27:05,118
SPEAKER_1:  I really want

02:27:05,698 --> 02:27:07,006
SPEAKER_1:  especially in this modern day.

02:27:07,394 --> 02:27:10,302
SPEAKER_1:  to just create a lot of clones of myself that generate.

02:27:11,394 --> 02:27:15,710
SPEAKER_1:  Lex sounding things and just get put so much information out there.

02:27:16,034 --> 02:27:17,566
SPEAKER_1:  I actually dox myself.

02:27:18,882 --> 02:27:20,030
SPEAKER_1:  all across the world.

02:27:20,130 --> 02:27:22,526
SPEAKER_0:  And then you're not a target, I guess. Just put it out there.

02:27:22,786 --> 02:27:27,550
SPEAKER_0:  I've always said that though. Like we do these searches in FBI houses and stuff like that. Someone just got like.

02:27:27,810 --> 02:27:28,126
SPEAKER_0:  Oh.

02:27:28,418 --> 02:27:32,030
SPEAKER_0:  box load of like 10 terabyte drives and just encrypted them.

02:27:32,706 --> 02:27:36,094
SPEAKER_0:  Oh my god, you know how long the FBI would spin their wheels trying to get that data off there?

02:27:36,450 --> 02:27:37,118
SPEAKER_0:  Be insane.

02:27:37,890 --> 02:27:39,486
SPEAKER_0:  Oh, so just give them.

02:27:39,938 --> 02:27:42,782
SPEAKER_0:  You don't even know which one you're looking for. Yeah.

02:27:42,882 --> 02:27:43,646
SPEAKER_1:

02:27:44,130 --> 02:27:48,094
SPEAKER_1:  That's true. That's true. So it's like me printing like a treasure map.

02:27:48,578 --> 02:27:50,462
SPEAKER_1:  to a random location, just get.

02:27:50,722 --> 02:27:52,734
SPEAKER_1:  people to go on goose chases.

02:27:53,986 --> 02:27:54,526
SPEAKER_1:  Yeah.

02:27:54,914 --> 02:27:55,774
SPEAKER_1:  uh... what

02:27:56,130 --> 02:27:59,134
SPEAKER_1:  What about operating system? What have you found?

02:27:59,554 --> 02:28:02,974
SPEAKER_1:  What's the most secure and what's the least secure operating system? Windows Linux.

02:28:04,002 --> 02:28:05,726
SPEAKER_1:  Is there no universal?

02:28:05,858 --> 02:28:14,302
SPEAKER_0:  There's no universal security. I mean, it changed. You people used to think that Macs were the most secure just because they just weren't out there. But now kids have had access to them.

02:28:14,946 --> 02:28:19,230
SPEAKER_0:  You know, I know you're a Linux guy, I like Linux too, but you know.

02:28:19,874 --> 02:28:21,726
SPEAKER_0:  It's tough to have run a business.

02:28:21,986 --> 02:28:31,006
SPEAKER_0:  on Linux, you know, people want to move more towards the Microsoft's and the Google's just because they don't, it's easier to communicate with other people that maybe aren't computer guys so...

02:28:31,586 --> 02:28:36,798
SPEAKER_0:  You have to just take what's best, what's easiest and secure the shit out of it as much as you can and just think about it.

02:28:37,250 --> 02:28:38,910
SPEAKER_1:  What are you doing these days at Naxo?

02:28:39,266 --> 02:28:41,374
SPEAKER_0:  So we just started Nexo, so I...

02:28:41,890 --> 02:28:47,262
SPEAKER_0:  left the government and went to a couple of consultancies and I started working.

02:28:47,490 --> 02:28:48,926
SPEAKER_0:  Really all the people I...

02:28:49,186 --> 02:28:50,878
SPEAKER_0:  I worked good in the government with.

02:28:51,266 --> 02:28:54,151
SPEAKER_0:  Um, I brought them out with me. Um, and now.

02:28:54,151 --> 02:28:55,651
SPEAKER_1:  You used to work for the man and now you're not.

02:28:55,651 --> 02:28:59,646
SPEAKER_0:  you're the man. Exactly. So, but now we formed a partnership and it's just a

02:28:59,874 --> 02:29:04,222
SPEAKER_0:  It's a new cybersecurity firm that we, our launch party is actually on Thursday. So it's going to be exciting.

02:29:04,674 --> 02:29:05,799
SPEAKER_0:  Do you want to give more detail?

02:29:05,799 --> 02:29:08,049
SPEAKER_1:  about the party so that somebody can hack into it.

02:29:08,049 --> 02:29:10,078
SPEAKER_0:  No, I don't even tell you where it is.

02:29:10,434 --> 02:29:13,406
SPEAKER_0:  You can come if you want, but don't bring the hackers.

02:29:13,634 --> 02:29:16,259
SPEAKER_0:  with us. Hector will be there. I can't believe you.

02:29:16,259 --> 02:29:21,246
SPEAKER_1:  Inviting me because you also say insider threat is the biggest threat.

02:29:21,986 --> 02:29:23,861
SPEAKER_1:  By the way, can you explain what the insider thread is?

02:29:23,861 --> 02:29:24,766
SPEAKER_0:  The biggest

02:29:25,058 --> 02:29:27,070
SPEAKER_0:  insider threat in my life is my children.

02:29:27,650 --> 02:29:34,025
SPEAKER_0:  My son's big into Minecraft and will download executables mindlessly and just run them on the network.

02:29:34,025 --> 02:29:36,275
SPEAKER_1:  Do you recommend against marriage and family?

02:29:36,275 --> 02:29:48,414
SPEAKER_0:  kids? No, no, I think from a security perspective, from a security perspective, absolutely. But no, I just segmentation. I mean, we do it in all businesses for years, starting segmenting networks, different networks.

02:29:48,802 --> 02:29:50,910
SPEAKER_0:  I just do it at home. My kid's on his own network.

02:29:51,330 --> 02:29:51,806
SPEAKER_0:  Um...

02:29:52,418 --> 02:30:14,718
SPEAKER_0:  It makes it a little bit easier to see what they're doing too. You can monitor traffic and then also throttle bandwidth. If, uh, if you know your Netflix isn't playing fast enough or buffers or something. So you can obviously change that a little too. You know, they're going to listen to this, right? Uh, you're going to get your tricks. Yeah, that's true. They'll definitely will listen, but there's nothing more humbling than your family. You think you've done something big and you go on a big podcast and

02:30:14,978 --> 02:30:17,374
SPEAKER_0:  talk to Les Freeman and they don't fucking care.

02:30:17,538 --> 02:30:17,982
SPEAKER_1:  Alright.

02:30:18,882 --> 02:30:20,007
SPEAKER_1:  Unless you're into...

02:30:20,007 --> 02:30:52,446
SPEAKER_0:  You'll show up on a YouTube feed or something like that. And I'll be like, oh yeah. Whatever, this guy's boring. My son does a podcast for his school. And it's still, I still can't get him to tell. So, so one of the, Hector and I just started a podcast talking about cyber security. We do a podcast called Hacker in the Fed. It just came out yesterday. So the first episode. So yeah, we got 13, 1300 downloads the first day. So pretty, we were at the top of Hacker News, which is a big website in our world. So it's called Hacker in the Fed. Hacker in the Fed's name is so.

02:30:52,898 --> 02:30:54,023
SPEAKER_0:  Go download and listen.

02:30:54,023 --> 02:31:01,694
SPEAKER_1:  to hack and if I can't wait to see what, because I don't think I've seen a video of you two together so I can't wait to see what the chemistry is like.

02:31:03,042 --> 02:31:07,582
SPEAKER_1:  It's not weird that you guys used to be enemies and now you're friends.

02:31:07,810 --> 02:31:09,150
SPEAKER_0:  So yeah, I mean, we just.

02:31:09,378 --> 02:31:17,214
SPEAKER_0:  did some, a trailer and all that. And the, the, our producer, we have a great producer guy named Phineas and he kind of pulls things out of me. And I said, I said, okay, I got one.

02:31:18,850 --> 02:31:23,550
SPEAKER_0:  My relationship with Hector is, you know, we're very close friends now. And then he's like...

02:31:23,810 --> 02:31:24,126
SPEAKER_0:  Oh.

02:31:24,706 --> 02:31:26,494
SPEAKER_0:  I arrested one of my closest friends.

02:31:27,234 --> 02:31:28,830
SPEAKER_0:  which is a very strange relationship.

02:31:28,994 --> 02:31:29,310
SPEAKER_1:  Yeah.

02:31:29,410 --> 02:31:29,982
SPEAKER_0:  It's weird.

02:31:30,210 --> 02:31:36,574
SPEAKER_0:  You know, but he, he says that I changed his life. I mean, he was going down a very dark path and I gave him an option that one night and he.

02:31:36,802 --> 02:31:42,878
SPEAKER_0:  He made the right choice. I mean, he's, he now does penetration testing. He does a lot of good work and, uh, you know, he's turned his life around.

02:31:44,610 --> 02:31:45,534
SPEAKER_1:  Do you worry about?

02:31:46,434 --> 02:31:48,510
SPEAKER_1:  cyber war in the 21st century.

02:31:48,674 --> 02:31:54,750
SPEAKER_0:  Absolutely. If there is a global war, it'll start with cyber, you know, if it's not already started.

02:31:55,778 --> 02:31:57,086
SPEAKER_1:  Do you feel like?

02:31:57,794 --> 02:31:58,302
SPEAKER_1:  There's a.

02:31:59,490 --> 02:32:00,606
SPEAKER_1:  like a boiling.

02:32:01,218 --> 02:32:03,614
SPEAKER_1:  like the drums of war beating.

02:32:04,162 --> 02:32:06,270
SPEAKER_1:  What's happening in Ukraine with Russia?

02:32:06,530 --> 02:32:07,518
SPEAKER_1:  It feels like.

02:32:07,746 --> 02:32:10,494
SPEAKER_1:  the United States becoming more and more involved.

02:32:11,042 --> 02:32:12,830
SPEAKER_1:  in the conflict in that part of the world.

02:32:13,090 --> 02:32:15,422
SPEAKER_1:  and China is watching very closely.

02:32:15,714 --> 02:32:16,286
SPEAKER_1:

02:32:16,546 --> 02:32:18,942
SPEAKER_1:  starting to get involved geopolitically.

02:32:19,522 --> 02:32:21,918
SPEAKER_1:  and probably in terms of cyber.

02:32:22,562 --> 02:32:24,158
SPEAKER_1:  Um, do you worry about

02:32:24,642 --> 02:32:28,830
SPEAKER_1:  this kind of thing happening in the next decade or two, like where it really escalates.

02:32:29,250 --> 02:32:31,422
SPEAKER_1:  People in the 1920s...

02:32:32,322 --> 02:32:32,926
SPEAKER_1:  or...

02:32:33,346 --> 02:32:35,646
SPEAKER_1:  completely terrible at predicting.

02:32:36,002 --> 02:32:36,862
SPEAKER_1:  the World War II.

02:32:37,986 --> 02:32:38,398
SPEAKER_1:  Do you think?

02:32:38,882 --> 02:32:41,406
SPEAKER_1:  We're at the precipice of war potentially.

02:32:41,698 --> 02:32:42,526
SPEAKER_0:  I think we could be.

02:32:42,754 --> 02:32:45,374
SPEAKER_0:  I mean, I would hate to just be.

02:32:46,082 --> 02:32:46,718
SPEAKER_0:  He notes.

02:32:47,010 --> 02:32:52,126
SPEAKER_0:  fear margarine out there, you know, COVID's over, so the next big thing in the media is war and all that, but

02:32:53,538 --> 02:32:54,718
SPEAKER_0:  There's some... some...

02:32:55,298 --> 02:32:56,190
SPEAKER_0:  flags going up.

02:32:56,642 --> 02:32:57,278
SPEAKER_0:  that are...

02:32:57,666 --> 02:33:00,190
SPEAKER_0:  Very strange to me. Is there ways to avoid this?

02:33:00,674 --> 02:33:06,878
SPEAKER_0:  I hope so. I hope smarter people than I are figuring it out. I hope people are playing their parts and talking to the right people.

02:33:07,202 --> 02:33:10,142
SPEAKER_0:  Um, because that's the war is the last thing I want.

02:33:10,658 --> 02:33:14,910
SPEAKER_1:  Well there's two things to be concerned about on the cyber side. One is the actual defense.

02:33:16,066 --> 02:33:19,262
SPEAKER_1:  on the technical side of cyber. And the other one is the panic.

02:33:19,746 --> 02:33:21,246
SPEAKER_1:  that might happen when...

02:33:21,954 --> 02:33:25,726
SPEAKER_1:  something like some dramatic event happened because of cyber.

02:33:26,178 --> 02:33:28,222
SPEAKER_1:  some major hack that becomes public.

02:33:29,346 --> 02:33:31,390
SPEAKER_1:  I'm honestly more concerned about the panic.

02:33:31,906 --> 02:33:32,830
SPEAKER_1:  because...

02:33:33,474 --> 02:33:36,638
SPEAKER_1:  I feel like if people don't think about this stuff, the panic can hit harder.

02:33:37,538 --> 02:33:41,598
SPEAKER_1:  Like if they're not conscious about the fact that they were constantly under attack.

02:33:42,178 --> 02:33:44,862
SPEAKER_1:  I feel like it'll come like a much harder surprise.

02:33:45,794 --> 02:33:47,678
SPEAKER_0:  Yeah, I think people will be...

02:33:47,906 --> 02:33:52,158
SPEAKER_0:  really shocked on things. I mean, so we talked about LulzSec today and LulzSec was 2011.

02:33:52,674 --> 02:33:53,566
SPEAKER_0:  They had.

02:33:54,210 --> 02:33:58,078
SPEAKER_0:  access into the water supply system of a major US city.

02:33:58,626 --> 02:34:05,374
SPEAKER_0:  They didn't do anything with it. They were sitting on it in case someone got arrested and they were going to maybe just expose that it's insecure.

02:34:05,858 --> 02:34:07,966
SPEAKER_0:  Maybe they were gonna do something to fuck with it, I don't know.

02:34:08,226 --> 02:34:08,702
SPEAKER_0:  but...

02:34:09,026 --> 02:34:13,406
SPEAKER_0:  You know, that's 2011. You know, I don't think it's gotten a lot better.

02:34:13,762 --> 02:34:14,206
SPEAKER_0:  Since then.

02:34:16,418 --> 02:34:20,446
SPEAKER_1:  And there's probably nation states or major organizations.

02:34:21,122 --> 02:34:23,006
SPEAKER_1:  that are sitting secretly on

02:34:23,106 --> 02:34:24,350
SPEAKER_0:  100%.

02:34:24,610 --> 02:34:27,262
SPEAKER_0:  100% they are sitting, serially waiting.

02:34:27,522 --> 02:34:28,286
SPEAKER_0:  to expose things.

02:34:30,402 --> 02:34:36,094
SPEAKER_0:  I mean, I, again, I don't want to scare this shit out of people, but people have to understand the cyber threat. I mean, there are, you know.

02:34:36,354 --> 02:34:37,854
SPEAKER_0:  there are.

02:34:38,082 --> 02:34:41,310
SPEAKER_0:  thousands of nation state hackers in some countries.

02:34:41,794 --> 02:34:43,669
SPEAKER_0:  I mean, we have them too. We have offensive hackers.

02:34:43,669 --> 02:34:45,886
SPEAKER_1:  You know, the terrorist attacks of 9-11.

02:34:47,074 --> 02:34:51,486
SPEAKER_1:  There's planes that actually hit actual buildings and it was visibly clear.

02:34:51,842 --> 02:34:52,862
SPEAKER_1:  and you can trace.

02:34:53,154 --> 02:34:53,982
SPEAKER_1:  the information.

02:34:54,658 --> 02:34:57,950
SPEAKER_1:  with cyber attacks, say something that would result in...

02:34:58,594 --> 02:35:00,478
SPEAKER_1:  and a major explosion in New York City.

02:35:01,858 --> 02:35:03,102
SPEAKER_1:  How the hell do you trace that?

02:35:03,970 --> 02:35:04,318
SPEAKER_1:  Like.

02:35:05,058 --> 02:35:07,934
SPEAKER_1:  If it's well done, it's going to be extremely difficult.

02:35:08,546 --> 02:35:09,470
SPEAKER_1:  the problem is.

02:35:10,178 --> 02:35:14,430
SPEAKER_1:  There's so many problems, one of which the US government in that case...

02:35:14,690 --> 02:35:16,894
SPEAKER_1:  has complete freedom to blame anybody they want.

02:35:18,370 --> 02:35:20,446
SPEAKER_1:  and then to go start war.

02:35:20,866 --> 02:35:21,662
SPEAKER_1:  Then you betity

02:35:22,850 --> 02:35:27,166
SPEAKER_1:  anybody that actually see Jupe, that's sorry.

02:35:27,426 --> 02:35:27,774
SPEAKER_1:  That's.

02:35:28,226 --> 02:35:29,758
SPEAKER_1:  one cynical take on it.

02:35:30,082 --> 02:35:35,454
SPEAKER_0:  No, but you're going down the right path. I mean, the guys that flew the planes in the buildings wanted attribution. They took credit for it.

02:35:35,810 --> 02:35:37,214
SPEAKER_0:  when we see the cyber attack.

02:35:37,538 --> 02:35:39,262
SPEAKER_0:  I doubt we're going to see attribution.

02:35:39,650 --> 02:35:44,446
SPEAKER_0:  Maybe the victim side, the US government on this side, might come out and try to blame somebody.

02:35:44,674 --> 02:35:45,086
SPEAKER_0:  but.

02:35:45,378 --> 02:35:50,046
SPEAKER_0:  You know, like you've brought up, they could blame anybody they want, there's not really a good way of verifying that.

02:35:51,138 --> 02:35:52,263
SPEAKER_0:  Can I just ask for your-

02:35:52,263 --> 02:35:54,814
SPEAKER_1:  advice so in my personal case

02:35:55,042 --> 02:35:56,446
SPEAKER_1:  Am I being tracked?

02:35:57,186 --> 02:35:58,142
SPEAKER_1:  How do I know?

02:35:59,170 --> 02:36:00,926
SPEAKER_1:  How do I protect myself should I care?

02:36:02,114 --> 02:36:03,326
SPEAKER_0:  You are being tracked.

02:36:03,618 --> 02:36:07,838
SPEAKER_0:  Um, I wouldn't say you're being tracked by the government. You're definitely being tracked by big tech.

02:36:08,514 --> 02:36:09,639
SPEAKER_0:  Uh, they- No, I mean-

02:36:09,639 --> 02:36:12,606
SPEAKER_1:  me personally, Lex and an escalated level.

02:36:13,026 --> 02:36:13,982
SPEAKER_1:  So like, uh...

02:36:14,722 --> 02:36:15,358
SPEAKER_1:  uh...

02:36:16,802 --> 02:36:18,910
SPEAKER_1:  Like you mentioned, there's an FBI file on people.

02:36:19,650 --> 02:36:21,630
SPEAKER_1:  I'd love to see what's in that file.

02:36:22,338 --> 02:36:22,878
SPEAKER_1:  Uh...

02:36:23,266 --> 02:36:24,734
SPEAKER_1:  awkward laughter

02:36:24,962 --> 02:36:29,534
SPEAKER_1:  Who did I have the argument for? Oh, let me ask you, FBI. Um,9

02:36:29,762 --> 02:36:31,454
SPEAKER_1:  How's the cafeteria food in FBI?

02:36:31,618 --> 02:36:32,734
SPEAKER_0:  At the Academy?

02:36:33,058 --> 02:36:42,185
SPEAKER_0:  It's bad. Yeah. What about like- At headquarters? Headquarters. Little bit better, cause that's what the director, I mean, he eats up on the seventh floor. Have you been like a go-

02:36:42,185 --> 02:36:45,630
SPEAKER_1:  Have you been to Silicon Valley, those cafes?

02:36:45,762 --> 02:36:49,137
SPEAKER_0:  I've been to the Google in Silicon Valley. I've been to the Google in New York.

02:36:49,137 --> 02:36:52,926
SPEAKER_1:  Yeah, the food is incredible. It is great. So, FBI is worse.

02:36:53,122 --> 02:36:57,630
SPEAKER_0:  Well, when you're going through the academy, they don't let you outside of the building, so you have to eat it.

02:36:58,050 --> 02:37:00,062
SPEAKER_0:  But I think that's the only reason people eat it.

02:37:00,866 --> 02:37:03,358
SPEAKER_0:  Um, it's, it's pretty bad.

02:37:03,714 --> 02:37:04,158
SPEAKER_0:  I got it.

02:37:04,802 --> 02:37:09,086
SPEAKER_0:  Okay. But there's also a bar inside the FBI Academy. If people don't know that.

02:37:09,602 --> 02:37:11,390
SPEAKER_0:  Alcohol bar? Yes, alcohol bar.

02:37:11,906 --> 02:37:15,710
SPEAKER_0:  And if you, as long as you've passed your PT and, uh,

02:37:16,226 --> 02:37:17,854
SPEAKER_0:  going well, you're allowed to go to the bar.

02:37:18,178 --> 02:37:18,526
SPEAKER_1:  Nice.

02:37:19,394 --> 02:37:21,278
SPEAKER_1:  It feels like if I was a hacker.

02:37:22,114 --> 02:37:26,846
SPEAKER_1:  I'll be going after like celebrities because they're a little bit easier, like celebrity celebrities like Hollywood.

02:37:27,330 --> 02:37:30,046
SPEAKER_0:  The Hollywood nudes were a big thing there for a long time.

02:37:30,594 --> 02:37:31,230
SPEAKER_1:  But now you-

02:37:31,394 --> 02:37:37,438
SPEAKER_0:  Yeah, I guess news. That's what they went after. I mean, all those guys, they socialized. They social engineered Apple to get

02:37:38,018 --> 02:37:43,550
SPEAKER_0:  backups to get the recoveries for backups. And then they just pulled all their news. And I mean, whole websites were dedicated to that.

02:37:44,290 --> 02:37:44,638
SPEAKER_1:  Yeah.

02:37:44,962 --> 02:37:47,870
SPEAKER_1:  See that? See, I wouldn't do that kind of stuff. It's very creepy.

02:37:48,098 --> 02:37:50,846
SPEAKER_1:  I would go, if I was a hacker, I would go after...

02:37:52,290 --> 02:37:52,830
SPEAKER_1:  like

02:37:54,178 --> 02:37:55,966
SPEAKER_1:  major, like powerful people.

02:37:56,482 --> 02:37:59,838
SPEAKER_1:  and like tweet something from their account and like something that

02:38:00,098 --> 02:38:00,958
SPEAKER_1:  like positive.

02:38:01,538 --> 02:38:01,982
SPEAKER_1:  Loving.

02:38:02,818 --> 02:38:04,350
SPEAKER_1:  but like for the walls.

02:38:04,610 --> 02:38:05,758
SPEAKER_1:  that obviously is a troll.

02:38:05,922 --> 02:38:07,518
SPEAKER_0:  God, you get busted so quick.

02:38:08,066 --> 02:38:09,406
SPEAKER_0:  by a bad hacker.

02:38:09,698 --> 02:38:11,573
SPEAKER_1:  Really? But why?

02:38:11,573 --> 02:38:13,726
SPEAKER_0:  Because hackers never put things out about love.

02:38:14,082 --> 02:38:14,718
SPEAKER_0:  Oh, okay.

02:38:15,106 --> 02:38:21,214
SPEAKER_0:  Oh, you mean like, this is clearly- Yeah, this is clearly Lex. He's about loving every podcast he does.

02:38:21,570 --> 02:38:24,862
SPEAKER_1:  I would just be like, no, oh, God damn it. Now somebody's gonna do it.

02:38:25,090 --> 02:38:26,430
SPEAKER_1:  You'll blame me.

02:38:26,850 --> 02:38:27,550
SPEAKER_1:  It wasn't me.

02:38:28,322 --> 02:38:30,910
SPEAKER_1:  Looking back at your life, is there something you regret?

02:38:31,842 --> 02:38:34,467
SPEAKER_0:  I'm only 44 years old, I'm already looking back. Is there-

02:38:34,467 --> 02:38:34,910
SPEAKER_1:  stuff.

02:38:35,170 --> 02:38:35,870
SPEAKER_1:  that...

02:38:36,514 --> 02:38:37,150
SPEAKER_1:  Um...

02:38:37,826 --> 02:38:38,558
SPEAKER_1:  You regret?

02:38:39,586 --> 02:38:40,606
SPEAKER_0:  AV Unit!

02:38:41,218 --> 02:38:42,782
SPEAKER_0:  I got away!

02:38:43,106 --> 02:38:44,702
SPEAKER_1:  That was the one that got away.

02:38:44,834 --> 02:38:50,846
SPEAKER_0:  Yeah, I mean, it took me a while into my law enforcement career to learn about like the compassionate side and

02:38:51,074 --> 02:38:53,886
SPEAKER_0:  It took Hector Monsiger to make me realize that

02:38:54,146 --> 02:38:56,702
SPEAKER_0:  Criminals aren't really criminals, they're human beings.

02:38:57,186 --> 02:39:01,790
SPEAKER_0:  That really humanized the whole thing for me, sitting with him for nine months.

02:39:02,338 --> 02:39:06,686
SPEAKER_0:  Um, I think that's maybe why I had a lot more compassion when I arrested Ross.

02:39:07,138 --> 02:39:15,006
SPEAKER_0:  Probably wouldn't have been so compassionate if it was before Hector. But yeah, he changed my life and showed me that humanity side of things.

02:39:15,202 --> 02:39:20,318
SPEAKER_1:  So would it be fair to say that all the criminals are most criminals are?

02:39:21,314 --> 02:39:21,662
SPEAKER_1:  I just.

02:39:22,306 --> 02:39:27,710
SPEAKER_1:  People that took a wrong turn at some point, they all have the capacity for good and for evil in them.

02:39:29,570 --> 02:39:38,110
SPEAKER_0:  I'd say 99% of the criminals that I've interacted with, yes, the people with the child exploitation, no, I don't have any place in my heart for them.

02:39:39,554 --> 02:39:41,662
SPEAKER_1:  What advice would you give to...

02:39:42,370 --> 02:39:44,094
SPEAKER_1:  people in college, people in high school.

02:39:44,514 --> 02:39:46,238
SPEAKER_1:  trying to figure out what they want to do with their life.

02:39:46,786 --> 02:39:50,462
SPEAKER_1:  how to have a life they can be proud of, how to have a career they can be proud of.

02:39:51,202 --> 02:39:52,062
SPEAKER_1:  All that kind of stuff.

02:39:52,482 --> 02:39:58,110
SPEAKER_0:  in the US budget that was just put forward, there's $18 billion for cybersecurity.

02:39:58,626 --> 02:40:02,430
SPEAKER_0:  We're about a million people short of where we really should be in the industry, if not more.

02:40:02,882 --> 02:40:07,742
SPEAKER_0:  If you want job security and want to work and see exciting stuff, subscribe.

02:40:08,002 --> 02:40:09,278
SPEAKER_0:  head toward cyber security.

02:40:09,634 --> 02:40:11,230
SPEAKER_0:  It's a good career.

02:40:11,746 --> 02:40:12,766
SPEAKER_0:  Um, and.

02:40:13,410 --> 02:40:22,750
SPEAKER_0:  You know, one thing I dislike about like, cybersecurity right now is they expect you to come out of college and have 10 years experience in protecting and knowing every different.

02:40:23,010 --> 02:40:25,502
SPEAKER_0:  Python script out there and everything available.

02:40:25,730 --> 02:40:32,798
SPEAKER_0:  um... you know the industry needs to change and let the lower people in in order to to broaden and get that those billion jobs filled

02:40:33,218 --> 02:40:35,134
SPEAKER_0:  But as far as their personal security

02:40:35,714 --> 02:40:36,958
SPEAKER_0:  Just remember it's all gonna follow you.

02:40:37,218 --> 02:40:38,814
SPEAKER_0:  I mean, uh, you know.

02:40:39,170 --> 02:40:43,358
SPEAKER_0:  There's laws out there now that you have to turn over your social media accounts in order to have certain things.

02:40:43,938 --> 02:40:48,222
SPEAKER_0:  They just changed that in New York State if you want to carry a gun you have to turn over your social media to Transcribed by All Brainages

02:40:48,450 --> 02:40:50,494
SPEAKER_0:  to figure if you're a good social.

02:40:51,106 --> 02:40:51,934
SPEAKER_0:  uh, character.

02:40:52,450 --> 02:40:52,926
SPEAKER_0:  Um...

02:40:53,250 --> 02:40:53,982
SPEAKER_0:  So...

02:40:54,242 --> 02:40:55,838
SPEAKER_0:  Hopefully you didn't say something.

02:40:56,194 --> 02:40:58,718
SPEAKER_0:  strange in the last few years and it's going to follow you forever.

02:40:59,138 --> 02:41:05,982
SPEAKER_0:  Um, I, I bet Ross Albrecht would tell you the same thing when in not, don't put Ross Albrecht at gmail.com on things. Cause it's going to last forever.

02:41:06,370 --> 02:41:11,742
SPEAKER_1:  Yeah, people sometimes for some reason they interact on social media as if they're talking to a couple of buddies.

02:41:12,898 --> 02:41:13,662
SPEAKER_1:  Like

02:41:14,018 --> 02:41:14,494
SPEAKER_1:  n-n-n-n...

02:41:14,722 --> 02:41:15,838
SPEAKER_1:  just shooting shit.

02:41:16,130 --> 02:41:17,566
SPEAKER_1:  and mocking and...

02:41:17,794 --> 02:41:18,750
SPEAKER_1:  And like...

02:41:19,394 --> 02:41:23,646
SPEAKER_1:  you know, what is that, busting each other's chops, like making fun of yourself, like being.

02:41:24,034 --> 02:41:25,566
SPEAKER_1:  especially gaming culture.

02:41:25,922 --> 02:41:26,462
SPEAKER_1:  Uh, like-

02:41:26,754 --> 02:41:27,806
SPEAKER_1:  people who stream.

02:41:28,322 --> 02:41:31,870
SPEAKER_0:  Thank God that's not recorded. Oh my God. The things people say on those streams.

02:41:32,674 --> 02:41:34,549
SPEAKER_0:  Yeah, but a lot of them are recorded.

02:41:34,549 --> 02:41:38,910
SPEAKER_1:  So there's a whole Twitch thing where people stream for many hours a day.

02:41:39,522 --> 02:41:40,446
SPEAKER_1:  And uh...

02:41:41,314 --> 02:41:41,982
SPEAKER_1:  I mean just...

02:41:43,106 --> 02:41:46,142
SPEAKER_1:  outside of the very offensive things they say.

02:41:47,586 --> 02:41:48,958
SPEAKER_1:  They just swear a lot.

02:41:49,442 --> 02:41:50,846
SPEAKER_1:  They're not the kind of person.

02:41:51,842 --> 02:41:55,262
SPEAKER_1:  that I would wanna hire, I wanna work with.

02:41:55,522 --> 02:41:56,862
SPEAKER_1:  Now I understand.

02:41:57,314 --> 02:41:57,662
SPEAKER_1:  that.

02:41:57,890 --> 02:42:02,046
SPEAKER_1:  Some of us might be that way privately, I guess.

02:42:02,306 --> 02:42:03,902
SPEAKER_1:  when you're shooting a shit with friends.

02:42:04,130 --> 02:42:04,862
SPEAKER_1:  Like, uh...

02:42:05,218 --> 02:42:08,190
SPEAKER_1:  playing a video game and talking shit to each other, maybe.

02:42:08,418 --> 02:42:12,126
SPEAKER_1:  but like that's all out there. You have to be conscious of the fact that that's all out there.

02:42:12,578 --> 02:42:15,326
SPEAKER_1:  And it's just not, it's not a good look. It's not like you're.

02:42:15,650 --> 02:42:19,198
SPEAKER_1:  You should, it's complicated, cause I'm like against hiding who you-

02:42:19,682 --> 02:42:20,318
SPEAKER_0:  R?

02:42:21,122 --> 02:42:22,782
SPEAKER_0:  But, like- I saw you hide some of it.

02:42:23,234 --> 02:42:24,862
SPEAKER_1:  Yeah, but like, I just feel like...

02:42:25,090 --> 02:42:26,910
SPEAKER_1:  It's going to be misinterpreted.

02:42:27,298 --> 02:42:30,014
SPEAKER_1:  when you talk shit to your friends while you're playing video games.

02:42:30,850 --> 02:42:34,014
SPEAKER_1:  It doesn't mean you're an asshole. It means you're an asshole to your friend.

02:42:34,882 --> 02:42:37,438
SPEAKER_1:  But that's how a lot of friends show love.

02:42:37,698 --> 02:42:40,062
SPEAKER_0:  Yeah, an outside person can't judge how I'm friends with you.

02:42:40,354 --> 02:42:49,022
SPEAKER_0:  If I want to be, this is our relationship. If that person can say that I'm an asshole to them, then that's fine, I'll take it. But you can't tell me I'm an asshole to them.

02:42:49,250 --> 02:42:51,614
SPEAKER_0:  Just because you saw my interaction. I agree with that.

02:42:51,874 --> 02:42:55,262
SPEAKER_1:  those words out of context and now that's considered.

02:42:55,810 --> 02:42:57,406
SPEAKER_1:  who you are is dangerous.

02:42:57,666 --> 02:43:00,286
SPEAKER_1:  And people take that very nausical outlay. People treat their-

02:43:00,738 --> 02:43:03,134
SPEAKER_1:  behave on the internet very, very carelessly.

02:43:03,490 --> 02:43:04,990
SPEAKER_1:  That's definitely something that people can do.

02:43:05,538 --> 02:43:09,758
SPEAKER_1:  You need to learn and take extremely seriously. Also, I think they're taking that seriously.

02:43:10,242 --> 02:43:13,278
SPEAKER_1:  will help you figure out who you, what you really stand for.

02:43:13,730 --> 02:43:15,294
SPEAKER_1:  If you use your language...

02:43:16,034 --> 02:43:16,830
SPEAKER_1:  carelessly.

02:43:17,762 --> 02:43:18,654
SPEAKER_1:  You'd never really ask.

02:43:18,946 --> 02:43:20,126
SPEAKER_1:  Like, what do I stand for?

02:43:20,546 --> 02:43:23,390
SPEAKER_1:  I feel like it's a good opportunity when you're young.

02:43:23,778 --> 02:43:24,542
SPEAKER_1:  to ask like.

02:43:25,090 --> 02:43:26,654
SPEAKER_1:  What are the things that are?

02:43:27,202 --> 02:43:28,126
SPEAKER_1:  Okay to say.

02:43:28,834 --> 02:43:31,614
SPEAKER_1:  What are the things, what are the ideas I stand behind?

02:43:32,674 --> 02:43:34,718
SPEAKER_1:  Like, what are, especially if they're controversial.

02:43:35,330 --> 02:43:37,790
SPEAKER_1:  and I'm willing to say that because I believe in them.

02:43:38,178 --> 02:43:40,702
SPEAKER_1:  versus just saying random shit for the lolz.

02:43:41,186 --> 02:43:43,934
SPEAKER_1:  is for the random shift of the law, skip that off the internet.

02:43:44,642 --> 02:43:50,014
SPEAKER_1:  That said, man, I was an idiot for most of my life, and I'm constantly learning and growing.

02:43:50,530 --> 02:43:53,342
SPEAKER_1:  I'd hate to be responsible for the kind of person.

02:43:54,434 --> 02:43:56,990
SPEAKER_1:  I was in my teens.

02:43:57,698 --> 02:44:01,534
SPEAKER_1:  in my 20s. I didn't do anything offensive but just...

02:44:01,858 --> 02:44:03,710
SPEAKER_1:  change as a person like I used to

02:44:04,322 --> 02:44:08,574
SPEAKER_1:  I guess I probably still do, but I used to read so much existential literature.

02:44:08,962 --> 02:44:09,342
SPEAKER_1:  Uh...

02:44:09,666 --> 02:44:12,094
SPEAKER_1:  That was a phase. There's like phases.

02:44:12,290 --> 02:44:19,102
SPEAKER_0:  Yeah, you grow and evolve as a person that changes you in the future. Yeah. I thank God there wasn't social media when I was in high school. Thank God.

02:44:20,066 --> 02:44:21,918
SPEAKER_0:  Oh my god, I would never have gotten the FBI.

02:44:22,146 --> 02:44:26,718
SPEAKER_1:  Would you recommend that people consider a career at a place like the FBI?

02:44:27,842 --> 02:44:40,638
SPEAKER_0:  I loved the FBI. I never thought I would go any place else but the FBI. I thought I was going to retire with the gold watch and everything from the FBI. That was my plan. You get a gold watch? No, but you know what it is. It's an expression.

02:44:41,154 --> 02:44:41,598
SPEAKER_0:  Um...

02:44:42,082 --> 02:44:47,006
SPEAKER_0:  You get a gold badge. You actually get your badge in Lucite and your creds, put it in Lucite and all that. So.

02:44:47,490 --> 02:44:48,894
SPEAKER_1:  Does it, does it by the way just...

02:44:49,218 --> 02:44:50,782
SPEAKER_1:  on a tangent since we like those.

02:44:51,618 --> 02:44:54,238
SPEAKER_1:  Does it hurt you that the FBI

02:44:55,010 --> 02:44:57,790
SPEAKER_1:  by certain people as distrusted or even hated.

02:44:58,306 --> 02:45:01,406
SPEAKER_0:  100%. It kills me. I like, I've never.

02:45:01,890 --> 02:45:03,870
SPEAKER_0:  Until recently, not- I-

02:45:04,130 --> 02:45:05,534
SPEAKER_0:  sometimes be embarrassed.

02:45:06,434 --> 02:45:06,846
SPEAKER_0:  about.

02:45:07,682 --> 02:45:08,862
SPEAKER_0:  FBI sometimes.

02:45:09,314 --> 02:45:10,206
SPEAKER_0:  which is really...

02:45:10,530 --> 02:45:21,630
SPEAKER_0:  really hard for me to say because I love that place. I love the people in it. I love the brotherhood that you have with all the guys in your squad, guys and girls.

02:45:22,274 --> 02:45:25,566
SPEAKER_0:  I developed a real drinking problem there because we were so social.

02:45:26,050 --> 02:45:27,998
SPEAKER_0:  of going out after, after work.

02:45:28,354 --> 02:45:30,622
SPEAKER_0:  continuing on. It really was a family.

02:45:31,266 --> 02:45:31,838
SPEAKER_0:  Um...

02:45:32,130 --> 02:45:33,950
SPEAKER_0:  You know, so I do miss that.

02:45:34,466 --> 02:45:34,910
SPEAKER_0:  Um...

02:45:35,906 --> 02:45:40,382
SPEAKER_0:  But yeah, I mean, if someone can become an FBI agent, I mean, it's pretty fucking cool, man.

02:45:40,898 --> 02:45:44,958
SPEAKER_0:  The day you graduate and walk out of the Academy with a gun and a badge and you know...

02:45:45,250 --> 02:45:49,598
SPEAKER_0:  the power to charge someone with a misdemeanor for flying a United States flag at night.

02:45:50,242 --> 02:45:51,367
SPEAKER_0:  That's awesome.

02:45:51,367 --> 02:45:51,774
SPEAKER_1:

02:45:52,290 --> 02:45:55,582
SPEAKER_1:  So there is a part of like representing loving your country.

02:45:55,906 --> 02:46:01,726
SPEAKER_1:  And especially if you're doing cybersecurity. So there's a lot of technical savvy in there and different places in the FBI.

02:46:02,082 --> 02:46:03,550
SPEAKER_0:  Yeah, I mean, there's different...

02:46:03,970 --> 02:46:08,030
SPEAKER_0:  Sometimes, you know, you'll see an older agent that's done, you know, not.

02:46:08,258 --> 02:46:21,790
SPEAKER_0:  cybercrime come over to cybercrime at the end so he can get a job once he goes out. But there's also some guys that come in. I won't name his name, but there was a guy, I think he was a hacker when he was a kid and now he's been an agent. Now he's way up in management.

02:46:22,018 --> 02:46:25,534
SPEAKER_0:  great guy. I love this guy and he knows who he is if he's listening.

02:46:25,826 --> 02:46:27,006
SPEAKER_0:  You know, the, the...

02:46:27,298 --> 02:46:29,118
SPEAKER_0:  You know, he had some skills.

02:46:29,794 --> 02:46:41,086
SPEAKER_0:  But we also lost a bunch of guys that had some skills because we had one guy in the squad that he had to leave the FBI because his wife became a doctor and she got her residency down in Houston and she couldn't move.

02:46:41,314 --> 02:46:44,830
SPEAKER_0:  He wasn't allowed to transfer so he decided to keep his family versus

02:46:45,378 --> 02:46:49,982
SPEAKER_0:  the FBI. So there's some stringent rules in the FBI that need to be relaxed a little bit.

02:46:50,338 --> 02:46:50,750
SPEAKER_0:  Yeah.

02:46:51,042 --> 02:46:52,574
SPEAKER_1:  I love hackers turned.

02:46:53,154 --> 02:46:55,550
SPEAKER_1:  like leaders, like one of my...

02:46:55,970 --> 02:46:57,790
SPEAKER_1:  quickly becoming good friends as much.

02:46:58,530 --> 02:47:00,734
SPEAKER_1:  It was a big hack in the 90s and then...

02:47:01,026 --> 02:47:02,590
SPEAKER_1:  uh, now was recently.

02:47:03,586 --> 02:47:04,126
SPEAKER_1:  Twitter.

02:47:04,866 --> 02:47:05,406
SPEAKER_1:  chief.

02:47:05,858 --> 02:47:07,454
SPEAKER_1:  Security Officer CSO.

02:47:08,162 --> 02:47:11,230
SPEAKER_1:  But he had a bunch of different leadership positions, including being my

02:47:12,002 --> 02:47:14,526
SPEAKER_1:  boss at Google, but.

02:47:15,426 --> 02:47:19,518
SPEAKER_1:  but originally a hacker. It's cool to see like hackers become like leaders.

02:47:19,746 --> 02:47:27,847
SPEAKER_0:  I just wonder what would cause him to stop doing it. Why he would then take like a managerial route for high tech companies. I think a lot of-

02:47:27,847 --> 02:47:32,414
SPEAKER_1:  those guys, so this is like the 90s, they really were about like the freedom.

02:47:33,218 --> 02:47:39,326
SPEAKER_1:  There's like a philosophy to it. And when I think the hacking culture evolved over the years.

02:47:39,746 --> 02:47:43,390
SPEAKER_1:  And I think when it leaves you behind, you start to realize like, oh,

02:47:43,842 --> 02:47:46,782
SPEAKER_1:  Actually what I want to do is I want to help the world and I can do that and

02:47:47,138 --> 02:47:48,574
SPEAKER_1:  legitimate routes and so on.

02:47:48,994 --> 02:47:51,806
SPEAKER_1:  But that's the story that then yeah, I would love to.

02:47:52,418 --> 02:47:54,174
SPEAKER_1:  Talk to him one day with dad

02:47:54,530 --> 02:47:56,030
SPEAKER_1:  I wonder how common that is too.

02:47:56,482 --> 02:47:58,334
SPEAKER_1:  like young hackers turn.

02:47:58,786 --> 02:48:03,093
SPEAKER_1:  turn good. You're saying it like pulls you in. If you're not careful it can really pull you in.

02:48:03,093 --> 02:48:12,350
SPEAKER_0:  Yeah, you're good at it, you become powerful, everyone's slapping you on the back and saying what a good job and all that, you know, at a very young age.

02:48:12,770 --> 02:48:16,958
SPEAKER_0:  Yeah, so yeah, I would love to get into my buddy's mind on why he stopped hacking

02:48:17,538 --> 02:48:19,710
SPEAKER_0:  and moved on. Ah, that's gonna be a good conversation.

02:48:20,258 --> 02:48:23,070
SPEAKER_1:  in his case maybe it's always about

02:48:23,298 --> 02:48:24,542
SPEAKER_1:  A great woman involved.

02:48:25,250 --> 02:48:27,390
SPEAKER_1:  family and so on that grounds you.

02:48:28,002 --> 02:48:28,574
SPEAKER_1:  Um...

02:48:29,794 --> 02:48:31,710
SPEAKER_1:  uh... because like we have

02:48:32,610 --> 02:48:38,974
SPEAKER_1:  There is a danger to hacking that once you're in a relationship, once you have family, maybe you're not willing to partake in.

02:48:40,290 --> 02:48:41,918
SPEAKER_1:  What's your story? What uh...

02:48:42,466 --> 02:48:44,638
SPEAKER_1:  from childhood what are some fun memories you have?

02:48:45,218 --> 02:48:46,878
SPEAKER_1:  Fond memories? Where did you grow up?

02:48:47,714 --> 02:48:49,086
SPEAKER_1:  Well, I don't give away that information.

02:48:49,666 --> 02:48:51,541
SPEAKER_1:  In the ISA's for you.

02:48:51,541 --> 02:48:53,022
SPEAKER_0:  here in Virginia.

02:48:53,314 --> 02:48:54,238
SPEAKER_1:  So yeah.

02:48:54,498 --> 02:48:57,278
SPEAKER_1:  What are some rough moments? What are some beautiful moments?

02:48:57,570 --> 02:48:58,046
SPEAKER_1:  the humor.

02:48:58,882 --> 02:48:59,230
SPEAKER_1:

02:48:59,842 --> 02:49:02,398
SPEAKER_0:  I had a very good family growing up.

02:49:03,170 --> 02:49:04,126
SPEAKER_0:  The, the...

02:49:04,610 --> 02:49:13,118
SPEAKER_0:  rough moment and I'll tell you a story that just happened to me two days ago and it fucked me up man it really didn't you'll be the first I've never told I tried to tell my wife this two nights ago and I couldn't get it out

02:49:13,794 --> 02:49:14,302
SPEAKER_0:  So.

02:49:14,818 --> 02:49:23,870
SPEAKER_0:  My father, he's a disabled veteran, he was a disabled veteran. He was in the army and got hurt and was in a wheelchair his whole life.

02:49:24,418 --> 02:49:25,470
SPEAKER_0:  all my growing up.

02:49:26,146 --> 02:49:27,102
SPEAKER_0:  Here.

02:49:28,002 --> 02:49:29,022
SPEAKER_0:  He was my biggest fan.

02:49:29,474 --> 02:49:40,446
SPEAKER_0:  He just wanted to know everything about, you know, what was going on in the FBI, my stories. I was a local cop before the FBI and I got to a high speed car chase, you know, foot chase and all that.

02:49:40,770 --> 02:49:44,030
SPEAKER_0:  kicking doors in, he wanted to hear none of those stories.

02:49:44,738 --> 02:49:48,670
SPEAKER_0:  At some points I was kind of too cool for school and, ah, dad, I just want a break and all that.

02:49:48,898 --> 02:49:50,398
SPEAKER_0:  and things going on.

02:49:51,106 --> 02:49:52,926
SPEAKER_0:  We lost my dad during COVID.

02:49:53,474 --> 02:49:59,838
SPEAKER_0:  Um, not because of COVID, but it was around that time, but it was right when COVID was kicking off and so he died in the hospital by himself.

02:50:00,322 --> 02:50:01,630
SPEAKER_0:  And I didn't get to see him then.

02:50:02,306 --> 02:50:03,742
SPEAKER_0:  Um, and then, uh...

02:50:04,578 --> 02:50:19,998
SPEAKER_0:  My mom had some people visiting her the other night, and Tom and Karen Roggerberg, and I'll say they're my second biggest fans, right behind my dad. They always askin' about me and my career, and they read the books and seen the movie. They'll even tell you that Silk Road movie was good.

02:50:20,258 --> 02:50:22,462
SPEAKER_0:  That's a lie to you, isn't it?

02:50:22,818 --> 02:50:23,646
SPEAKER_0:  But, and so.

02:50:23,906 --> 02:50:25,310
SPEAKER_0:  They came over and uh...

02:50:25,922 --> 02:50:35,070
SPEAKER_0:  And I helped them with something and my mom was like, called me back a couple of days later and she said, I appreciate you helping them. I know fixing someone's Apple phone over the phone really isn't.

02:50:35,298 --> 02:50:43,454
SPEAKER_0:  what you do for a living, it's kind of beneath you and all that, but I appreciate it." She said, oh, they loved hearing the stories about, you know.

02:50:43,682 --> 02:50:46,558
SPEAKER_0:  Silk Road and all those things. And she goes, you know.

02:50:46,850 --> 02:50:58,270
SPEAKER_0:  Your dad, he loved those stories. He just, I just wish he could have heard them. I mean, he even would tell me, he would say, you know, maybe Chris will come home and I'll get him drunk and he'll tell me the stories.

02:50:58,754 --> 02:50:59,134
SPEAKER_0:  Um...

02:50:59,362 --> 02:51:00,190
SPEAKER_0:  But then she goes.

02:51:00,738 --> 02:51:03,486
SPEAKER_0:  Maybe one day in heaven you can tell him those stories.

02:51:04,162 --> 02:51:05,022
SPEAKER_0:  fucking lost it.

02:51:06,082 --> 02:51:08,382
SPEAKER_0:  I literally stood in my shower sobbing.

02:51:08,674 --> 02:51:10,430
SPEAKER_0:  Yeah. Like a child.

02:51:10,946 --> 02:51:12,958
SPEAKER_0:  Like, just thinking about, like...

02:51:13,570 --> 02:51:15,486
SPEAKER_0:  All my dad wanted was those stories.

02:51:15,874 --> 02:51:16,254
SPEAKER_0:  Yeah.

02:51:17,186 --> 02:51:21,886
SPEAKER_0:  And now I'm on a fucking podcast telling the stories to the world. And I did tell them. Yeah.

02:51:22,242 --> 02:51:22,718
SPEAKER_0:  So.

02:51:23,810 --> 02:51:26,238
SPEAKER_1:  Did you ever have like a long heart to heart with him?

02:51:26,754 --> 02:51:27,518
SPEAKER_1:  about like...

02:51:28,482 --> 02:51:29,214
SPEAKER_1:  bots.

02:51:29,762 --> 02:51:30,750
SPEAKER_1:  such stories.

02:51:30,914 --> 02:51:33,502
SPEAKER_0:  He was in the hospital one time and I went through and uh...

02:51:33,826 --> 02:51:35,134
SPEAKER_0:  I want to know about his history.

02:51:35,426 --> 02:51:36,478
SPEAKER_0:  his life what he did.

02:51:36,898 --> 02:51:39,422
SPEAKER_0:  And I think he may be sensationalized some of it.

02:51:39,874 --> 02:51:43,646
SPEAKER_0:  But that's what you want you dad's a hero. So you want to hear those things is a good storyteller

02:51:44,002 --> 02:51:44,510
SPEAKER_0:  Um...

02:51:45,090 --> 02:51:50,494
SPEAKER_0:  Yeah, again, I don't know what was true and not true, but you know, some of it was really good.

02:51:50,722 --> 02:51:53,150
SPEAKER_0:  And it was just good to hear his life, but.

02:51:53,570 --> 02:51:56,195
SPEAKER_0:  you know, we lost him and now the story.

02:51:56,195 --> 02:51:56,830
SPEAKER_1:  are gone.

02:51:57,602 --> 02:51:58,110
SPEAKER_1:  You miss him?

02:52:01,730 --> 02:52:03,390
SPEAKER_1:  What did he teach you about?

02:52:03,746 --> 02:52:05,054
SPEAKER_1:  what it means to be a man.

02:52:07,234 --> 02:52:07,582
SPEAKER_1:

02:52:08,674 --> 02:52:09,150
SPEAKER_0:  So.

02:52:09,410 --> 02:52:10,846
SPEAKER_0:  My dad... um...

02:52:12,002 --> 02:52:13,342
SPEAKER_0:  He was an engineer.

02:52:13,826 --> 02:52:14,782
SPEAKER_0:  And so...

02:52:15,266 --> 02:52:18,014
SPEAKER_0:  Part of his job, we worked for Vermont.

02:52:18,818 --> 02:52:20,638
SPEAKER_0:  power and electric or whatever it was.

02:52:21,218 --> 02:52:24,574
SPEAKER_0:  I mean, he, when he first got married to my mom and all that.

02:52:25,154 --> 02:52:27,646
SPEAKER_0:  like he flew around in a helicopter, checking out like.

02:52:27,938 --> 02:52:35,966
SPEAKER_0:  power lines and dams. He would just swim inside to scuba into dams to check to make sure they were functioning properly and all that. Pretty cool shit.

02:52:36,226 --> 02:52:36,766
SPEAKER_1:

02:52:37,378 --> 02:52:38,206
SPEAKER_1:  And then...

02:52:38,434 --> 02:52:39,454
SPEAKER_0:  He couldn't walk anymore.

02:52:41,346 --> 02:52:48,030
SPEAKER_0:  I probably would have killed myself if my life switched like that so bad. My dad probably went through some dark points, but he hid that from me maybe.

02:52:48,322 --> 02:52:56,510
SPEAKER_0:  Um, and so to, to get through that struggle, to teach me like, you know, you press on, you have a family, people count on you, you do what you got to do.

02:52:56,962 --> 02:52:57,438
SPEAKER_0:  Um...

02:52:57,858 --> 02:52:59,102
SPEAKER_0:  That was big.

02:53:01,538 --> 02:53:02,663
SPEAKER_1:  I'm sure you make him proud.

02:53:02,663 --> 02:53:04,798
SPEAKER_0:  I'm sure I do but

02:53:05,762 --> 02:53:06,558
SPEAKER_0:  I don't think he knew that.

02:53:06,978 --> 02:53:07,422
SPEAKER_0:  that I knew.

02:53:09,666 --> 02:53:12,222
SPEAKER_1:  Well, you get to pass on that love to your kids now.

02:53:12,898 --> 02:53:14,174
SPEAKER_0:  I try, I try, but...

02:53:14,594 --> 02:53:19,710
SPEAKER_0:  I can't impress him as much as my dad impressed me. I can try all I want.

02:53:19,842 --> 02:53:20,382
SPEAKER_1:  but...

02:53:20,866 --> 02:53:23,166
SPEAKER_1:  Well, what do you think is the role of love?

02:53:24,322 --> 02:53:31,902
SPEAKER_1:  Cause you gave me some grief, you busted my balls a little bit for talking about love a lot. What do you think is the role of love in the human condition?

02:53:32,098 --> 02:53:37,406
SPEAKER_0:  I think it's the greatest thing. I think everyone should be searching for it. If you don't have it, find it and get it as soon as you can.

02:53:37,730 --> 02:53:38,270
SPEAKER_0:  Um...

02:53:38,594 --> 02:53:40,574
SPEAKER_0:  I love my wife. I really do.

02:53:41,218 --> 02:53:44,126
SPEAKER_0:  I had no idea what love was until my kids were born.

02:53:44,674 --> 02:53:46,590
SPEAKER_0:  My son came out and...

02:53:47,266 --> 02:53:58,398
SPEAKER_0:  This is a funny story. He came out and I just wanted him to be safe and be healthy and all that. And I said to the doctor, I said, 10 and 10 doc, 10 fingers, 10 toes, everything good. And he goes, nine and nine.

02:53:59,234 --> 02:54:20,318
SPEAKER_0:  I was like, what the fuck? He's like, oh, this is gonna suck. Okay, we'll deal with it and all that. He was talking about the apnocharge cord or some score about breathing and color and all that. And I was like, oh shit, but no one told me this. But so I'm just sobbing. I couldn't even cut the umbilical cord. Like just fell in love with my kids when I saw them. And that to me really is what love is.

02:54:21,538 --> 02:54:22,270
SPEAKER_0:  For them, man.

02:54:22,402 --> 02:54:25,790
SPEAKER_1:  And I see that through your career that love developed, which is awesome.

02:54:26,146 --> 02:54:26,814
SPEAKER_1:  The...

02:54:27,458 --> 02:54:27,934
SPEAKER_1:  The...

02:54:28,194 --> 02:54:30,814
SPEAKER_1:  the being able to see the humanity in people.

02:54:31,650 --> 02:54:34,494
SPEAKER_0:  I didn't when I was young. The foolishness of youth.

02:54:35,330 --> 02:54:50,942
SPEAKER_0:  I needed to learn that lesson hard. I mean, when I was young in my career, it was just about career goals and resting people became stats. You rest someone, you get a good stat, you get an atta boy, maybe the boss likes it and you get a better job or you move up the chain.

02:54:52,194 --> 02:54:55,838
SPEAKER_0:  It took a real change in my life to see that humanity.

02:54:56,834 --> 02:55:00,574
SPEAKER_1:  And I can't wait to listen to your talk.

02:55:01,506 --> 02:55:03,422
SPEAKER_1:  probably hilarious and insightful.

02:55:03,970 --> 02:55:04,510
SPEAKER_1:  Um...

02:55:05,186 --> 02:55:07,454
SPEAKER_1:  given the life of the two of you lived.

02:55:07,778 --> 02:55:10,206
SPEAKER_1:  and given how much you've changed each other's lives.

02:55:11,202 --> 02:55:18,327
SPEAKER_1:  I can't wait to listen by the way. Thank you so much. This is a huge honor. You're an amazing person with an amazing life. This is an awesome conversation.

02:55:18,327 --> 02:55:22,174
SPEAKER_0:  Dude, huge fan. I love the podcast. Glad I could be here. Thanks for the invite.

02:55:22,690 --> 02:55:26,622
SPEAKER_0:  So exercising the brain too, it was great. Great conversation.

02:55:26,978 --> 02:55:30,654
SPEAKER_0:  and the heart too, right? Oh yeah, yeah, you got some tears there at the end.

02:55:31,938 --> 02:55:33,063
SPEAKER_0:  Thanks for listening to this conversation.

02:55:33,063 --> 02:55:34,110
SPEAKER_1:  with Crystal Bell.

02:55:34,498 --> 02:55:37,758
SPEAKER_1:  To support this podcast, please check out our sponsors in the description.

02:55:38,242 --> 02:55:38,718
SPEAKER_1:  And now...

02:55:38,978 --> 02:55:41,438
SPEAKER_1:  Let me leave you with some words from Benjamin Franklin.

02:55:42,242 --> 02:55:44,350
SPEAKER_1:  They who can give up essential liberty.

02:55:44,706 --> 02:55:46,590
SPEAKER_1:  to obtain a little temporary safety.

02:55:47,042 --> 02:55:48,606
SPEAKER_1:  deserve neither liberty

02:55:48,898 --> 02:55:49,822
SPEAKER_1:  North Safety.

02:55:50,850 --> 02:55:51,646
SPEAKER_1:  Thank you for listening.

02:55:51,874 --> 02:55:52,702
SPEAKER_1:  and hope to see you.

02:55:53,058 --> 02:55:53,534
SPEAKER_1:  next time.
