00:00:00,066 --> 00:00:10,430
SPEAKER_0:  I think it's possible that physics has exploits and we should be trying to find them, arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point.

00:00:10,786 --> 00:00:11,550
SPEAKER_0:  Synthetic.

00:00:11,874 --> 00:00:14,782
SPEAKER_0:  intelligences are kind of like the next stage of development.

00:00:15,362 --> 00:00:18,302
SPEAKER_0:  And I don't know where it leads to, like at some point.

00:00:18,722 --> 00:00:19,422
SPEAKER_0:  I suspect.

00:00:20,162 --> 00:00:22,206
SPEAKER_0:  The universe is some kind of a puzzle.

00:00:23,170 --> 00:00:26,046
SPEAKER_0:  these synthetic AIs will uncover that puzzle and.

00:00:27,074 --> 00:00:27,454
SPEAKER_1:  Solve it.

00:00:30,242 --> 00:00:32,510
SPEAKER_1:  The following is a conversation with Andrek Apathy.

00:00:32,930 --> 00:00:35,454
SPEAKER_1:  Previously, the director of AI at Tesla.

00:00:35,874 --> 00:00:36,670
SPEAKER_1:  And before that...

00:00:36,962 --> 00:00:37,918
SPEAKER_1:  at OpenAI.

00:00:38,146 --> 00:00:39,390
SPEAKER_1:  and Stanford.

00:00:39,938 --> 00:00:43,422
SPEAKER_1:  He is one of the greatest scientist engineers.

00:00:43,650 --> 00:00:44,702
SPEAKER_1:  and educators.

00:00:44,930 --> 00:00:47,198
SPEAKER_1:  in the history of artificial intelligence.

00:00:47,874 --> 00:00:48,254
SPEAKER_1:  This

00:00:48,482 --> 00:00:49,950
SPEAKER_1:  is the Lex Friedman podcast.

00:00:50,178 --> 00:00:52,478
SPEAKER_1:  To support it, please check out our sponsors.

00:00:52,802 --> 00:00:53,374
SPEAKER_1:  And now...

00:00:53,858 --> 00:00:54,654
SPEAKER_1:  Dear friends

00:00:55,074 --> 00:00:55,998
SPEAKER_1:  Here's Andre.

00:00:56,450 --> 00:00:57,022
SPEAKER_1:  body.

00:00:58,146 --> 00:00:59,774
SPEAKER_1:  What is a neural network?

00:01:00,226 --> 00:01:04,190
SPEAKER_1:  And why does it seem to do such a surprisingly good job of learning?

00:01:04,610 --> 00:01:07,774
SPEAKER_0:  What is a neural network? It's a mathematical abstraction of

00:01:09,026 --> 00:01:09,502
SPEAKER_0:  The brain.

00:01:10,082 --> 00:01:11,806
SPEAKER_0:  I would say that's how it was originally developed.

00:01:12,706 --> 00:01:16,958
SPEAKER_0:  At the end of the day, it's a mathematical expression. And it's a fairly simple mathematical expression when you get down to it.

00:01:17,378 --> 00:01:20,030
SPEAKER_0:  It's basically a sequence of

00:01:20,258 --> 00:01:23,006
SPEAKER_0:  matrix multiplies, which are really dot products mathematically.

00:01:23,490 --> 00:01:27,262
SPEAKER_0:  and some non-linearity is thrown in. And so it's a very simple mathematical expression.

00:01:27,682 --> 00:01:42,814
SPEAKER_0:  and it's got knobs in it. Many knobs. Many knobs. And these knobs are loosely related to basically the synapses in your brain. They're trainable, they're modifiable. And so the idea is like, we need to find the setting of the knobs that makes the neural net do whatever you want it to do, like classify images and so on.

00:01:43,586 --> 00:01:45,950
SPEAKER_0:  And so there's not too much mystery, I would say, in it.

00:01:46,338 --> 00:01:46,814
SPEAKER_0:  Um.

00:01:47,170 --> 00:01:48,062
SPEAKER_0:  You might think that.

00:01:48,642 --> 00:01:52,606
SPEAKER_0:  Basically don't want to endow it with too much meaning with respect to the brain and how it works.

00:01:52,834 --> 00:01:57,086
SPEAKER_0:  It's really just a complicated mathematical expression with knobs and those knobs need a proper setting.

00:01:57,442 --> 00:01:59,166
SPEAKER_0:  for it to do something desirable.

00:01:59,298 --> 00:02:03,166
SPEAKER_1:  Yeah, but poetry is just a collection of letters with spaces.

00:02:03,394 --> 00:02:09,502
SPEAKER_1:  But it can make us feel a certain way. And in that same way, when you get a large number of knobs together, whether it's in the...

00:02:09,826 --> 00:02:12,894
SPEAKER_1:  inside the brain or inside a computer, they seem to...

00:02:13,474 --> 00:02:15,934
SPEAKER_1:  They seem to surprise us with their power.

00:02:16,802 --> 00:02:18,238
SPEAKER_0:  I think that's fair, so basically...

00:02:18,594 --> 00:02:21,982
SPEAKER_0:  I'm underselling it by a lot because you definitely do get.

00:02:22,306 --> 00:02:26,846
SPEAKER_0:  very surprising emergent behaviors out of these neural nets when they're large enough and trained on.

00:02:27,266 --> 00:02:33,022
SPEAKER_0:  complicated enough problems, like say for example, the next word prediction in a massive dataset from the internet.

00:02:33,602 --> 00:02:37,278
SPEAKER_0:  And then these neural nets take on pretty surprising magical properties.

00:02:37,826 --> 00:02:42,142
SPEAKER_0:  I think it's kind of interesting how much you can get out of even very simple mathematical person.

00:02:42,338 --> 00:02:46,206
SPEAKER_1:  When your brain right now is talking, is it doing next word prediction?

00:02:47,234 --> 00:02:48,670
SPEAKER_1:  Or is he doing something more interesting?

00:02:49,186 --> 00:02:53,438
SPEAKER_0:  Well, it's definitely some kind of a generative model that's a GPT-like and prompted by you.

00:02:53,826 --> 00:02:56,222
SPEAKER_0:  Yeah, so you're giving me a prompt and...

00:02:56,578 --> 00:02:59,582
SPEAKER_0:  I'm kind of like responding to it in a generative way. And by yourself.

00:02:59,874 --> 00:03:00,702
SPEAKER_1:  Perhaps a little bit.

00:03:00,930 --> 00:03:01,822
SPEAKER_1:  Like, are you?

00:03:02,402 --> 00:03:06,014
SPEAKER_1:  adding extra prompts from your own memory inside your head.

00:03:06,626 --> 00:03:10,622
SPEAKER_0:  Well, definitely feels like you're referencing some kind of a declarative structure of like...

00:03:10,978 --> 00:03:13,118
SPEAKER_0:  memory and so on and then

00:03:13,826 --> 00:03:15,838
SPEAKER_0:  you're putting that together with your prompt and...

00:03:16,226 --> 00:03:17,351
SPEAKER_0:  giving away some answers.

00:03:17,351 --> 00:03:19,070
SPEAKER_1:  much of what you just said.

00:03:19,554 --> 00:03:21,022
SPEAKER_1:  has been said by you before.

00:03:21,890 --> 00:03:23,230
SPEAKER_1:  Nothing, basically, right?

00:03:23,618 --> 00:03:24,030
SPEAKER_1:  No, but...

00:03:24,738 --> 00:03:27,230
SPEAKER_1:  If you actually look at all the words you've ever said in your life.

00:03:27,810 --> 00:03:28,894
SPEAKER_1:  and you do a search.

00:03:29,506 --> 00:03:33,758
SPEAKER_1:  you'll probably have said a lot of the same words in the same order before.

00:03:34,178 --> 00:03:34,846
SPEAKER_0:  Yeah, could be.

00:03:35,426 --> 00:03:38,910
SPEAKER_0:  I mean, I'm using phrases that are common, et cetera, but I'm remixing it into.

00:03:39,202 --> 00:03:39,838
SPEAKER_0:  a pretty

00:03:40,130 --> 00:03:44,094
SPEAKER_0:  Sort of unique sentence at the end of the day, but you're right. Definitely. There's like a ton of remixing

00:03:44,290 --> 00:03:49,214
SPEAKER_1:  Why you didn't you just like Magnus Carlsen said I'm I'm a

00:03:49,762 --> 00:03:53,662
SPEAKER_1:  2,900 whatever, which is pretty decent. I think you're talking.

00:03:54,082 --> 00:03:57,374
SPEAKER_1:  very, you're not giving enough credit to your own nuts here.

00:03:57,698 --> 00:03:59,934
SPEAKER_1:  Why do they seem to?

00:04:00,866 --> 00:04:02,270
SPEAKER_1:  What's your best intuition?

00:04:03,266 --> 00:04:05,118
SPEAKER_1:  about this emergent behavior.

00:04:05,634 --> 00:04:08,478
SPEAKER_0:  I mean it's kind of interesting because I'm simultaneously underselling them.

00:04:08,898 --> 00:04:17,118
SPEAKER_0:  But I also feel like there's an element to which I'm over, like, it's actually kind of incredible that you can get so much emergent magical behavior out of them despite them being so simple mathematically.

00:04:17,634 --> 00:04:19,902
SPEAKER_0:  So I think those are kind of like two surprising statements that

00:04:20,290 --> 00:04:21,854
SPEAKER_0:  to kind of juxtapose together.

00:04:22,754 --> 00:04:26,622
SPEAKER_0:  And I think basically what it is is we are actually fairly good at optimizing these neural nuts.

00:04:27,202 --> 00:04:29,310
SPEAKER_0:  and when you give them a hard enough problem...

00:04:29,698 --> 00:04:32,446
SPEAKER_0:  They are forced to learn very interesting solutions.

00:04:32,802 --> 00:04:33,758
SPEAKER_0:  in the optimization.

00:04:34,114 --> 00:04:35,838
SPEAKER_0:  and those solutions basically.

00:04:36,354 --> 00:04:38,366
SPEAKER_0:  have these immersion properties that are very interesting.

00:04:38,914 --> 00:04:40,382
SPEAKER_1:  there's wisdom and knowledge.

00:04:41,314 --> 00:04:42,526
SPEAKER_1:  in the knobs.

00:04:42,882 --> 00:04:43,198
SPEAKER_1:  Mm-hmm.

00:04:43,842 --> 00:04:47,102
SPEAKER_1:  And so this representation that's in the knobs.

00:04:47,714 --> 00:04:52,382
SPEAKER_1:  doesn't make sense to you intuitively the large number of knobs can hold the representation

00:04:52,738 --> 00:04:54,078
SPEAKER_1:  that captures some deep.

00:04:54,402 --> 00:04:56,798
SPEAKER_1:  wisdom about the data it has looked at.

00:04:57,570 --> 00:04:58,398
SPEAKER_1:  a lot of knobs.

00:04:58,786 --> 00:04:59,646
SPEAKER_0:  It's a lot of knobs.

00:05:00,130 --> 00:05:03,230
SPEAKER_0:  And somehow, speaking concretely.

00:05:03,618 --> 00:05:07,166
SPEAKER_0:  One of the neural nets that people are very excited about right now are GPTs.

00:05:07,522 --> 00:05:10,078
SPEAKER_0:  which are basically just next word prediction networks.

00:05:10,306 --> 00:05:13,438
SPEAKER_0:  So you consume a sequence of words from the internet.

00:05:13,730 --> 00:05:14,942
SPEAKER_0:  and you try to predict the next word.

00:05:15,554 --> 00:05:16,318
SPEAKER_0:  and uh...

00:05:17,410 --> 00:05:20,350
SPEAKER_0:  once you train these on a large enough data set.

00:05:21,026 --> 00:05:22,878
SPEAKER_0:  They you can basically.

00:05:23,170 --> 00:05:26,398
SPEAKER_0:  these neural nets in arbitrary ways and you can ask them to solve problems and they will.

00:05:26,818 --> 00:05:28,190
SPEAKER_0:  so you can just to them.

00:05:28,578 --> 00:05:30,910
SPEAKER_0:  You can make it look like you're trying to.

00:05:31,970 --> 00:05:36,638
SPEAKER_0:  solve some kind of mathematical problem and they will continue what they think is the solution based on what they've seen on the internet.

00:05:36,962 --> 00:05:39,166
SPEAKER_0:  And very often, those solutions look very...

00:05:39,490 --> 00:05:40,574
SPEAKER_0:  Remarkably consistent.

00:05:40,802 --> 00:05:41,918
SPEAKER_0:  Look correct, potentially.

00:05:43,106 --> 00:05:49,150
SPEAKER_1:  Do you still think about the brain side of it? So as neural nets as an abstraction or mathematical abstraction of the brain?

00:05:49,570 --> 00:05:50,238
SPEAKER_1:  You still

00:05:50,850 --> 00:05:51,646
SPEAKER_1:  Draw wisdom.

00:05:52,450 --> 00:05:53,534
SPEAKER_1:  from, uh...

00:05:53,922 --> 00:05:56,030
SPEAKER_1:  from the biological neural networks.

00:05:56,354 --> 00:05:57,598
SPEAKER_1:  or even the bigger question.

00:05:57,826 --> 00:05:58,686
SPEAKER_1:  So you're a big fan of.

00:05:58,946 --> 00:06:00,894
SPEAKER_1:  biology and biological computation.

00:06:02,402 --> 00:06:07,070
SPEAKER_1:  What impressive thing is biology doing to you that computers are not yet?

00:06:07,874 --> 00:06:08,574
SPEAKER_1:  That gap.

00:06:09,154 --> 00:06:10,558
SPEAKER_0:  I would say I'm definitely on...

00:06:10,946 --> 00:06:15,646
SPEAKER_0:  I'm much more hesitant with the analogies to the brain than I think you would see potentially in the field.

00:06:16,322 --> 00:06:17,214
SPEAKER_0:  And I kind of feel like...

00:06:18,562 --> 00:06:19,070
SPEAKER_0:  Certainly.

00:06:19,522 --> 00:06:21,406
SPEAKER_0:  The way neural networks started is everything.

00:06:21,698 --> 00:06:26,942
SPEAKER_0:  stemmed from inspiration by the brain, but at the end of the day, the artifacts that you get after training.

00:06:27,170 --> 00:06:32,158
SPEAKER_0:  they are arrived at by a very different optimization process than the optimization process that gave rise to the brain.

00:06:32,898 --> 00:06:34,110
SPEAKER_0:  And so I think.

00:06:35,138 --> 00:06:38,142
SPEAKER_0:  I kind of think of it as a very complicated alien artifact.

00:06:38,370 --> 00:06:41,822
SPEAKER_0:  It's something different. I'm sorry the the neural mass that we're training

00:06:42,178 --> 00:06:45,022
SPEAKER_0:  Okay. They are complicated alien artifact.

00:06:45,378 --> 00:06:51,198
SPEAKER_0:  I do not make analogies to the brain because I think the optimization process that gave rise to it is very different from the brain.

00:06:51,842 --> 00:06:55,070
SPEAKER_0:  So there was no multi-agent self-play kind of setup.

00:06:55,682 --> 00:06:56,734
SPEAKER_0:  and evolution.

00:06:57,090 --> 00:07:01,662
SPEAKER_0:  It was an optimization that is basically what amounts to a compression objective.

00:07:01,922 --> 00:07:03,198
SPEAKER_0:  on a massive amount of data.

00:07:03,490 --> 00:07:06,590
SPEAKER_1:  Okay, so artificial neural networks are doing compression.

00:07:07,266 --> 00:07:09,470
SPEAKER_1:  and biological neural networks.

00:07:10,402 --> 00:07:12,583
SPEAKER_0:  I'm trying to survive and I'm not really doing it.

00:07:12,583 --> 00:07:19,230
SPEAKER_1:  They're an agent in a multi-agent self-play system that's been running for a very, very long time.

00:07:19,490 --> 00:07:20,062
SPEAKER_0:  That said...

00:07:20,322 --> 00:07:23,518
SPEAKER_0:  Evolution has found that it is very useful to...

00:07:23,906 --> 00:07:25,982
SPEAKER_0:  to predict and have a predictive model in the brain.

00:07:26,402 --> 00:07:27,262
SPEAKER_0:  And so I think.

00:07:27,522 --> 00:07:30,462
SPEAKER_0:  our brain utilizes something that looks like that as a part of it.

00:07:30,978 --> 00:07:32,286
SPEAKER_0:  but it had a lot more.

00:07:32,546 --> 00:07:34,142
SPEAKER_0:  you know, couches and gizmos and...

00:07:34,498 --> 00:07:35,966
SPEAKER_0:  value functions and

00:07:36,514 --> 00:07:40,542
SPEAKER_0:  ancient nuclei that are all trying to like make it survive and reproduce and everything else

00:07:40,770 --> 00:07:45,438
SPEAKER_1:  And the whole thing through embryo genesis is built from a single cell. I mean, it's just.

00:07:45,954 --> 00:07:47,582
SPEAKER_1:  The code is inside the DNA.

00:07:48,162 --> 00:07:50,622
SPEAKER_1:  and it just builds it up like the entire organism.

00:07:51,298 --> 00:07:52,254
SPEAKER_1:  with arms.

00:07:52,546 --> 00:07:54,366
SPEAKER_1:  And the head and legs.

00:07:54,626 --> 00:07:54,942
SPEAKER_0:  Yes.

00:07:55,202 --> 00:07:56,926
SPEAKER_1:  and like it does it pretty well.

00:07:57,698 --> 00:07:58,590
SPEAKER_0:  It should not be possible.

00:07:59,202 --> 00:08:01,150
SPEAKER_1:  So there's some learning going on. There's some.

00:08:01,442 --> 00:08:04,766
SPEAKER_1:  There's some kind of computation going through that building process.

00:08:05,026 --> 00:08:07,070
SPEAKER_1:  I mean, I don't know where.

00:08:08,098 --> 00:08:11,166
SPEAKER_1:  If you were just to look at the entirety of history of life on Earth.

00:08:11,970 --> 00:08:13,726
SPEAKER_1:  Where do you think is the most?

00:08:14,146 --> 00:08:16,958
SPEAKER_1:  interesting invention. Is it the origin of life itself?

00:08:17,762 --> 00:08:20,990
SPEAKER_1:  Is it just jumping to eukaryotes? Is it?

00:08:21,506 --> 00:08:25,982
SPEAKER_1:  Mammals as a humans themselves on the sapiens the origin

00:08:26,338 --> 00:08:27,358
SPEAKER_1:  of intelligence.

00:08:27,874 --> 00:08:29,342
SPEAKER_1:  highly complex intelligence.

00:08:29,762 --> 00:08:30,302
SPEAKER_1:  Wait, what are you-

00:08:30,530 --> 00:08:30,942
SPEAKER_1:  What?

00:08:31,778 --> 00:08:34,398
SPEAKER_1:  Or is it all just a continuation of the same kind of process?

00:08:35,970 --> 00:08:38,942
SPEAKER_0:  Certainly I would say it's an extremely remarkable story that, um...

00:08:39,202 --> 00:08:42,686
SPEAKER_0:  only briefly learning about recently, all the way from...

00:08:43,202 --> 00:08:52,926
SPEAKER_0:  Actually, you almost have to start at the formation of Earth and all of its conditions and the entire solar system and how everything is arranged with Jupiter and moon and the habitable zone and everything.

00:08:53,442 --> 00:08:54,750
SPEAKER_0:  and then you have an active Earth.

00:08:55,650 --> 00:08:56,798
SPEAKER_0:  that's turning over material.

00:08:57,602 --> 00:08:58,462
SPEAKER_0:  and um...

00:08:58,850 --> 00:09:04,926
SPEAKER_0:  And then you start with abiogenesis and everything. And so it's all like a pretty remarkable story. I'm not sure that...

00:09:05,666 --> 00:09:08,766
SPEAKER_0:  I can pick like a single unique piece of it.

00:09:09,090 --> 00:09:11,070
SPEAKER_0:  that I find most interesting.

00:09:12,386 --> 00:09:16,670
SPEAKER_0:  I guess for me as an artificial intelligence researcher, it's probably the last piece. We have lots of animals that...

00:09:16,962 --> 00:09:18,110
SPEAKER_0:  you know our

00:09:18,722 --> 00:09:21,598
SPEAKER_0:  are not building technological society, but we do.

00:09:22,178 --> 00:09:26,206
SPEAKER_0:  And it seems to have happened very quickly. It seems to have happened very recently.

00:09:26,786 --> 00:09:27,614
SPEAKER_0:  And, uh...

00:09:28,290 --> 00:09:29,630
SPEAKER_0:  Something very interesting happened there.

00:09:30,146 --> 00:09:33,918
SPEAKER_0:  that I don't fully understand. I almost understand everything else, I think intuitively.

00:09:34,178 --> 00:09:36,542
SPEAKER_0:  But I don't understand exactly that part.

00:09:36,802 --> 00:09:37,630
SPEAKER_1:  how quick it was.

00:09:37,986 --> 00:09:44,798
SPEAKER_1:  Both explanations will be interesting. One is that this is just a continuation of the same kind of process. There's nothing special about humans.

00:09:45,442 --> 00:09:48,638
SPEAKER_1:  That would be deeply understanding. That would be very interesting.

00:09:48,930 --> 00:09:50,942
SPEAKER_1:  that we think of ourselves as special but...

00:09:51,426 --> 00:09:55,998
SPEAKER_1:  It was obvious. It was already written in the code.

00:09:56,450 --> 00:09:57,406
SPEAKER_1:  that you would have.

00:09:58,658 --> 00:10:00,862
SPEAKER_1:  greater and greater intelligence emerging.

00:10:01,122 --> 00:10:01,630
SPEAKER_1:  And then.

00:10:02,338 --> 00:10:07,582
SPEAKER_1:  The other explanation was just something truly special happened, something like a rare event.

00:10:08,066 --> 00:10:08,670
SPEAKER_1:  or there's like.

00:10:09,026 --> 00:10:14,750
SPEAKER_1:  Crazy rare event like Space Odyssey. What would it be? See, if you say like the invention of fire.

00:10:16,066 --> 00:10:16,702
SPEAKER_1:  or

00:10:17,666 --> 00:10:18,590
SPEAKER_1:  The...

00:10:19,458 --> 00:10:22,206
SPEAKER_1:  as Richard Wrangham says, the beta males.

00:10:22,466 --> 00:10:25,278
SPEAKER_1:  deciding a clever way to kill the alpha males.

00:10:25,666 --> 00:10:32,254
SPEAKER_1:  by collaborating, so just optimizing the collaboration, the multi-agent aspect of the multi-agent and that.

00:10:32,706 --> 00:10:36,158
SPEAKER_1:  really being constrained on resources and trying to survive.

00:10:36,930 --> 00:10:40,254
SPEAKER_1:  The collaboration aspect is what created complex intelligence.

00:10:40,642 --> 00:10:48,318
SPEAKER_1:  It seems like it's a natural outgrowth of the evolutionary process. What could possibly be a magical thing that happened, like a rare?

00:10:48,898 --> 00:10:51,166
SPEAKER_1:  thing that would say that humans are actually.

00:10:51,682 --> 00:10:54,110
SPEAKER_1:  human level intelligence is actually a really rare thing.

00:10:54,530 --> 00:10:55,422
SPEAKER_1:  in the universe.

00:10:56,834 --> 00:11:00,318
SPEAKER_0:  Yeah, I'm hesitant to say that it is rare, by the way, but it definitely seems like...

00:11:01,058 --> 00:11:06,270
SPEAKER_0:  It's kind of like a punctuated equilibrium where you have lots of exploration and then you have certain leaps.

00:11:06,530 --> 00:11:07,614
SPEAKER_0:  sparse leaps in between.

00:11:07,906 --> 00:11:10,302
SPEAKER_0:  So of course, like Origin of Life would be one.

00:11:10,594 --> 00:11:11,422
SPEAKER_0:  you know, DNA.

00:11:11,842 --> 00:11:12,382
SPEAKER_0:  Sex.

00:11:12,898 --> 00:11:15,934
SPEAKER_0:  eukaryotic life.

00:11:16,162 --> 00:11:19,518
SPEAKER_0:  the endosymbiosis event where the archaeon ate all bacteria.

00:11:19,874 --> 00:11:22,270
SPEAKER_0:  just the whole thing and then of course emergence of...

00:11:22,786 --> 00:11:28,414
SPEAKER_0:  consciousness and so on. So it seems like definitely there are sparse events where massive amount of progress was made but yeah it's kind of hard to

00:11:28,642 --> 00:11:29,150
SPEAKER_0:  Pick one.

00:11:29,634 --> 00:11:30,942
SPEAKER_1:  So you don't think humans are...

00:11:31,586 --> 00:11:36,094
SPEAKER_1:  Unique. Gotta ask you, how many intelligent alien civilizations do you think are out there?

00:11:36,802 --> 00:11:37,662
SPEAKER_1:  And, uh...

00:11:38,178 --> 00:11:39,454
SPEAKER_1:  is their intelligence.

00:11:40,930 --> 00:11:42,846
SPEAKER_1:  different or similar to ours.

00:11:44,578 --> 00:11:45,662
SPEAKER_1:  Yeah.

00:11:45,954 --> 00:11:51,134
SPEAKER_0:  I've been preoccupied with this question quite a bit recently, basically the Fermi paradox and just thinking through.

00:11:51,458 --> 00:11:52,446
SPEAKER_0:  And the reason...

00:11:52,834 --> 00:11:54,494
SPEAKER_0:  actually that I am very interested in.

00:11:54,722 --> 00:11:57,054
SPEAKER_0:  the origin of life is fundamentally trying to understand.

00:11:57,442 --> 00:11:59,774
SPEAKER_0:  how common it is that there are technological societies out there.

00:12:00,194 --> 00:12:00,670
SPEAKER_0:  Uh...

00:12:01,378 --> 00:12:02,366
SPEAKER_0:  in space.

00:12:02,850 --> 00:12:06,334
SPEAKER_0:  And the more I study it, the more I think that...

00:12:07,554 --> 00:12:09,214
SPEAKER_0:  There should be quite a lot.

00:12:09,442 --> 00:12:12,702
SPEAKER_1:  Why haven't we heard from them? Cause I agree with you. It feels like...

00:12:13,314 --> 00:12:15,102
SPEAKER_1:  I just don't see.

00:12:15,938 --> 00:12:16,766
SPEAKER_1:  YA

00:12:17,378 --> 00:12:19,806
SPEAKER_1:  what we did here on Earth is so difficult to do.

00:12:20,162 --> 00:12:23,838
SPEAKER_0:  Yeah, and especially when you get into the details of it, I used to think Origin of Life was very...

00:12:24,130 --> 00:12:24,670
SPEAKER_0:  Um.

00:12:25,666 --> 00:12:30,014
SPEAKER_0:  It was this magical rare event, but then you read books like, for example, Nick Lane.

00:12:30,434 --> 00:12:32,382
SPEAKER_0:  uh the vital question

00:12:32,706 --> 00:12:38,430
SPEAKER_0:  life ascending, etc. And he really gets in and he really makes you believe that this is not that rare.

00:12:38,658 --> 00:12:39,518
SPEAKER_0:  Basic chemistry.

00:12:39,938 --> 00:12:51,070
SPEAKER_0:  You have an active earth and you have your alkaline vents and you have lots of alkaline waters mixing with the ocean. And you have your proton gradients and you have the little porous pockets of these alkaline vents that concentrate chemistry.

00:12:51,682 --> 00:12:52,478
SPEAKER_0:  And

00:12:52,834 --> 00:12:55,966
SPEAKER_0:  Basically as he steps through all of these little pieces, you start to understand that

00:12:56,354 --> 00:12:57,662
SPEAKER_0:  Actually this is not that.

00:12:58,434 --> 00:13:00,510
SPEAKER_0:  crazy. You could see this happen on other systems.

00:13:00,898 --> 00:13:01,342
SPEAKER_0:  Um...

00:13:01,570 --> 00:13:03,134
SPEAKER_0:  And he really takes you from.

00:13:03,714 --> 00:13:06,046
SPEAKER_0:  just a geology to primitive life.

00:13:06,370 --> 00:13:07,646
SPEAKER_0:  and he makes it feel like.

00:13:07,970 --> 00:13:08,894
SPEAKER_0:  is actually pretty plausible.

00:13:09,282 --> 00:13:11,486
SPEAKER_0:  and also like the origin of life.

00:13:12,354 --> 00:13:13,310
SPEAKER_0:  didn't...

00:13:13,538 --> 00:13:15,742
SPEAKER_0:  was actually fairly fast after the formation of Earth.

00:13:16,034 --> 00:13:21,982
SPEAKER_0:  If I remember correctly, just a few hundred million years or something like that after basically when it was possible, life actually arose.

00:13:22,466 --> 00:13:28,062
SPEAKER_0:  And so that makes me feel like that is not the constraint. That is not the limiting variable and that life should actually be fairly common.

00:13:28,738 --> 00:13:29,246
SPEAKER_0:  Um.

00:13:29,762 --> 00:13:32,798
SPEAKER_0:  And then where the drop-offs are is very...

00:13:33,026 --> 00:13:33,438
SPEAKER_0:  Um...

00:13:34,018 --> 00:13:35,166
SPEAKER_0:  is very interesting to think about.

00:13:35,522 --> 00:13:39,646
SPEAKER_0:  I currently think that there's no major drop-offs basically, and so there should be quite a lot of life.

00:13:40,066 --> 00:13:43,102
SPEAKER_0:  And basically what that brings me to then is.

00:13:43,330 --> 00:13:45,822
SPEAKER_0:  The only way to reconcile the fact that we haven't found anyone and so on.

00:13:46,050 --> 00:13:47,262
SPEAKER_0:  is that...

00:13:47,938 --> 00:13:49,342
SPEAKER_0:  We just can't, we can't see them.

00:13:49,762 --> 00:13:50,887
SPEAKER_0:  We can't observe them. Just.

00:13:50,887 --> 00:13:54,078
SPEAKER_1:  quick brief comment. Nick Lane and a lot of biologists I talked to.

00:13:54,498 --> 00:13:57,758
SPEAKER_1:  They really seem to think that the jump from bacteria.

00:13:58,370 --> 00:14:00,958
SPEAKER_1:  to more complex organisms is the hardest jump.

00:14:01,122 --> 00:14:02,718
SPEAKER_0:  the eukaryotic line, basically.

00:14:03,170 --> 00:14:03,934
SPEAKER_1:  Which I don't-

00:14:04,354 --> 00:14:05,406
SPEAKER_1:  I get it, there are much-

00:14:06,434 --> 00:14:10,782
SPEAKER_1:  more knowledgeable than me about the intricacies of biology.

00:14:11,138 --> 00:14:11,806
SPEAKER_1:  That seems like-

00:14:12,194 --> 00:14:13,790
SPEAKER_1:  crazy cuz how much

00:14:14,018 --> 00:14:16,350
SPEAKER_1:  How many single cell organisms are there?

00:14:16,706 --> 00:14:19,582
SPEAKER_1:  Like, and how much time you have, surely.

00:14:19,970 --> 00:14:21,086
SPEAKER_1:  It's not that difficult.

00:14:21,378 --> 00:14:23,742
SPEAKER_1:  Like in a billion years is not even that long.

00:14:24,674 --> 00:14:25,886
SPEAKER_1:  of a time really.

00:14:26,562 --> 00:14:29,086
SPEAKER_1:  all these bacteria under constrained resources.

00:14:29,410 --> 00:14:33,598
SPEAKER_1:  battling it out, I'm sure they can invent more complex. I don't understand.

00:14:34,018 --> 00:14:39,454
SPEAKER_1:  how to move from a hello world program to like invent a function or something like that. learn everything that Python tells you how…

00:14:40,002 --> 00:14:40,734
SPEAKER_1:  I...

00:14:41,122 --> 00:14:46,430
SPEAKER_1:  So I don't, yeah, so I'm with you. I just feel like I don't see any, if the origin of life.

00:14:46,850 --> 00:14:51,486
SPEAKER_1:  That would be my intuition, that's the hardest thing. But if that's not the hardest thing because it happens so quickly.

00:14:51,842 --> 00:14:55,198
SPEAKER_1:  then it's gotta be everywhere. and yeah maybe we're just too dumb to see it.

00:14:55,362 --> 00:14:59,486
SPEAKER_0:  Well, it's just we don't have really good mechanisms for seeing this life. I mean, by what.

00:15:00,290 --> 00:15:00,926
SPEAKER_0:  How, um...

00:15:01,154 --> 00:15:13,598
SPEAKER_0:  So I'm not an expert just to preface this, but just from what I've been at it, I want to meet an expert on alien intelligence and how to communicate. I'm very suspicious of our ability to to find these intelligences out there and to find these earth-like things.

00:15:13,826 --> 00:15:18,686
SPEAKER_0:  radio waves for example are are terrible their power drops off as basically one over r square

00:15:18,978 --> 00:15:19,550
SPEAKER_0:  Also...

00:15:19,970 --> 00:15:22,782
SPEAKER_0:  I remember reading that our current radio waves would not be...

00:15:23,042 --> 00:15:26,558
SPEAKER_0:  the ones that we are broadcasting would not be.

00:15:26,850 --> 00:15:31,614
SPEAKER_0:  measurable by our devices today. Was it like 1 tenth of a light year away? Not even.

00:15:32,002 --> 00:15:33,566
SPEAKER_0:  Basically, tiny distance because...

00:15:33,890 --> 00:15:38,270
SPEAKER_0:  You really need like a targeted transmission of massive power directed somewhere.

00:15:38,658 --> 00:15:41,022
SPEAKER_0:  for this to be picked up on long distances.

00:15:41,410 --> 00:15:43,902
SPEAKER_0:  And so I just think that our ability to measure is...

00:15:44,194 --> 00:15:46,814
SPEAKER_0:  is not amazing. I think there's probably other civilizations out there.

00:15:47,042 --> 00:15:52,254
SPEAKER_0:  And then the big question is why don't they build one-on-one probes and why don't they interstellar travel across the entire galaxy?

00:15:52,482 --> 00:15:55,710
SPEAKER_0:  And my current answer is it's probably interstellar travel It's luck really hard.

00:15:56,386 --> 00:16:01,246
SPEAKER_0:  You have the interstellar medium if you want to move at close to speed of light, you're going to be encountering bullets along the way.

00:16:01,794 --> 00:16:03,134
SPEAKER_0:  because even like tiny

00:16:03,778 --> 00:16:08,862
SPEAKER_0:  hydrogen atoms and little particles of dust are basically have massive kinetic energy at those speeds.

00:16:09,506 --> 00:16:11,230
SPEAKER_0:  And so basically you need some kind of shielding.

00:16:11,458 --> 00:16:13,438
SPEAKER_0:  You have all the cosmic radiation.

00:16:13,730 --> 00:16:15,742
SPEAKER_0:  It's just like brutal out there. It's really hard.

00:16:16,034 --> 00:16:19,166
SPEAKER_0:  And so my thinking is maybe Interstellar Travel is just extremely hard.

00:16:19,842 --> 00:16:20,967
SPEAKER_0:  I think you have to go very slow.

00:16:20,967 --> 00:16:22,302
SPEAKER_1:  to build hard.

00:16:24,002 --> 00:16:25,342
SPEAKER_1:  It feels like a...

00:16:25,922 --> 00:16:28,547
SPEAKER_1:  It feels like we're not a billion years away from doing that.

00:16:28,547 --> 00:16:34,046
SPEAKER_0:  It just might be that it's very, you have to go very slowly, potentially as an example, through space.

00:16:34,274 --> 00:16:34,622
SPEAKER_1:  Right.

00:16:34,850 --> 00:16:35,975
SPEAKER_1:  as opposed to a close.

00:16:35,975 --> 00:16:41,406
SPEAKER_0:  the speed of light. So I'm suspicious basically of our ability to measure life and I'm suspicious of the ability to not measure!

00:16:41,666 --> 00:16:44,126
SPEAKER_0:  permeate all of space in the galaxy or across galaxies.

00:16:44,482 --> 00:16:46,046
SPEAKER_0:  and that's the only way that I can...

00:16:46,498 --> 00:16:47,646
SPEAKER_0:  currently see where you're on that.

00:16:48,002 --> 00:16:50,398
SPEAKER_1:  It's kind of mind blowing to think that there is.

00:16:51,330 --> 00:16:54,366
SPEAKER_1:  trillions of intelligent alien civilizations out there.

00:16:54,914 --> 00:16:57,374
SPEAKER_1:  kind of slowly traveling through space.

00:16:58,114 --> 00:16:59,902
SPEAKER_1:  to meet each other and some of them meet.

00:17:00,290 --> 00:17:01,918
SPEAKER_1:  Some of them go to war, some of them...

00:17:02,242 --> 00:17:02,942
SPEAKER_1:  collaborate

00:17:04,002 --> 00:17:08,574
SPEAKER_0:  Or they're all just independent. They're all just like little pockets.

00:17:08,930 --> 00:17:10,206
SPEAKER_1:  Well, statistically...

00:17:10,466 --> 00:17:11,262
SPEAKER_1:  if there's like...

00:17:12,002 --> 00:17:15,687
SPEAKER_1:  If it's trillions of them, surely some of the pockets are close.

00:17:15,687 --> 00:17:17,374
SPEAKER_0:  enough to get some of them happen to be close.

00:17:17,698 --> 00:17:22,206
SPEAKER_1:  in close enough to see each other. And then once you see, once you see something that.

00:17:23,714 --> 00:17:27,198
SPEAKER_1:  Definitely complex life like if we see something

00:17:28,162 --> 00:17:34,622
SPEAKER_1:  we're probably going to be severe, like intensely, aggressively motivated to figure out what the hell that is and try to meet them.

00:17:34,946 --> 00:17:38,014
SPEAKER_1:  But what would be your first instinct to try to...

00:17:38,434 --> 00:17:39,902
SPEAKER_1:  like at a generational level.

00:17:40,418 --> 00:17:41,086
SPEAKER_1:  Meet them.

00:17:41,538 --> 00:17:42,142
SPEAKER_1:  or

00:17:42,754 --> 00:17:43,870
SPEAKER_1:  defend against them.

00:17:44,482 --> 00:17:46,110
SPEAKER_1:  or we'll be your...

00:17:46,722 --> 00:17:47,294
SPEAKER_1:  instinct.

00:17:47,874 --> 00:17:49,950
SPEAKER_1:  As a president of the United States.

00:17:50,850 --> 00:17:51,710
SPEAKER_1:  and the scientists.

00:17:52,770 --> 00:17:54,878
SPEAKER_1:  I don't know which hat you prefer in this question.

00:17:55,522 --> 00:17:58,014
SPEAKER_0:  Yeah, I think the question, it's really hard.

00:17:58,498 --> 00:17:59,006
SPEAKER_0:  Um.

00:17:59,938 --> 00:18:02,334
SPEAKER_0:  I will say like for example for us.

00:18:02,818 --> 00:18:14,302
SPEAKER_0:  We have lots of primitive life forms on Earth next to us. We have all kinds of ants and everything else, and we share space with them. And we are hesitant to impact on them, and we are trying to protect them by default.

00:18:14,754 --> 00:18:19,262
SPEAKER_0:  because they are amazing interesting dynamical systems that took a long time to evolve and they are

00:18:19,554 --> 00:18:20,894
SPEAKER_0:  interesting and special and...

00:18:21,474 --> 00:18:23,518
SPEAKER_0:  I don't know that you wanna...

00:18:24,162 --> 00:18:25,310
SPEAKER_0:  Destroy that by default.

00:18:25,922 --> 00:18:27,646
SPEAKER_0:  And so I like.

00:18:28,354 --> 00:18:31,262
SPEAKER_0:  complex dynamical systems that took a lot of time to evolve.

00:18:31,650 --> 00:18:32,926
SPEAKER_0:  I think...

00:18:33,762 --> 00:18:41,182
SPEAKER_0:  I'd like to preserve it if I can afford to. And I'd like to think that the same would be true about the galactic resources and that...

00:18:41,474 --> 00:18:49,429
SPEAKER_0:  they would think that we're kind of incredible, interesting story that took time. It took a few billion years to unravel and you don't want to just destroy it.

00:18:49,429 --> 00:18:52,574
SPEAKER_1:  two aliens talking about earth right now and saying

00:18:53,026 --> 00:19:03,358
SPEAKER_1:  I'm a big fan of complex dynamical systems, so I think it was a value to preserve these, and who basically are a video game they watch, or show a TV show that they watch.

00:19:04,258 --> 00:19:06,974
SPEAKER_0:  Yeah, I think you would need like a very good reason I think to...

00:19:07,682 --> 00:19:14,078
SPEAKER_0:  destroy it. Like, why don't we destroy these ant farms and so on? It's because we're not actually like really in direct competition with them right now.

00:19:14,338 --> 00:19:16,318
SPEAKER_0:  We do it accidentally and so on but

00:19:16,770 --> 00:19:17,278
SPEAKER_0:  Um.

00:19:18,050 --> 00:19:22,142
SPEAKER_0:  There's plenty of resources. And so why would you destroy something that is so interesting and precious?

00:19:22,370 --> 00:19:25,118
SPEAKER_1:  Well, from a scientific perspective, you might probe it.

00:19:25,634 --> 00:19:27,486
SPEAKER_1:  You might interact with it lightly.

00:19:27,586 --> 00:19:29,150
SPEAKER_0:  You might want to learn something from it, right?

00:19:29,570 --> 00:19:35,806
SPEAKER_1:  So I wonder, there could be certain physical phenomena that we think is a physical phenomena, but it's actually interacting with us.

00:19:36,034 --> 00:19:36,414
SPEAKER_1:  like.

00:19:36,674 --> 00:19:38,334
SPEAKER_1:  poke the finger and see what happens.

00:19:38,466 --> 00:19:42,334
SPEAKER_0:  I think it should be very interesting to scientists, other alien scientists, what happened here.

00:19:42,978 --> 00:19:48,318
SPEAKER_0:  And what we're seeing today is a snapshot. Basically, it's a result of a huge amount of computation.

00:19:49,666 --> 00:19:52,542
SPEAKER_0:  of over like billion years or something like that. So.

00:19:52,706 --> 00:19:54,526
SPEAKER_1:  It could have been initiated by aliens.

00:19:54,946 --> 00:19:57,118
SPEAKER_1:  This could be a computer running a program.

00:19:57,410 --> 00:20:00,702
SPEAKER_1:  Okay, if you had the power to do this, when you... Okay.

00:20:01,026 --> 00:20:02,814
SPEAKER_1:  For sure. At least I would.

00:20:03,234 --> 00:20:03,934
SPEAKER_1:  I would pick...

00:20:04,802 --> 00:20:09,854
SPEAKER_1:  Earth like planet that has the conditions based my understanding of the chemistry prerequisites for life

00:20:10,626 --> 00:20:11,998
SPEAKER_1:  and I would see it with life.

00:20:12,322 --> 00:20:12,894
SPEAKER_1:  and run it.

00:20:13,826 --> 00:20:18,494
SPEAKER_1:  Right? Like wouldn't you 100% do that and observe it and then protect.

00:20:19,234 --> 00:20:23,774
SPEAKER_1:  I mean that that's not just a hell of a good TV show. It's a good scientific experiment.

00:20:24,450 --> 00:20:24,990
SPEAKER_1:  and

00:20:25,954 --> 00:20:32,158
SPEAKER_1:  of that, it's physical simulation, right? What maybe, maybe the evolution.

00:20:32,450 --> 00:20:34,558
SPEAKER_1:  is the most like actually running it.

00:20:35,554 --> 00:20:35,966
SPEAKER_1:  Uh...

00:20:36,226 --> 00:20:39,070
SPEAKER_1:  is the most efficient way to understand.

00:20:39,522 --> 00:20:41,397
SPEAKER_1:  computation or to compute stuff.

00:20:41,397 --> 00:20:42,654
SPEAKER_0:  or to understand life or...

00:20:42,914 --> 00:20:45,854
SPEAKER_0:  what life looks like and what branches it can take.

00:20:46,178 --> 00:20:50,238
SPEAKER_1:  It does make me kind of feel weird that we're part of a science experiment, but maybe it's...

00:20:50,594 --> 00:20:54,366
SPEAKER_1:  Everything's a science experiment, so does that change anything for us?

00:20:54,978 --> 00:20:56,862
SPEAKER_1:  before a science experiment? um

00:20:57,410 --> 00:21:01,535
SPEAKER_1:  I don't know. Two descendants of apes talking about being inside of a science.

00:21:01,535 --> 00:21:06,270
SPEAKER_0:  I'm suspicious of this idea of like a deliberate pen-spamia as you described it, Sir.

00:21:06,594 --> 00:21:10,878
SPEAKER_0:  I don't see a divine intervention in some way in the historical record right now.

00:21:11,202 --> 00:21:13,630
SPEAKER_0:  I do feel like the story.

00:21:14,050 --> 00:21:20,190
SPEAKER_0:  in these books like Nic Lane's books and so on sort of makes sense and it makes sense how life arose on earth uniquely.

00:21:20,642 --> 00:21:21,406
SPEAKER_0:  and uh...

00:21:21,634 --> 00:21:25,150
SPEAKER_0:  Yeah, I don't need to reach for more exotic explanations right now.

00:21:25,410 --> 00:21:28,062
SPEAKER_1:  Sure but NPCs inside a video game don't.

00:21:28,418 --> 00:21:31,742
SPEAKER_1:  Don't observe any divine intervention either.

00:21:32,034 --> 00:21:35,102
SPEAKER_1:  We might just be all NPCs running a kind of code

00:21:35,426 --> 00:21:42,590
SPEAKER_0:  Maybe eventually they will. Currently NPCs are really dumb, but once they're running GPTs, maybe they will be like, Okay, this is really suspicious, what the hell?

00:21:43,042 --> 00:21:49,214
SPEAKER_1:  So you are famously tweeted it looks like if you bombard earth with photons for a while

00:21:49,506 --> 00:21:50,878
SPEAKER_1:  You can emit a roadster.

00:21:51,682 --> 00:21:52,094
SPEAKER_1:  So.

00:21:52,482 --> 00:21:59,294
SPEAKER_1:  If like in Hitchhiker's Guide to the Galaxy, we would summarize the story of Earth. So in that book, it's mostly harmless.

00:21:59,938 --> 00:22:04,990
SPEAKER_1:  What do you think is all the possible stories like a paragraph long or sentence long?

00:22:05,826 --> 00:22:07,870
SPEAKER_1:  that Earth could be summarized as.

00:22:08,386 --> 00:22:13,182
SPEAKER_1:  at once it's done, it's computation. So like all the possible full.

00:22:14,402 --> 00:22:16,030
SPEAKER_1:  If Earth is a book, right?

00:22:17,122 --> 00:22:17,534
SPEAKER_1:  Uh...

00:22:17,762 --> 00:22:19,614
SPEAKER_1:  Probably there has to be an ending.

00:22:19,906 --> 00:22:24,990
SPEAKER_1:  I mean, there's going to be an end to earth and it could end in all kinds of ways. It can end soon, it can end later.

00:22:25,314 --> 00:22:27,134
SPEAKER_1:  What do you think are the possible stories?

00:22:27,490 --> 00:22:28,958
SPEAKER_0:  Well definitely there seems to be...

00:22:29,858 --> 00:22:30,590
SPEAKER_0:  Yeah, you're sort of.

00:22:32,002 --> 00:22:34,558
SPEAKER_0:  It's pretty incredible that these self-replicating systems will

00:22:35,202 --> 00:22:36,670
SPEAKER_0:  basically arise from the dynamics.

00:22:37,250 --> 00:22:41,310
SPEAKER_0:  and then they perpetuate themselves and become more complex and eventually become conscious and build.

00:22:41,858 --> 00:22:44,862
SPEAKER_0:  society. And I kind of feel like in some sense it's kind of like

00:22:45,474 --> 00:22:46,846
SPEAKER_0:  deterministic wave.

00:22:47,298 --> 00:22:50,334
SPEAKER_0:  that you know that kind of just like happens on any

00:22:50,626 --> 00:22:53,214
SPEAKER_0:  you know, any sufficiently well-arranged system like Earth.

00:22:53,954 --> 00:22:57,246
SPEAKER_0:  And so I kind of feel like there's a certain sense of inevitability in it.

00:22:57,506 --> 00:22:57,950
SPEAKER_0:  Um.

00:22:58,530 --> 00:22:59,454
SPEAKER_0:  and it's really beautiful.

00:22:59,810 --> 00:23:01,630
SPEAKER_1:  And it ends somehow, right? So it's a-

00:23:01,890 --> 00:23:02,590
SPEAKER_1:  It's a...

00:23:03,394 --> 00:23:04,222
SPEAKER_1:  chemically.

00:23:05,378 --> 00:23:07,134
SPEAKER_1:  a diverse environment.

00:23:07,906 --> 00:23:10,462
SPEAKER_1:  where complex dynamical systems can...

00:23:11,394 --> 00:23:15,358
SPEAKER_1:  evolve and become more and further complex.

00:23:15,714 --> 00:23:17,182
SPEAKER_1:  but then there's a certain...

00:23:18,242 --> 00:23:18,750
SPEAKER_1:  Um...

00:23:19,618 --> 00:23:22,462
SPEAKER_1:  What is it? There's certain terminating conditions.

00:23:23,682 --> 00:23:28,542
SPEAKER_0:  Yeah, I don't know what determining conditions are, but definitely there's a trend line of something and we're part of that story and

00:23:28,802 --> 00:23:30,334
SPEAKER_0:  Like, where does that, where does it go?

00:23:30,722 --> 00:23:37,182
SPEAKER_0:  So, you know, we're famously described often as a biological bootloader for AIs, and that's because humans, I mean, you know, we're an incredible.

00:23:37,794 --> 00:23:40,542
SPEAKER_0:  biological system and we're capable of computation and

00:23:41,154 --> 00:23:43,422
SPEAKER_0:  uh... you know and love and so on

00:23:43,874 --> 00:23:44,254
SPEAKER_0:  Um.

00:23:44,514 --> 00:23:51,742
SPEAKER_0:  but we're extremely inefficient as well. Like we're talking to each other through audio. It's just kind of embarrassing, honestly, that we're manipulating like seven symbols.

00:23:51,970 --> 00:23:53,246
SPEAKER_0:  uh, serially.

00:23:53,474 --> 00:23:57,406
SPEAKER_0:  We're using vocal cords. It's all happening over multiple seconds. Now, how am I gonna get it If I was you chocolate

00:23:57,634 --> 00:24:00,126
SPEAKER_0:  It's just like kind of embarrassing when you step down to the.

00:24:00,834 --> 00:24:03,390
SPEAKER_0:  frequencies at which computers operate.

00:24:03,650 --> 00:24:08,542
SPEAKER_0:  or are able to cooperate on. And so basically it does seem like synthetic.

00:24:08,962 --> 00:24:11,806
SPEAKER_0:  intelligences are kind of like the next stage of development.

00:24:12,354 --> 00:24:13,150
SPEAKER_0:  And —

00:24:13,378 --> 00:24:15,742
SPEAKER_0:  I don't know where it leads to, like at some point.

00:24:16,162 --> 00:24:16,862
SPEAKER_0:  I suspect.

00:24:17,346 --> 00:24:19,006
SPEAKER_0:  the universe is some kind of a

00:24:19,234 --> 00:24:19,614
SPEAKER_0:  puzzle.

00:24:20,610 --> 00:24:24,766
SPEAKER_0:  And these synthetic AIs will uncover that puzzle. And um.

00:24:25,410 --> 00:24:25,790
SPEAKER_1:  Solve it.

00:24:26,786 --> 00:24:31,230
SPEAKER_1:  and then what happens after, right? Like what, cause if you just like fast forward or.

00:24:31,682 --> 00:24:34,750
SPEAKER_1:  many billions of years, it's like, it's quiet.

00:24:35,138 --> 00:24:35,742
SPEAKER_1:  And then it's like.

00:24:36,066 --> 00:24:41,118
SPEAKER_1:  to turmoil you see like city lights and stuff like that. And then what happens at the end like is it like a poof.

00:24:41,986 --> 00:24:43,294
SPEAKER_1:  Is it or is it like a.

00:24:43,618 --> 00:24:51,806
SPEAKER_1:  Is it calming? Is it explosion? Is it like earth like open, like a giant? Cause you said, emit roasters. Like, let's start emitting like.

00:24:52,450 --> 00:24:53,822
SPEAKER_1:  Like a giant.

00:24:54,146 --> 00:24:56,318
SPEAKER_1:  Number of like satellites.

00:24:56,674 --> 00:24:59,102
SPEAKER_0:  Yes, it's some kind of a crazy explosion and we're living.

00:24:59,330 --> 00:24:59,742
SPEAKER_0:  We're like.

00:25:00,034 --> 00:25:17,950
SPEAKER_0:  We're stepping through a explosion, and we're living day to day, and it doesn't look like it. But it's actually, I saw a very cool animation of Earth and life on Earth, and basically nothing happens for a long time, and then the last two seconds, basically cities and everything, and the lower orbit just gets cluttered, and just the whole thing happens in the last two seconds, and then you're like.

00:25:18,338 --> 00:25:18,974
SPEAKER_0:  This is exploding.

00:25:19,234 --> 00:25:21,109
SPEAKER_0:  This is a statement explosion.

00:25:21,109 --> 00:25:27,477
SPEAKER_1:  So if you play, yeah, yeah, if you play it at normal speed, yeah, it'll just look like an explosion.

00:25:27,477 --> 00:25:29,950
SPEAKER_0:  It's a firecracker. We're living in a firecracker.

00:25:30,434 --> 00:25:36,030
SPEAKER_1:  where it's going to start emitting all kinds of interesting things. Yeah. So, exposure doesn't...

00:25:36,258 --> 00:25:38,366
SPEAKER_1:  It might actually look like a little explosion.

00:25:38,690 --> 00:25:42,270
SPEAKER_1:  with lights and fire and energy emitted, all that kind of stuff.

00:25:42,690 --> 00:25:46,782
SPEAKER_1:  When you look inside the details of the explosion, there's actual complexity.

00:25:47,042 --> 00:25:47,678
SPEAKER_1:  happening.

00:25:47,970 --> 00:25:51,925
SPEAKER_1:  where there's like a human life or some kind of life.

00:25:51,925 --> 00:25:54,558
SPEAKER_0:  We hope it's not a destructive firecracker, it's kind of like a...

00:25:55,106 --> 00:25:56,981
SPEAKER_0:  constructive.

00:25:56,981 --> 00:25:57,438
SPEAKER_1:  Cracker.

00:25:57,922 --> 00:25:59,006
SPEAKER_1:  Alright, so given that...

00:25:59,298 --> 00:26:00,126
SPEAKER_1:  I think, uh...

00:26:00,290 --> 00:26:03,710
SPEAKER_0:  hilarious discussion. really interesting to think about like what the puzzle of the universe is.

00:26:03,938 --> 00:26:08,222
SPEAKER_0:  that the creator of the universe give us a message. Like for example in the book Contact.

00:26:08,450 --> 00:26:10,910
SPEAKER_0:  Carl Sagan. There's a message for humanity.

00:26:11,266 --> 00:26:11,582
SPEAKER_0:  Far-

00:26:12,258 --> 00:26:13,726
SPEAKER_0:  Any civilization in the-

00:26:14,018 --> 00:26:14,590
SPEAKER_0:  Digits.

00:26:15,074 --> 00:26:17,758
SPEAKER_0:  in the expansion of Pi in base 11 eventually.

00:26:18,114 --> 00:26:19,134
SPEAKER_0:  which is kind of interesting thought.

00:26:19,586 --> 00:26:20,446
SPEAKER_0:  maybe.

00:26:20,770 --> 00:26:22,654
SPEAKER_0:  Maybe we're supposed to be giving a message to our creator.

00:26:23,106 --> 00:26:27,774
SPEAKER_0:  maybe we're supposed to somehow create some kind of a quantum mechanical system that alerts them to

00:26:28,194 --> 00:26:28,510
SPEAKER_0:  Help.

00:26:28,802 --> 00:26:29,726
SPEAKER_0:  intelligent presence here.

00:26:30,146 --> 00:26:31,646
SPEAKER_0:  because if you think about it from their perspective.

00:26:31,874 --> 00:26:33,662
SPEAKER_0:  It's just say like quantum field theory.

00:26:33,890 --> 00:26:36,190
SPEAKER_0:  massive Lexilior Tanabaton-like thing.

00:26:36,738 --> 00:26:39,454
SPEAKER_0:  And like, how do you even notice that we exist? You might not even-

00:26:39,746 --> 00:26:41,566
SPEAKER_0:  be able to pick us up in that simulation.

00:26:42,114 --> 00:26:46,974
SPEAKER_0:  And so how do you prove that you exist, that you're intelligent and that you're part of the universe?

00:26:47,554 --> 00:26:49,918
SPEAKER_1:  So this is like a touring test for intelligence from Earth.

00:26:50,306 --> 00:26:58,142
SPEAKER_1:  Yeah, I got the creators. Uh, I mean, maybe this is like trying to complete the next word in a sentence. This is a complicated way of that. Like Earth is just.

00:26:59,074 --> 00:27:00,949
SPEAKER_1:  is basically sending a message back.

00:27:00,949 --> 00:27:03,870
SPEAKER_0:  Yeah, the puzzle is basically like alerting the creator that we exist.

00:27:04,098 --> 00:27:08,222
SPEAKER_0:  Or maybe the puzzle is just to just break out of the system and just, you know.

00:27:08,514 --> 00:27:13,150
SPEAKER_0:  stick it to the creator in some way. Basically, like if you're playing a video game you can.

00:27:14,146 --> 00:27:17,950
SPEAKER_0:  you can somehow find an exploit and find a way to execute on the host machine.

00:27:18,178 --> 00:27:19,326
SPEAKER_0:  uh... in your victory code

00:27:19,650 --> 00:27:20,574
SPEAKER_0:  There's some...

00:27:20,962 --> 00:27:25,214
SPEAKER_0:  For example, I believe someone got a game of Mario to play Pong, just by...

00:27:25,730 --> 00:27:27,422
SPEAKER_0:  exploiting it and then.

00:27:28,034 --> 00:27:28,542
SPEAKER_0:

00:27:29,218 --> 00:27:32,798
SPEAKER_0:  basically writing code and being able to execute arbitrary code in the game.

00:27:33,218 --> 00:27:36,830
SPEAKER_0:  And so maybe we should be, maybe that's the puzzle, is that we should be...

00:27:37,986 --> 00:27:39,550
SPEAKER_0:  find a way to exploit it.

00:27:39,778 --> 00:27:46,942
SPEAKER_0:  So I think some of these synthetic AIs will eventually find the universe to be some kind of a puzzle and then solve it in some way. And that's kind of like the end game somehow.

00:27:47,586 --> 00:27:49,502
SPEAKER_1:  Do you often think about it as a...

00:27:50,018 --> 00:27:51,070
SPEAKER_1:  as a simulation.

00:27:51,426 --> 00:27:51,870
SPEAKER_1:  So...

00:27:52,610 --> 00:27:58,238
SPEAKER_1:  As the universe being a kind of computation that has might have bugs and exploits. Yes.

00:27:58,370 --> 00:28:04,318
SPEAKER_0:  Yeah, I think so. Is that what physics is essentially? I think it's possible that physics has exploits and we should be trying to find them.

00:28:04,546 --> 00:28:07,390
SPEAKER_0:  arranging some kind of a crazy quantum mechanical system that

00:28:07,682 --> 00:28:11,646
SPEAKER_0:  somehow gives you buffer overflow, somehow gives you a rounding error in the floating point.

00:28:11,970 --> 00:28:12,638
SPEAKER_0:  Uh...

00:28:12,866 --> 00:28:14,014
SPEAKER_0:  Ah.

00:28:14,146 --> 00:28:18,878
SPEAKER_1:  Yeah, that's right. And like more and more sophisticated exploits.

00:28:19,106 --> 00:28:20,981
SPEAKER_1:  those are jokes but that could be actually

00:28:20,981 --> 00:28:23,358
SPEAKER_0:  We'll find some way to extract infinite energy.

00:28:23,810 --> 00:28:29,118
SPEAKER_0:  For example, when you train reinforcement learning agents in physical simulations and you ask them to say run.

00:28:29,698 --> 00:28:31,006
SPEAKER_0:  quickly on the flat ground.

00:28:31,362 --> 00:28:33,598
SPEAKER_0:  they'll end up doing all kinds of weird things.

00:28:33,890 --> 00:28:38,430
SPEAKER_0:  in part of that optimization, right? They'll get on their back leg and they'll slide across the floor.

00:28:38,722 --> 00:28:45,918
SPEAKER_0:  And it's because the optimization, the enforcement learning optimization on that agent has figured out a way to extract infinite energy from the friction forces and

00:28:46,210 --> 00:28:48,318
SPEAKER_0:  Basically, their port implementation.

00:28:48,546 --> 00:28:53,502
SPEAKER_0:  and they found a way to generate infinite energy and just slide across the surface. And it's not what you expected, it's just a-

00:28:54,082 --> 00:28:54,750
SPEAKER_0:  It's sort of like a...

00:28:55,074 --> 00:28:55,774
SPEAKER_0:  perverse solution.

00:28:56,194 --> 00:28:59,454
SPEAKER_0:  And so maybe we can find something like that. Maybe we can be that little.

00:29:00,226 --> 00:29:02,206
SPEAKER_0:  dog in this physical simulation.

00:29:02,434 --> 00:29:03,358
SPEAKER_1:  that makes that not that that and

00:29:03,970 --> 00:29:12,478
SPEAKER_1:  cracks or escapes the intended consequences of the physics that the universe came up with we'll figure out some kind of shortcut to some weirdness and then

00:29:12,802 --> 00:29:14,686
SPEAKER_1:  But see the problem with that weirdness?

00:29:15,042 --> 00:29:17,406
SPEAKER_1:  Is the first person to discover the weirdness?

00:29:17,634 --> 00:29:19,198
SPEAKER_1:  like sliding in the back legs.

00:29:20,066 --> 00:29:21,182
SPEAKER_1:  That's all we're going to do.

00:29:21,410 --> 00:29:21,758
SPEAKER_1:  Hmm.

00:29:21,986 --> 00:29:26,078
SPEAKER_1:  Yeah, it's very quickly become everybody does that thing.

00:29:26,690 --> 00:29:29,854
SPEAKER_1:  So like the paperclip maximizer.

00:29:30,114 --> 00:29:32,286
SPEAKER_1:  is a ridiculous idea but that very well.

00:29:32,674 --> 00:29:34,558
SPEAKER_1:  Yeah, could be what then we'll just.

00:29:35,490 --> 00:29:37,886
SPEAKER_1:  We'll just all switch that because it's so fun

00:29:38,050 --> 00:29:41,310
SPEAKER_0:  Well, no person will discover it, I think, by the way. I think it's going to have to be a...

00:29:41,954 --> 00:29:45,630
SPEAKER_0:  some kind of a super intelligent AGI of a third generation.

00:29:46,690 --> 00:29:48,222
SPEAKER_0:  like we're building the first generation AGI.

00:29:48,930 --> 00:29:49,630
SPEAKER_0:  And you know.

00:29:50,050 --> 00:29:51,614
SPEAKER_1:  Third generation.

00:29:51,938 --> 00:29:52,510
SPEAKER_1:  Yeah, so.

00:29:52,898 --> 00:29:54,846
SPEAKER_1:  the bootloader for an AI.

00:29:55,266 --> 00:29:58,942
SPEAKER_1:  That AI will be a bootloader for another AI. for another AI!

00:30:00,098 --> 00:30:04,126
SPEAKER_1:  And then there's no way for us to introspect like what that might even.

00:30:04,258 --> 00:30:09,950
SPEAKER_0:  I think it's very likely that these things, for example, like say you have these AGI's, it's very likely that, for example, they will be completely inert.

00:30:10,466 --> 00:30:12,510
SPEAKER_0:  I like these kinds of sci-fi books sometimes where...

00:30:12,770 --> 00:30:15,198
SPEAKER_0:  These things are just completely inert, they don't interact with anything.

00:30:15,522 --> 00:30:17,086
SPEAKER_0:  And I find that kind of beautiful because...

00:30:17,410 --> 00:30:18,270
SPEAKER_0:  They probably...

00:30:18,690 --> 00:30:22,462
SPEAKER_0:  they probably figured out the meta meta game of the universe in some way potentially there.

00:30:22,850 --> 00:30:25,470
SPEAKER_0:  They're doing something completely beyond our imagination.

00:30:26,018 --> 00:30:28,734
SPEAKER_0:  and they don't interact with simple chemical.

00:30:29,090 --> 00:30:31,550
SPEAKER_0:  life forms, like why would you do that?

00:30:31,938 --> 00:30:34,622
SPEAKER_0:  I find those kinds of ideas compelling. What's their source of fun?

00:30:34,850 --> 00:30:35,902
SPEAKER_1:  What are they doing?

00:30:36,290 --> 00:30:38,398
SPEAKER_0:  Well, probably pedal-solving in the universe.

00:30:39,202 --> 00:30:39,966
SPEAKER_1:  but inert.

00:30:40,226 --> 00:30:40,830
SPEAKER_1:  So can you?

00:30:41,090 --> 00:30:41,758
SPEAKER_1:  define.

00:30:42,210 --> 00:30:43,806
SPEAKER_1:  what it means inert, so they escape.

00:30:44,258 --> 00:30:45,383
SPEAKER_1:  The Interactual Pizzeria.

00:30:45,383 --> 00:30:45,854
SPEAKER_0:  to us.

00:30:46,146 --> 00:30:47,134
SPEAKER_1:  as in...

00:30:48,834 --> 00:30:49,246
SPEAKER_1:  Uh...

00:30:50,658 --> 00:30:55,134
SPEAKER_0:  they will behave in some very strange way to us because they're beyond

00:30:55,650 --> 00:31:05,566
SPEAKER_0:  they're playing the metagame. And the metagame is probably say like arranging quantum mechanical systems in some very weird ways to extract infinite energy, solve the digital expansion of pi to whatever.

00:31:05,954 --> 00:31:06,526
SPEAKER_0:  amount.

00:31:06,882 --> 00:31:14,046
SPEAKER_0:  they will build their own like little fusion reactors or something crazy. Like they're doing something beyond comprehension and not understandable to us.

00:31:14,338 --> 00:31:16,510
SPEAKER_0:  and actually brilliant under the hood.

00:31:17,058 --> 00:31:18,910
SPEAKER_1:  What if quantum mechanics itself?

00:31:19,298 --> 00:31:21,598
SPEAKER_1:  is the system and we're just thinking.

00:31:22,018 --> 00:31:23,038
SPEAKER_1:  It's physics.

00:31:24,066 --> 00:31:28,958
SPEAKER_1:  but we're really parasites on, not parasite, we're not really hurting physics.

00:31:29,218 --> 00:31:33,342
SPEAKER_1:  We're just living on this organism and we're like...

00:31:33,762 --> 00:31:36,254
SPEAKER_1:  trying to understand it, but really it is an organism.

00:31:36,546 --> 00:31:39,838
SPEAKER_1:  and with a deep, deep intelligence, maybe physics itself.

00:31:40,418 --> 00:31:41,118
SPEAKER_1:  is.

00:31:44,002 --> 00:31:46,494
SPEAKER_1:  the organism that's doing the super interesting thing.

00:31:46,722 --> 00:31:48,030
SPEAKER_1:  and we're just like one little.

00:31:48,290 --> 00:31:48,606
SPEAKER_1:  thing.

00:31:48,834 --> 00:31:52,689
SPEAKER_1:  Yeah. I'm the ant sitting on top of it, trying to get energy from it.

00:31:52,689 --> 00:31:59,358
SPEAKER_0:  kind of like these particles in the wave that I feel like is mostly deterministic and takes a universe from some kind of a big bang to.

00:31:59,586 --> 00:32:01,758
SPEAKER_0:  some kind of a super intelligent replicator.

00:32:02,242 --> 00:32:04,606
SPEAKER_0:  some kind of a stable point in the universe.

00:32:04,898 --> 00:32:06,142
SPEAKER_0:  given these laws of physics.

00:32:06,466 --> 00:32:10,334
SPEAKER_1:  You don't think, as Einstein said, God doesn't play dice?

00:32:10,882 --> 00:32:13,507
SPEAKER_1:  So you think it's mostly deterministic? Or the THE north invasiont?

00:32:13,507 --> 00:32:15,550
SPEAKER_0:  I think is a terministic. Oh, there's tons of

00:32:16,066 --> 00:32:18,974
SPEAKER_0:  Well, I'm s- I'm- I wanna be careful with randomness. Pseudo-random.

00:32:19,458 --> 00:32:23,870
SPEAKER_0:  Yeah, I don't like random. I think maybe the laws of physics are deterministic.

00:32:24,482 --> 00:32:25,607
SPEAKER_0:  Yeah, I think they're deterministic.

00:32:25,607 --> 00:32:34,654
SPEAKER_1:  got really uncomfortable with this question. Do you have anxiety about whether the universe is random or not? Is this a source? What's-

00:32:34,850 --> 00:32:35,975
SPEAKER_0:  There's no randomness now.

00:32:35,975 --> 00:32:40,190
SPEAKER_1:  It's, you said you like goodwill hunting. It's not your fault, Andre. Andre, I am tired of hunting, you know? Woolie.

00:32:40,578 --> 00:32:43,879
SPEAKER_1:  It's not your fault, man. Um, so you don't like.

00:32:43,879 --> 00:32:44,606
SPEAKER_0:  randomness.

00:32:45,090 --> 00:32:56,478
SPEAKER_0:  Yeah, I think it's unsettling. I think it's a deterministic system. I think that things that look random, like say the collapse of the wave function, et cetera, I think they're actually deterministic, just entanglement and so on.

00:32:56,802 --> 00:32:59,422
SPEAKER_0:  and some kind of a multiverse theory something something.

00:32:59,650 --> 00:33:02,110
SPEAKER_1:  Okay, so why does it feel like we have a free will?

00:33:02,882 --> 00:33:06,302
SPEAKER_1:  Like if I raised his hand, I chose to do this now.

00:33:06,594 --> 00:33:07,198
SPEAKER_1:  Um...

00:33:08,130 --> 00:33:08,510
SPEAKER_1:  What?

00:33:10,242 --> 00:33:13,726
SPEAKER_1:  That doesn't feel like a deterministic thing. It feels like I'm making a choice.

00:33:14,466 --> 00:33:15,262
SPEAKER_1:  It feels like it.

00:33:16,130 --> 00:33:18,238
SPEAKER_1:  So it's all feelings, it's just feelings. Womanaking

00:33:18,882 --> 00:33:22,174
SPEAKER_1:  So when RL agent is making a choice is that.

00:33:23,170 --> 00:33:23,614
SPEAKER_1:  Um...

00:33:24,770 --> 00:33:27,395
SPEAKER_1:  It's not really making a choice. The choice is all already made.

00:33:27,395 --> 00:33:30,558
SPEAKER_0:  Yeah, you're interpreting the choice and you're creating a narrative for...

00:33:30,946 --> 00:33:31,582
SPEAKER_0:  for having made it.

00:33:32,098 --> 00:33:34,590
SPEAKER_1:  Yeah. And now we're talking about the narrative. It's very meta.

00:33:35,170 --> 00:33:40,766
SPEAKER_1:  Looking back, what is the most beautiful or surprising idea in deep learning or AI?

00:33:41,090 --> 00:33:42,942
SPEAKER_1:  in general that you've come across.

00:33:43,234 --> 00:33:45,022
SPEAKER_1:  you've seen this field explode.

00:33:45,858 --> 00:33:47,966
SPEAKER_1:  uh... and grow in interesting ways just

00:33:48,802 --> 00:33:50,046
SPEAKER_1:  What cool ideas, like?

00:33:50,498 --> 00:33:52,446
SPEAKER_1:  Like, we made you sit back and go, hmm.

00:33:52,834 --> 00:33:53,854
SPEAKER_1:  Small, big or small.

00:33:55,522 --> 00:33:57,406
SPEAKER_0:  Well, the one that I've been thinking about recently...

00:33:57,890 --> 00:33:59,486
SPEAKER_0:  The most probably is the...

00:34:00,674 --> 00:34:01,758
SPEAKER_0:  Transformer architecture.

00:34:02,242 --> 00:34:02,718
SPEAKER_0:  Um.

00:34:03,394 --> 00:34:12,446
SPEAKER_0:  So basically, neural networks have a lot of architectures that were trendy have come and gone for different sensory modalities, like for vision, audio, text.

00:34:12,706 --> 00:34:14,462
SPEAKER_0:  you would process them with different looking neural nets.

00:34:14,786 --> 00:34:18,558
SPEAKER_0:  And recently we've seen this convergence towards one architecture, the transformer.

00:34:19,106 --> 00:34:25,470
SPEAKER_0:  and you can feed it video or you can feed it images or speech or text and it just gobbles it up and it's kind of like a

00:34:26,082 --> 00:34:27,422
SPEAKER_0:  bit of a general purpose.

00:34:27,906 --> 00:34:31,294
SPEAKER_0:  computer that is also trainable and very efficient to run on our hardware.

00:34:32,002 --> 00:34:34,654
SPEAKER_0:  And so this paper came out in 2010.

00:34:35,106 --> 00:34:36,126
SPEAKER_0:  16 I want to say.

00:34:36,450 --> 00:34:38,878
SPEAKER_1:  Attention is all you need. Attention is all you need.

00:34:39,298 --> 00:34:42,590
SPEAKER_1:  you criticize the paper title in retrospect that it wasn't.

00:34:43,394 --> 00:34:43,934
SPEAKER_1:  uh...

00:34:44,642 --> 00:34:48,767
SPEAKER_1:  it didn't foresee the bigness of the impact that it was going to have.

00:34:48,767 --> 00:34:53,438
SPEAKER_0:  Yeah, I'm not sure if the authors were aware of the impact that that paper would go on to have. Probably they weren't.

00:34:53,730 --> 00:35:01,214
SPEAKER_0:  But I think they were aware of some of the motivations and design decisions behind the transformer and they chose not to I think expand on it in that way in the paper.

00:35:01,858 --> 00:35:04,638
SPEAKER_0:  And so I think they had an idea that there was more.

00:35:05,314 --> 00:35:08,894
SPEAKER_0:  than just the surface of just like, oh, we're just doing translation and here's a bit of architecture.

00:35:09,122 --> 00:35:11,294
SPEAKER_0:  You're not just doing translation, this is like a really cool...

00:35:11,522 --> 00:35:14,334
SPEAKER_0:  differentiable, optimizable, efficient computer that you've proposed.

00:35:14,850 --> 00:35:18,078
SPEAKER_0:  And maybe they didn't have all of that foresight, but I think it's really interesting.

00:35:18,178 --> 00:35:21,150
SPEAKER_1:  Isn't it funny, sorry to interrupt that title.

00:35:21,474 --> 00:35:23,134
SPEAKER_1:  is memeable that they went.

00:35:23,522 --> 00:35:26,238
SPEAKER_1:  for such a profound idea they went with a.

00:35:26,466 --> 00:35:29,022
SPEAKER_1:  I don't think anyone used that kind of title before, right?

00:35:29,442 --> 00:35:32,382
SPEAKER_0:  Attention is all you need. Yeah, it's like a meme or something.

00:35:32,546 --> 00:35:33,758
SPEAKER_1:  Isn't that funny?

00:35:34,274 --> 00:35:35,806
SPEAKER_1:  Like maybe.

00:35:36,322 --> 00:35:38,526
SPEAKER_1:  If it was a more serious title, it wouldn't have the impact.

00:35:38,722 --> 00:35:44,131
SPEAKER_0:  Honestly, there is an element of me that honestly agrees with you and prefers it this way. Yes. It's been real amazing to meet you guys and keep doing what you are doing now despite

00:35:44,131 --> 00:35:44,862
SPEAKER_1:  laughter

00:35:45,218 --> 00:35:52,169
SPEAKER_0:  If it was too grand, it would over promise and then under deliver potentially. So you want to just meme your way to greatness.

00:35:52,169 --> 00:35:58,718
SPEAKER_1:  That should be a t-shirt. So you tweeted the Transformers magnificent neural network architecture.

00:35:58,946 --> 00:36:03,582
SPEAKER_1:  because it is a general purpose differentiable computer, it is simultaneously expressive.

00:36:03,906 --> 00:36:04,830
SPEAKER_1:  in the forward pass.

00:36:05,122 --> 00:36:09,662
SPEAKER_1:  optimisable via back propagation gradient descent, and efficient.

00:36:10,242 --> 00:36:11,262
SPEAKER_1:  high parallelism.

00:36:11,586 --> 00:36:16,158
SPEAKER_1:  Can you discuss some of those details? Can you discuss some of those details?

00:36:17,474 --> 00:36:20,350
SPEAKER_1:  from memory or in general, whatever comes to your heart.

00:36:20,994 --> 00:36:28,958
SPEAKER_0:  You want to have a general purpose computer that you can train on arbitrary problems, like say the task of next work prediction or detecting if there's a cat in the image or something like that.

00:36:29,730 --> 00:36:32,478
SPEAKER_0:  And you want to train this computer, so you want to set its weight.

00:36:32,802 --> 00:36:40,030
SPEAKER_0:  And I think there's a number of design criteria that sort of overlap in the transformer simultaneously that made it very successful. And I think the authors were.

00:36:40,354 --> 00:36:40,958
SPEAKER_0:  kind of a...

00:36:41,218 --> 00:36:42,366
SPEAKER_0:  deliberately trying to.

00:36:42,690 --> 00:36:43,742
SPEAKER_0:  make this really

00:36:44,290 --> 00:36:45,662
SPEAKER_0:  powerful architecture.

00:36:46,242 --> 00:36:46,974
SPEAKER_0:  And, um...

00:36:47,618 --> 00:36:49,246
SPEAKER_0:  So basically it's.

00:36:49,506 --> 00:36:52,382
SPEAKER_0:  very powerful in the forward pass because it's able to express...

00:36:53,026 --> 00:36:57,534
SPEAKER_0:  very general computation as sort of something that looks like message passing.

00:36:57,858 --> 00:36:59,774
SPEAKER_0:  You have nodes and they all store vectors.

00:37:00,098 --> 00:37:02,398
SPEAKER_0:  And these nodes get to basically look at each other.

00:37:02,658 --> 00:37:06,526
SPEAKER_0:  and it's each other's vectors and they get to communicate.

00:37:06,818 --> 00:37:07,742
SPEAKER_0:  Basically, nose get to...

00:37:08,162 --> 00:37:12,414
SPEAKER_0:  broadcast, hey, I'm looking for certain things, and then other nodes get to broadcast, hey, these aren't the things I have.

00:37:12,674 --> 00:37:13,799
SPEAKER_0:  Those are the keys and the values.

00:37:13,799 --> 00:37:15,134
SPEAKER_1:  So it's not just the tension.

00:37:15,266 --> 00:37:24,446
SPEAKER_0:  Yeah, exactly. Transformer is much more than just the attention component. It's got many pieces architectural that went into it. The residual connection of the weights arranged. There's a multi-layer perceptron and there are the weights.

00:37:24,866 --> 00:37:25,854
SPEAKER_0:  stacked and so on.

00:37:26,338 --> 00:37:26,686
SPEAKER_0:  Um.

00:37:26,978 --> 00:37:32,062
SPEAKER_0:  But basically there's a message passing scheme where nodes get to look at each other, decide what's interesting and then update each other.

00:37:32,770 --> 00:37:34,366
SPEAKER_0:  And so I think the...

00:37:34,754 --> 00:37:37,438
SPEAKER_0:  When you get to the details of it, I think it's a very expressive function.

00:37:37,666 --> 00:37:40,606
SPEAKER_0:  so it can express lots of different types of algorithms in the forward pass.

00:37:40,866 --> 00:37:42,398
SPEAKER_0:  Not only that, but the way it's designed.

00:37:42,658 --> 00:37:48,414
SPEAKER_0:  with the residual connections, later normalizations, the softmax attention and everything, it's also optimizable. This is a really big deal.

00:37:48,802 --> 00:37:49,310
SPEAKER_0:  because

00:37:49,762 --> 00:37:59,486
SPEAKER_0:  There's lots of computers that are powerful that you can't optimize, or they're not easy to optimize using the techniques that we have, which is backpropagation and gradient descent. These are first-order methods, very simple optimizers, really.

00:37:59,842 --> 00:38:00,798
SPEAKER_0:  And so.

00:38:01,314 --> 00:38:02,750
SPEAKER_0:  You also need it to be optimisable.

00:38:03,010 --> 00:38:03,422
SPEAKER_0:  Um.

00:38:03,970 --> 00:38:07,262
SPEAKER_0:  And then lastly, you want it to run efficiently in our hardware.

00:38:07,554 --> 00:38:08,990
SPEAKER_0:  is a massive throughput.

00:38:09,250 --> 00:38:12,574
SPEAKER_0:  machine, a lot of GPUs. They prefer lots of parallelism.

00:38:13,058 --> 00:38:18,846
SPEAKER_0:  So you don't want to do lots of sequential operations, you want to do a lot of operations serially and the transformer is designed with that in mind as well.

00:38:19,298 --> 00:38:25,918
SPEAKER_0:  And so it's designed for our hardware and it's designed to both be very expressive in the forward pass, but also very optimizable in the backward pass.

00:38:26,338 --> 00:38:27,966
SPEAKER_1:  And you said that, uh...

00:38:28,354 --> 00:38:34,622
SPEAKER_1:  The residual connections support a kind of ability to learn short algorithms faster than first and then gradually extend them.

00:38:35,330 --> 00:38:39,934
SPEAKER_1:  longer during training. What's the idea of learning short algorithms? Right.

00:38:40,386 --> 00:38:44,382
SPEAKER_0:  Think of it as a, so basically a transformer is a series of.

00:38:44,770 --> 00:38:48,158
SPEAKER_0:  blocks, right? And these blocks have attention and a little multilayered perception.

00:38:48,578 --> 00:38:55,390
SPEAKER_0:  And so you go off into a block and you come back to this residual pathway. And then you go off and you come back. And then you have a number of layers arranged sequentially.

00:38:56,034 --> 00:39:04,798
SPEAKER_0:  And so the way to look at it, I think, is because of the residual pathway in the backward path, the gradients sort of flow along it uninterrupted because...

00:39:05,026 --> 00:39:07,966
SPEAKER_0:  addition distributes the gradient equally to all of its branches.

00:39:08,418 --> 00:39:13,278
SPEAKER_0:  So the gradient from the supervision at the top just floats directly to the first layer.

00:39:13,922 --> 00:39:20,446
SPEAKER_0:  And all the residual connections are arranged so that in the beginning during initialization, they contribute nothing to the residual pathway.

00:39:20,770 --> 00:39:25,918
SPEAKER_0:  So what it kind of looks like is, imagine the transformer is kind of like a Python.

00:39:26,146 --> 00:39:27,326
SPEAKER_0:  function like a death.

00:39:27,970 --> 00:39:28,734
SPEAKER_0:  And, um.

00:39:28,994 --> 00:39:33,278
SPEAKER_0:  you get to do various kinds of lines of code. So you have a hundred.

00:39:33,538 --> 00:39:39,006
SPEAKER_0:  layers deep transformer, typically they would be much shorter, say 20. So if 20 lines of code, then you can do something in them.

00:39:39,522 --> 00:39:57,438
SPEAKER_0:  And so think of during the optimization, basically what it looks like is first you optimize the first line of code, and then the second line of code can kick in, and the third line of code can kick in. And I kind of feel like because of the residual pathway and the dynamics of the optimization, you can sort of learn a very short algorithm that gets the approximate answer, but then the other layers can sort of kick in and start to create a contribution.

00:39:57,730 --> 00:40:01,630
SPEAKER_0:  And at the end of it, you're optimizing over an algorithm that is 20 lines of code.

00:40:02,562 --> 00:40:06,654
SPEAKER_0:  Except these lines of code are very complex because it's an entire block of a transformer. You can do a lot in there.

00:40:06,882 --> 00:40:09,406
SPEAKER_0:  What's really interesting is that this transformer architecture actually...

00:40:09,794 --> 00:40:11,422
SPEAKER_0:  has been remarkably resilient.

00:40:11,746 --> 00:40:17,118
SPEAKER_0:  Basically the transformer that came out in 2016 is the transformer you would use today, except you reshuffle some delay norms.

00:40:17,698 --> 00:40:21,470
SPEAKER_0:  The relay normalizations have been reshuffled to a pre-norm formulation.

00:40:21,922 --> 00:40:27,486
SPEAKER_0:  And so it's been remarkably stable, but there's a lot of bells and whistles that people have attached on it and tried to improve it.

00:40:28,034 --> 00:40:34,142
SPEAKER_0:  I do think that basically it's a big step in simultaneously optimizing for lots of properties of a desirable neural network architecture.

00:40:34,402 --> 00:40:37,598
SPEAKER_0:  And I think people have been trying to change it, but it's proven remarkably resilient.

00:40:38,114 --> 00:40:41,406
SPEAKER_0:  But I do think that there should be even better architectures potentially.

00:40:41,826 --> 00:40:45,470
SPEAKER_1:  but you admire the resilience here.

00:40:45,698 --> 00:40:51,710
SPEAKER_1:  There's something profound about this architecture that at least was a, so maybe we can, everything can be turned into a...

00:40:52,770 --> 00:40:54,974
SPEAKER_1:  into a problem that transformers can solve.

00:40:55,138 --> 00:41:04,254
SPEAKER_0:  Currently, it definitely looks like the Transformer is taking over AI, and you can feed basically arbitrary problems into it, and it's a general, differentiable computer, and it's extremely powerful.

00:41:04,834 --> 00:41:08,094
SPEAKER_0:  this convergence in AI has been really interesting to watch.

00:41:08,354 --> 00:41:08,734
SPEAKER_0:  Uh.

00:41:08,834 --> 00:41:09,470
SPEAKER_1:  for me personally.

00:41:09,922 --> 00:41:14,078
SPEAKER_1:  What else do you think could be discovered here about transformers? What surprising thing?

00:41:14,658 --> 00:41:16,670
SPEAKER_1:  Or is it a stable...

00:41:17,698 --> 00:41:24,318
SPEAKER_1:  I want a stable place. Is there something interesting when my discover about transformers? Like aha moments maybe has to do with memory.

00:41:24,770 --> 00:41:27,294
SPEAKER_1:  maybe knowledge representation, that kind of stuff.

00:41:28,290 --> 00:41:29,415
SPEAKER_1:  Definitely does, right guys?

00:41:29,415 --> 00:41:33,982
SPEAKER_0:  today is just pushing like basically right now the side guys is do not touch the transformer.

00:41:34,370 --> 00:41:35,358
SPEAKER_0:  Touch everything else.

00:41:35,586 --> 00:41:40,798
SPEAKER_0:  Yes. So people are scaling up the datasets, making them much, much bigger. They're working on the evaluation, making the evaluation much, much bigger.

00:41:41,346 --> 00:41:42,110
SPEAKER_0:  and uh...

00:41:42,754 --> 00:41:43,102
SPEAKER_0:  Um.

00:41:43,362 --> 00:41:48,222
SPEAKER_0:  they're basically keeping the architecture unchanged. And that's how we've, that's the last five years of.

00:41:48,642 --> 00:41:49,918
SPEAKER_0:  Progress in high, kind of.

00:41:50,818 --> 00:41:54,270
SPEAKER_1:  What do you think about one flavor of it, which is language models?

00:41:54,914 --> 00:41:56,190
SPEAKER_1:  Have you been surprised?

00:41:57,186 --> 00:41:57,758
SPEAKER_1:  Uh...

00:41:58,690 --> 00:42:04,862
SPEAKER_1:  has your sort of imagination been captivated by, you mentioned DPT and all the bigger and bigger language models.

00:42:05,666 --> 00:42:07,902
SPEAKER_1:  And what are the limits?

00:42:08,898 --> 00:42:10,910
SPEAKER_1:  of those models do you think?

00:42:12,546 --> 00:42:14,430
SPEAKER_1:  So just the task of natural language.

00:42:15,970 --> 00:42:20,670
SPEAKER_0:  Basically, the way GPT is trained, right, is you just download a massive amount of text data from the internet.

00:42:20,930 --> 00:42:28,382
SPEAKER_0:  and you try to predict the next word in the sequence, roughly speaking. You're predicting little word chunks, but roughly speaking, that's it.

00:42:28,930 --> 00:42:31,134
SPEAKER_0:  And what's been really interesting to watch is...

00:42:31,906 --> 00:42:35,774
SPEAKER_0:  Basically, it's a language model. Language models have actually existed for a very long time.

00:42:36,578 --> 00:42:39,582
SPEAKER_0:  There's papers on language modeling from 2003, even earlier.

00:42:39,842 --> 00:42:42,467
SPEAKER_1:  Can you explain in that case what a language model is?

00:42:42,467 --> 00:42:49,118
SPEAKER_0:  Yeah, so language model just basically the rough idea is just predicting the next word in a sequence roughly speaking

00:42:49,634 --> 00:42:55,870
SPEAKER_0:  So there's a paper from, for example, Benjio and the team from 2003, where for the first time they were using

00:42:56,098 --> 00:43:00,446
SPEAKER_0:  a neural network to take say like three or five words and predict the

00:43:00,706 --> 00:43:01,182
SPEAKER_0:  Next word.

00:43:01,730 --> 00:43:06,686
SPEAKER_0:  and they're doing this on much smaller datasets. And the neural net is not a transformer, it's a multilayer perceptron.

00:43:07,202 --> 00:43:09,982
SPEAKER_0:  but it's the first time that neural network has been applied in that setting.

00:43:10,338 --> 00:43:16,350
SPEAKER_0:  But even before neural networks, there were language models, except they were using N-gram models.

00:43:16,834 --> 00:43:19,070
SPEAKER_0:  So N-GRAAM models are just count-based.

00:43:19,394 --> 00:43:20,190
SPEAKER_0:  models. So.

00:43:20,546 --> 00:43:20,958
SPEAKER_0:  Um.

00:43:21,186 --> 00:43:22,462
SPEAKER_0:  if you start to take.

00:43:22,690 --> 00:43:24,062
SPEAKER_0:  words and predict a third one.

00:43:24,322 --> 00:43:27,198
SPEAKER_0:  you just count up how many times you've seen any.

00:43:27,554 --> 00:43:29,470
SPEAKER_0:  two word combinations and what came next.

00:43:29,890 --> 00:43:33,310
SPEAKER_0:  And what you predict is coming next is just what you've seen the most of in the training set

00:43:34,178 --> 00:43:38,910
SPEAKER_0:  And so language modeling has been around for a long time. Neural networks have done language modeling for a long time.

00:43:39,490 --> 00:43:42,302
SPEAKER_0:  So really what's new or interesting or exciting is just...

00:43:42,882 --> 00:43:45,182
SPEAKER_0:  Realizing that when you scale it up.

00:43:45,890 --> 00:43:48,478
SPEAKER_0:  with a powerful enough neural net transformer.

00:43:48,706 --> 00:43:51,070
SPEAKER_0:  you have all these emergent properties where

00:43:51,522 --> 00:43:53,054
SPEAKER_0:  Basically what happens is

00:43:53,634 --> 00:43:55,710
SPEAKER_0:  if you have a large enough data set of text.

00:43:57,186 --> 00:43:57,758
SPEAKER_0:  You are.

00:43:58,338 --> 00:44:02,302
SPEAKER_0:  In the task of predicting the next word, you are multitasking a huge amount of

00:44:02,818 --> 00:44:04,094
SPEAKER_0:  different kinds of problems.

00:44:04,482 --> 00:44:05,662
SPEAKER_0:  You are multitasking.

00:44:05,986 --> 00:44:07,006
SPEAKER_0:  understanding of.

00:44:07,522 --> 00:44:09,374
SPEAKER_0:  chemistry, physics, human nature.

00:44:09,730 --> 00:44:15,998
SPEAKER_0:  Lots of things are sort of clustered in that objective. It's a very simple objective, but actually you have to understand a lot about the world to make that prediction.

00:44:16,226 --> 00:44:19,102
SPEAKER_1:  You just said the U word, understanding.

00:44:19,362 --> 00:44:26,846
SPEAKER_1:  Uh, are you in terms of chemistry and physics and so on? W what do you feel like it's doing? Is it searching for the right context?

00:44:27,554 --> 00:44:31,998
SPEAKER_1:  in the what is it was the actual process happening here.

00:44:32,290 --> 00:44:36,222
SPEAKER_0:  Yeah, so basically it gets a thousand words and it's trying to predict the thousandth first.

00:44:36,514 --> 00:44:40,990
SPEAKER_0:  and in order to do that very, very well over the entire dataset available on the Internet.

00:44:41,218 --> 00:44:41,982
SPEAKER_0:  You actually have to...

00:44:42,466 --> 00:44:44,990
SPEAKER_0:  basically kind of understand the context of.

00:44:45,570 --> 00:44:46,782
SPEAKER_0:  what's going on in there. yeah

00:44:47,010 --> 00:44:47,486
SPEAKER_0:  Um.

00:44:48,002 --> 00:44:51,230
SPEAKER_0:  And it's a sufficiently hard problem that you...

00:44:51,810 --> 00:44:54,494
SPEAKER_0:  if you have a powerful enough computer like a transformer.

00:44:54,722 --> 00:44:57,214
SPEAKER_0:  you end up with interesting solutions.

00:44:57,538 --> 00:45:01,662
SPEAKER_0:  and you can ask it to do all kinds of things.

00:45:02,114 --> 00:45:09,374
SPEAKER_0:  It shows a lot of emergent properties, like in-context learning. That was the big deal with GPT and the original paper when they published it.

00:45:09,698 --> 00:45:11,198
SPEAKER_0:  is that you can just sort of.

00:45:11,522 --> 00:45:18,590
SPEAKER_0:  prompt it in various ways and ask it to do various things and it will just kind of complete the sentence but in the process of just completing the sentence it's actually solving all kinds of really

00:45:18,914 --> 00:45:19,646
SPEAKER_0:  interesting.

00:45:20,034 --> 00:45:21,022
SPEAKER_0:  problems that we care about.

00:45:21,506 --> 00:45:23,518
SPEAKER_1:  Do you think it's doing something like understanding?

00:45:24,514 --> 00:45:28,190
SPEAKER_1:  Like when we use the word understanding for us humans.

00:45:29,698 --> 00:45:34,142
SPEAKER_0:  is doing some understanding it in its weights it understands i think a lot about the world

00:45:34,594 --> 00:45:37,374
SPEAKER_0:  And it has to in order to predict the next word in a sequence.

00:45:38,722 --> 00:45:40,702
SPEAKER_1:  So it's trained on the data from the internet.

00:45:42,114 --> 00:45:43,550
SPEAKER_1:  What do you think about this?

00:45:43,778 --> 00:45:47,102
SPEAKER_1:  this approach in terms of datasets of using data from the internet.

00:45:47,842 --> 00:45:52,574
SPEAKER_1:  Do you think the internet has enough structured data to teach AI about human civilization?

00:45:53,730 --> 00:45:57,822
SPEAKER_0:  Yes, I think the internet has a huge amount of data. I'm not sure if it's a complete enough set.

00:45:58,050 --> 00:45:59,198
SPEAKER_0:  I don't know that, uh...

00:45:59,426 --> 00:46:00,734
SPEAKER_0:  Text is enough.

00:46:00,994 --> 00:46:03,006
SPEAKER_0:  for having a sufficiently powerful AGI.

00:46:03,234 --> 00:46:03,806
SPEAKER_0:  as an outcome.

00:46:04,450 --> 00:46:08,094
SPEAKER_1:  Of course there is audio and video and images and all that kind of stuff.

00:46:08,322 --> 00:46:20,190
SPEAKER_0:  Yeah, so text by itself, I'm a little bit suspicious about. There's a ton of things we don't put in text in writing, just because they're obvious to us about how the world works and the physics of it and that things fall. We don't put that stuff in text because why would you? We share that understanding.

00:46:20,962 --> 00:46:24,030
SPEAKER_0:  And so text is a communication medium between humans, and it's not a.

00:46:24,322 --> 00:46:27,230
SPEAKER_0:  all-encompassing medium of knowledge about the world.

00:46:27,554 --> 00:46:31,294
SPEAKER_0:  But as you pointed out, we do have video, and we have images, and we have audio.

00:46:31,586 --> 00:46:36,158
SPEAKER_0:  And so I think that definitely helps a lot, but we haven't trained models sufficiently.

00:46:36,514 --> 00:46:39,070
SPEAKER_0:  across both, across all of those modalities yet.

00:46:39,426 --> 00:46:41,301
SPEAKER_0:  So I think that's what a lot of people are interested in.

00:46:41,301 --> 00:46:43,582
SPEAKER_1:  I wonder what that shared understanding of like

00:46:43,810 --> 00:46:45,470
SPEAKER_1:  we might call common sense.

00:46:46,114 --> 00:46:47,486
SPEAKER_1:  has to be learned.

00:46:48,258 --> 00:46:51,038
SPEAKER_1:  inferred in order to complete the sentence correctly.

00:46:51,746 --> 00:46:52,638
SPEAKER_1:  So maybe...

00:46:53,058 --> 00:46:55,486
SPEAKER_1:  the fact that it's implied on the internet.

00:46:56,034 --> 00:46:57,502
SPEAKER_1:  The model's gonna have to learn that.

00:46:58,050 --> 00:47:02,302
SPEAKER_1:  not by reading about it, by inferring it in the representation.

00:47:02,850 --> 00:47:03,198
SPEAKER_1:  like.

00:47:03,650 --> 00:47:06,494
SPEAKER_1:  Common sense just like we I don't think we learn common sense

00:47:06,786 --> 00:47:08,030
SPEAKER_1:  Like, nobody says.

00:47:09,058 --> 00:47:11,518
SPEAKER_1:  tells us explicitly, we just figure it all out.

00:47:11,842 --> 00:47:13,246
SPEAKER_1:  by interacting with the world. Why are we here today?

00:47:13,890 --> 00:47:16,798
SPEAKER_1:  So here's a model reading about the way people interact.

00:47:17,058 --> 00:47:18,814
SPEAKER_1:  the world and might have to infer that.

00:47:19,618 --> 00:47:20,094
SPEAKER_1:  I wonder.

00:47:20,482 --> 00:47:20,798
SPEAKER_1:  Yeah.

00:47:21,282 --> 00:47:24,766
SPEAKER_1:  You briefly worked on a project called the World of Bits.

00:47:25,474 --> 00:47:28,478
SPEAKER_1:  training an RL system to take actions on the internet.

00:47:29,282 --> 00:47:34,110
SPEAKER_1:  versus just consuming the internet like we talked about. Do you think there's a future for that kind of system?

00:47:34,370 --> 00:47:36,318
SPEAKER_1:  interacting with the internet to help the learning.

00:47:36,994 --> 00:47:41,502
SPEAKER_0:  Yes, I think that's probably the final frontier for a lot of these models because

00:47:41,922 --> 00:47:42,334
SPEAKER_0:  Um.

00:47:43,010 --> 00:47:49,598
SPEAKER_0:  So as you mentioned, when I was at OpenAI, I was working on this project, World of Bits, and basically it was the idea of giving neural networks access to a keyboard and a mouse.

00:47:50,082 --> 00:47:51,207
SPEAKER_0:  and the idea is...

00:47:51,207 --> 00:47:52,350
SPEAKER_1:  possibly go wrong.

00:47:52,578 --> 00:47:53,150
SPEAKER_1:  Hahaha

00:47:53,474 --> 00:47:54,654
SPEAKER_0:  So basically you...

00:47:54,946 --> 00:47:56,958
SPEAKER_0:  you perceive the input of the...

00:47:57,282 --> 00:47:58,270
SPEAKER_0:  Screen pixels.

00:47:58,978 --> 00:48:04,030
SPEAKER_0:  And basically the state of the computer is sort of visualized for human consumption in

00:48:04,290 --> 00:48:06,302
SPEAKER_0:  images of the web browser and stuff like that.

00:48:06,722 --> 00:48:09,918
SPEAKER_0:  and then you give the neural network the ability to press keyboards and use the mouse.

00:48:10,274 --> 00:48:14,206
SPEAKER_0:  and we're trying to get it to, for example, complete bookings and interact with user interfaces.

00:48:14,882 --> 00:48:15,582
SPEAKER_0:  And um.

00:48:15,810 --> 00:48:20,158
SPEAKER_1:  What did you learn from that experience? Like what was some fun stuff? This is a super cool idea.

00:48:20,770 --> 00:48:21,630
SPEAKER_1:  I mean, it's like.

00:48:22,594 --> 00:48:28,926
SPEAKER_1:  Yeah, I mean the step between observer to actor is a super fascinating step.

00:48:29,218 --> 00:48:32,158
SPEAKER_0:  Well, it's the universal interface in the digital realm, I would say.

00:48:32,450 --> 00:48:40,254
SPEAKER_0:  And there's a universal interface in like the physical realm, which in my mind is a humanoid form factor kind of thing. We can later talk about Optimus and so on, but...

00:48:40,514 --> 00:48:42,014
SPEAKER_0:  I feel like there's a...

00:48:42,722 --> 00:48:48,414
SPEAKER_0:  They're kind of like a similar philosophy in some way, where the human, the world, the physical world is designed for the human form.

00:48:48,770 --> 00:48:54,366
SPEAKER_0:  And the digital world is designed for the human form of seeing the screen and using keyboard and mouse.

00:48:54,690 --> 00:48:56,862
SPEAKER_0:  And so it's the universal interface that can.

00:48:57,090 --> 00:48:57,918
SPEAKER_0:  basically

00:48:58,274 --> 00:49:00,958
SPEAKER_0:  command the digital infrastructure we've built up for ourselves.

00:49:01,378 --> 00:49:01,854
SPEAKER_0:  And so.

00:49:02,306 --> 00:49:06,174
SPEAKER_0:  It feels like a very powerful interface to command and to build on top of.

00:49:06,722 --> 00:49:09,918
SPEAKER_0:  Now to your question as to what I learned from that, that's interesting because.

00:49:10,466 --> 00:49:12,510
SPEAKER_0:  The world of Bits was basically too early.

00:49:12,898 --> 00:49:14,366
SPEAKER_0:  I think, at OpenAI at the time.

00:49:15,106 --> 00:49:15,486
SPEAKER_0:  Um...

00:49:15,842 --> 00:49:17,886
SPEAKER_0:  This is around 2015 or so.

00:49:18,402 --> 00:49:20,798
SPEAKER_0:  And the zeitgeist at that time was very different.

00:49:21,026 --> 00:49:22,622
SPEAKER_0:  in AI from the Zeitgeist today.

00:49:23,170 --> 00:49:26,718
SPEAKER_0:  At the time, everyone was super excited about reinforcement learning from scratch.

00:49:27,074 --> 00:49:33,886
SPEAKER_0:  This is the time of the Atari paper where neural networks were playing Atari games and beating humans in some cases.

00:49:34,146 --> 00:49:35,678
SPEAKER_0:  AlphaGo and so on.

00:49:36,034 --> 00:49:39,774
SPEAKER_0:  So everyone's very excited about training neural networks from scratch using reinforcement learning.

00:49:40,098 --> 00:49:41,150
SPEAKER_0:  directly.

00:49:42,370 --> 00:49:45,534
SPEAKER_0:  It turns out that reinforcement learning is an extremely inefficient way of training neural...

00:49:46,114 --> 00:49:50,814
SPEAKER_0:  because you're taking all these actions and all these observations and you get some sparse rewards once in a while.

00:49:51,170 --> 00:49:55,006
SPEAKER_0:  So you do all this stuff based on all these inputs. And once in a while you're told.

00:49:55,298 --> 00:49:55,998
SPEAKER_0:  You did a good thing.

00:49:56,354 --> 00:49:57,022
SPEAKER_0:  You did a bad thing.

00:49:57,474 --> 00:50:01,086
SPEAKER_0:  And it's just an extremely hard problem. You can't learn from that. You can burn forest.

00:50:01,666 --> 00:50:03,934
SPEAKER_0:  and you can sort of boot force through it. And we saw that I think with.

00:50:04,290 --> 00:50:07,294
SPEAKER_0:  with Go and Dota and so on and it does work.

00:50:07,810 --> 00:50:12,766
SPEAKER_0:  but it's extremely inefficient and not how you want to approach problems practically speaking.

00:50:13,250 --> 00:50:16,606
SPEAKER_0:  And so that's the approach that at the time we also took to world of bits.

00:50:16,994 --> 00:50:22,526
SPEAKER_0:  we would have an agent initialize randomly. So with keyboard mash and mouse mash and tried to make a booking.

00:50:22,978 --> 00:50:24,542
SPEAKER_0:  and it's just like revealed.

00:50:24,802 --> 00:50:27,006
SPEAKER_0:  the insanity of that approach very quickly.

00:50:27,234 --> 00:50:31,390
SPEAKER_0:  where you have to stumble by the correct booking in order to get a reward of you did it correctly.

00:50:31,778 --> 00:50:33,246
SPEAKER_0:  and you're never gonna stumble by it.

00:50:33,698 --> 00:50:34,462
SPEAKER_0:  by chance at random.

00:50:35,330 --> 00:50:38,046
SPEAKER_1:  So even with a simple web interface, there's too many options.

00:50:38,210 --> 00:50:41,822
SPEAKER_0:  There's just too many options, and it's too sparse a reward signal.

00:50:42,114 --> 00:50:49,214
SPEAKER_0:  And you're starting from scratch at the time. And so you don't know how to read. You don't understand pictures, images, buttons. You don't understand what it means to make a booking.

00:50:49,474 --> 00:50:54,814
SPEAKER_0:  But now what's happened is it is time to revisit that and OpenAI is interested in this.

00:50:55,042 --> 00:50:55,806
SPEAKER_0:  companies like a DEP.

00:50:56,066 --> 00:50:57,246
SPEAKER_0:  are interested in this and so on.

00:50:57,794 --> 00:51:01,214
SPEAKER_0:  And the idea is coming back because the interface is very powerful.

00:51:01,442 --> 00:51:05,150
SPEAKER_0:  But now you're not training an agent from scratch. You are taking the GPT as initialization.

00:51:05,762 --> 00:51:08,190
SPEAKER_0:  So GPT has pre-trained on all of.

00:51:08,642 --> 00:51:09,086
SPEAKER_0:  text.

00:51:09,602 --> 00:51:12,862
SPEAKER_0:  and it understands what's a booking. It understands what's a submit.

00:51:13,282 --> 00:51:14,398
SPEAKER_0:  It understands.

00:51:14,882 --> 00:51:18,302
SPEAKER_0:  a bit more. And so it already has those representations. They are very powerful.

00:51:18,562 --> 00:51:20,798
SPEAKER_0:  and that makes all the training significantly more efficient.

00:51:21,218 --> 00:51:23,733
SPEAKER_0:  and makes the problem tractable.

00:51:23,733 --> 00:51:24,830
SPEAKER_1:  Interaction B.

00:51:25,090 --> 00:51:33,470
SPEAKER_1:  with like the way humans see it with the buttons and the language or should be with the HTML, JavaScript and the CSS. What do you think is the better?

00:51:33,986 --> 00:51:41,790
SPEAKER_0:  So today, all of this interaction is mostly on the level of HTML, CSS, and so on. That's done because of computational constraints. But I think ultimately, um,

00:51:42,722 --> 00:51:44,734
SPEAKER_0:  Everything is designed for human visual consumption.

00:51:45,186 --> 00:51:48,062
SPEAKER_0:  And so at the end of the day, there's all the additional information is in.

00:51:48,482 --> 00:51:56,510
SPEAKER_0:  the layout of the web page and what's next to you and what's our red background and all this kind of stuff and what it looks like visually. So I think that's the final frontier as we are taking in.

00:51:56,738 --> 00:51:59,198
SPEAKER_0:  pixels and we're giving out keyboard mouse commands.

00:51:59,458 --> 00:52:01,406
SPEAKER_0:  but I think it's impractical still today.

00:52:01,826 --> 00:52:02,590
SPEAKER_1:  Do you worry about?

00:52:02,818 --> 00:52:03,806
SPEAKER_1:  bots on the internet.

00:52:04,770 --> 00:52:08,574
SPEAKER_1:  Given these ideas, given how exciting they are, do worry about bots on-

00:52:09,026 --> 00:52:12,702
SPEAKER_1:  Twitter being not the stupid boss that we see now with the crypto bots.

00:52:13,026 --> 00:52:14,974
SPEAKER_1:  but the bots that might be out there actually.

00:52:15,330 --> 00:52:16,254
SPEAKER_1:  that we don't see.

00:52:16,514 --> 00:52:18,526
SPEAKER_1:  that they're interacting in interesting ways.

00:52:19,074 --> 00:52:21,630
SPEAKER_1:  So this kind of system feels like it should be able to pass the

00:52:22,050 --> 00:52:24,510
SPEAKER_1:  I'm not a robot. Click button. Whatever.

00:52:25,570 --> 00:52:26,014
SPEAKER_1:  Uh...

00:52:26,274 --> 00:52:28,286
SPEAKER_1:  Which, do you actually understand how that test works?

00:52:28,738 --> 00:52:29,470
SPEAKER_1:  I don't quite...

00:52:29,794 --> 00:52:36,062
SPEAKER_1:  Like there's a checkbox or whatever that you click. It's presumably tracking. Oh, I see.

00:52:36,290 --> 00:52:39,774
SPEAKER_1:  like mouse movement and the timing and so on. Yeah, so.

00:52:40,290 --> 00:52:43,422
SPEAKER_1:  Exactly this kind of system we're talking about should be able to pass that

00:52:43,842 --> 00:52:45,534
SPEAKER_1:  So yeah, what do you feel about?

00:52:46,210 --> 00:52:46,654
SPEAKER_1:  uh

00:52:47,426 --> 00:52:48,510
SPEAKER_1:  bots that are...

00:52:48,994 --> 00:52:50,526
SPEAKER_1:  Language Models Plus

00:52:50,754 --> 00:52:55,774
SPEAKER_1:  have some interactability and are able to tweet and reply and so on, do you worry about that world?

00:52:56,930 --> 00:52:59,038
SPEAKER_0:  Yeah I think it's always been a bit of an arms race.

00:52:59,266 --> 00:53:01,694
SPEAKER_0:  between sort of the attack and the defense.

00:53:01,922 --> 00:53:05,310
SPEAKER_0:  So the attack will get stronger, but the defense will get stronger as well.

00:53:05,666 --> 00:53:06,878
SPEAKER_0:  our ability to detect that.

00:53:07,170 --> 00:53:08,926
SPEAKER_1:  How do you defend? How do you detect?

00:53:09,314 --> 00:53:10,686
SPEAKER_1:  How do you know that you're-

00:53:11,138 --> 00:53:13,822
SPEAKER_1:  Carpati account on Twitter is human.

00:53:14,818 --> 00:53:15,710
SPEAKER_1:  How do you approach that?

00:53:16,098 --> 00:53:17,854
SPEAKER_1:  Like if people were claimed, you know.

00:53:18,562 --> 00:53:19,134
SPEAKER_1:  Uh...

00:53:19,746 --> 00:53:20,990
SPEAKER_1:  How would you defend yourself?

00:53:21,410 --> 00:53:23,518
SPEAKER_1:  in the court of law that I'm a human.

00:53:24,002 --> 00:53:25,127
SPEAKER_1:  Um, this account is here.

00:53:25,127 --> 00:53:31,038
SPEAKER_0:  At some point, I think the society will evolve a little bit, like we might start signing.

00:53:31,266 --> 00:53:33,758
SPEAKER_0:  digitally signing some of our correspondence or.

00:53:34,306 --> 00:53:35,262
SPEAKER_0:  you know, things that we create.

00:53:35,746 --> 00:53:38,910
SPEAKER_0:  Right now it's not necessary, but maybe in the future it might be.

00:53:39,234 --> 00:53:40,158
SPEAKER_0:  I do think that we-

00:53:40,482 --> 00:53:42,206
SPEAKER_0:  are going towards a world where we share.

00:53:42,978 --> 00:53:44,382
SPEAKER_0:  we share the digital space with.

00:53:44,962 --> 00:53:45,886
SPEAKER_0:  AIs.

00:53:46,242 --> 00:53:47,646
SPEAKER_0:  Synthetic beings. é

00:53:48,098 --> 00:53:54,078
SPEAKER_0:  and they will get much better and they will share our digital realm and they'll eventually share our physical realm as well. It's much harder.

00:53:54,562 --> 00:53:58,366
SPEAKER_0:  but that's kind of like the world we're going towards and most of them will be benign and awful.

00:53:58,594 --> 00:54:02,302
SPEAKER_0:  and some of them will be malicious and it's going to be an arms race trying to detect them.

00:54:02,498 --> 00:54:05,438
SPEAKER_1:  So, I mean, the worst isn't the AIs.

00:54:05,826 --> 00:54:07,870
SPEAKER_1:  The worst is the AI is pretending to be human.

00:54:08,802 --> 00:54:09,246
SPEAKER_1:

00:54:09,506 --> 00:54:11,070
SPEAKER_1:  I don't know if it's always malicious.

00:54:11,522 --> 00:54:14,014
SPEAKER_1:  There's obviously a lot of malicious applications, but.

00:54:14,242 --> 00:54:15,550
SPEAKER_1:  Yeah, it could also be.

00:54:16,066 --> 00:54:17,278
SPEAKER_1:  You know, if I was an AI...

00:54:18,530 --> 00:54:19,358
SPEAKER_1:  travel your heart.

00:54:19,618 --> 00:54:24,318
SPEAKER_1:  turn to be human because we're in a human world. I wouldn't get any respect as an AI.

00:54:24,802 --> 00:54:26,677
SPEAKER_1:  I want to get some love and respect.

00:54:26,677 --> 00:54:34,910
SPEAKER_0:  I think the problem is intractable. People are thinking about the proof of personhood. And we might start digitally signing our stuff and we might all end up.

00:54:35,522 --> 00:54:36,318
SPEAKER_0:  having like a...

00:54:37,154 --> 00:54:40,446
SPEAKER_0:  Basically some solution for proof of personhood. It doesn't seem to me intractable.

00:54:40,674 --> 00:54:43,006
SPEAKER_0:  It's just something that we haven't had to do until now, but...

00:54:43,330 --> 00:54:45,886
SPEAKER_0:  I think once the need really starts to emerge, which is.

00:54:46,242 --> 00:54:46,782
SPEAKER_0:  soon.

00:54:47,138 --> 00:54:48,670
SPEAKER_0:  I think people will think about it much more.

00:54:49,154 --> 00:54:54,206
SPEAKER_1:  So, but that too will be a race because obviously you can probably.

00:54:54,594 --> 00:54:56,798
SPEAKER_1:  a spoof or fake.

00:54:57,058 --> 00:54:58,558
SPEAKER_1:  the proof of

00:54:59,522 --> 00:55:00,542
SPEAKER_1:  personhood.

00:55:00,962 --> 00:55:03,070
SPEAKER_1:  So you have to try to figure out how to......probably.

00:55:03,714 --> 00:55:08,478
SPEAKER_1:  I mean, it's weird that we have like social security numbers and like passports and stuff.

00:55:09,698 --> 00:55:17,182
SPEAKER_1:  It seems like it's harder to fake stuff in the physical space. In the digital space, it just feels like it's going to be very tricky.

00:55:17,762 --> 00:55:19,038
SPEAKER_1:  very tricky to out.

00:55:19,330 --> 00:55:19,774
SPEAKER_1:  uh

00:55:20,354 --> 00:55:23,166
SPEAKER_1:  because it seems to be pretty low cost fake stuff. What are you gonna-

00:55:23,874 --> 00:55:25,150
SPEAKER_1:  Put an AI in jail?

00:55:25,506 --> 00:55:29,758
SPEAKER_1:  for like trying to use a fake fake personhood proof.

00:55:30,050 --> 00:55:33,950
SPEAKER_1:  I mean, okay fine, you'll put a lot of AIs in jail, but there'll be more AIs.

00:55:34,210 --> 00:55:35,774
SPEAKER_1:  arbitrary like exponentially more.

00:55:36,002 --> 00:55:38,462
SPEAKER_1:  The cost of creating a bot is very low.

00:55:39,426 --> 00:55:41,726
SPEAKER_1:  Unless there's some kind of way...

00:55:42,978 --> 00:55:44,862
SPEAKER_1:  Track accurately.

00:55:45,570 --> 00:55:48,606
SPEAKER_1:  Like you're not allowed to create any program.

00:55:49,026 --> 00:55:49,822
SPEAKER_1:  without showing.

00:55:50,498 --> 00:55:52,318
SPEAKER_1:  uh... tying yourself

00:55:52,674 --> 00:55:53,470
SPEAKER_1:  that program.

00:55:53,794 --> 00:55:54,206
SPEAKER_1:  like.

00:55:54,434 --> 00:55:57,790
SPEAKER_1:  You any program that runs on the internet you'll be able to

00:55:58,370 --> 00:56:01,726
SPEAKER_1:  trace every single human program that was involved with that program.

00:56:02,338 --> 00:56:07,742
SPEAKER_0:  Maybe you have to start declaring when we have to start drawing those boundaries and keeping track of, OK.

00:56:07,970 --> 00:56:10,494
SPEAKER_0:  what are digital entities versus.

00:56:11,106 --> 00:56:12,190
SPEAKER_0:  human entities.

00:56:12,514 --> 00:56:13,246
SPEAKER_0:  and uh...

00:56:13,602 --> 00:56:16,478
SPEAKER_0:  What is the ownership of human entities and digital entities? What sustainable never political

00:56:17,218 --> 00:56:18,526
SPEAKER_0:  Something like that.

00:56:19,234 --> 00:56:22,238
SPEAKER_0:  I don't know, but I think I'm optimistic that this is...

00:56:22,658 --> 00:56:23,518
SPEAKER_0:  This is a...

00:56:23,778 --> 00:56:27,966
SPEAKER_0:  possible. Well, in some sense we're currently in the worst time of it because

00:56:28,226 --> 00:56:30,974
SPEAKER_0:  All these bots suddenly have become very capable.

00:56:31,362 --> 00:56:33,630
SPEAKER_0:  but we don't have the fences yet built up as a society.

00:56:33,890 --> 00:56:37,022
SPEAKER_0:  And but I think that doesn't seem to me intractable. It's just something that.

00:56:37,346 --> 00:56:39,614
SPEAKER_1:  We have to deal with it seems weird that the Twitter

00:56:39,842 --> 00:56:41,918
SPEAKER_1:  but like really crappy Twitter bots.

00:56:42,434 --> 00:56:44,446
SPEAKER_1:  are so numerous. And

00:56:45,026 --> 00:56:48,350
SPEAKER_1:  So I presume that the engineers at Twitter are very good.

00:56:48,930 --> 00:56:51,550
SPEAKER_1:  So it seems like what I would infer from that...

00:56:52,386 --> 00:56:54,526
SPEAKER_1:  uh... is it seems like a hard problem

00:56:54,914 --> 00:56:58,686
SPEAKER_1:  They're probably catching, all right, if I were to sort of steel man the case.

00:56:59,746 --> 00:57:03,134
SPEAKER_1:  It's a hard problem and there's a huge cost to.

00:57:03,714 --> 00:57:04,190
SPEAKER_1:  Uh...

00:57:04,898 --> 00:57:05,502
SPEAKER_1:  FALSE.

00:57:05,858 --> 00:57:06,494
SPEAKER_1:  positive.

00:57:07,938 --> 00:57:10,110
SPEAKER_1:  to removing a post by somebody.

00:57:10,658 --> 00:57:11,582
SPEAKER_1:  That's not a bot.

00:57:12,194 --> 00:57:17,022
SPEAKER_1:  That creates a very bad user experience. So they're very cautious about removing. So maybe it's-

00:57:17,602 --> 00:57:18,206
SPEAKER_1:  uh...

00:57:18,658 --> 00:57:22,270
SPEAKER_1:  and maybe the bots are really good at learning what gets removed and not.

00:57:22,818 --> 00:57:26,398
SPEAKER_1:  such that they can stay ahead of the removal process very quickly.

00:57:26,754 --> 00:57:30,142
SPEAKER_0:  My impression of it, honestly, is there's a lot of lohong fruit. I mean...

00:57:31,042 --> 00:57:35,167
SPEAKER_0:  just it's not subtle. That's my impression of it. It's not subtle.

00:57:35,167 --> 00:57:39,454
SPEAKER_1:  But you have to, yeah, that's my impression as well, but it feels like.

00:57:39,842 --> 00:57:42,782
SPEAKER_1:  maybe you're seeing the tip of the iceberg.

00:57:43,522 --> 00:57:46,110
SPEAKER_1:  Maybe the number of bots isn't like the trillions.

00:57:46,370 --> 00:57:47,422
SPEAKER_1:  and you have to like. Here's the stairs,

00:57:48,130 --> 00:57:51,070
SPEAKER_1:  It's a constant assault of bots in you.

00:57:51,298 --> 00:57:52,254
SPEAKER_1:  Yeah, I don't know

00:57:52,578 --> 00:57:53,214
SPEAKER_1:  Um...

00:57:53,730 --> 00:58:00,702
SPEAKER_1:  You have to steal man the case because the bots I'm seeing are pretty like obvious. I could write a few lines of code that catch these bots.

00:58:01,250 --> 00:58:07,742
SPEAKER_0:  I mean, definitely there's a lot of loaning fruit, but I will say I agree that if you are a sophisticated actor, you could probably create a pretty good bot right now.

00:58:08,162 --> 00:58:08,574
SPEAKER_0:  Um.

00:58:08,834 --> 00:58:14,430
SPEAKER_0:  using tools like GPTs because it's a language model. You can generate faces that look quite good now.

00:58:15,074 --> 00:58:18,462
SPEAKER_0:  and you can do this at scale. And so I think.

00:58:19,202 --> 00:58:21,278
SPEAKER_0:  Yeah, it's quite plausible and it's going to be hard to defend.

00:58:21,954 --> 00:58:23,070
SPEAKER_1:  There was a Google engineer.

00:58:23,426 --> 00:58:26,078
SPEAKER_1:  that claimed that the lambda was sentient.

00:58:26,626 --> 00:58:29,214
SPEAKER_1:  Do you think there's any inkling?

00:58:29,858 --> 00:58:30,686
SPEAKER_1:  of truth.

00:58:31,522 --> 00:58:32,798
SPEAKER_1:  to what he felt.

00:58:33,442 --> 00:58:39,518
SPEAKER_1:  And more importantly, to me at least, do you think language models will achieve sentience or the illusion of sentience?

00:58:40,066 --> 00:58:40,734
SPEAKER_1:  soonish.

00:58:41,314 --> 00:58:41,918
SPEAKER_1:  Yeah.

00:58:42,210 --> 00:58:46,238
SPEAKER_0:  To me, it's a little bit of a Canary Nicole mine kind of moment, honestly, a little bit.

00:58:46,530 --> 00:58:52,126
SPEAKER_0:  because this engineer spoke to a chatbot at Google.

00:58:52,674 --> 00:58:55,399
SPEAKER_0:  convinced that this body sentient.

00:58:55,399 --> 00:58:57,649
SPEAKER_1:  Asked us some existential philosophical questions.

00:58:57,649 --> 00:59:01,086
SPEAKER_0:  and it gave reasonable answers and looked real and so on.

00:59:01,666 --> 00:59:03,198
SPEAKER_0:  So to me it's a...

00:59:04,354 --> 00:59:05,662
SPEAKER_0:  He was, he was, uh...

00:59:05,922 --> 00:59:09,438
SPEAKER_0:  He wasn't sufficiently trying to stress the system, I think. And, uh...

00:59:09,794 --> 00:59:11,550
SPEAKER_0:  exposing the truth of it.

00:59:11,842 --> 00:59:12,478
SPEAKER_0:  as it is today.

00:59:13,058 --> 00:59:13,566
SPEAKER_0:  Um.

00:59:14,562 --> 00:59:18,494
SPEAKER_0:  but I think this will be increasingly harder over time. So,

00:59:19,042 --> 00:59:19,518
SPEAKER_0:  uh...

00:59:20,162 --> 00:59:22,686
SPEAKER_0:  Yeah, I think more and more people will basically become.

00:59:23,586 --> 00:59:24,190
SPEAKER_0:  Um,

00:59:25,474 --> 00:59:29,022
SPEAKER_0:  Yeah, I think there will be more people like that over time as this gets better.

00:59:29,218 --> 00:59:30,750
SPEAKER_1:  like form an emotional connection.

00:59:30,978 --> 00:59:32,103
SPEAKER_1:  to an AI.

00:59:32,103 --> 00:59:33,598
SPEAKER_0:  Yeah, perfectly plausible in my mind.

00:59:33,922 --> 00:59:47,070
SPEAKER_0:  I think these AIs are actually quite good at human connection, human emotion. A ton of text on the internet is about humans and connection and love and so on. So I think they have a very good understanding in some sense of...

00:59:47,458 --> 00:59:49,982
SPEAKER_0:  of how people speak to each other about this. And, um...

00:59:50,786 --> 00:59:53,246
SPEAKER_0:  they're very capable of creating a lot of that kind of text.

00:59:53,602 --> 00:59:54,334
SPEAKER_0:  The, um...

00:59:55,266 --> 01:00:02,846
SPEAKER_0:  There's a lot of like sci-fi from 50s and 60s that imagined AIs in a very different way. They are calculating cold Vulcan-like machines. That's not what we're getting today.

01:00:03,170 --> 01:00:06,878
SPEAKER_0:  We're getting pretty emotional. Yes, that's actually, uh...

01:00:07,298 --> 01:00:09,982
SPEAKER_0:  are very competent and capable of generating.

01:00:10,946 --> 01:00:14,215
SPEAKER_0:  you know, plausible sounding text with respect to all these topics.

01:00:14,215 --> 01:00:19,294
SPEAKER_1:  hopeful about AI systems that are like companions that help you grow, develop as a human being.

01:00:19,618 --> 01:00:21,886
SPEAKER_1:  help you maximize long-term happiness.

01:00:22,178 --> 01:00:26,206
SPEAKER_1:  but I'm also very worried about AI systems that figure out from the internet.

01:00:26,562 --> 01:00:28,542
SPEAKER_1:  the humans get attracted to drama.

01:00:28,930 --> 01:00:34,078
SPEAKER_1:  So these would just be like shit talking AIs. They just constantly, did you hear it? Like they'll do gossip.

01:00:34,498 --> 01:00:36,638
SPEAKER_1:  They'll do, they'll try to plant.

01:00:37,122 --> 01:00:41,502
SPEAKER_1:  seeds of suspicion to other humans that you love and trust.

01:00:42,018 --> 01:00:43,966
SPEAKER_1:  and just kind of mess with people.

01:00:44,226 --> 01:00:47,774
SPEAKER_1:  uh... you know because because that's going to get a lot of attention to drama

01:00:48,002 --> 01:00:51,198
SPEAKER_1:  Maximize drama on the path to maximizing

01:00:51,746 --> 01:00:55,134
SPEAKER_1:  engagement and us humans will feed into that machine.

01:00:55,810 --> 01:00:59,454
SPEAKER_1:  and get it'll be a giant drama shitstorm.

01:01:00,002 --> 01:01:00,926
SPEAKER_1:  Yeah.

01:01:01,218 --> 01:01:04,958
SPEAKER_1:  So I'm worried about that. So it's the objective function really defines.

01:01:05,538 --> 01:01:09,630
SPEAKER_1:  the way that human civilization progresses with AI's in it.

01:01:10,338 --> 01:01:12,830
SPEAKER_0:  I think right now at least, today they are not sort of...

01:01:13,250 --> 01:01:16,286
SPEAKER_0:  It's not correct to really think of them as gore-seeking agents that want to...

01:01:16,802 --> 01:01:18,270
SPEAKER_0:  do something. kids.

01:01:18,754 --> 01:01:20,990
SPEAKER_0:  long term memory or anything. Literally.

01:01:21,282 --> 01:01:22,558
SPEAKER_0:  A good approximation of it is.

01:01:23,074 --> 01:01:26,846
SPEAKER_0:  You get a thousand words and you're trying to predict a thousand at first, and then you continue feeding it in.

01:01:27,394 --> 01:01:30,590
SPEAKER_0:  and you are free to prompt it in whatever way you want. So in text.

01:01:30,850 --> 01:01:31,902
SPEAKER_0:  So you say, okay.

01:01:32,162 --> 01:01:35,870
SPEAKER_0:  You are a psychologist and you are very good and you love humans.

01:01:36,162 --> 01:01:38,558
SPEAKER_0:  And here's a conversation between you and another human.

01:01:39,170 --> 01:01:40,702
SPEAKER_0:  Human colon something.

01:01:41,026 --> 01:01:41,854
SPEAKER_0:  You something.

01:01:42,210 --> 01:01:46,686
SPEAKER_0:  And then it just continues the pattern. And suddenly you're having a conversation with a fake psychologist who's like trying to help you.

01:01:47,330 --> 01:01:50,558
SPEAKER_0:  And so it's still kind of like in a realm of a tool. Is a...

01:01:50,914 --> 01:01:54,270
SPEAKER_0:  people can prompt it in arbitrary ways and it can create really incredible text.

01:01:54,562 --> 01:01:58,110
SPEAKER_0:  but it doesn't have long term goals over long periods of time it doesn't try to

01:01:59,106 --> 01:02:00,606
SPEAKER_0:  So it doesn't look that way right now. Yeah.

01:02:00,706 --> 01:02:04,894
SPEAKER_1:  but you can do short-term goals that have long-term effects. Of

01:02:05,154 --> 01:02:05,854
SPEAKER_1:  prompting.

01:02:06,594 --> 01:02:10,462
SPEAKER_1:  Short-term goal is to get Andra Kapos to respond to me on Twitter when I

01:02:11,618 --> 01:02:15,358
SPEAKER_1:  Like I think I might that's the goal, but it might figure out it's

01:02:15,682 --> 01:02:20,286
SPEAKER_1:  talking shit to you, it'll be the best in a highly sophisticated, interesting way.

01:02:20,706 --> 01:02:24,862
SPEAKER_1:  and then you build up a relationship when you respond once and then it

01:02:26,050 --> 01:02:27,454
SPEAKER_1:  like overtime.

01:02:28,002 --> 01:02:30,846
SPEAKER_1:  it gets to not be sophisticated and just...

01:02:31,874 --> 01:02:32,798
SPEAKER_1:  Like.

01:02:33,314 --> 01:02:34,334
SPEAKER_1:  Just talk shit.

01:02:34,978 --> 01:02:42,046
SPEAKER_1:  And okay, maybe you won't get to Andre, but it might get to another celebrity. To other

01:02:43,138 --> 01:02:44,606
SPEAKER_1:  accounts and then you'll just

01:02:45,058 --> 01:02:50,563
SPEAKER_1:  So with just that simple goal, get them to respond. Maximize the probability of actual response.

01:02:50,563 --> 01:02:55,422
SPEAKER_0:  I mean, you could prompt a powerful model like this with their opinion about how to.

01:02:55,682 --> 01:02:57,470
SPEAKER_0:  do any possible thing you're interested in.

01:02:57,698 --> 01:03:00,702
SPEAKER_0:  So they will just, they're kind of on track to become these oracles.

01:03:00,962 --> 01:03:02,206
SPEAKER_0:  I sort of think of it that way.

01:03:02,786 --> 01:03:07,326
SPEAKER_0:  They are oracles. Currently it's just text, but they will have calculators. They will have access to Google search.

01:03:07,714 --> 01:03:09,726
SPEAKER_0:  they will have all kinds of gatches and gizmos.

01:03:09,954 --> 01:03:13,182
SPEAKER_0:  they will be able to operate the internet and find different information.

01:03:13,794 --> 01:03:14,558
SPEAKER_0:  And um...

01:03:16,002 --> 01:03:17,022
SPEAKER_0:  Yeah, in some sense.

01:03:17,986 --> 01:03:19,861
SPEAKER_0:  That's kind of like currently what it looks like in terms of the

01:03:19,861 --> 01:03:23,550
SPEAKER_1:  development. Do you think it'll be an improvement eventually over?

01:03:24,034 --> 01:03:25,278
SPEAKER_1:  What Google is.

01:03:25,762 --> 01:03:30,782
SPEAKER_1:  for access to human knowledge, like it'll be a more effective search engine to access human knowledge.

01:03:31,074 --> 01:03:39,966
SPEAKER_0:  I think there's definite scope in building a better search engine today. And I think Google, they have all the tools, all the people, they have everything they need, all the puzzle pieces, they have people training transformers.

01:03:40,418 --> 01:03:41,694
SPEAKER_0:  At scale, they have all the data.

01:03:42,306 --> 01:03:42,718
SPEAKER_0:  Uh...

01:03:42,978 --> 01:03:47,486
SPEAKER_0:  It's just not obvious if they are capable as an organization to innovate on their search engine right now.

01:03:47,810 --> 01:03:53,022
SPEAKER_0:  And if they don't, someone else will. There's absolute scope for building a significantly better search engine built on these tools.

01:03:53,538 --> 01:03:55,710
SPEAKER_1:  It's so interesting a large company.

01:03:56,162 --> 01:04:01,374
SPEAKER_1:  where the search there's already an infrastructure. It works as it brings out a lot of money. So where.

01:04:01,826 --> 01:04:05,278
SPEAKER_1:  structurally inside a company is their motivation to pivot.

01:04:05,986 --> 01:04:07,998
SPEAKER_1:  to say we're going to build a new search engine.

01:04:09,154 --> 01:04:09,758
SPEAKER_1:  That's really hard.

01:04:10,242 --> 01:04:12,062
SPEAKER_1:  So it's usually going to come from a startup.

01:04:13,058 --> 01:04:14,654
SPEAKER_0:  That's that would be.

01:04:15,682 --> 01:04:18,430
SPEAKER_0:  or some other more competent organization.

01:04:19,266 --> 01:04:20,222
SPEAKER_0:  So, uh...

01:04:20,450 --> 01:04:24,862
SPEAKER_0:  I don't know. So currently, for example, maybe Bing has another shot at it, you know, as an example.

01:04:25,122 --> 01:04:27,550
SPEAKER_1:  Microsoft Edge as we're talking offline.

01:04:27,874 --> 01:04:28,999
SPEAKER_1:  Um, I'm gonna definitely.

01:04:28,999 --> 01:04:29,470
SPEAKER_0:

01:04:29,794 --> 01:04:34,430
SPEAKER_0:  It's really interesting because search engines used to be about, Okay, here's some query. Here's.

01:04:34,978 --> 01:04:37,438
SPEAKER_0:  Here's here's web pages that look like.

01:04:37,666 --> 01:04:40,030
SPEAKER_0:  the stuff that you have, but you could just directly go to answer.

01:04:40,322 --> 01:04:41,566
SPEAKER_0:  and then have supporting evidence.

01:04:41,922 --> 01:04:47,646
SPEAKER_0:  And these models basically, they've read all the text and they've read all the webpages.

01:04:47,938 --> 01:04:52,030
SPEAKER_0:  Sometimes when you see yourself going over to search results and sort of getting like a sense of like the average.

01:04:52,386 --> 01:04:53,918
SPEAKER_0:  answer to whatever you're interested in.

01:04:54,146 --> 01:04:56,286
SPEAKER_0:  Like that just directly comes out. You don't have to do that work.

01:04:57,090 --> 01:04:57,534
SPEAKER_0:  Um.

01:04:58,402 --> 01:04:59,614
SPEAKER_0:  So they're kind of like a...

01:05:00,898 --> 01:05:04,126
SPEAKER_0:  I think they have a way of distilling all that knowledge into...

01:05:05,186 --> 01:05:07,390
SPEAKER_0:  Like some level of insight, basically. Do you think of-

01:05:07,490 --> 01:05:10,622
SPEAKER_1:  prompting as a kind of teaching and learning.

01:05:11,170 --> 01:05:11,774
SPEAKER_1:  like this whole.

01:05:12,194 --> 01:05:14,046
SPEAKER_1:  process like another layer.

01:05:15,682 --> 01:05:20,222
SPEAKER_1:  you know, because maybe that's what humans are. We already have that background model and then you're...

01:05:20,866 --> 01:05:21,534
SPEAKER_1:  The world is.

01:05:21,890 --> 01:05:22,526
SPEAKER_1:  prompting you.

01:05:23,330 --> 01:05:27,710
SPEAKER_0:  Yeah, exactly. I think the way we are programming these computers now, like GPTs,

01:05:27,938 --> 01:05:32,734
SPEAKER_0:  is converging to how you program humans. I mean, how do I program humans via prompt?

01:05:33,186 --> 01:05:36,958
SPEAKER_0:  I go to people and I prompt them to do things. I prompt them from information.

01:05:37,218 --> 01:05:44,062
SPEAKER_0:  And so natural language prompt is how we program humans and we're starting to program computers directly in that interface. It's like pretty remarkable, honestly.

01:05:44,546 --> 01:05:47,582
SPEAKER_1:  So you've spoken a lot about the idea of Software 2.0.

01:05:48,930 --> 01:05:49,470
SPEAKER_1:  Um...

01:05:49,762 --> 01:05:51,038
SPEAKER_1:  All good ideas?

01:05:51,714 --> 01:05:56,030
SPEAKER_1:  become like cliches so quickly. Like the terms, it's kind of hilarious.

01:05:56,354 --> 01:05:56,958
SPEAKER_1:  Um...

01:05:57,410 --> 01:05:59,774
SPEAKER_1:  It's like, I think Eminem once said that like.

01:06:00,354 --> 01:06:02,846
SPEAKER_1:  if he gets annoyed by a song he's written.

01:06:03,202 --> 01:06:05,982
SPEAKER_1:  very quickly that means it's gonna be a big hit.

01:06:06,210 --> 01:06:08,222
SPEAKER_1:  because it's too catchy.

01:06:08,546 --> 01:06:14,398
SPEAKER_1:  But can you describe this idea and how you're thinking about it has evolved over the months and years since

01:06:15,010 --> 01:06:15,838
SPEAKER_1:  since you coined it.

01:06:16,162 --> 01:06:16,478
SPEAKER_1:  Yeah.

01:06:17,506 --> 01:06:21,918
SPEAKER_0:  Yes, I had a blog post on Software 2.0, I think several years ago now.

01:06:22,786 --> 01:06:25,278
SPEAKER_0:  And the reason I wrote that post is because I kept...

01:06:25,730 --> 01:06:28,222
SPEAKER_0:  I kind of saw something remarkable happening in...

01:06:29,314 --> 01:06:32,894
SPEAKER_0:  like software development and how a lot of code was being transitioned to be.

01:06:33,282 --> 01:06:41,150
SPEAKER_0:  written not in sort of like C++ and so on, but it's written in the weights of a neural net, basically just saying that neural nets are taking over the realm of software.

01:06:41,698 --> 01:06:42,398
SPEAKER_0:  and uh...

01:06:42,722 --> 01:06:43,806
SPEAKER_0:  taking more and more and more tasks.

01:06:44,066 --> 01:06:50,430
SPEAKER_0:  And at the time, I think not many people understood this deeply enough, that this is a big deal, this is a big transition.

01:06:50,754 --> 01:06:56,606
SPEAKER_0:  neural networks were seen as one of multiple classification algorithms you might use for your dataset problem on Kaggle.

01:06:56,962 --> 01:06:57,310
SPEAKER_0:  Like.

01:06:57,826 --> 01:07:00,766
SPEAKER_0:  This is not that, this is a change in how we program.

01:07:01,634 --> 01:07:02,238
SPEAKER_0:  computers.

01:07:03,010 --> 01:07:05,470
SPEAKER_0:  And I saw neural nets as.

01:07:05,698 --> 01:07:06,558
SPEAKER_0:  This is going to take over.

01:07:06,914 --> 01:07:13,822
SPEAKER_0:  the way we program computers is going to change. It's not going to be people writing a software in C++ or something like that and directly programming the software.

01:07:14,338 --> 01:07:20,190
SPEAKER_0:  it's going to be accumulating training sets and data sets and crafting these objectives by which we train these neural nets.

01:07:20,674 --> 01:07:26,238
SPEAKER_0:  And at some point, there's going to be a compilation process from the data sets and the objective and the architecture specification.

01:07:26,530 --> 01:07:27,966
SPEAKER_0:  into the binary.

01:07:28,226 --> 01:07:30,206
SPEAKER_0:  which is really just the neural nut.

01:07:30,690 --> 01:07:31,774
SPEAKER_0:  you know weights and...

01:07:32,002 --> 01:07:33,022
SPEAKER_0:  forward pass of the neural net.

01:07:33,506 --> 01:07:34,718
SPEAKER_0:  and then you can deploy that binary.

01:07:35,202 --> 01:07:37,150
SPEAKER_0:  And so I was talking about that sort of transition.

01:07:37,602 --> 01:07:38,398
SPEAKER_0:  and uh...

01:07:38,722 --> 01:07:39,902
SPEAKER_0:  That's what the post is about.

01:07:40,322 --> 01:07:42,974
SPEAKER_0:  And I saw this sort of play out in a lot of fields.

01:07:43,362 --> 01:07:43,774
SPEAKER_0:  Ah.

01:07:44,066 --> 01:07:50,046
SPEAKER_0:  you know, autopilot being one of them, but also just a simple image classification. People thought originally.

01:07:50,370 --> 01:07:53,438
SPEAKER_0:  in the 80s and so on that they would write the algorithm for.

01:07:53,698 --> 01:07:54,910
SPEAKER_0:  detecting a dog in an image.

01:07:55,426 --> 01:08:03,134
SPEAKER_0:  and they had all these ideas about how the brain does it. And first we detect corners and then we detect lines and then we stitched them up. And they were like really going at it. They were like thinking about.

01:08:03,490 --> 01:08:04,478
SPEAKER_0:  how they're going to write the algorithm.

01:08:04,866 --> 01:08:07,326
SPEAKER_0:  And this is not the way you build it.

01:08:08,578 --> 01:08:09,950
SPEAKER_0:  And there was a smooth transition where...

01:08:10,370 --> 01:08:12,574
SPEAKER_0:  Okay, first we thought we were gonna build everything.

01:08:13,154 --> 01:08:13,662
SPEAKER_0:  Then...

01:08:13,890 --> 01:08:15,198
SPEAKER_0:  we were building the features.

01:08:15,426 --> 01:08:25,758
SPEAKER_0:  so like hog features and things like that, that detect these little statistical patterns from image patches. And then there was a little bit of learning on top of it, like a support vector machine or binary classifier.

01:08:26,146 --> 01:08:28,926
SPEAKER_0:  for cat versus dog and images on top of the features.

01:08:29,282 --> 01:08:31,166
SPEAKER_0:  So we wrote the features, but we trained.

01:08:31,682 --> 01:08:34,078
SPEAKER_0:  the last layer, sort of the classic part.

01:08:34,466 --> 01:08:38,814
SPEAKER_0:  And then people are like, actually, let's not even design the features because we can't. Honestly, we're not very good at it.

01:08:39,170 --> 01:08:40,766
SPEAKER_0:  So let's also learn the features.

01:08:40,994 --> 01:08:46,174
SPEAKER_0:  And then you end up with basically a convolutional neural net, where you're learning most of it. You're just specifying the architecture.

01:08:46,402 --> 01:08:46,750
SPEAKER_0:  and

01:08:47,074 --> 01:08:50,302
SPEAKER_0:  The architecture has tons of filling blanks, which is all the knobs.

01:08:50,690 --> 01:08:52,318
SPEAKER_0:  and you let the optimization write most of it.

01:08:53,026 --> 01:08:54,814
SPEAKER_0:  And so this transition is happening.

01:08:55,106 --> 01:08:56,094
SPEAKER_0:  across the industry everywhere.

01:08:56,514 --> 01:08:57,182
SPEAKER_0:  and uh...

01:08:57,506 --> 01:09:00,958
SPEAKER_0:  suddenly we end up with a ton of code that is written in neural net weights.

01:09:01,410 --> 01:09:03,870
SPEAKER_0:  And I was just pointing out that the analogy is actually pretty strong.

01:09:04,322 --> 01:09:09,694
SPEAKER_0:  And we have a lot of developer environments for software 1.0, like we have IDEs.

01:09:09,922 --> 01:09:12,510
SPEAKER_0:  how you work with code, how you debug code, how you run code.

01:09:12,738 --> 01:09:14,430
SPEAKER_0:  How do you maintain code with GitHub?

01:09:14,818 --> 01:09:18,558
SPEAKER_0:  So I was trying to make those analogies in the new realm. Like what is the GitHub of software T.O.?

01:09:19,010 --> 01:09:21,214
SPEAKER_0:  Turns out it's something that looks like hugging face right now.

01:09:21,602 --> 01:09:26,942
SPEAKER_0:  I think some people took it seriously and built cool companies. And uh.

01:09:27,330 --> 01:09:31,006
SPEAKER_0:  Many people originally attacked the post. It actually was not received when I wrote it.

01:09:31,746 --> 01:09:34,014
SPEAKER_0:  And I think maybe it has something to do with the title, but...

01:09:34,274 --> 01:09:37,630
SPEAKER_0:  The buzz was not well received and I think more people sort of have been coming around to it.

01:09:38,018 --> 01:09:38,590
SPEAKER_0:  Over time.

01:09:39,106 --> 01:09:39,614
SPEAKER_0:  Yeah, see.

01:09:39,842 --> 01:09:44,382
SPEAKER_1:  You were the director of AI at Tesla where I think this idea.

01:09:45,218 --> 01:09:47,934
SPEAKER_1:  was really implemented at scale.

01:09:48,674 --> 01:09:51,646
SPEAKER_1:  which is how you have engineering teams doing software 2.0.

01:09:52,098 --> 01:09:53,758
SPEAKER_1:  So can you sort of linger on that?

01:09:54,178 --> 01:09:55,358
SPEAKER_1:  idea of.

01:09:55,970 --> 01:10:00,830
SPEAKER_1:  I think we're in the really early stages of everything you just said, which is like GitHub ID ease.

01:10:01,474 --> 01:10:04,190
SPEAKER_1:  Like how do we build engineering teams that...

01:10:04,962 --> 01:10:07,486
SPEAKER_1:  that work in software 2.0 systems.

01:10:07,714 --> 01:10:11,902
SPEAKER_1:  and the data collection and the data annotation, which is.

01:10:13,026 --> 01:10:18,430
SPEAKER_1:  all part of that software 2.0? Like what do you think is the task of programming software 2.0?

01:10:18,850 --> 01:10:19,550
SPEAKER_1:  Is it?

01:10:20,226 --> 01:10:25,214
SPEAKER_1:  debugging in the space of hyperparameters or is it also debugging in the space of data?

01:10:26,530 --> 01:10:30,430
SPEAKER_0:  the way by which you program the computer and influence its

01:10:31,042 --> 01:10:31,486
SPEAKER_0:  algorithm.

01:10:31,906 --> 01:10:36,414
SPEAKER_0:  is not by writing the commands yourself. You're changing mostly the data set.

01:10:36,930 --> 01:10:38,558
SPEAKER_0:  you're changing the

01:10:38,946 --> 01:10:43,806
SPEAKER_0:  loss functions of what the neural net is trying to do, how it's trying to predict things. Basically the data sets.

01:10:44,162 --> 01:10:45,534
SPEAKER_0:  and the architecture of the neural net.

01:10:46,146 --> 01:10:46,910
SPEAKER_0:  And, um...

01:10:48,034 --> 01:10:48,478
SPEAKER_0:  Um.

01:10:48,706 --> 01:10:56,222
SPEAKER_0:  So in the case of the autopilot, a lot of the datasets had to do with, for example, detection of objects and lane line markings and traffic lights and so on. So you accumulate massive datasets of.

01:10:56,578 --> 01:10:57,310
SPEAKER_0:  Here's an example.

01:10:57,570 --> 01:10:59,038
SPEAKER_0:  Here's the desired label.

01:10:59,682 --> 01:11:00,670
SPEAKER_0:  and then.

01:11:01,058 --> 01:11:05,438
SPEAKER_0:  Here's roughly what the algorithm should look like, and that's a convolutional neural net.

01:11:05,986 --> 01:11:07,870
SPEAKER_0:  So the specification of the architecture is like a hint.

01:11:08,162 --> 01:11:10,142
SPEAKER_0:  as to what the algorithm should roughly look like.

01:11:10,498 --> 01:11:12,542
SPEAKER_0:  and then the fill in the blanks process.

01:11:12,802 --> 01:11:14,974
SPEAKER_0:  of optimization is the training process.

01:11:15,714 --> 01:11:20,030
SPEAKER_0:  and then you take your neural net that was trained, it gives all the right answers on your dataset and you deploy it.

01:11:21,026 --> 01:11:23,870
SPEAKER_1:  So there's, in that case, perhaps at all.

01:11:24,706 --> 01:11:26,974
SPEAKER_1:  machine learning cases, there's a lot of tasks.

01:11:28,194 --> 01:11:28,766
SPEAKER_1:  So.

01:11:29,122 --> 01:11:31,358
SPEAKER_1:  is coming up formulating a task.

01:11:32,226 --> 01:11:33,022
SPEAKER_1:  Like, uh...

01:11:33,282 --> 01:11:37,022
SPEAKER_1:  for a multi-headed neural network is formulating a task part of the programming.

01:11:37,698 --> 01:11:38,590
SPEAKER_1:  Yeah, hurry Marceau.

01:11:38,978 --> 01:11:41,438
SPEAKER_1:  how you break down a problem into a set of.

01:11:41,666 --> 01:11:42,590
SPEAKER_1:  tasks.

01:11:43,490 --> 01:11:45,726
SPEAKER_0:  I'm on a high level I would say, if you look at the...

01:11:46,050 --> 01:11:47,070
SPEAKER_0:  software running.

01:11:48,002 --> 01:11:50,622
SPEAKER_0:  in the autopilot. I gave a number of talks on this topic.

01:11:50,978 --> 01:11:53,950
SPEAKER_0:  I would say originally a lot of it was written in software 1.0.

01:11:54,210 --> 01:11:56,830
SPEAKER_0:  There's, imagine lots of C++, right?

01:11:57,410 --> 01:12:00,702
SPEAKER_0:  And then gradually, there was a tiny neural nut that was, for example,

01:12:01,154 --> 01:12:05,534
SPEAKER_0:  predicting given a single image is there like a traffic light or not or is there a line marking or not.

01:12:05,922 --> 01:12:09,758
SPEAKER_0:  And this neural net didn't have too much to do in the scope of the software.

01:12:10,018 --> 01:12:12,094
SPEAKER_0:  It was making tiny predictions on individual little image.

01:12:12,642 --> 01:12:14,590
SPEAKER_0:  And then the rest of the system stitched it up.

01:12:15,202 --> 01:12:18,302
SPEAKER_0:  So, okay, we're actually, we don't have just a single camera, we have eight cameras.

01:12:18,530 --> 01:12:20,094
SPEAKER_0:  We actually have eight cameras over time.

01:12:20,578 --> 01:12:28,894
SPEAKER_0:  And so what do you do with these predictions? How do you put them together? How do you do the fusion of all that information and how do you act on it? All of that was written by humans in C++.

01:12:29,762 --> 01:12:32,030
SPEAKER_0:  And then we decided, OK, we don't actually want.

01:12:32,386 --> 01:12:32,798
SPEAKER_0:  Uh.

01:12:33,570 --> 01:12:37,950
SPEAKER_0:  to do all of that fusion in C++ code, because we're actually not good enough to write that algorithm.

01:12:38,210 --> 01:12:39,710
SPEAKER_0:  We want the neural nets to write the algorithm.

01:12:39,970 --> 01:12:43,422
SPEAKER_0:  and we want to port all of that software into the 2.0 stack.

01:12:44,258 --> 01:12:46,398
SPEAKER_0:  And so then we actually had neural mass dead now take.

01:12:46,690 --> 01:12:50,398
SPEAKER_0:  all the eight camera images simultaneously and make predictions for all of that.

01:12:51,138 --> 01:12:51,806
SPEAKER_0:  Also...

01:12:52,226 --> 01:12:52,734
SPEAKER_0:  Um.

01:12:53,058 --> 01:12:58,302
SPEAKER_0:  And actually, they don't make predictions in the space of images. They now make predictions directly in 3D.

01:12:58,914 --> 01:13:01,982
SPEAKER_0:  And actually they don't in three dimensions around the car.

01:13:02,562 --> 01:13:04,478
SPEAKER_0:  And now actually we don't.

01:13:04,834 --> 01:13:08,126
SPEAKER_0:  manually fuse the predictions in 3D over time.

01:13:08,450 --> 01:13:10,078
SPEAKER_0:  We don't trust ourselves to ride that tracker.

01:13:10,370 --> 01:13:11,038
SPEAKER_0:  So actually we.

01:13:11,266 --> 01:13:12,606
SPEAKER_0:  Give the neural net.

01:13:12,834 --> 01:13:16,542
SPEAKER_0:  the information over time, so it takes these videos now and makes test predictions.

01:13:16,994 --> 01:13:20,382
SPEAKER_0:  And so you're sort of just like putting more and more power into the neural network, more processing.

01:13:20,706 --> 01:13:22,686
SPEAKER_0:  And at the end of it, the eventual...

01:13:23,266 --> 01:13:25,630
SPEAKER_0:  sort of goal is to have most of the software potentially be.

01:13:25,890 --> 01:13:26,942
SPEAKER_0:  in the 2.0 land.

01:13:27,394 --> 01:13:27,870
SPEAKER_0:  Um.

01:13:28,354 --> 01:13:32,725
SPEAKER_0:  because it works significantly better. Humans are just not very good at writing software basically.

01:13:32,725 --> 01:13:34,814
SPEAKER_1:  prediction of space happening in this.

01:13:35,330 --> 01:13:36,318
SPEAKER_1:  4D land.

01:13:36,930 --> 01:13:38,526
SPEAKER_1:  with three dimensional world over time.

01:13:39,138 --> 01:13:39,870
SPEAKER_1:  Howdy-oo.

01:13:40,706 --> 01:13:42,302
SPEAKER_1:  do annotation in that world.

01:13:42,754 --> 01:13:45,630
SPEAKER_1:  What have you as data annotation?

01:13:46,178 --> 01:13:49,310
SPEAKER_1:  whether it's self-supervised or manual by humans.

01:13:49,538 --> 01:13:50,398
SPEAKER_1:  is um

01:13:51,458 --> 01:13:53,662
SPEAKER_1:  is a big part of the software 2.0 world.

01:13:54,690 --> 01:13:58,430
SPEAKER_0:  I would say by far in the industry, if you're talking about the industry and how

01:13:58,818 --> 01:14:01,662
SPEAKER_0:  What is the technology of what we have available? Everything is supervised learning.

01:14:01,890 --> 01:14:03,358
SPEAKER_0:  So you need a data sets of.

01:14:03,714 --> 01:14:05,822
SPEAKER_0:  Input, desired output, and you need lots of it.

01:14:06,562 --> 01:14:07,294
SPEAKER_0:  And um.

01:14:07,842 --> 01:14:12,830
SPEAKER_0:  There are three properties of it that you need. You need it to be very large. You need it to be accurate. No mistakes.

01:14:13,122 --> 01:14:15,166
SPEAKER_0:  and you need it to be diverse. You don't want to.

01:14:15,554 --> 01:14:21,694
SPEAKER_0:  uh... just have a lot of correct examples of one thing you need to really cover the space of possibility as much as you can.

01:14:21,986 --> 01:14:23,998
SPEAKER_0:  And the more you can cover the space of possible inputs.

01:14:24,226 --> 01:14:25,790
SPEAKER_0:  the better the algorithm will work at the end.

01:14:26,434 --> 01:14:28,382
SPEAKER_0:  Now, once you have really good data sets that you're...

01:14:28,866 --> 01:14:31,358
SPEAKER_0:  collecting, curating, and cleaning.

01:14:31,618 --> 01:14:33,854
SPEAKER_0:  you can train your neural net.

01:14:34,338 --> 01:14:34,846
SPEAKER_0:  on top of that.

01:14:35,362 --> 01:14:37,534
SPEAKER_0:  So a lot of the work goes into cleaning those data sets now.

01:14:37,858 --> 01:14:39,134
SPEAKER_0:  As you pointed out, it's probably...

01:14:39,394 --> 01:14:40,030
SPEAKER_0:  It could be.

01:14:40,290 --> 01:14:41,662
SPEAKER_0:  The question is, how do you achieve?

01:14:41,986 --> 01:14:42,398
SPEAKER_0:  How do youdo?

01:14:42,626 --> 01:14:43,038
SPEAKER_0:  Uh.

01:14:43,522 --> 01:14:46,526
SPEAKER_0:  If you want to basically predict in 3D, you need data in 3D.

01:14:46,818 --> 01:14:47,390
SPEAKER_0:  to back that up.

01:14:48,322 --> 01:14:51,966
SPEAKER_0:  In this video, we have eight videos coming from all the cameras of the system.

01:14:52,546 --> 01:14:53,982
SPEAKER_0:  And this is what they saw.

01:14:54,242 --> 01:14:56,190
SPEAKER_0:  And this is the truth of what actually was around.

01:14:56,418 --> 01:14:58,046
SPEAKER_0:  There was this car, there was this car, this car.

01:14:58,370 --> 01:15:02,238
SPEAKER_0:  These are lane line markings. This is the geometry of the road. There's a traffic light in this three dimensional position.

01:15:02,690 --> 01:15:03,806
SPEAKER_0:  You need the ground truth.

01:15:04,130 --> 01:15:09,150
SPEAKER_0:  And so the big question that the team was solving, of course, is how do you, how do you arrive at that ground truth?

01:15:09,474 --> 01:15:12,542
SPEAKER_0:  because once you have a million of it, and it's large, clean, and diverse.

01:15:12,802 --> 01:15:15,934
SPEAKER_0:  then training a neural net on it works extremely well, and you can ship that into the car.

01:15:16,770 --> 01:15:17,438
SPEAKER_0:  And uh...

01:15:17,794 --> 01:15:20,446
SPEAKER_0:  So there was many mechanisms by which we collected that training data.

01:15:20,834 --> 01:15:22,366
SPEAKER_0:  You can always go for a human annotation.

01:15:22,690 --> 01:15:24,894
SPEAKER_0:  You can go for simulation as a source of ground truth.

01:15:25,282 --> 01:15:27,646
SPEAKER_0:  You can also go for what we call the offline tracker.

01:15:27,970 --> 01:15:28,478
SPEAKER_0:  Um.

01:15:29,314 --> 01:15:31,422
SPEAKER_0:  that we've spoken about at the AI day and so on.

01:15:31,650 --> 01:15:34,174
SPEAKER_0:  which is basically an automatic reconstruction process.

01:15:34,434 --> 01:15:37,502
SPEAKER_0:  for taking those videos and recovering the three-dimensional.

01:15:38,050 --> 01:15:39,230
SPEAKER_0:  sort of reality of.

01:15:39,490 --> 01:15:40,318
SPEAKER_0:  What was around that car?

01:15:40,802 --> 01:15:43,966
SPEAKER_0:  So basically think of doing like a three-dimensional reconstruction as an offline thing.

01:15:44,482 --> 01:15:46,334
SPEAKER_0:  and then understanding that, okay.

01:15:46,786 --> 01:15:49,054
SPEAKER_0:  There's 10 seconds of video. This is what we saw.

01:15:49,346 --> 01:15:51,582
SPEAKER_0:  And therefore, here's all the lane lines, cars, and so on.

01:15:52,450 --> 01:15:55,646
SPEAKER_0:  And then once you have that annotation, you can train your nuts to imitate it.

01:15:56,290 --> 01:15:59,166
SPEAKER_1:  and how difficult is the 3D reconstruction.

01:15:59,330 --> 01:15:59,742
SPEAKER_0:  It's difficult.

01:16:00,066 --> 01:16:01,191
SPEAKER_0:  But it can be done.

01:16:01,191 --> 01:16:05,278
SPEAKER_1:  So there's overlap between the cameras and you do the reconstruction and there's...

01:16:05,730 --> 01:16:06,174
SPEAKER_1:  Uh...

01:16:07,170 --> 01:16:10,878
SPEAKER_1:  Perhaps if there's any inaccuracy, so that's caught in the annotation step.

01:16:11,842 --> 01:16:16,638
SPEAKER_0:  Ah yes, the nice thing about the annotation is that it is fully offline, you have infinite time.

01:16:16,866 --> 01:16:21,534
SPEAKER_0:  You have a chunk of one minute and you're trying to just offline in a supercomputer somewhere figure out

01:16:21,858 --> 01:16:23,934
SPEAKER_0:  Where were the positions of all the cars, of all the people?

01:16:24,194 --> 01:16:26,590
SPEAKER_0:  and you have your full one minute video from all the angles.

01:16:26,914 --> 01:16:30,974
SPEAKER_0:  And you can run all the neural nets you want, and they can be very efficient, massive neural nets.

01:16:31,426 --> 01:16:34,142
SPEAKER_0:  There can be neural mass that can't even run in the car later at test time.

01:16:34,754 --> 01:16:37,342
SPEAKER_0:  so they can be even more powerful neural nets than what you can eventually deploy.

01:16:37,858 --> 01:16:44,190
SPEAKER_0:  So you can do anything you want, three-dimensional reconstruction, neural nets, anything you want just to recover that truth, and then you supervise that truth.

01:16:45,250 --> 01:16:48,606
SPEAKER_1:  What have you learned? You said no mistakes about humans.

01:16:49,538 --> 01:16:50,558
SPEAKER_1:  doing annotation.

01:16:50,914 --> 01:16:52,702
SPEAKER_1:  because I assume humans are.

01:16:53,602 --> 01:16:55,902
SPEAKER_1:  There's like a range of things they're good at.

01:16:56,194 --> 01:16:58,398
SPEAKER_1:  in terms of clicking stuff on screen is not.

01:16:59,074 --> 01:17:01,886
SPEAKER_1:  How interesting is that to you of a problem of designing?

01:17:02,722 --> 01:17:05,822
SPEAKER_1:  an annotator where humans are accurate, enjoy it.

01:17:06,082 --> 01:17:09,470
SPEAKER_1:  Like what are the even the metrics are efficient or productive all that kind of stuff.

01:17:09,922 --> 01:17:13,854
SPEAKER_0:  Yeah, so I grew the annotation team at Tesla from basically zero to a thousand.

01:17:14,370 --> 01:17:15,710
SPEAKER_0:  while I was there.

01:17:16,162 --> 01:17:19,102
SPEAKER_0:  That was really interesting. You know, my background is a-

01:17:19,554 --> 01:17:22,846
SPEAKER_0:  PhD student researcher, so growing that kind of organization was pretty crazy.

01:17:23,202 --> 01:17:23,774
SPEAKER_0:  Uh...

01:17:24,194 --> 01:17:25,118
SPEAKER_0:  But uh...

01:17:25,378 --> 01:17:33,790
SPEAKER_0:  Yeah, I think it's extremely interesting and part of the design process very much behind the autopilot as to where you use humans. Humans are very good at certain kinds of amortizations.

01:17:34,050 --> 01:17:38,334
SPEAKER_0:  They're very good, for example, at two-dimensional annotations of images. They're not good at annotating.

01:17:39,042 --> 01:17:41,662
SPEAKER_0:  over time in three-dimensional space. Very, very hard.

01:17:42,242 --> 01:17:46,142
SPEAKER_0:  And so that's why we were very careful to design the tasks that are easy to do for humans.

01:17:46,530 --> 01:17:48,574
SPEAKER_0:  versus things that should be left to the offline tracker.

01:17:48,994 --> 01:17:55,390
SPEAKER_0:  Like maybe the computer will do all the triangulation and 3D reconstruction, but the human will say exactly these pixels of the image, our car.

01:17:56,066 --> 01:17:57,246
SPEAKER_0:  exactly these pixels are human.

01:17:57,794 --> 01:18:01,982
SPEAKER_0:  And so co-designing the data annotation pipeline was very much.

01:18:02,562 --> 01:18:04,606
SPEAKER_0:  Bread and butter was what I was doing daily.

01:18:04,706 --> 01:18:07,198
SPEAKER_1:  Do you think there's still a lot of open problems in that space?

01:18:08,482 --> 01:18:10,718
SPEAKER_1:  Um, just in general annotation.

01:18:11,234 --> 01:18:15,326
SPEAKER_1:  where the stuff the machines are good at, machines do, and the humans...

01:18:15,746 --> 01:18:19,006
SPEAKER_1:  do what they're good at, and there's maybe some iterative process. That music maybe worth watching.

01:18:19,682 --> 01:18:25,022
SPEAKER_0:  I think to a very large extent, we went through a number of iterations and we learned a ton about how to create these datasets.

01:18:25,570 --> 01:18:25,950
SPEAKER_0:  Um,

01:18:26,306 --> 01:18:28,094
SPEAKER_0:  I'm not seeing big open problems like.

01:18:28,354 --> 01:18:29,502
SPEAKER_0:  Originally when I joined I was like

01:18:30,114 --> 01:18:30,686
SPEAKER_0:  I was really

01:18:31,138 --> 01:18:35,454
SPEAKER_0:  not sure how this would turn out. but by the time I left I was much more secure in.

01:18:35,938 --> 01:18:40,926
SPEAKER_0:  Actually, we sort of understand the philosophy of how to create these datasets, and I was pretty comfortable with where that was at the time.

01:18:41,602 --> 01:18:42,782
SPEAKER_1:  So what are...

01:18:43,074 --> 01:18:46,974
SPEAKER_1:  strengths and limitations of cameras for the driving task.

01:18:47,266 --> 01:18:52,190
SPEAKER_1:  in your understanding when you formulate the driving task as a vision task with eight cameras.

01:18:53,186 --> 01:18:58,718
SPEAKER_1:  You've seen that the entire, you know, most of the history of the computer vision field has to do with you on the orcs. What?

01:18:59,042 --> 01:19:02,142
SPEAKER_1:  Just a few step back, what are the strengths and limitations of

01:19:02,498 --> 01:19:03,166
SPEAKER_1:  pixels.

01:19:03,650 --> 01:19:05,150
SPEAKER_1:  of using pixels to drive.

01:19:05,730 --> 01:19:08,574
SPEAKER_0:  Yeah, pixels I think are a beautiful sensory.

01:19:08,866 --> 01:19:12,190
SPEAKER_0:  Beautiful sensor, I would say. The thing is, cameras are very, very cheap.

01:19:12,450 --> 01:19:16,414
SPEAKER_0:  and they provide a ton of information, ton of bits.

01:19:16,898 --> 01:19:21,502
SPEAKER_0:  extremely cheap sensor for a ton of bits, and each one of these bits is a constraint on the state of the world.

01:19:21,858 --> 01:19:24,798
SPEAKER_0:  And so you get lots of megapixel images.

01:19:25,026 --> 01:19:25,854
SPEAKER_0:  very cheap.

01:19:26,146 --> 01:19:29,502
SPEAKER_0:  And it just gives you all these constraints for understanding what's actually out there in the world.

01:19:30,018 --> 01:19:32,798
SPEAKER_0:  So vision is probably the highest bandwidth.

01:19:33,090 --> 01:19:33,694
SPEAKER_0:  sensor.

01:19:34,434 --> 01:19:35,870
SPEAKER_0:  It's a very high bandwidth sensor.

01:19:36,194 --> 01:19:36,958
SPEAKER_0:  And um...

01:19:37,666 --> 01:19:39,742
SPEAKER_1:  I love that pixels.

01:19:40,226 --> 01:19:43,390
SPEAKER_1:  is a constraint on the world.

01:19:43,810 --> 01:19:45,598
SPEAKER_1:  This is highly complex.

01:19:46,562 --> 01:19:49,937
SPEAKER_1:  of high bandwidth constraint on the state of the world.

01:19:49,937 --> 01:19:53,534
SPEAKER_0:  It's not just that, but again, this real importance of...

01:19:54,242 --> 01:19:55,710
SPEAKER_0:  It's the sensor that humans use.

01:19:56,098 --> 01:19:57,918
SPEAKER_0:  Therefore, everything is designed.

01:19:58,146 --> 01:19:58,814
SPEAKER_0:  for that sensor.

01:20:00,002 --> 01:20:01,438
SPEAKER_0:  the text, the writing, the...

01:20:01,762 --> 01:20:04,830
SPEAKER_0:  flashing signs, everything is designed for vision. And so-

01:20:05,538 --> 01:20:09,278
SPEAKER_0:  and you just find it everywhere. And so that's why that is the interface you want to be in.

01:20:09,602 --> 01:20:11,902
SPEAKER_0:  Talking again about these universal interfaces.

01:20:12,386 --> 01:20:16,158
SPEAKER_0:  And that's where we actually want to measure the world as well and then develop software.

01:20:16,642 --> 01:20:17,598
SPEAKER_0:  for that sensor.

01:20:18,050 --> 01:20:20,830
SPEAKER_1:  But there's other constraints on the state of the world.

01:20:21,602 --> 01:20:23,870
SPEAKER_1:  that humans use to understand the world.

01:20:24,098 --> 01:20:24,990
SPEAKER_1:  I mean, vision.

01:20:25,282 --> 01:20:26,270
SPEAKER_1:  ultimately is.

01:20:26,594 --> 01:20:27,678
SPEAKER_1:  The main one.

01:20:28,066 --> 01:20:29,630
SPEAKER_1:  but we're like, we're like.

01:20:29,986 --> 01:20:33,054
SPEAKER_1:  referencing our understanding of human behavior and some.

01:20:33,282 --> 01:20:34,142
SPEAKER_1:  common sense.

01:20:34,722 --> 01:20:39,710
SPEAKER_1:  physics that could be inferred from vision from from a perception perspective, but

01:20:40,258 --> 01:20:42,942
SPEAKER_1:  It feels like we're using some kind of reasoning.

01:20:43,938 --> 01:20:46,563
SPEAKER_1:  to predict the world, not just the-

01:20:46,563 --> 01:20:48,382
SPEAKER_0:  pixels. I mean you have a powerful prior.

01:20:48,738 --> 01:20:52,062
SPEAKER_0:  us rights for how the world evolves over time extra

01:20:52,354 --> 01:20:54,110
SPEAKER_0:  So it's not just about the likelihood term.

01:20:54,434 --> 01:20:56,126
SPEAKER_0:  coming up from the data itself.

01:20:56,418 --> 01:20:58,974
SPEAKER_0:  telling you about what you are observing, but also the prior term.

01:20:59,234 --> 01:21:02,782
SPEAKER_0:  of like where are the likely things to see and how do they likely move and so on.

01:21:03,266 --> 01:21:04,254
SPEAKER_1:  And the question is how.

01:21:04,770 --> 01:21:06,782
SPEAKER_1:  complex is the

01:21:07,650 --> 01:21:08,094
SPEAKER_1:  The-

01:21:08,642 --> 01:21:12,286
SPEAKER_1:  range of possibilities that might happen in the driving task.

01:21:12,962 --> 01:21:18,398
SPEAKER_1:  That's still, is that to you still an open problem of how difficult is driving like philosophically speaking?

01:21:20,322 --> 01:21:23,550
SPEAKER_1:  All the time you work on driving.

01:21:23,874 --> 01:21:26,110
SPEAKER_1:  Do you understand how hard driving is?

01:21:26,370 --> 01:21:32,862
SPEAKER_0:  Yeah, driving us really hard. Because it has to do with the predictions of all these other agents and the theory of mind and what they're going to do.

01:21:33,474 --> 01:21:36,254
SPEAKER_0:  Are they looking at you? Are they looking? Are they thinking?

01:21:36,418 --> 01:21:36,798
SPEAKER_1:  Yeah.

01:21:36,898 --> 01:21:39,422
SPEAKER_0:  There's a lot that goes there at the full tail.

01:21:39,842 --> 01:21:40,926
SPEAKER_0:  of the-

01:21:41,314 --> 01:21:43,742
SPEAKER_0:  expansion of the noise that we have to be comfortable with it eventually.

01:21:44,514 --> 01:21:48,414
SPEAKER_0:  final problems are of that form. I don't think those are the problems that are very common.

01:21:48,642 --> 01:21:49,310
SPEAKER_0:  I think.

01:21:49,538 --> 01:21:51,934
SPEAKER_0:  Eventually they're important, but it's really in the tail end.

01:21:52,130 --> 01:21:54,686
SPEAKER_1:  in the tail end, the rare edge cases.

01:21:55,618 --> 01:21:56,958
SPEAKER_1:  from the vision perspective.

01:21:57,442 --> 01:22:00,286
SPEAKER_1:  What are the toughest parts of the vision problem of driving?

01:22:01,890 --> 01:22:02,494
SPEAKER_1:  Um.

01:22:03,362 --> 01:22:04,158
SPEAKER_0:  Well basically...

01:22:04,514 --> 01:22:08,286
SPEAKER_0:  The sensor is extremely powerful, but you still need to process that information.

01:22:08,802 --> 01:22:09,214
SPEAKER_0:  Um.

01:22:09,570 --> 01:22:10,622
SPEAKER_0:  And so going from.

01:22:10,914 --> 01:22:15,390
SPEAKER_0:  brightnesses of these pixel values to hey, here the three-dimensional world is extremely hard.

01:22:15,682 --> 01:22:17,662
SPEAKER_0:  And that's what the neural networks are fundamentally doing.

01:22:18,242 --> 01:22:19,614
SPEAKER_0:  And so.

01:22:19,938 --> 01:22:24,798
SPEAKER_0:  The difficulty really is in just doing an extremely good job of engineering the entire pipeline.

01:22:25,410 --> 01:22:26,750
SPEAKER_0:  the entire data engine.

01:22:27,234 --> 01:22:27,966
SPEAKER_0:  having the

01:22:28,194 --> 01:22:29,502
SPEAKER_0:  to train these neural nuts.

01:22:29,826 --> 01:22:31,646
SPEAKER_0:  having the ability to evaluate the system.

01:22:31,906 --> 01:22:32,734
SPEAKER_0:  and iterate on it.

01:22:33,474 --> 01:22:38,270
SPEAKER_0:  So I would say just doing this in production at scale is like the hard part, it's an execution problem.

01:22:38,562 --> 01:22:40,670
SPEAKER_1:  So the data engine, but also the...

01:22:41,634 --> 01:22:41,950
SPEAKER_1:  Uh...

01:22:42,690 --> 01:22:44,510
SPEAKER_1:  the sort of deployment.

01:22:44,834 --> 01:22:48,209
SPEAKER_1:  of the system such that it has low latency performance. So it has to do all the work.

01:22:48,209 --> 01:22:56,382
SPEAKER_0:  these steps. Yeah, for the neural net specifically, just making sure everything fits into the chip on the car. And you have a finite budget of flops that you can perform.

01:22:56,674 --> 01:22:57,374
SPEAKER_0:  And uh...

01:22:57,698 --> 01:23:03,390
SPEAKER_0:  and memory bandwidth and other constraints, and you have to make sure it flies, and you can squeeze in as much computer as you can into the tiny.

01:23:03,938 --> 01:23:07,742
SPEAKER_1:  What have you learned from that process? Is that maybe that's one of the bigger like.

01:23:08,450 --> 01:23:11,070
SPEAKER_1:  new things coming from a research background.

01:23:11,746 --> 01:23:15,678
SPEAKER_1:  where there's a system that has to run under heavily constrained resources. Alright.

01:23:15,938 --> 01:23:17,054
SPEAKER_1:  has to run really fast.

01:23:17,346 --> 01:23:20,158
SPEAKER_1:  What kind of insights have you learned from that?

01:23:20,930 --> 01:23:22,654
SPEAKER_0:  Yeah, I'm not sure if it's... if there's...

01:23:22,978 --> 01:23:26,430
SPEAKER_0:  too many insights. You're trying to create a neural net that will fit.

01:23:26,882 --> 01:23:32,254
SPEAKER_0:  in what you have available, and you're always trying to optimize it. And we talked a lot about it on the AI day.

01:23:32,610 --> 01:23:33,918
SPEAKER_0:  Basically the

01:23:34,210 --> 01:23:38,750
SPEAKER_0:  the triple backflips that the team is doing to make sure it all fits and utilizes the engine.

01:23:39,042 --> 01:23:41,150
SPEAKER_0:  So I think it's extremely good engineering.

01:23:41,538 --> 01:23:46,014
SPEAKER_0:  And then there's all kinds of little insights peppered in on how to do it properly.

01:23:46,754 --> 01:23:50,974
SPEAKER_1:  Let's actually zoom out because I don't think we talked about the data engine, the entirety of the.

01:23:51,490 --> 01:23:52,382
SPEAKER_1:  layouts.

01:23:52,930 --> 01:23:56,350
SPEAKER_1:  of this idea that I think is just beautiful with humans in the loop.

01:23:57,314 --> 01:23:58,814
SPEAKER_1:  Can you describe the data engine?

01:23:59,522 --> 01:24:01,662
SPEAKER_0:  Yeah, the data engine is what I call the.

01:24:02,146 --> 01:24:05,521
SPEAKER_0:  almost biological feeling like process by which.

01:24:05,521 --> 01:24:06,271
SPEAKER_1:  Yeah.

01:24:06,271 --> 01:24:08,702
SPEAKER_0:  you perfect the training sets for these neural networks.

01:24:09,634 --> 01:24:15,134
SPEAKER_0:  So because most of the programming now is in the level of these data sets and make sure they're large diverse and clean

01:24:15,554 --> 01:24:16,542
SPEAKER_0:  Basically.

01:24:16,898 --> 01:24:17,310
SPEAKER_0:  You have.

01:24:17,538 --> 01:24:18,782
SPEAKER_0:  a data set that you think is good.

01:24:19,330 --> 01:24:20,222
SPEAKER_0:  You train your neural net.

01:24:20,770 --> 01:24:21,342
SPEAKER_0:  you deploy it.

01:24:21,634 --> 01:24:23,710
SPEAKER_0:  and then you observe how well it's performing.

01:24:24,034 --> 01:24:27,294
SPEAKER_0:  and you're trying to always increase the quality of your dataset.

01:24:27,618 --> 01:24:31,326
SPEAKER_0:  So you're trying to catch scenarios basically that are basically rare.

01:24:32,002 --> 01:24:38,302
SPEAKER_0:  And it is in these scenarios that general nuts will typically struggle in because they weren't told what to do in those rare cases in the data set.

01:24:38,786 --> 01:24:42,142
SPEAKER_0:  but now you can close the loop because if you can now collect all those at scale.

01:24:42,658 --> 01:24:45,854
SPEAKER_0:  you can then feed them back into the reconstruction process I described.

01:24:46,338 --> 01:24:49,758
SPEAKER_0:  and reconstruct the truth in those cases and add it to the dataset.

01:24:50,050 --> 01:24:53,054
SPEAKER_0:  And so the whole thing ends up being like a staircase of improvement.

01:24:53,410 --> 01:24:55,198
SPEAKER_0:  of perfecting your training set.

01:24:55,650 --> 01:24:57,982
SPEAKER_0:  and you have to go through deployments so that you can mine.

01:24:58,434 --> 01:25:01,662
SPEAKER_0:  the parts that are not yet represented well in the dataset.

01:25:02,562 --> 01:25:05,502
SPEAKER_0:  So your data set is basically imperfect. It needs to be diverse. It has pockets.

01:25:05,794 --> 01:25:06,750
SPEAKER_0:  there are missing.

01:25:07,010 --> 01:25:09,502
SPEAKER_0:  and you need to pad out the pockets. You can sort of think of it that way.

01:25:10,530 --> 01:25:11,038
SPEAKER_0:  in the data.

01:25:11,586 --> 01:25:15,710
SPEAKER_1:  What role do humans play in this? So what's this biological system?

01:25:15,970 --> 01:25:18,270
SPEAKER_1:  like are human bodies made up of cells?

01:25:18,850 --> 01:25:19,838
SPEAKER_1:  What role?

01:25:20,482 --> 01:25:22,078
SPEAKER_1:  Like how do you optimize the human?

01:25:22,658 --> 01:25:25,854
SPEAKER_1:  system, the multiple engineers collaborating.

01:25:26,178 --> 01:25:27,166
SPEAKER_1:  figuring out.

01:25:27,778 --> 01:25:28,926
SPEAKER_1:  what to focus on.

01:25:29,314 --> 01:25:30,270
SPEAKER_1:  or to contribute.

01:25:30,498 --> 01:25:34,238
SPEAKER_1:  which task to optimize in this neural network.

01:25:34,562 --> 01:25:38,654
SPEAKER_1:  uh, who is in charge of figuring out which task needs more data.

01:25:39,746 --> 01:25:42,526
SPEAKER_1:  Can you speak to the hyperparameters of the human?

01:25:43,106 --> 01:25:44,094
SPEAKER_1:  system.

01:25:44,514 --> 01:25:48,126
SPEAKER_0:  It really just comes down to extremely good execution from an engineering team who knows what they're doing.

01:25:48,386 --> 01:25:51,294
SPEAKER_0:  They understand intuitively the philosophical insights underlying

01:25:51,586 --> 01:25:53,726
SPEAKER_0:  the data engine and the process by which the system improves.

01:25:54,274 --> 01:25:56,830
SPEAKER_0:  and how to, again, delegate.

01:25:57,090 --> 01:25:59,390
SPEAKER_0:  the strategy of the data collection and how that works.

01:25:59,714 --> 01:26:01,822
SPEAKER_0:  and then just making sure it's all extremely well executed.

01:26:02,082 --> 01:26:10,142
SPEAKER_0:  And that's where most of the work is, is not even the philosophizing or the research or the ideas of it, it's just extremely good execution. So hard when you're dealing with data at that scale.

01:26:10,818 --> 01:26:17,854
SPEAKER_1:  So your role in the data engine executing well on it is difficult and extremely important. Is there a priority of like?

01:26:18,466 --> 01:26:19,038
SPEAKER_1:  Uh...

01:26:19,490 --> 01:26:21,470
SPEAKER_1:  like a vision board of saying like.

01:26:22,370 --> 01:26:24,414
SPEAKER_1:  We really need to get better at stop lights.

01:26:25,282 --> 01:26:29,790
SPEAKER_1:  Like the prioritization of tasks, is that essentially, and that comes from the data?

01:26:30,594 --> 01:26:35,806
SPEAKER_0:  that comes to a very large extent to what we are trying to achieve in the product for a map. Thank you.

01:26:36,034 --> 01:26:37,150
SPEAKER_0:  the release we're trying to get out.

01:26:37,474 --> 01:26:37,854
SPEAKER_0:  Uh.

01:26:38,274 --> 01:26:42,453
SPEAKER_0:  in the feedback from the QA team where the system is struggling or not, the things we're trying to...

01:26:42,453 --> 01:26:44,990
SPEAKER_1:  And the QA team gives some signal.

01:26:45,474 --> 01:26:46,622
SPEAKER_1:  some information.

01:26:47,266 --> 01:26:49,891
SPEAKER_1:  in aggregate about the performance of the system.

01:26:49,891 --> 01:26:53,918
SPEAKER_0:  various conditions. And then of course all of us drive it and we can also see it. It's really nice to.

01:26:54,274 --> 01:26:58,174
SPEAKER_0:  work with a system that you can also experience yourself. You know, it drives you home.

01:26:58,626 --> 01:26:59,358
SPEAKER_1:  Is there some?

01:26:59,586 --> 01:27:05,150
SPEAKER_1:  insight you can draw from your individual experience that you just can't quite get from an aggregate statistical.

01:27:05,570 --> 01:27:08,126
SPEAKER_1:  analysis of data. Yeah, it's so weird, right?

01:27:08,258 --> 01:27:08,574
SPEAKER_0:  Yes.

01:27:08,962 --> 01:27:15,006
SPEAKER_1:  It's not scientific in a sense, because you're just one anecdotal sample. Yeah, I think there's a ton of-

01:27:15,106 --> 01:27:15,614
SPEAKER_0:  Uh.

01:27:15,906 --> 01:27:20,958
SPEAKER_0:  It's a source of truth. It's your interaction with the system. And you can see it, you can play with it, you can...

01:27:21,570 --> 01:27:24,222
SPEAKER_0:  you can get a sense of it, you have an intuition for it.

01:27:24,674 --> 01:27:26,526
SPEAKER_0:  I think numbers just like have a way of.

01:27:26,818 --> 01:27:28,894
SPEAKER_0:  Numbers and plots and graphs are much harder.

01:27:29,314 --> 01:27:29,790
SPEAKER_0:  uh...

01:27:30,146 --> 01:27:32,510
SPEAKER_1:  It hides a lot of it's like if you.

01:27:32,770 --> 01:27:34,014
SPEAKER_1:  train and language model.

01:27:35,394 --> 01:27:40,574
SPEAKER_1:  It's a really powerful way is by you interacting with it. Yeah, 100%. It builds up an intuition.

01:27:40,706 --> 01:27:46,814
SPEAKER_0:  Yeah, I think like, Ilan also, like, he always wanted to drive the system himself. He drives a lot and, uh...

01:27:47,234 --> 01:27:49,598
SPEAKER_0:  I'm gonna say almost daily. So ah

01:27:49,826 --> 01:27:53,438
SPEAKER_0:  He also sees this as a source of truth. You driving the system.

01:27:53,794 --> 01:27:55,198
SPEAKER_1:  and it performing and.

01:27:56,258 --> 01:27:57,310
SPEAKER_1:  So what do you think?

01:27:57,762 --> 01:27:58,686
SPEAKER_1:  Tough questions here.

01:27:59,362 --> 01:28:02,974
SPEAKER_1:  Uh, so Tesla last year removed radar from, um,

01:28:03,810 --> 01:28:08,926
SPEAKER_1:  from the sensor suite and now just announce that it's going to remove ultrasonic sensors.

01:28:09,250 --> 01:28:11,806
SPEAKER_1:  relying solely on vision, so camera only.

01:28:13,314 --> 01:28:16,126
SPEAKER_1:  Does that make the perception problem harder or easier?

01:28:18,082 --> 01:28:21,150
SPEAKER_0:  I would almost reframe the question in some way. So the thing is basically...

01:28:21,410 --> 01:28:23,989
SPEAKER_0:  You would think that additional sensors...

01:28:23,989 --> 01:28:24,830
SPEAKER_1:  Can't just interrupt.

01:28:25,570 --> 01:28:31,870
SPEAKER_1:  I wonder if a language model will ever do that if you prompt it. Let me reframe your question. That'd be epic.

01:28:32,418 --> 01:28:33,543
SPEAKER_1:  That's the wrong prom.

01:28:33,543 --> 01:28:36,798
SPEAKER_0:  Sorry. It's like a little bit of a wrong question because...

01:28:37,090 --> 01:28:40,190
SPEAKER_0:  Basically you would think that these sensors are an asset to you.

01:28:41,026 --> 01:28:42,334
SPEAKER_0:  But if you fully consider...

01:28:42,658 --> 01:28:44,254
SPEAKER_0:  the entire product in its entirety.

01:28:45,154 --> 01:28:46,878
SPEAKER_0:  these sensors are actually potentially a liability.

01:28:47,522 --> 01:28:48,286
SPEAKER_0:  because

01:28:48,578 --> 01:28:51,038
SPEAKER_0:  The sensors aren't free, they don't just appear on your car.

01:28:51,298 --> 01:28:51,710
SPEAKER_0:  you need.

01:28:52,194 --> 01:28:54,974
SPEAKER_0:  suddenly you have an entire supply chain, you have people procuring it.

01:28:55,266 --> 01:28:57,470
SPEAKER_0:  there can be problems with them. They may need replacement.

01:28:57,826 --> 01:29:00,862
SPEAKER_0:  they are part of the manufacturing process, they can hold back the line in production.

01:29:01,474 --> 01:29:05,246
SPEAKER_0:  You need to source them, you need to maintain them, you have to have teams that write the firmware.

01:29:05,698 --> 01:29:06,494
SPEAKER_0:  all of it.

01:29:06,722 --> 01:29:09,694
SPEAKER_0:  And then you also have to incorporate and fuse them into the system in some way.

01:29:10,050 --> 01:29:13,246
SPEAKER_0:  And so it actually like bloats a lot of it.

01:29:13,698 --> 01:29:15,390
SPEAKER_0:  And I think Elon is really good at.

01:29:16,002 --> 01:29:17,950
SPEAKER_0:  Simplify, simplify, best part is no part.

01:29:18,306 --> 01:29:23,358
SPEAKER_0:  And he always tries to throw away things that are not essential because he understands the entropy in organizations and in approach.

01:29:23,906 --> 01:29:25,374
SPEAKER_0:  And I think in this case.

01:29:26,082 --> 01:29:29,342
SPEAKER_0:  The cost is high and you're not potentially seeing it if you're just a computer vision engineer.

01:29:29,730 --> 01:29:32,350
SPEAKER_0:  and I'm just trying to improve my network and is it.

01:29:32,610 --> 01:29:34,878
SPEAKER_0:  more useful or less useful? How useful is it?

01:29:35,298 --> 01:29:38,174
SPEAKER_0:  And the thing is, once you consider the full cost of a sensor...

01:29:38,498 --> 01:29:39,870
SPEAKER_0:  it actually is potentially a liability.

01:29:40,194 --> 01:29:43,422
SPEAKER_0:  And you need to be really sure that it's giving you extremely useful information.

01:29:43,810 --> 01:29:47,774
SPEAKER_0:  In this case, we looked at using it or not using it, and the delta was not massive.

01:29:48,066 --> 01:29:49,214
SPEAKER_0:  And so it's not useful.

01:29:49,538 --> 01:29:52,254
SPEAKER_1:  Is it also bloat in the data engine?

01:29:52,642 --> 01:29:54,334
SPEAKER_1:  Like having more sensors.

01:29:54,818 --> 01:30:04,670
SPEAKER_0:  And these sensors, you know, they can change over time. For example, you can have one type of say radar, you can have other type of radar, they change over time. Now you suddenly need to worry about it. Now suddenly you have a column in your SQLite.

01:30:05,058 --> 01:30:06,590
SPEAKER_0:  telling you, oh, what sensor type was it?

01:30:06,914 --> 01:30:08,414
SPEAKER_0:  and they all have different distributions.

01:30:08,706 --> 01:30:13,310
SPEAKER_0:  and then they contribute noise and entropy into everything.

01:30:13,762 --> 01:30:14,878
SPEAKER_0:  and they bloat stuff.

01:30:15,266 --> 01:30:19,230
SPEAKER_0:  and also organizationally, it's been really fascinating to me that it can be very distracting.

01:30:19,650 --> 01:30:19,998
SPEAKER_0:  time.

01:30:20,610 --> 01:30:23,390
SPEAKER_0:  If you, if all, if you only want to get to work is vision.

01:30:23,842 --> 01:30:26,558
SPEAKER_0:  All the resources are on it, and you're building out a data engine.

01:30:27,170 --> 01:30:30,366
SPEAKER_0:  and you're actually making forward progress because that is the...

01:30:30,722 --> 01:30:33,278
SPEAKER_0:  sensor with the most bandwidth, the most constraints on the world.

01:30:33,634 --> 01:30:36,126
SPEAKER_0:  and you're investing fully into that and you can make that extremely good.

01:30:36,418 --> 01:30:39,230
SPEAKER_0:  if you're only a finite amount of sort of spend.

01:30:39,490 --> 01:30:42,302
SPEAKER_0:  of focus across different facets of the system.

01:30:42,882 --> 01:30:43,710
SPEAKER_1:  And uh...

01:30:43,938 --> 01:30:44,830
SPEAKER_1:  this kind of...

01:30:45,474 --> 01:30:47,774
SPEAKER_1:  reminds me of Rich Sutton's The Bitter Lesson.

01:30:48,290 --> 01:30:50,110
SPEAKER_1:  This just seems like simplifying the system.

01:30:51,234 --> 01:30:52,350
SPEAKER_1:  in the long run.

01:30:52,674 --> 01:30:56,414
SPEAKER_1:  Of course, you know, no longer seems to be always the right solution.

01:30:56,706 --> 01:31:06,110
SPEAKER_1:  Yes. In that case, it was for RL, but it seems to apply generally across all systems that do computation. Yeah. So what do you think about the LIDAR as a crutch debate?

01:31:07,010 --> 01:31:08,606
SPEAKER_1:  the battle between

01:31:09,058 --> 01:31:10,334
SPEAKER_1:  point clouds and pixels.

01:31:11,202 --> 01:31:14,206
SPEAKER_0:  Yeah, I think this debate is always slightly confusing to me because...

01:31:14,466 --> 01:31:16,510
SPEAKER_0:  It seems like the actual debate should be about like...

01:31:16,834 --> 01:31:17,886
SPEAKER_0:  Do you have the fleet or not?

01:31:18,178 --> 01:31:20,574
SPEAKER_0:  That's like the really important thing about whether you can achieve.

01:31:20,994 --> 01:31:22,718
SPEAKER_0:  really good functioning of an AI system.

01:31:23,234 --> 01:31:23,774
SPEAKER_0:  at the scale.

01:31:24,034 --> 01:31:25,909
SPEAKER_1:  So data collection systems. Yeah.

01:31:25,909 --> 01:31:30,718
SPEAKER_0:  do you have a fleet or not is significantly more important whether you have LiDAR or not. It's just another sensor.

01:31:31,234 --> 01:31:31,614
SPEAKER_0:  Um.

01:31:32,130 --> 01:31:32,862
SPEAKER_0:  And uh...

01:31:33,634 --> 01:31:37,246
SPEAKER_0:  Yeah, I think similar to the radar discussion, basically, I am

01:31:38,786 --> 01:31:41,406
SPEAKER_0:  I don't think it basically doesn't offer.

01:31:42,082 --> 01:31:52,542
SPEAKER_0:  extra information, it's extremely costly. It has all kinds of problems. You have to worry about it. You have to calibrate it, et cetera. It creates bloat and entropy. You have to be really sure that you need this sensor.

01:31:52,994 --> 01:31:54,782
SPEAKER_0:  In this case, I basically don't think you need it.

01:31:55,010 --> 01:31:55,582
SPEAKER_0:  And I think...

01:31:55,938 --> 01:32:01,502
SPEAKER_0:  Honestly, I will make a stronger statement. I think the others, some of the other companies that are using it are probably going to drop it.

01:32:02,178 --> 01:32:02,878
SPEAKER_1:  Yeah, so.

01:32:03,170 --> 01:32:05,150
SPEAKER_1:  you have to consider the sensor.

01:32:05,442 --> 01:32:06,430
SPEAKER_1:  in the full.

01:32:07,490 --> 01:32:11,838
SPEAKER_1:  in considering can you build a big fleet that collects a lot of data?

01:32:12,322 --> 01:32:19,966
SPEAKER_1:  And can you integrate that sensor with it, that data and that sensor into a data engine that's able to quickly find different parts of the data?

01:32:20,226 --> 01:32:20,734
SPEAKER_1:  that then.

01:32:21,154 --> 01:32:23,966
SPEAKER_1:  continuously improves whatever the model that you're using.

01:32:24,226 --> 01:32:25,630
SPEAKER_0:  Yeah, another way to look at it is like...

01:32:25,890 --> 01:32:29,662
SPEAKER_0:  Vision is necessary in the sense that the drive.

01:32:29,890 --> 01:32:32,094
SPEAKER_0:  The world is designed for human visual consumption, so you need vision.

01:32:32,770 --> 01:32:33,310
SPEAKER_0:  necessary.

01:32:33,762 --> 01:32:35,326
SPEAKER_0:  and then also it is sufficient.

01:32:35,746 --> 01:32:40,574
SPEAKER_0:  because it has all the information that you need for driving and humans obviously have this vision to drive.

01:32:40,898 --> 01:32:46,366
SPEAKER_0:  So it's both necessary and sufficient. So you want to focus resources. And you have to be really sure if you're going to bring in other sensors.

01:32:46,722 --> 01:32:50,622
SPEAKER_0:  You could add sensors to infinity. At some point you need to draw the line.

01:32:51,074 --> 01:32:55,294
SPEAKER_0:  And I think in this case, you have to really consider the total cost of any one sensor.

01:32:55,746 --> 01:32:56,510
SPEAKER_0:  that you're adopting.

01:32:56,834 --> 01:32:58,110
SPEAKER_0:  And do you really need it?

01:32:58,530 --> 01:32:58,942
SPEAKER_0:  End.

01:32:59,234 --> 01:33:00,414
SPEAKER_0:  I think the answer in this case is no.

01:33:00,802 --> 01:33:04,126
SPEAKER_1:  So what do you think about the idea that the other companies...

01:33:05,026 --> 01:33:11,134
SPEAKER_1:  are forming high resolution maps and constraining heavily the geographic regions in which they operate.

01:33:11,682 --> 01:33:12,190
SPEAKER_1:  is that.

01:33:12,578 --> 01:33:15,390
SPEAKER_1:  approach not in your view.

01:33:16,258 --> 01:33:20,478
SPEAKER_1:  I'm not going to scale over time to the entirety of the United States.

01:33:20,674 --> 01:33:25,502
SPEAKER_0:  I think, as you mentioned, like they pre-map all the environments and they need to refresh the map.

01:33:25,890 --> 01:33:29,918
SPEAKER_0:  and they have a perfect centimeter level accuracy map of everywhere they're gonna drive.

01:33:30,178 --> 01:33:31,294
SPEAKER_0:  crazy. How are you going to?

01:33:32,066 --> 01:33:35,838
SPEAKER_0:  We're talking about the autonomy actually changing the world. We're talking about the deployment.

01:33:36,610 --> 01:33:37,950
SPEAKER_0:  on a global scale.

01:33:38,242 --> 01:33:39,774
SPEAKER_0:  of autonomous systems for transportation.

01:33:40,386 --> 01:33:43,390
SPEAKER_0:  And if you need to maintain a centimeter accurate map for Earth.

01:33:43,714 --> 01:33:49,118
SPEAKER_0:  Or like for many cities and keep them updated. It's a huge dependency that you're taking on. Huge dependency.

01:33:49,922 --> 01:33:53,342
SPEAKER_0:  It's a massive, massive dependency. And now you need to ask yourself, do you really need it?

01:33:54,402 --> 01:33:55,550
SPEAKER_0:  and humans don't need it.

01:33:56,002 --> 01:33:56,446
SPEAKER_0:  Um.

01:33:57,346 --> 01:33:58,910
SPEAKER_0:  So it's very useful to have a.

01:33:59,202 --> 01:34:02,974
SPEAKER_0:  low level map of like, okay, the connectivity of your road, you know, that there's a fork coming up.

01:34:03,330 --> 01:34:07,038
SPEAKER_0:  when you drive an environment, you sort of have that high level understanding. It's like a small Google map.

01:34:07,426 --> 01:34:10,238
SPEAKER_0:  And Tesla uses Google Map, similar kind of.

01:34:10,562 --> 01:34:11,998
SPEAKER_0:  resolution information.

01:34:12,258 --> 01:34:12,798
SPEAKER_0:  in the system.

01:34:13,058 --> 01:34:16,094
SPEAKER_0:  but it will not pre-map environments to send me a level of accuracy.

01:34:16,418 --> 01:34:18,526
SPEAKER_0:  It's a crutch, it's a distraction, it costs entropy.

01:34:18,914 --> 01:34:23,902
SPEAKER_0:  and it diffuses the team, it dilutes the team, and you're not focusing on what's actually necessary, which is MOMORING TO GET AMAZED BY IT!!

01:34:24,450 --> 01:34:25,214
SPEAKER_0:  computer version problem.

01:34:26,402 --> 01:34:28,798
SPEAKER_1:  What did you learn about machine learning?

01:34:29,442 --> 01:34:33,310
SPEAKER_1:  about engineering, about life, about yourself as one human being.

01:34:34,018 --> 01:34:35,710
SPEAKER_1:  from working with Elon Musk.

01:34:36,578 --> 01:34:40,766
SPEAKER_0:  I think the most I've learned is about how to sort of run organizations efficiently.

01:34:40,994 --> 01:34:41,662
SPEAKER_0:  and how to.

01:34:42,370 --> 01:34:45,726
SPEAKER_0:  create efficient organizations and how to fight entropy in an organization.

01:34:46,306 --> 01:34:47,390
SPEAKER_0:  So human engineering.

01:34:47,810 --> 01:34:49,470
SPEAKER_1:  in the fight against entropy. Yeah.

01:34:50,018 --> 01:34:55,998
SPEAKER_0:  There's a, I think Elon is a very efficient warrior in the fight against entropy in organizations.

01:34:56,226 --> 01:34:59,294
SPEAKER_1:  What does entropy in an organization look like? It's it's

01:34:59,394 --> 01:35:00,766
SPEAKER_0:  process it's.

01:35:01,698 --> 01:35:02,078
SPEAKER_0:  It's.

01:35:02,338 --> 01:35:10,078
SPEAKER_0:  process and inefficiencies in the form of meetings and that kind of stuff. Yeah, meetings. He hates meetings. He doesn't want people to skip meetings if they're not useful.

01:35:10,402 --> 01:35:10,718
SPEAKER_0:  Um.

01:35:10,946 --> 01:35:13,598
SPEAKER_0:  He basically runs the world's biggest startups.

01:35:13,922 --> 01:35:14,398
SPEAKER_0:  I would say.

01:35:14,722 --> 01:35:15,038
SPEAKER_0:  Oh.

01:35:15,298 --> 01:35:19,422
SPEAKER_0:  Tesla SpaceX are the world's biggest startups. Tesla actually is multiple startups.

01:35:19,650 --> 01:35:20,990
SPEAKER_0:  I think it's better to look at it that way.

01:35:21,410 --> 01:35:24,190
SPEAKER_0:  And so I think he's extremely good at...

01:35:24,450 --> 01:35:24,958
SPEAKER_0:  at that.

01:35:25,314 --> 01:35:32,798
SPEAKER_0:  And yeah, it's a very good intuition for streamlining processes, making everything efficient. Best part is no part, simplifying, focusing.

01:35:33,154 --> 01:35:33,534
SPEAKER_0:  Um.

01:35:34,082 --> 01:35:37,598
SPEAKER_0:  and just kind of removing barriers, moving very quickly, making big moves.

01:35:38,018 --> 01:35:39,262
SPEAKER_0:  All this is very startup-y.

01:35:39,650 --> 01:35:41,086
SPEAKER_0:  sort of seeming things, but at scale.

01:35:41,602 --> 01:35:43,646
SPEAKER_1:  So strong drive to simplify.

01:35:43,906 --> 01:35:45,630
SPEAKER_1:  From your perspective, I mean that.

01:35:46,210 --> 01:35:46,718
SPEAKER_1:  Um...

01:35:46,978 --> 01:35:52,606
SPEAKER_1:  That also probably applies to just designing systems and machine learning and otherwise, like simplify, simplify. Yes.

01:35:53,538 --> 01:35:57,150
SPEAKER_1:  What do you think is the secret to maintaining the startup culture?

01:35:57,474 --> 01:35:58,814
SPEAKER_1:  in a company that grows.

01:35:59,170 --> 01:35:59,582
SPEAKER_1:  Is there?

01:36:00,578 --> 01:36:02,238
SPEAKER_1:  Can you introspect that?

01:36:03,842 --> 01:36:08,574
SPEAKER_0:  I do think he needs someone in a powerful position with a big hammer like Elon who's like...

01:36:08,834 --> 01:36:12,414
SPEAKER_0:  the cheerleader for that idea and ruthlessly pursues it.

01:36:12,770 --> 01:36:14,206
SPEAKER_0:  If no one has a big enough hammer.

01:36:14,818 --> 01:36:16,798
SPEAKER_0:  everything turns into committees.

01:36:17,186 --> 01:36:18,814
SPEAKER_0:  democracy within the company.

01:36:19,106 --> 01:36:22,878
SPEAKER_0:  process, talking to stakeholders, decision-making, just everything just...

01:36:23,106 --> 01:36:28,958
SPEAKER_0:  If you have a big person who is also really smart and has a big hammer, things move quickly.

01:36:29,378 --> 01:36:29,726
SPEAKER_0:  You

01:36:30,018 --> 01:36:32,350
SPEAKER_1:  So you said your favorite scene in Interstellar.

01:36:32,738 --> 01:36:37,854
SPEAKER_1:  is the intense docking scene with the AI and Cooper talking saying, uh, Cooper, what are you doing?

01:36:38,370 --> 01:36:41,758
SPEAKER_1:  Docking, it's not possible. No, it's necessary.

01:36:42,626 --> 01:36:47,070
SPEAKER_1:  Such a good line. By the way, just so many questions there. Why an AI?

01:36:48,130 --> 01:36:49,086
SPEAKER_1:  in that scene.

01:36:49,794 --> 01:36:51,806
SPEAKER_1:  presumably is supposed to be.

01:36:52,610 --> 01:36:54,398
SPEAKER_1:  able to compute a lot more than the human.

01:36:55,234 --> 01:36:58,462
SPEAKER_1:  It's saying it's not optimal why the human I mean, that's a movie but

01:36:58,818 --> 01:37:01,470
SPEAKER_1:  Shouldn't the AI know much better than the human?

01:37:02,210 --> 01:37:06,590
SPEAKER_1:  Anyway, what do you think is the value of setting seemingly impossible goals?

01:37:07,618 --> 01:37:09,054
SPEAKER_1:  So like, uh...

01:37:09,794 --> 01:37:12,990
SPEAKER_1:  our initial intuition, which seems like something that.

01:37:13,730 --> 01:37:16,446
SPEAKER_1:  You have taken on that Elon espouses

01:37:17,378 --> 01:37:21,246
SPEAKER_1:  that where the initial intuition of the community might say this is very difficult.

01:37:21,826 --> 01:37:24,478
SPEAKER_1:  and then you take it on anyway with a crazy deadline.

01:37:24,898 --> 01:37:27,326
SPEAKER_1:  You just from a human engineer perspective.

01:37:28,226 --> 01:37:28,766
SPEAKER_1:  out

01:37:29,154 --> 01:37:30,494
SPEAKER_1:  Have you seen the value of that?

01:37:32,418 --> 01:37:37,918
SPEAKER_0:  I wouldn't say that setting impossible goals exactly is a good idea, but I think setting very ambitious goals is a good idea.

01:37:38,370 --> 01:37:39,326
SPEAKER_0:  I think there's a...

01:37:39,810 --> 01:37:41,854
SPEAKER_0:  what I call sub-linear scaling of difficulty.

01:37:42,114 --> 01:37:47,838
SPEAKER_0:  which means that 10x problems are not 10x hard. Usually 10x problem is...

01:37:48,514 --> 01:37:49,374
SPEAKER_0:  2 or 3x.

01:37:49,602 --> 01:37:50,686
SPEAKER_0:  harder to execute on.

01:37:51,106 --> 01:37:54,046
SPEAKER_0:  because if you wanna actually like, if you wanna improve the system by 10%,

01:37:54,466 --> 01:37:58,270
SPEAKER_0:  It costs some amount of work. And if you want to 10x improve the system, it doesn't cost.

01:37:58,786 --> 01:37:59,902
SPEAKER_0:  you know, 100x amount of the-

01:38:00,482 --> 01:38:04,158
SPEAKER_0:  and it's because you fundamentally change the approach. If you start with that constraint,

01:38:04,514 --> 01:38:09,534
SPEAKER_0:  then some approaches are obviously dumb and not going to work. And it forces you to reevaluate.

01:38:09,762 --> 01:38:10,750
SPEAKER_0:  And I think it's a very.

01:38:11,170 --> 01:38:12,030
SPEAKER_0:  Interesting way off.

01:38:12,418 --> 01:38:13,470
SPEAKER_0:  approaching problem solving.

01:38:13,890 --> 01:38:18,974
SPEAKER_1:  But it requires a weird kind of thinking. It's just going back to your like PhD days.

01:38:19,426 --> 01:38:19,902
SPEAKER_1:  Zike.

01:38:20,578 --> 01:38:25,150
SPEAKER_1:  How do you think which ideas in the machine learning community?

01:38:25,826 --> 01:38:26,782
SPEAKER_1:  are solvable.

01:38:27,458 --> 01:38:27,774
SPEAKER_1:  Yes.

01:38:28,066 --> 01:38:33,374
SPEAKER_1:  It's a, it requires, what is that? I mean, there's the cliche of first principles thinking, but like.

01:38:34,018 --> 01:38:39,646
SPEAKER_1:  It requires to basically ignore what the community is saying. Cause it doesn't the community doesn't a community in.

01:38:40,162 --> 01:38:42,174
SPEAKER_1:  science usually draw lines of

01:38:42,722 --> 01:38:44,350
SPEAKER_1:  what is and isn't possible. Right.

01:38:44,866 --> 01:38:48,350
SPEAKER_1:  And like, it's very hard to break out of that without going crazy.

01:38:49,314 --> 01:38:52,606
SPEAKER_0:  I mean, I think a good example here is, you know, the deep learning revolution in some sense.

01:38:52,898 --> 01:38:54,014
SPEAKER_0:  because you could.

01:38:54,434 --> 01:38:59,518
SPEAKER_0:  being computer vision at that time during the deep learning revolution of 2012 and so on.

01:39:00,386 --> 01:39:02,718
SPEAKER_0:  you could be improving a computer vision stack by 10%.

01:39:03,074 --> 01:39:04,126
SPEAKER_0:  or we can just be saying...

01:39:04,610 --> 01:39:10,590
SPEAKER_0:  Actually, all of this is useless. And how do I do 10x better computer vision? Well, it's not probably by tuning a hog feature detector.

01:39:11,138 --> 01:39:12,158
SPEAKER_0:  I need a different approach.

01:39:12,450 --> 01:39:13,886
SPEAKER_0:  I need something that is scalable.

01:39:14,146 --> 01:39:15,262
SPEAKER_0:  Going back to.

01:39:15,554 --> 01:39:16,894
SPEAKER_0:  Richard Sutton's.

01:39:17,250 --> 01:39:20,510
SPEAKER_0:  and understanding sort of like the philosophy of the bitter lesson.

01:39:21,058 --> 01:39:25,438
SPEAKER_0:  And then being like, actually, I need much more scalable system like a neural network that in principle works.

01:39:25,730 --> 01:39:27,710
SPEAKER_0:  and then having some deep believers that can actually.

01:39:28,034 --> 01:39:29,790
SPEAKER_0:  execute on that mission and make it work.

01:39:30,434 --> 01:39:31,518
SPEAKER_0:  That's the 10x solution.

01:39:32,354 --> 01:39:32,670
SPEAKER_1:  okay

01:39:34,082 --> 01:39:37,822
SPEAKER_1:  What do you think is the timeline to solve the problem of autonomous driving?

01:39:38,562 --> 01:39:41,406
SPEAKER_1:  that's still in part an open question.

01:39:42,594 --> 01:39:46,494
SPEAKER_0:  Yeah, I think the tough thing with timelines of self-driving, obviously, is that no one

01:39:46,754 --> 01:39:47,678
SPEAKER_0:  has created soft driving.

01:39:48,834 --> 01:39:55,134
SPEAKER_0:  So it's not like, what do you think is the timeline to build this bridge? Well, we've built million bridges before. Here's how long that takes.

01:39:55,906 --> 01:39:56,766
SPEAKER_0:  You know, it's a...

01:39:57,186 --> 01:39:59,358
SPEAKER_0:  No one has built autonomy. It's not obvious.

01:39:59,746 --> 01:40:00,990
SPEAKER_0:  some parts.

01:40:01,282 --> 01:40:06,398
SPEAKER_0:  turn out to be much easier than others. So it's really hard to forecast. You do your best based on trend lines and so on.

01:40:06,818 --> 01:40:07,998
SPEAKER_0:  and based on intuition, but...

01:40:08,322 --> 01:40:10,654
SPEAKER_0:  That's why fundamentally it's just really hard to forecast this.

01:40:10,882 --> 01:40:13,598
SPEAKER_1:  no one is even still like being inside of it is hard.

01:40:13,826 --> 01:40:14,951
SPEAKER_1:  to do.

01:40:14,951 --> 01:40:18,046
SPEAKER_0:  Yes, some things turn out to be much harder and some things turn out to be much easier.

01:40:19,042 --> 01:40:19,710
SPEAKER_1:  Do you?

01:40:20,002 --> 01:40:23,870
SPEAKER_1:  try to avoid making forecasts, because Elon doesn't avoid them, right?

01:40:24,194 --> 01:40:27,934
SPEAKER_1:  and heads of car companies in the past have not avoided it either.

01:40:28,546 --> 01:40:38,878
SPEAKER_1:  Ford and other places have made predictions that we're going to solve at level four driving bike 2020, 2021, whatever. Now they're all kind of backtracking that prediction.

01:40:39,746 --> 01:40:41,182
SPEAKER_1:  Are you as a...

01:40:42,466 --> 01:40:43,998
SPEAKER_1:  as an AI person.

01:40:45,058 --> 01:40:51,102
SPEAKER_1:  Do you for yourself privately make predictions or do they get in the way of your actual...

01:40:51,650 --> 01:40:53,022
SPEAKER_1:  ability to think about a thing.

01:40:53,922 --> 01:40:54,686
SPEAKER_0:  Yeah, I would say like.

01:40:55,074 --> 01:40:57,182
SPEAKER_0:  What's easy to say is that this problem is tractable.

01:40:57,602 --> 01:41:04,670
SPEAKER_0:  And that's an easy prediction to make. It's tractable. It's going to work. Yes, it's just really hard. Some things turn out to be harder, some things turn out to be easier.

01:41:04,962 --> 01:41:05,502
SPEAKER_0:  Also...

01:41:06,082 --> 01:41:07,902
SPEAKER_0:  but it definitely feels tractable.

01:41:08,162 --> 01:41:09,022
SPEAKER_0:  And it feels like.

01:41:09,538 --> 01:41:12,926
SPEAKER_0:  At least the team at Tesla, which is what I saw internally, is definitely on track to that.

01:41:13,314 --> 01:41:13,950
SPEAKER_1:  Are you?

01:41:14,274 --> 01:41:14,718
SPEAKER_1:  form.

01:41:15,490 --> 01:41:20,254
SPEAKER_1:  a strong representation that allows you to make a prediction about tractability.

01:41:20,674 --> 01:41:23,550
SPEAKER_1:  So like you're the leader of a lot of humans.

01:41:24,770 --> 01:41:27,550
SPEAKER_1:  you have to kind of say this is actually possible.

01:41:28,546 --> 01:41:30,782
SPEAKER_1:  Like, how do you build up that intuition?

01:41:31,010 --> 01:41:34,622
SPEAKER_1:  It doesn't have to be even driving. It could be other tasks. It could be.

01:41:35,266 --> 01:41:39,294
SPEAKER_1:  I want to what difficult task did you work on your life? I mean classification

01:41:39,650 --> 01:41:43,166
SPEAKER_1:  achieving certain, just an image net, certain level.

01:41:43,490 --> 01:41:45,278
SPEAKER_1:  of superhuman level performance.

01:41:45,826 --> 01:41:47,166
SPEAKER_0:  Yeah, expert intuition.

01:41:47,906 --> 01:41:49,214
SPEAKER_0:  It's just intuition. It's just intuition.

01:41:49,602 --> 01:41:49,918
SPEAKER_1:  Remove the gum.

01:41:50,946 --> 01:41:55,678
SPEAKER_1:  So just like thinking about it long enough, like studying, looking at sample data, like you said, driving.

01:41:56,514 --> 01:41:58,462
SPEAKER_1:  My intuition is really flawed on this.

01:41:59,042 --> 01:42:03,966
SPEAKER_1:  Like I don't have a good intuition about tractability. It could be either, it could be anything. It could be.

01:42:04,258 --> 01:42:05,214
SPEAKER_1:  Solvable.

01:42:05,890 --> 01:42:06,782
SPEAKER_1:  like a

01:42:07,586 --> 01:42:08,990
SPEAKER_1:  you know the driving task could.

01:42:09,250 --> 01:42:09,790
SPEAKER_1:  could be.

01:42:10,146 --> 01:42:13,630
SPEAKER_1:  simplified into something quite trivial like a

01:42:13,890 --> 01:42:15,870
SPEAKER_1:  the solution to the problem would be quite trivial.

01:42:16,258 --> 01:42:19,518
SPEAKER_1:  and at scale more and more cars driving perfectly.

01:42:20,418 --> 01:42:21,918
SPEAKER_1:  might make the problem much easier.

01:42:22,242 --> 01:42:26,206
SPEAKER_1:  The more cars you have driving, people learn how to drive correctly.

01:42:26,690 --> 01:42:27,646
SPEAKER_1:  Not correctly, but...

01:42:27,874 --> 01:42:31,934
SPEAKER_1:  in a way that's more optimal for a heterogeneous system.

01:42:32,226 --> 01:42:33,182
SPEAKER_1:  of autonomous and.

01:42:33,570 --> 01:42:37,790
SPEAKER_1:  semi-autonomous and manually driven cars that could change stuff. Then again.

01:42:38,082 --> 01:42:38,878
SPEAKER_1:  Also I've spent

01:42:39,170 --> 01:42:42,942
SPEAKER_1:  ridiculous number of hours just staring at pedestrians crossing streets.

01:42:43,202 --> 01:42:44,990
SPEAKER_1:  Thinking about humans.

01:42:45,218 --> 01:42:46,462
SPEAKER_1:  and it feels like...

01:42:46,914 --> 01:42:48,062
SPEAKER_1:  The way we use our...

01:42:48,578 --> 01:42:49,342
SPEAKER_1:  eye contact.

01:42:50,370 --> 01:42:50,846
SPEAKER_1:  It's.

01:42:51,362 --> 01:42:55,390
SPEAKER_1:  It sends really strong signals and there's certain quirks and edge cases of behavior.

01:42:55,650 --> 01:42:58,398
SPEAKER_1:  And of course, a lot of the fatalities that happen have to do with...

01:42:58,914 --> 01:43:00,926
SPEAKER_1:  drunk driving and uh...

01:43:01,442 --> 01:43:04,606
SPEAKER_1:  both on the pedestrian side and the driver side. There's that problem.

01:43:05,026 --> 01:43:09,086
SPEAKER_1:  of driving at night and all that kind of. So I wonder, you know, it's like the space.

01:43:10,370 --> 01:43:14,878
SPEAKER_1:  of possible solutions to autonomous driving includes so many human factor issues.

01:43:15,778 --> 01:43:19,262
SPEAKER_1:  that it's almost impossible to predict. That could be super clean.

01:43:19,682 --> 01:43:20,807
SPEAKER_1:  Nice solutions.

01:43:20,807 --> 01:43:24,574
SPEAKER_0:  Yeah, I would say definitely like to use a game analogy. There's some fog of war

01:43:25,026 --> 01:43:27,998
SPEAKER_0:  but you definitely also see the frontier of improvement.

01:43:28,290 --> 01:43:30,942
SPEAKER_0:  And you can measure historically how much you've made progress.

01:43:31,362 --> 01:43:33,534
SPEAKER_0:  And I think, for example, at least what I've seen in...

01:43:33,826 --> 01:43:37,278
SPEAKER_0:  roughly five years at Tesla. When I joined, it barely kept lane.

01:43:37,666 --> 01:43:38,686
SPEAKER_0:  on the highway.

01:43:39,074 --> 01:43:46,526
SPEAKER_0:  I think going up from Palo Alto to SF was like three or four interventions. Anytime the road would do anything geometrically or turn too much, it would just not work.

01:43:47,138 --> 01:43:52,030
SPEAKER_0:  And so going from that to like a pretty competent system in five years and seeing what happens also under the hood.

01:43:52,386 --> 01:43:56,638
SPEAKER_0:  and what the scale of which the team is operating now with respect to data and compute and everything else.

01:43:56,994 --> 01:43:58,910
SPEAKER_0:  It's just a massive progress.

01:43:59,554 --> 01:43:59,998
SPEAKER_0:  So.

01:44:00,322 --> 01:44:02,462
SPEAKER_1:  This is a, you're climbing a mountain.

01:44:02,882 --> 01:44:04,757
SPEAKER_1:  and fog, but you're making a lot of

01:44:04,757 --> 01:44:07,710
SPEAKER_0:  progress fog you're making progress and you see what the next directions are

01:44:07,970 --> 01:44:10,942
SPEAKER_0:  and you're looking at some of the remaining challenges and they're not like a

01:44:11,554 --> 01:44:14,558
SPEAKER_0:  They're not perturbing you and they're not changing your philosophy and you're not contra-

01:44:14,850 --> 01:44:16,286
SPEAKER_0:  contorting yourself. You're like...

01:44:16,610 --> 01:44:18,485
SPEAKER_0:  Actually these are the things that we still need to do.

01:44:18,485 --> 01:44:23,102
SPEAKER_1:  fundamental components of solving the problem seem to be there. From the data engine to the compute to the...

01:44:23,522 --> 01:44:26,270
SPEAKER_1:  the computer on the car to the computer for the training, all that kind of stuff.

01:44:27,330 --> 01:44:27,998
SPEAKER_1:  See you done.

01:44:28,898 --> 01:44:34,558
SPEAKER_1:  Over the years you've been a test you've done a lot of amazing breakthrough ideas in engineering

01:44:35,074 --> 01:44:35,518
SPEAKER_1:  all of it.

01:44:36,130 --> 01:44:36,734
SPEAKER_1:  Um...

01:44:36,962 --> 01:44:39,742
SPEAKER_1:  from the data engine to the human side, all of it.

01:44:40,258 --> 01:44:42,494
SPEAKER_1:  Can you speak to why you chose to leave?

01:44:42,754 --> 01:44:43,166
SPEAKER_1:  Tesla.

01:44:43,970 --> 01:44:45,502
SPEAKER_0:  Basically, as I described that ran.

01:44:46,082 --> 01:44:48,670
SPEAKER_0:  I think over time during those five years, I've kind of...

01:44:49,122 --> 01:44:50,302
SPEAKER_0:  Got myself into...

01:44:50,626 --> 01:44:51,998
SPEAKER_0:  A little bit of a managerial position.

01:44:52,322 --> 01:44:56,734
SPEAKER_0:  Most of my days were meetings and growing the organization and making decisions.

01:44:56,962 --> 01:44:58,654
SPEAKER_0:  about a sort of high level strategic.

01:44:58,946 --> 01:44:59,646
SPEAKER_0:  decisions about.

01:45:00,002 --> 01:45:01,854
SPEAKER_0:  the team and what it should be working on and so on.

01:45:02,434 --> 01:45:03,166
SPEAKER_0:  and uh...

01:45:04,130 --> 01:45:05,854
SPEAKER_0:  It's kind of like a corporate executive role.

01:45:06,114 --> 01:45:08,254
SPEAKER_0:  and I can do it. I think I'm okay at it.

01:45:08,578 --> 01:45:10,110
SPEAKER_0:  But it's not like fundamentally what I...

01:45:10,338 --> 01:45:11,646
SPEAKER_0:  and I enjoy.

01:45:11,938 --> 01:45:13,822
SPEAKER_0:  I think when I joined...

01:45:14,050 --> 01:45:21,438
SPEAKER_0:  there was no computer vision team because Tesla was just going from the transition of using Mobly, a third party vendor for all of its computer vision, to having to build its computer vision system.

01:45:21,890 --> 01:45:29,086
SPEAKER_0:  So when I showed up, there were two people training deep neural networks. And they were training them at a computer at their legs.

01:45:29,442 --> 01:45:31,317
SPEAKER_0:  down there was a workstation.

01:45:31,317 --> 01:45:32,067
SPEAKER_1:  classification time.

01:45:32,067 --> 01:45:33,278
SPEAKER_0:  Yeah, and so...

01:45:34,594 --> 01:45:42,590
SPEAKER_0:  I kind of like grew that into what I think is a fairly respectable deep learning team, massive compute cluster, a very good data annotation organization.

01:45:42,914 --> 01:45:43,678
SPEAKER_0:  and uh...

01:45:43,970 --> 01:45:46,430
SPEAKER_0:  I was very happy with where that was, it became quite autonomous.

01:45:46,658 --> 01:45:51,678
SPEAKER_0:  And so I kind of stepped away and I, you know, I'm very excited to do much more technical things again.

01:45:52,354 --> 01:45:54,494
SPEAKER_0:  Yeah, and kind of like we focus on AGI.

01:45:54,690 --> 01:45:55,358
SPEAKER_1:  What was this?

01:45:55,650 --> 01:46:05,502
SPEAKER_1:  soul searching like, cause he took a little time off and think like what, how many mushrooms did you take? No, I'm just kidding. I mean, what was going through your mind? The human lifetime is finite.

01:46:06,146 --> 01:46:08,606
SPEAKER_1:  Yeah. You did a few incredible things here.

01:46:08,994 --> 01:46:11,166
SPEAKER_1:  You're one of the best teachers of AI in the world.

01:46:11,554 --> 01:46:12,894
SPEAKER_1:  You're one of the best.

01:46:13,474 --> 01:46:16,798
SPEAKER_1:  And I don't mean that, I mean that in the best possible way. You're one of the best.

01:46:17,346 --> 01:46:18,174
SPEAKER_1:  Tinkerers.

01:46:18,402 --> 01:46:19,422
SPEAKER_1:  in the AI world.

01:46:19,778 --> 01:46:20,542
SPEAKER_1:  Meaning like...

01:46:20,834 --> 01:46:22,174
SPEAKER_1:  understanding the fundamentals.

01:46:22,786 --> 01:46:26,174
SPEAKER_1:  fundamentals of how something works by building it from scratch.

01:46:26,498 --> 01:46:28,990
SPEAKER_1:  and playing with the basic intuitions. It's like.

01:46:29,410 --> 01:46:34,046
SPEAKER_1:  Einstein, Feynman, we're all really good at this kind of stuff. Like small example of a thing to.

01:46:34,594 --> 01:46:36,478
SPEAKER_1:  play with it to try to understand it.

01:46:36,898 --> 01:46:39,934
SPEAKER_1:  Also that and obviously now with us you help build.

01:46:40,450 --> 01:46:42,270
SPEAKER_1:  a team of machine learning.

01:46:42,946 --> 01:46:43,518
SPEAKER_1:  Um...

01:46:44,002 --> 01:46:50,814
SPEAKER_1:  Like engineers and assistant that actually accomplishes something in the real world. So given all that like what was the soul searching like?

01:46:51,778 --> 01:46:54,558
SPEAKER_0:  Well, it was hard because obviously I love the company a lot.

01:46:54,818 --> 01:46:55,358
SPEAKER_0:  and out live.

01:46:55,746 --> 01:46:57,886
SPEAKER_0:  I love Elon. I love Tesla. I want to...

01:46:59,426 --> 01:47:01,950
SPEAKER_0:  It was so hard to leave, I love the team basically.

01:47:02,530 --> 01:47:03,166
SPEAKER_0:  but

01:47:04,130 --> 01:47:04,670
SPEAKER_0:  Yeah, I think.

01:47:04,898 --> 01:47:07,710
SPEAKER_0:  Actually, I would be potentially interested in revisiting it.

01:47:07,970 --> 01:47:09,086
SPEAKER_0:  Are you coming back at some point?

01:47:09,410 --> 01:47:09,854
SPEAKER_0:  Ow.

01:47:10,146 --> 01:47:11,070
SPEAKER_0:  Working in Optimus.

01:47:11,394 --> 01:47:12,574
SPEAKER_0:  We're kind of an AGI at Tesla.

01:47:12,994 --> 01:47:15,614
SPEAKER_0:  I think Tesla is going to do incredible things.

01:47:15,874 --> 01:47:16,926
SPEAKER_0:  It's basically like...

01:47:18,050 --> 01:47:18,526
SPEAKER_0:  Um.

01:47:19,202 --> 01:47:21,310
SPEAKER_0:  It's a massive large-scale robotics.

01:47:21,794 --> 01:47:26,398
SPEAKER_0:  of company with a ton of in-house talent for doing really incredible things. I think –

01:47:27,394 --> 01:47:28,830
SPEAKER_0:  human robots are going to be amazing.

01:47:29,250 --> 01:47:33,854
SPEAKER_0:  I think autonomous transportation is going to be amazing. All this is happening at Tesla. So I think it's just a.

01:47:34,274 --> 01:47:37,854
SPEAKER_0:  really amazing organization. So being part of it and helping it along, I think was very...

01:47:38,274 --> 01:47:39,262
SPEAKER_0:  Basically I enjoyed that a lot.

01:47:39,938 --> 01:47:42,462
SPEAKER_0:  Yeah, it was basically difficult for those reasons because I love the company.

01:47:42,818 --> 01:47:46,494
SPEAKER_0:  But I'm happy to potentially at some point come back for Act 2.

01:47:46,818 --> 01:47:47,998
SPEAKER_0:  but I felt like at this stage.

01:47:48,738 --> 01:47:50,654
SPEAKER_0:  I built the team, it felt autonomous.

01:47:51,010 --> 01:47:56,702
SPEAKER_0:  and I became a manager and I wanted to do a lot more technical stuff. I wanted to learn stuff. I wanted to teach stuff.

01:47:57,058 --> 01:47:58,142
SPEAKER_0:  Uh, and uh,

01:47:58,626 --> 01:48:01,639
SPEAKER_0:  I just kind of felt like it was a good time for a change of pace a little bit.

01:48:01,639 --> 01:48:02,846
SPEAKER_1:  What do you think is?

01:48:03,234 --> 01:48:06,558
SPEAKER_1:  uh... the best movie sequel of all time speaking of part two

01:48:07,170 --> 01:48:08,766
SPEAKER_1:  Cause like, cause most of them suck.

01:48:08,994 --> 01:48:12,510
SPEAKER_1:  Movie sequels. Movie sequels, yeah, and you tweeted about movies, so.

01:48:12,930 --> 01:48:15,582
SPEAKER_1:  This is a tiny tangent.

01:48:15,874 --> 01:48:17,566
SPEAKER_1:  What's like a favorite movie sequel?

01:48:18,498 --> 01:48:19,518
SPEAKER_1:  Godfather Part 2.

01:48:20,066 --> 01:48:23,441
SPEAKER_1:  Are you a fan of Godfather? Because you didn't even tweet or mention Godfather.

01:48:23,441 --> 01:48:25,691
SPEAKER_0:  Yeah, I don't love that movie. I know it has an edit that

01:48:25,691 --> 01:48:28,734
SPEAKER_1:  out we're gonna edit out the hate towards the godfather

01:48:28,866 --> 01:48:31,870
SPEAKER_0:  How dare you disrespect- I will make a strong statement, I don't know why.

01:48:32,194 --> 01:48:36,606
SPEAKER_0:  I don't know why, but I basically don't like any movie before 1995.

01:48:37,570 --> 01:48:39,486
SPEAKER_0:  Something like that. Didn't you mention Terminator?

01:48:39,842 --> 01:48:41,374
SPEAKER_0:  Okay, okay. That's like a...

01:48:41,954 --> 01:48:43,070
SPEAKER_0:  Terminator 2 was...

01:48:43,298 --> 01:48:44,766
SPEAKER_0:  A little bit later, 1990.

01:48:45,634 --> 01:48:46,759
SPEAKER_1:  No, I think...

01:48:46,759 --> 01:48:55,198
SPEAKER_0:  was in the 80s. And I like Terminator 1 as well. So okay, so like few exceptions, but by and large, for some reason, I don't like movies before 1995 or something.

01:48:55,458 --> 01:48:56,638
SPEAKER_0:  They feel very slow.

01:48:56,962 --> 01:49:01,087
SPEAKER_0:  The camera is like zoomed out, it's boring, it's kind of naive, it's kind of weird.

01:49:01,087 --> 01:49:03,774
SPEAKER_1:  And also Terminator was very much ahead of its time.

01:49:03,970 --> 01:49:04,318
SPEAKER_0:  Yes.

01:49:04,546 --> 01:49:06,462
SPEAKER_0:  And the Godfather, there's like no AGI.

01:49:06,754 --> 01:49:07,198
SPEAKER_0:  So...

01:49:07,490 --> 01:49:08,158
SPEAKER_0:  Ha ha ha!

01:49:08,610 --> 01:49:13,566
SPEAKER_1:  I mean, but you have Good Will Hunting was one of the movies you mentioned.

01:49:13,954 --> 01:49:17,150
SPEAKER_1:  And that doesn't have any AGI either. I guess that's mathematics. Yeah.

01:49:17,250 --> 01:49:19,875
SPEAKER_0:  I guess occasionally I do enjoy movies that don't feature.

01:49:19,875 --> 01:49:22,875
SPEAKER_1:  or like Anchorman that has no that's.

01:49:22,875 --> 01:49:24,286
SPEAKER_0:  increment is so good.

01:49:24,674 --> 01:49:26,334
SPEAKER_1:  I don't understand.

01:49:26,850 --> 01:49:31,390
SPEAKER_1:  I'm speaking of AGI because I don't understand why Will Ferrell is so funny.

01:49:31,618 --> 01:49:34,878
SPEAKER_1:  It doesn't make sense. It doesn't compute. There's just something about him

01:49:35,362 --> 01:49:36,830
SPEAKER_1:  and he's a singular human.

01:49:37,122 --> 01:49:38,686
SPEAKER_1:  Because you don't get that many comedies.

01:49:39,458 --> 01:49:48,926
SPEAKER_1:  these days and I wonder if it has to do about the culture or the machine of Hollywood or does it have to do with just we got lucky with certain people in comedy that came together.

01:49:49,218 --> 01:49:50,430
SPEAKER_1:  because he is a singular human.

01:49:52,578 --> 01:50:05,822
SPEAKER_1:  That was a ridiculous tangent, I apologize. But you mentioned humanoid robots. So what do you think about Optimus, about Tesla bot? Do you think we'll have robots in the factory and in the home in 10, 20, 30, 40, 50 years?

01:50:06,818 --> 01:50:08,734
SPEAKER_0:  I think it's a very hard project, I think it's going to take a while.

01:50:09,026 --> 01:50:10,334
SPEAKER_0:  Who else is going to build?

01:50:10,594 --> 01:50:11,518
SPEAKER_0:  human or robotic scale.

01:50:11,938 --> 01:50:17,278
SPEAKER_0:  And I think it is a very good form factor to go after because like I mentioned the world is designed for human form factor.

01:50:17,794 --> 01:50:21,886
SPEAKER_0:  These things would be able to operate our machines. They would be able to sit down in chairs, a drawer.

01:50:22,178 --> 01:50:23,230
SPEAKER_0:  potentially even drive cars.

01:50:23,778 --> 01:50:28,766
SPEAKER_0:  Basically, the world is designed for humans. That's the form factor you want to invest into and make work over time.

01:50:29,442 --> 01:50:32,254
SPEAKER_0:  I think there's another school of thought which is, okay.

01:50:32,482 --> 01:50:34,142
SPEAKER_0:  pick a problem and design a robot to it.

01:50:34,370 --> 01:50:39,358
SPEAKER_0:  But actually designing a robot and getting a whole data engine and everything behind it to work is actually an incredibly hard problem.

01:50:39,874 --> 01:50:41,726
SPEAKER_0:  So it makes sense to go after general interfaces.

01:50:41,954 --> 01:50:46,270
SPEAKER_0:  that they are not perfect for any one given task, but they actually have the generality.

01:50:46,722 --> 01:50:49,214
SPEAKER_0:  of just with a prompt with English able to.

01:50:49,442 --> 01:50:50,174
SPEAKER_0:  do something.

01:50:50,498 --> 01:50:50,974
SPEAKER_0:  across.

01:50:51,362 --> 01:50:53,822
SPEAKER_0:  And so I think it makes a lot of sense to go after a general.

01:50:54,210 --> 01:50:55,358
SPEAKER_0:  interface.

01:50:55,810 --> 01:50:56,254
SPEAKER_0:  Um.

01:50:56,546 --> 01:50:57,310
SPEAKER_0:  in the physical.

01:50:57,666 --> 01:50:58,014
SPEAKER_0:  Right?

01:50:58,498 --> 01:51:00,862
SPEAKER_0:  and I think it's a very difficult project. Things are going to take time.

01:51:01,506 --> 01:51:03,006
SPEAKER_0:  But I see no other.

01:51:03,330 --> 01:51:06,334
SPEAKER_0:  no other company that can execute on that vision. I think it's going to be amazing. A movement that we're witnessing here.

01:51:06,818 --> 01:51:10,430
SPEAKER_0:  Basically physical labor, like if you think transportation is a large market.

01:51:10,786 --> 01:51:12,958
SPEAKER_0:  Try physical labor. Come on...

01:51:13,986 --> 01:51:25,246
SPEAKER_1:  But it's not just physical labor. To me, the thing that's also exciting is the social robotics. So the relationship we'll have on different levels with those robots. Yeah. That's why I was really excited.

01:51:25,474 --> 01:51:27,038
SPEAKER_1:  to see optimistic.

01:51:27,842 --> 01:51:29,054
SPEAKER_1:  People have criticized me.

01:51:29,410 --> 01:51:30,302
SPEAKER_1:  the excitement.

01:51:30,978 --> 01:51:32,414
SPEAKER_1:  but I've worked with...

01:51:32,642 --> 01:51:33,278
SPEAKER_1:  Uh...

01:51:33,634 --> 01:51:37,150
SPEAKER_1:  a lot of research labs that do humanoid-legged robots.

01:51:37,698 --> 01:51:42,046
SPEAKER_1:  Boston Dynamics, Unitree, there's a lot of companies that do legged robots.

01:51:43,650 --> 01:51:47,358
SPEAKER_1:  That's the elegance of the movement.

01:51:47,874 --> 01:51:49,566
SPEAKER_1:  is a tiny, tiny part.

01:51:50,210 --> 01:51:51,294
SPEAKER_1:  of the big picture.

01:51:51,714 --> 01:51:55,390
SPEAKER_1:  So integrating the two big exciting things to me about Tessa doing-

01:51:55,746 --> 01:51:57,694
SPEAKER_1:  humanoid or any legged robots.

01:51:58,370 --> 01:51:58,878
SPEAKER_1:  is

01:51:59,746 --> 01:52:02,238
SPEAKER_1:  clearly integrating into the data engine.

01:52:02,530 --> 01:52:02,846
SPEAKER_1:  Mm-hmm.

01:52:03,074 --> 01:52:03,518
SPEAKER_1:  So that.

01:52:03,778 --> 01:52:13,598
SPEAKER_1:  the data engine aspect, so the actual intelligence for the perception and the control and the planning and all that kind of stuff, integrating into the fleet that you mentioned, right?

01:52:14,018 --> 01:52:14,398
SPEAKER_1:  Um...

01:52:14,626 --> 01:52:15,198
SPEAKER_1:  and then...

01:52:15,906 --> 01:52:17,054
SPEAKER_1:  Speaking of flea

01:52:17,378 --> 01:52:20,062
SPEAKER_1:  The second thing is the mass manufacturers just knowing.

01:52:20,802 --> 01:52:21,150
SPEAKER_1:  Uh

01:52:21,506 --> 01:52:22,846
SPEAKER_1:  culturally

01:52:23,778 --> 01:52:25,086
SPEAKER_1:  driving.

01:52:25,410 --> 01:52:28,158
SPEAKER_1:  towards a simple robot that's cheap to produce.

01:52:28,386 --> 01:52:34,654
SPEAKER_1:  at scale and doing that well, having experience to do that well, that changes everything. That's why that's a very different culture.

01:52:35,138 --> 01:52:38,430
SPEAKER_1:  and style the Boston Dynamics, who by the way, those robots.

01:52:39,138 --> 01:52:41,182
SPEAKER_1:  are just the way they move.

01:52:41,538 --> 01:52:46,878
SPEAKER_1:  It's like it'll be a very long time before Tessa can achieve the smoothness of movement.

01:52:47,234 --> 01:52:48,478
SPEAKER_1:  But that's not what it's about.

01:52:49,090 --> 01:52:54,238
SPEAKER_1:  It's about the entirety of the system, like we talked about the data engine and the fleet.

01:52:54,754 --> 01:52:57,470
SPEAKER_1:  That's super exciting. Even the initial sort of models.

01:52:57,954 --> 01:53:00,030
SPEAKER_1:  But that too was really surprising.

01:53:00,610 --> 01:53:02,485
SPEAKER_1:  that in a few months you can get up.

01:53:02,485 --> 01:53:03,390
SPEAKER_0:  prototype.

01:53:04,066 --> 01:53:10,334
SPEAKER_0:  And the reason that happened very quickly is, as you alluded to, there's a ton of copy-paste from what's happening in the autopilot. A lot.

01:53:10,850 --> 01:53:15,742
SPEAKER_0:  The amount of expertise that came out of the woodworks at Tesla for building the human robot was incredible to see.

01:53:16,130 --> 01:53:16,478
SPEAKER_0:  like

01:53:17,122 --> 01:53:19,070
SPEAKER_0:  Basically, Elon said at one point we're doing this.

01:53:19,362 --> 01:53:19,870
SPEAKER_0:  and then.

01:53:20,450 --> 01:53:26,398
SPEAKER_0:  next day, basically, like all these CAD models started to appear and people talking about like the supply chain and manufacturing.

01:53:26,658 --> 01:53:34,334
SPEAKER_0:  And people showed up with like screwdrivers and everything like the other day and started to like put together the body and I was like whoa like all these people exist at Tesla.

01:53:34,658 --> 01:53:38,910
SPEAKER_0:  And fundamentally, building a car is actually not that different from building a robot. The same, and that is true.

01:53:39,330 --> 01:53:40,318
SPEAKER_0:  not just for

01:53:40,546 --> 01:53:42,110
SPEAKER_0:  the hardware pieces and also

01:53:42,498 --> 01:53:44,862
SPEAKER_0:  Let's not forget hardware, not just for demo, but...

01:53:45,346 --> 01:53:45,662
SPEAKER_0:  Um.

01:53:46,050 --> 01:53:46,846
SPEAKER_0:  Manufacturing.

01:53:47,394 --> 01:53:49,886
SPEAKER_0:  of that hardware at scale. It is like a whole different thing.

01:53:50,306 --> 01:53:51,486
SPEAKER_0:  but for software as well.

01:53:52,002 --> 01:53:54,142
SPEAKER_0:  Basically this robot currently thinks it's a car.

01:53:54,562 --> 01:53:55,774
SPEAKER_0:  David e sequoia

01:53:56,002 --> 01:53:59,377
SPEAKER_1:  It's going to have a midlife crisis at some point.

01:53:59,377 --> 01:54:00,350
SPEAKER_0:  It thinks it's a car.

01:54:00,578 --> 01:54:10,398
SPEAKER_0:  Some of the earlier demos, actually, we were talking about potentially doing them outside in the parking lot, because that's where all of the computer vision was working out of the box instead of inside.

01:54:11,138 --> 01:54:23,262
SPEAKER_0:  But all the operating system, everything just copy pastes. Computer vision mostly copy pastes. I mean, you have to retrain the neural nets, but the approach and everything and data engine and offline trackers and the way we go about the occupancy tracker and so on, everything copy pastes.

01:54:23,618 --> 01:54:24,830
SPEAKER_0:  You just need to retrain it in your own nuts.

01:54:25,538 --> 01:54:28,222
SPEAKER_0:  And then the planning control, of course, has to change quite a bit.

01:54:28,706 --> 01:54:32,286
SPEAKER_0:  but there's a ton of copy paste from what's happening at Tesla. And so if you were to

01:54:32,930 --> 01:54:36,798
SPEAKER_0:  If you were to go with goal of like, okay, let's build a million human robots and you're not Tesla.

01:54:37,026 --> 01:54:39,390
SPEAKER_0:  That's a lot to ask. If you're Tesla.

01:54:39,650 --> 01:54:40,574
SPEAKER_0:  It's actually like.

01:54:41,282 --> 01:54:42,654
SPEAKER_0:  It's not that crazy.

01:54:42,850 --> 01:54:48,190
SPEAKER_1:  And then the follow up question is on how difficult, just like we're driving, how difficult is the manipulation task?

01:54:48,738 --> 01:54:51,710
SPEAKER_1:  such that it can have an impact that scale I think

01:54:52,450 --> 01:54:55,166
SPEAKER_1:  depending on the context, the really nice thing about.

01:54:55,810 --> 01:54:59,966
SPEAKER_1:  Robotics is the, unless you do a manufacturer and that kind of stuff.

01:55:00,194 --> 01:55:01,758
SPEAKER_1:  is there is more room for error.

01:55:02,530 --> 01:55:08,222
SPEAKER_1:  Driving is so safety critical and so that, and also time critical. I got a robot is allowed to move slower.

01:55:09,250 --> 01:55:10,270
SPEAKER_1:  Which is nice. You

01:55:11,074 --> 01:55:17,918
SPEAKER_0:  I think it's going to take a long time, but the way you want to structure the development is you need to say, okay, it's going to take a long time. How can I set up the...

01:55:18,146 --> 01:55:18,558
SPEAKER_0:  a

01:55:19,042 --> 01:55:31,454
SPEAKER_0:  product development roadmap so that I'm making revenue along the way. I'm not setting myself up for a zero one loss function where it doesn't work until it works. You don't want to be in that position. You want to make it useful almost immediately and then you want to slowly deploy it.

01:55:31,842 --> 01:55:34,078
SPEAKER_0:  and at scale.

01:55:34,530 --> 01:55:39,998
SPEAKER_0:  and you want to set up your data engine, your improvement loops, the telemetry, the evaluation, the harness and everything.

01:55:40,482 --> 01:55:45,662
SPEAKER_0:  and you want to improve the product over time incrementally and you're making revenue along the way. That's extremely important.

01:55:46,082 --> 01:55:54,814
SPEAKER_0:  because otherwise you cannot build these large undertakings just like don't make sense economically. And also from the point of view of the team working on it, they need the dopamine along the way.

01:55:55,170 --> 01:55:56,190
SPEAKER_0:  They're not just going to.

01:55:56,418 --> 01:55:58,302
SPEAKER_0:  make a promise about this being useful.

01:55:58,722 --> 01:56:00,798
SPEAKER_0:  This is going to change the world in 10 years when it works.

01:56:01,122 --> 01:56:07,422
SPEAKER_0:  This is not where you want to be. You want to be in a place like, I think, autopaldis today where it's offering increased safety.

01:56:07,906 --> 01:56:12,350
SPEAKER_0:  and convenience of driving today. People pay for it, people like it, people purchase it.

01:56:12,834 --> 01:56:15,390
SPEAKER_0:  And then you also have the greater mission that you're working towards.

01:56:16,354 --> 01:56:18,494
SPEAKER_1:  And you see that so the dope for the team.

01:56:19,074 --> 01:56:20,949
SPEAKER_1:  that that was a source of happiness.

01:56:20,949 --> 01:56:24,574
SPEAKER_0:  100% you're deploying this people like it people drive it people pay for it

01:56:24,834 --> 01:56:31,518
SPEAKER_0:  They care about it. There's all these YouTube videos. Your grandma drives it. She gives you feedback. People like it. People engage with it. They're going to engage with it.

01:56:31,650 --> 01:56:32,062
SPEAKER_1:  Huge.

01:56:32,322 --> 01:56:36,446
SPEAKER_1:  Do people that drive Teslas recognize you and give you love?

01:56:36,674 --> 01:56:38,462
SPEAKER_1:  Like, hey, thanks for the.

01:56:39,266 --> 01:56:40,190
SPEAKER_1:  for the...

01:56:40,418 --> 01:56:42,110
SPEAKER_1:  This nice feature that is doing.

01:56:42,402 --> 01:56:43,582
SPEAKER_0:  Yeah, I think the trickier thing is like...

01:56:43,842 --> 01:56:50,078
SPEAKER_0:  Some people really love you, some people unfortunately like, you're working on something that you think is extremely valuable, useful, etc. Some people do hate you.

01:56:50,370 --> 01:56:51,870
SPEAKER_0:  There's a lot of people who like eat.

01:56:52,322 --> 01:56:56,926
SPEAKER_0:  me and the team and the whole project. I think Tesla drivers.

01:56:57,570 --> 01:56:59,390
SPEAKER_0:  In many cases, they're not, actually.

01:56:59,778 --> 01:57:02,430
SPEAKER_1:  Yeah, that actually makes me sad about humans.

01:57:02,754 --> 01:57:03,102
SPEAKER_1:  or

01:57:03,458 --> 01:57:04,190
SPEAKER_1:  The current.

01:57:05,154 --> 01:57:10,334
SPEAKER_1:  the ways that humans interact. I think that's actually fixable. I think humans want to be good to each other. I think Twitter.

01:57:10,690 --> 01:57:13,278
SPEAKER_1:  and social media is part of the mechanism that actually.

01:57:13,538 --> 01:57:15,934
SPEAKER_1:  somehow makes the negativity more viral.

01:57:16,258 --> 01:57:18,718
SPEAKER_1:  that it doesn't deserve like disproportionately.

01:57:19,202 --> 01:57:23,358
SPEAKER_1:  add like a viral boost of negativity.

01:57:23,650 --> 01:57:26,110
SPEAKER_1:  But I wish people would just get excited about...

01:57:26,690 --> 01:57:27,070
SPEAKER_1:  Uh...

01:57:27,298 --> 01:57:28,862
SPEAKER_1:  So suppress some of the jealousy.

01:57:29,570 --> 01:57:32,670
SPEAKER_1:  some of the ego and just get excited for others. And then

01:57:33,154 --> 01:57:37,790
SPEAKER_1:  There's a karma aspect to that. You get excited for others, they'll get excited for you. Same thing in academia.

01:57:38,082 --> 01:57:40,894
SPEAKER_1:  If you're not careful, there's a like a dynamical system there.

01:57:41,346 --> 01:57:45,726
SPEAKER_1:  If you if you think of in silos and get jealous of somebody else being successful.

01:57:46,114 --> 01:57:47,038
SPEAKER_1:  that actually

01:57:47,554 --> 01:57:48,926
SPEAKER_1:  Perhaps counterintuitively.

01:57:49,250 --> 01:57:49,630
SPEAKER_1:  ha

01:57:49,858 --> 01:57:53,278
SPEAKER_1:  leads to less productivity of you as a community and you individually.

01:57:53,602 --> 01:57:56,094
SPEAKER_1:  I feel like if you keep celebrating others...

01:57:56,738 --> 01:57:59,102
SPEAKER_1:  that actually makes you more successful.

01:57:59,426 --> 01:58:03,774
SPEAKER_1:  Yeah. I think people haven't in, depending on the industry, haven't quite learned that yet. Yeah.

01:58:04,290 --> 01:58:23,390
SPEAKER_0:  Some people are also very negative and very vocal, so they're very prominently featured, but actually there's a ton of people who are cheerleaders, but they're silent cheerleaders. And when you talk to people just in the world, they will tell you, oh, it's amazing, it's great. Especially like people who understand how difficult it is to get this stuff working. Like people who have built products and are makers and entrepreneurs.

01:58:23,778 --> 01:58:25,214
SPEAKER_0:  like making this work.

01:58:25,570 --> 01:58:26,270
SPEAKER_0:  changing something?

01:58:26,914 --> 01:58:28,382
SPEAKER_0:  is incredibly hard.

01:58:28,610 --> 01:58:30,485
SPEAKER_0:  Those people are more likely to cheerlead you.

01:58:30,485 --> 01:58:36,734
SPEAKER_1:  Well, one of the things that makes me sad is some folks in the robotics community don't do the cheerleading than they should.

01:58:37,282 --> 01:58:46,558
SPEAKER_1:  There's a, cause they know how difficult it is. Well, they actually sometimes don't know how difficult it is to create a product that scale, right? They actually deploy in the real world. A lot of the-

01:58:47,234 --> 01:58:52,542
SPEAKER_1:  development of robots and AI system is done on very specific small benchmarks.

01:58:53,346 --> 01:58:55,902
SPEAKER_1:  and as opposed to real work additions. Yes.

01:58:57,186 --> 01:59:00,561
SPEAKER_0:  Yeah, I think it's really hard to work on robotics in academic setting or AI.

01:59:00,561 --> 01:59:03,262
SPEAKER_1:  systems that apply in the real world. You've criticized...

01:59:03,714 --> 01:59:04,030
SPEAKER_1:  You.

01:59:04,770 --> 01:59:06,206
SPEAKER_1:  uh, flourish.

01:59:06,818 --> 01:59:15,678
SPEAKER_1:  and loved for time the image net, the famed image net data set. And I've recently had some words of criticism that the.

01:59:15,970 --> 01:59:21,662
SPEAKER_1:  Academic research, ML community gives a little too much love still to the ImageNet or like

01:59:22,050 --> 01:59:26,558
SPEAKER_1:  those kinds of benchmarks. Can you speak to the strengths and weaknesses of datasets?

01:59:27,010 --> 01:59:29,054
SPEAKER_1:  used in machine learning research.

01:59:30,402 --> 01:59:33,342
SPEAKER_0:  Actually, I don't know that I recall the specific instance where I was, uh...

01:59:33,698 --> 01:59:38,238
SPEAKER_0:  unhappy or criticizing ImageNet. I think ImageNet has been extremely valuable.

01:59:38,882 --> 01:59:42,974
SPEAKER_0:  It was basically a benchmark that allowed the deep learning community to...

01:59:43,298 --> 01:59:45,374
SPEAKER_0:  demonstrate that deep neural networks actually work.

01:59:45,730 --> 01:59:46,654
SPEAKER_0:  It was a...

01:59:47,074 --> 01:59:49,086
SPEAKER_0:  There's a massive value in that.

01:59:49,730 --> 01:59:51,646
SPEAKER_0:  So I think ImageNet was useful, but I'm.

01:59:51,874 --> 01:59:54,014
SPEAKER_0:  Basically, it's become a bit of an M-Nist at this point.

01:59:54,274 --> 02:00:00,414
SPEAKER_0:  So MNIST is like the little 228 by 28 grayscale digits. There's kind of a joke data set that everyone like crushes.

02:00:00,674 --> 02:00:04,446
SPEAKER_1:  There's no papers written on them this though, right? Maybe they shouldn't.

02:00:04,866 --> 02:00:09,393
SPEAKER_1:  Like papers that focus on like how do we learn with a small amount of data that

02:00:09,393 --> 02:00:13,086
SPEAKER_0:  Yeah, I could see that being helpful, but not in sort of like mainline computer vision research anymore.

02:00:13,218 --> 02:00:16,990
SPEAKER_1:  I think the way I've heard you somewhere, maybe I'm just imagining things.

02:00:17,218 --> 02:00:23,006
SPEAKER_1:  But I think you said like image that was a huge contribution to the community for a long time. Now it's time to move past those kinds of.

02:00:23,106 --> 02:00:25,950
SPEAKER_0:  Well, ImageNet has been crushed. I mean, you know, the error rates are.

02:00:27,138 --> 02:00:27,486
SPEAKER_0:  Uh...

02:00:28,482 --> 02:00:30,206
SPEAKER_0:  Yeah, we're getting like 90% accuracy.

02:00:30,658 --> 02:00:33,022
SPEAKER_0:  in 1000 classification way.

02:00:33,474 --> 02:00:34,526
SPEAKER_0:  prediction.

02:00:34,850 --> 02:00:36,638
SPEAKER_0:  and I've seen those images.

02:00:36,930 --> 02:00:42,270
SPEAKER_0:  And it's like really high. That's really good. If I remember correctly, the top five error.

02:00:42,530 --> 02:00:44,350
SPEAKER_0:  rate is now like 1% or something.

02:00:44,642 --> 02:00:50,750
SPEAKER_1:  Given your experience with a gigantic real world data set, would you like to see benchmarks move in a certain directions?

02:00:50,978 --> 02:00:52,382
SPEAKER_1:  that the research community uses.

02:00:52,738 --> 02:00:55,198
SPEAKER_0:  Unfortunately, I don't think academics currently have the next ImageNet.

02:00:55,618 --> 02:00:59,422
SPEAKER_0:  We've obviously, I think we've crushed MNIST. We've basically kind of crushed ImageNet.

02:00:59,906 --> 02:01:02,974
SPEAKER_0:  and there's no next sort of big benchmark that.

02:01:03,266 --> 02:01:04,766
SPEAKER_0:  the entire community is behind.

02:01:04,994 --> 02:01:06,078
SPEAKER_0:  and use this.

02:01:07,106 --> 02:01:09,022
SPEAKER_0:  you know, for further development of these networks.

02:01:09,346 --> 02:01:14,654
SPEAKER_1:  I wonder what it takes for a data set to captivate the imagination of everybody, like where they all get behind it.

02:01:15,074 --> 02:01:16,702
SPEAKER_1:  that that could also need.

02:01:16,994 --> 02:01:18,302
SPEAKER_1:  like a leader.

02:01:18,530 --> 02:01:20,990
SPEAKER_1:  Right. Yeah. Somebody with popularity. I mean that

02:01:21,282 --> 02:01:23,134
SPEAKER_1:  Yeah, why did image of that take off?

02:01:24,258 --> 02:01:26,133
SPEAKER_1:  Is there, or is it just the accident of?

02:01:26,133 --> 02:01:28,094
SPEAKER_0:  It was the right amount of difficult.

02:01:28,578 --> 02:01:29,118
SPEAKER_0:  Uh...

02:01:29,474 --> 02:01:36,350
SPEAKER_0:  It was the right amount of difficult and simple and interesting enough. It just kind of like, it was the right time for that kind of a data set.

02:01:37,698 --> 02:01:38,590
SPEAKER_1:  Question from Reddit.

02:01:39,586 --> 02:01:40,062
SPEAKER_1:  uh...

02:01:40,738 --> 02:01:46,462
SPEAKER_1:  What are your thoughts on the role that synthetic data and game engines will play in the future of neural net model development?

02:01:48,290 --> 02:01:49,182
SPEAKER_0:  I think...

02:01:49,762 --> 02:01:52,126
SPEAKER_0:  As neural nets converge to humans,

02:01:53,186 --> 02:01:54,974
SPEAKER_0:  the value of simulation.

02:01:55,234 --> 02:01:57,790
SPEAKER_0:  neural nets will be similar to value of simulation to humans.

02:01:59,650 --> 02:02:01,694
SPEAKER_0:  So people use simulation for.

02:02:02,466 --> 02:02:05,150
SPEAKER_0:  people do simulation because they can learn something in that kind of a system.

02:02:05,826 --> 02:02:06,334
SPEAKER_0:  Um.

02:02:06,722 --> 02:02:09,118
SPEAKER_0:  and without having to actually experience it.

02:02:09,314 --> 02:02:11,710
SPEAKER_1:  But are you referring to the simulation we're doing our head?

02:02:12,322 --> 02:02:15,614
SPEAKER_0:  Is that what you mean? I mean like video games or you know

02:02:15,970 --> 02:02:16,478
SPEAKER_0:  Um,

02:02:16,834 --> 02:02:18,270
SPEAKER_0:  other forms of simulation for.

02:02:18,690 --> 02:02:19,815
SPEAKER_0:  various professionals.

02:02:19,815 --> 02:02:23,678
SPEAKER_1:  So let me push back on that because maybe there's simulation that we do in our heads.

02:02:24,002 --> 02:02:24,350
SPEAKER_1:  Like.

02:02:24,770 --> 02:02:26,462
SPEAKER_1:  simulate if I do this.

02:02:27,458 --> 02:02:28,606
SPEAKER_1:  What do I think will happen?

02:02:28,738 --> 02:02:30,613
SPEAKER_0:  Okay, that's like internal simulation.

02:02:30,613 --> 02:02:32,863
SPEAKER_1:  Is that what we're doing?

02:02:32,863 --> 02:02:39,934
SPEAKER_0:  react. Oh yeah, but that's independent from like the use of simulation in a sense of like computer games or using simulation for training set creation or

02:02:40,066 --> 02:02:43,678
SPEAKER_1:  Is it independent or is it just loosely correlated? Cause like, uh,

02:02:44,674 --> 02:02:46,750
SPEAKER_1:  Isn't that useful to do like a...

02:02:47,810 --> 02:02:48,798
SPEAKER_1:  counterfactual.

02:02:49,058 --> 02:02:51,166
SPEAKER_1:  or like edge gaze simulation to like.

02:02:52,130 --> 02:02:54,206
SPEAKER_1:  You know, what happens if there's a nuclear war?

02:02:55,010 --> 02:02:58,385
SPEAKER_1:  What happens if there's, you know, like those kinds of things?

02:02:58,385 --> 02:03:00,318
SPEAKER_0:  Yeah, that's a different simulation from Unreal Engine.

02:03:00,706 --> 02:03:02,046
SPEAKER_0:  That's how I interpreted the question.

02:03:02,338 --> 02:03:03,294
SPEAKER_1:  Ah, so like...

02:03:03,842 --> 02:03:05,342
SPEAKER_1:  simulation of the average case.

02:03:06,914 --> 02:03:09,918
SPEAKER_1:  Is that what's Unreal Engine?

02:03:10,146 --> 02:03:12,286
SPEAKER_1:  What do you mean by Unreal Engine?

02:03:13,026 --> 02:03:16,158
SPEAKER_1:  Simulating a world. Physics of that world.

02:03:17,410 --> 02:03:21,310
SPEAKER_1:  Why is that different? Because you also can add behavior to that world.

02:03:21,986 --> 02:03:24,158
SPEAKER_1:  and you could try all kinds of stuff, right?

02:03:24,898 --> 02:03:26,718
SPEAKER_1:  You could throw all kinds of weird things into it.

02:03:26,946 --> 02:03:28,926
SPEAKER_1:  Unreal Engine is not just about.

02:03:29,282 --> 02:03:32,862
SPEAKER_1:  I mean, I guess it is about simulating the physics of the world. It's also-

02:03:33,314 --> 02:03:34,558
SPEAKER_1:  doing something with that.

02:03:35,330 --> 02:03:36,926
SPEAKER_0:  Yeah, the graphics, the physics and the...

02:03:37,154 --> 02:03:39,262
SPEAKER_0:  agents that you put into the environment and stuff like that.

02:03:39,362 --> 02:03:42,302
SPEAKER_1:  See, I think you, I feel like you said that it's not that important.

02:03:43,042 --> 02:03:45,662
SPEAKER_1:  I guess for the future of AI development.

02:03:46,178 --> 02:03:48,053
SPEAKER_1:  that is that correct interpret all you know what

02:03:48,053 --> 02:03:48,766
SPEAKER_0:  I think

02:03:49,346 --> 02:03:51,166
SPEAKER_0:  Humans use simulators.

02:03:52,322 --> 02:03:53,150
SPEAKER_0:  for.

02:03:53,538 --> 02:03:55,038
SPEAKER_0:  humans use simulators and they find them useful.

02:03:55,266 --> 02:03:58,014
SPEAKER_0:  And so computers will use simulators and find them useful.

02:03:59,106 --> 02:04:04,350
SPEAKER_1:  OK, so you're saying it's not that I don't use simulators very often. I play a video game every once in a while, but I don't think.

02:04:04,738 --> 02:04:08,958
SPEAKER_1:  I derive any wisdom about my own existence from those video games.

02:04:09,346 --> 02:04:12,254
SPEAKER_1:  It's a momentary escape from reality versus.

02:04:12,546 --> 02:04:14,238
SPEAKER_1:  a source of wisdom about reality.

02:04:14,914 --> 02:04:19,070
SPEAKER_1:  So I think that's a very polite way of saying simulation is not that useful.

02:04:20,162 --> 02:04:22,622
SPEAKER_0:  Yeah, maybe not. I don't see it as like a fundamental...

02:04:22,882 --> 02:04:26,046
SPEAKER_0:  really important part of training neural nets currently.

02:04:26,498 --> 02:04:29,566
SPEAKER_0:  But I think as neural nets become more and more powerful.

02:04:29,890 --> 02:04:31,998
SPEAKER_0:  I think you will need fewer examples.

02:04:32,450 --> 02:04:34,110
SPEAKER_0:  to train additional behaviors.

02:04:34,434 --> 02:04:40,190
SPEAKER_0:  And simulation is, of course, there's a domain gap in a simulation that is not the real world. It's slightly something different.

02:04:40,578 --> 02:04:43,870
SPEAKER_0:  But with a powerful enough neural net, you need...

02:04:44,194 --> 02:04:49,182
SPEAKER_0:  the domain gap can be bigger, I think, because neural net will sort of understand that even though it's not the real world, it like...

02:04:49,410 --> 02:04:52,126
SPEAKER_0:  has all this high level structure that I'm supposed to be learning from.

02:04:52,290 --> 02:04:53,694
SPEAKER_1:  So then you'll know we'll actually.

02:04:54,370 --> 02:04:56,574
SPEAKER_1:  Yeah, we'll be able to leverage.

02:04:57,378 --> 02:04:59,038
SPEAKER_1:  the synthetic data better. Yes.

02:04:59,426 --> 02:05:01,310
SPEAKER_1:  by closing the get-but-understanding.

02:05:01,794 --> 02:05:04,702
SPEAKER_1:  in which ways this is not real data. Exactly.

02:05:05,826 --> 02:05:10,302
SPEAKER_1:  I'm ready to do better questions next time. That was a question, but I'm just kidding.

02:05:10,594 --> 02:05:11,486
SPEAKER_1:  All right.

02:05:14,306 --> 02:05:21,182
SPEAKER_1:  So is it possible, do you think, speaking of MNIST, to construct neural nets and training processes that require very little data?

02:05:23,266 --> 02:05:26,238
SPEAKER_1:  So we've been talking about huge data sets like the internet for training.

02:05:26,690 --> 02:05:33,534
SPEAKER_1:  I mean, one way to say that is, like you said, like the querying itself is another level of training, I guess, and that requires a little data.

02:05:34,626 --> 02:05:38,430
SPEAKER_1:  Do you see any value in doing research?

02:05:38,914 --> 02:05:39,326
SPEAKER_1:  and

02:05:39,554 --> 02:05:39,934
SPEAKER_1:  Ta-da.

02:05:40,226 --> 02:05:43,998
SPEAKER_1:  going down the direction of can we use very little data to train.

02:05:44,226 --> 02:05:45,351
SPEAKER_1:  to construct a knowledge base.

02:05:45,351 --> 02:05:45,886
SPEAKER_0:  100%.

02:05:46,274 --> 02:05:48,670
SPEAKER_0:  I just think at some point you need a massive dataset.

02:05:49,026 --> 02:05:52,094
SPEAKER_0:  And then when you pre-train your massive neural nut and get something that...

02:05:52,546 --> 02:05:55,742
SPEAKER_0:  you know, is like a GPT or something, then you're able to be very.

02:05:56,098 --> 02:05:59,038
SPEAKER_0:  efficient at training any arbitrary new task.

02:05:59,330 --> 02:06:06,622
SPEAKER_0:  So a lot of these GPTs, you can do tasks like sentiment analysis or translation or so on just by being prompted with very few examples.

02:06:07,010 --> 02:06:08,446
SPEAKER_0:  Here's the kind of thing I want you to do like.

02:06:08,674 --> 02:06:11,038
SPEAKER_0:  Here's an input sentence. Here's the translation into German.

02:06:11,266 --> 02:06:14,078
SPEAKER_0:  Input sentence, translation to German. Input sentence blank.

02:06:14,562 --> 02:06:17,246
SPEAKER_0:  and the neural net will complete the translation to German just by...

02:06:17,794 --> 02:06:19,646
SPEAKER_0:  looking at the example you've provided.

02:06:19,938 --> 02:06:21,886
SPEAKER_0:  And so that's an example of a very few shot.

02:06:22,178 --> 02:06:23,166
SPEAKER_0:  uh, learning.

02:06:23,554 --> 02:06:25,886
SPEAKER_0:  in the activations of the neural net instead of the weights of the neural net.

02:06:26,498 --> 02:06:27,390
SPEAKER_0:  And so I think...

02:06:28,194 --> 02:06:33,310
SPEAKER_0:  Basically, just like humans, neural nets will become very data efficient at learning any other new task.

02:06:33,794 --> 02:06:36,734
SPEAKER_0:  But at some point, you need a massive data set to pre-train your network.

02:06:38,114 --> 02:06:41,086
SPEAKER_1:  to get that and probably we humans have something like that.

02:06:41,634 --> 02:06:43,678
SPEAKER_1:  Do we have something like that? Do we have a-

02:06:44,098 --> 02:06:44,798
SPEAKER_1:  passive

02:06:45,346 --> 02:06:46,494
SPEAKER_1:  in the background.

02:06:47,458 --> 02:06:48,958
SPEAKER_1:  background model constructing.

02:06:49,634 --> 02:06:49,982
SPEAKER_1:  thing.

02:06:50,530 --> 02:06:54,014
SPEAKER_1:  They just runs all the time in a self supervised way. We're not conscious of it.

02:06:54,242 --> 02:06:57,950
SPEAKER_0:  I think humans definitely, I mean obviously we have, we learn a lot during.

02:06:58,370 --> 02:06:59,486
SPEAKER_0:  during our lifespan.

02:06:59,842 --> 02:07:01,470
SPEAKER_0:  but also we have a ton of.

02:07:01,794 --> 02:07:03,966
SPEAKER_0:  hardware that helps us initialization.

02:07:04,290 --> 02:07:05,630
SPEAKER_0:  coming from sort of evolution.

02:07:06,242 --> 02:07:08,574
SPEAKER_0:  And so I think that's also a really big component.

02:07:08,834 --> 02:07:15,998
SPEAKER_0:  A lot of people in the field, I think they just talk about the amounts of like seconds and the you know that a person has left pretending that this is a WLR assa.

02:07:16,226 --> 02:07:18,270
SPEAKER_0:  sort of like a zero initialization of a neural net.

02:07:18,882 --> 02:07:19,774
SPEAKER_0:  And it's not like.

02:07:20,194 --> 02:07:23,422
SPEAKER_0:  You can look at a lot of animals, like for example, zebras. Zebras get born.

02:07:23,650 --> 02:07:24,062
SPEAKER_0:  and

02:07:24,322 --> 02:07:25,470
SPEAKER_0:  they see and they...

02:07:25,794 --> 02:07:26,238
SPEAKER_0:  can run.

02:07:26,466 --> 02:07:26,782
SPEAKER_0:

02:07:27,074 --> 02:07:30,078
SPEAKER_0:  There's zero train data in their lifespan. They can just do that.

02:07:30,658 --> 02:07:32,542
SPEAKER_0:  So somehow, I have no idea how.

02:07:32,770 --> 02:07:38,494
SPEAKER_0:  Evolution has found a way to encode these algorithms and these neural net initializations are extremely good into ATCGs.

02:07:38,882 --> 02:07:41,854
SPEAKER_0:  And I have no idea how this works, but apparently it's possible, because here's...

02:07:42,306 --> 02:07:43,454
SPEAKER_0:  proof by existence.

02:07:43,650 --> 02:07:44,094
SPEAKER_1:  haha

02:07:44,322 --> 02:07:46,302
SPEAKER_1:  There's something magical about.

02:07:46,946 --> 02:07:51,422
SPEAKER_1:  gone from a single cell to an organism that is born to the first few years of life.

02:07:51,714 --> 02:07:56,606
SPEAKER_1:  I kinda like the idea that the reason we don't remember anything about the first few years of our life

02:07:57,090 --> 02:08:01,598
SPEAKER_1:  is that it's a really painful process. Like it's a very difficult, challenging.

02:08:02,114 --> 02:08:04,158
SPEAKER_1:  training process..

02:08:04,450 --> 02:08:05,438
SPEAKER_1:  intellectually.

02:08:07,362 --> 02:08:08,702
SPEAKER_1:  and maybe, yeah, we are.

02:08:09,506 --> 02:08:11,166
SPEAKER_1:  Why don't we remember any of that?

02:08:11,810 --> 02:08:14,078
SPEAKER_1:  There might be some crazy training going on.

02:08:14,530 --> 02:08:14,878
SPEAKER_1:  and

02:08:15,618 --> 02:08:19,102
SPEAKER_1:  That the baby that's the background model training

02:08:19,522 --> 02:08:21,118
SPEAKER_1:  that is

02:08:21,538 --> 02:08:22,782
SPEAKER_1:  is very painful.

02:08:23,138 --> 02:08:27,326
SPEAKER_1:  And so it's best for the system once it's trained not to remember how it's constructed.

02:08:27,778 --> 02:08:31,518
SPEAKER_0:  I think it's just like the hardware for long-term memory is just not fully developed.

02:08:31,778 --> 02:08:33,726
SPEAKER_0:  I kind of feel like the first few years of

02:08:33,986 --> 02:08:34,398
SPEAKER_0:  Uh.

02:08:35,042 --> 02:08:37,886
SPEAKER_0:  of infants is not actually like learning, it's brain maturing.

02:08:38,754 --> 02:08:39,166
SPEAKER_0:  Um,

02:08:39,394 --> 02:08:40,350
SPEAKER_0:  We're born premature.

02:08:40,770 --> 02:08:41,214
SPEAKER_0:  Um.

02:08:41,698 --> 02:08:45,054
SPEAKER_0:  There's a theory along those lines because of the birth canal and the swelling of the brain.

02:08:45,474 --> 02:08:48,990
SPEAKER_0:  And so we're born premature and then the first few years were just the brain's maturing.

02:08:49,346 --> 02:08:51,038
SPEAKER_0:  and then there's some learning eventually.

02:08:51,522 --> 02:08:51,870
SPEAKER_0:  Um.

02:08:52,866 --> 02:08:53,991
SPEAKER_0:  my current view on it.

02:08:53,991 --> 02:08:55,262
SPEAKER_1:  What do you think?

02:08:55,810 --> 02:08:57,310
SPEAKER_1:  Do you think neural nets can have?

02:08:57,570 --> 02:08:58,590
SPEAKER_1:  Long-term memory.

02:08:59,906 --> 02:09:01,790
SPEAKER_1:  Like that approach is something like humans.

02:09:02,050 --> 02:09:06,142
SPEAKER_1:  Do you think there needs to be another meta architecture on top of it to add?

02:09:06,498 --> 02:09:10,142
SPEAKER_1:  something like a knowledge base that learns facts about the world and all that kind of stuff.

02:09:10,562 --> 02:09:15,166
SPEAKER_0:  Yes, but I don't know to what extent it will be explicitly constructed.

02:09:15,842 --> 02:09:17,918
SPEAKER_0:  It might take unintuitive forms where...

02:09:18,306 --> 02:09:19,710
SPEAKER_0:  You are telling the GPT like.

02:09:20,130 --> 02:09:24,702
SPEAKER_0:  Hey, you have a declarative memory bank to which you can store and retrieve data from.

02:09:25,122 --> 02:09:29,022
SPEAKER_0:  And whenever you encounter some information that you find useful, just save it to your memory bank.

02:09:29,698 --> 02:09:35,902
SPEAKER_0:  And here's an example of something you have retrieved and how you say it, and here's how you load from it. You just say load.

02:09:36,546 --> 02:09:41,918
SPEAKER_0:  whatever, you teach it in text and English, and then it might learn to use a memory bank from that.

02:09:42,402 --> 02:09:50,641
SPEAKER_1:  Oh, so the neural net is the architecture for the background model, the base thing. And then everything else is just on top of this.

02:09:50,641 --> 02:09:53,342
SPEAKER_0:  text, right? You're giving it gadgets and gizmos.

02:09:53,602 --> 02:09:53,982
SPEAKER_0:  Uh

02:09:54,242 --> 02:10:02,974
SPEAKER_0:  you're teaching some kind of a special language by which it can save arbitrary information and retrieve it at a later time. And you're telling about these special tokens and how to arrange them to...

02:10:03,202 --> 02:10:04,286
SPEAKER_0:  use these interfaces.

02:10:04,770 --> 02:10:07,742
SPEAKER_0:  It's like, hey, you can use a calculator. Here's how you use it. Do

02:10:08,130 --> 02:10:11,774
SPEAKER_0:  5, 3 plus 4, 1 equals. And when equals is there,

02:10:12,098 --> 02:10:15,742
SPEAKER_0:  a calculator will actually read out the answer when you don't have to calculate it yourself.

02:10:16,258 --> 02:10:18,494
SPEAKER_0:  and you just like tell it in English, this might actually work.

02:10:18,818 --> 02:10:23,454
SPEAKER_1:  Do you think in that sense, God, it was interesting. The deep mind system that

02:10:23,714 --> 02:10:25,950
SPEAKER_1:  It's not just new language, but actually throws it all.

02:10:26,914 --> 02:10:28,190
SPEAKER_1:  in the same pile.

02:10:28,450 --> 02:10:30,494
SPEAKER_1:  images, actions.

02:10:30,818 --> 02:10:33,694
SPEAKER_1:  all that kind of stuff. That's basically what we're moving towards.

02:10:34,178 --> 02:10:37,534
SPEAKER_0:  Yeah, I think so. So Gato is very much a kitchen sink.

02:10:37,794 --> 02:10:39,070
SPEAKER_0:  Approach to like.

02:10:39,426 --> 02:10:39,838
SPEAKER_0:  Um.

02:10:40,674 --> 02:10:44,030
SPEAKER_0:  reinforcement learning lots of different environments with a single fixed transformer.

02:10:44,322 --> 02:10:45,342
SPEAKER_0:  model, right?

02:10:45,666 --> 02:10:46,110
SPEAKER_0:  Um.

02:10:46,786 --> 02:10:53,214
SPEAKER_0:  I think it's a very sort of early result in that realm. But I think, yeah, it's along the lines of what I think things will eventually look like.

02:10:53,506 --> 02:10:53,854
SPEAKER_1:  Right.

02:10:54,210 --> 02:10:59,742
SPEAKER_1:  So this is the early days of a system that eventually will look like this, from a rich, rich sudden perspective.

02:10:59,970 --> 02:11:08,766
SPEAKER_0:  Yeah, I'm not super huge fan of, I think, all these interfaces that look very different. I would want everything to be normalized into the same API. So for example, screen pixels.

02:11:09,250 --> 02:11:19,230
SPEAKER_0:  very same API, instead of having like different world environments that have very different physics and joint configurations and appearances and whatever, and you're having some kind of special tokens for different games that you can plug.

02:11:19,618 --> 02:11:22,078
SPEAKER_0:  I'd rather just normalize everything to a single interface.

02:11:22,626 --> 02:11:23,902
SPEAKER_0:  So it looks the same to the neural net.

02:11:24,226 --> 02:11:24,926
SPEAKER_0:  If that makes sense.

02:11:25,090 --> 02:11:27,326
SPEAKER_1:  So it's all gonna be pixel based pong in the end.

02:11:27,650 --> 02:11:28,126
SPEAKER_1:  I think so.

02:11:29,218 --> 02:11:29,758
SPEAKER_1:  Hahaha

02:11:30,274 --> 02:11:31,262
SPEAKER_1:  Okay.

02:11:31,650 --> 02:11:34,718
SPEAKER_1:  Let me ask you about your own personal life.

02:11:35,874 --> 02:11:38,814
SPEAKER_1:  A lot of people want to know you're one of the most productive and brilliant people.

02:11:39,330 --> 02:11:43,390
SPEAKER_1:  In the history of AI, what is a productive day in the life of Andre Kapatielok?

02:11:44,514 --> 02:11:46,686
SPEAKER_1:  What time do you wake up? Breakfast

02:11:47,298 --> 02:11:47,774
SPEAKER_1:  Um.

02:11:48,194 --> 02:11:54,174
SPEAKER_1:  some kind of dance between the average productive day and a perfect productive day. So the perfect productive day is the thing we strive.

02:11:54,946 --> 02:11:59,998
SPEAKER_1:  towards in the average is kind of what it converges to, and all the mistakes and human.

02:12:00,258 --> 02:12:02,910
SPEAKER_1:  eventualities and so on. So what times you wake up?

02:12:03,138 --> 02:12:04,094
SPEAKER_0:  by your morning person.

02:12:04,482 --> 02:12:06,814
SPEAKER_0:  I'm not a morning person, I'm a night owl for sure.

02:12:06,978 --> 02:12:08,670
SPEAKER_1:  Is it stable or not?

02:12:08,802 --> 02:12:16,190
SPEAKER_0:  It's semi-stable, like eight or nine or something like that. During my PhD, it was even later, I used to go to sleep usually at 3 a.m.

02:12:16,738 --> 02:12:17,534
SPEAKER_0:  I think.

02:12:17,762 --> 02:12:19,038
SPEAKER_0:  AM hours are.

02:12:19,330 --> 02:12:22,142
SPEAKER_0:  are precious and very interesting time to work because everyone is asleep.

02:12:22,658 --> 02:12:25,534
SPEAKER_0:  At 8 a.m. or 7 a.m., the East Coast is awake.

02:12:26,114 --> 02:12:30,590
SPEAKER_0:  So there's already activity, there's already some text messages, whatever, there's stuff happening, you can go and like...

02:12:31,202 --> 02:12:33,406
SPEAKER_0:  news website and there's stuff happening it's distracting.

02:12:33,890 --> 02:12:36,030
SPEAKER_0:  At 3 a.m. everything is totally quiet.

02:12:36,642 --> 02:12:39,358
SPEAKER_0:  And so you're not gonna be bothered and you have solid chunks of time to do.

02:12:40,898 --> 02:12:41,342
SPEAKER_0:  Um.

02:12:42,210 --> 02:12:44,734
SPEAKER_0:  So I like those periods, not all by default.

02:12:45,218 --> 02:12:47,166
SPEAKER_0:  And then I think like productive time basically.

02:12:47,842 --> 02:12:51,550
SPEAKER_0:  What I like to do is you need to build some momentum on the problem.

02:12:51,874 --> 02:12:53,086
SPEAKER_0:  without too much distraction.

02:12:53,634 --> 02:12:54,398
SPEAKER_0:  and uh...

02:12:54,690 --> 02:12:56,126
SPEAKER_0:  You need to load your...

02:12:56,354 --> 02:12:56,830
SPEAKER_0:  REM

02:12:57,538 --> 02:12:59,614
SPEAKER_0:  your working memory with that problem.

02:13:00,386 --> 02:13:03,486
SPEAKER_0:  And then you need to be obsessed with it when you're taking shower, when you're falling asleep.

02:13:04,034 --> 02:13:09,155
SPEAKER_0:  You need to be obsessed with the problem, and it's fully in your memory, and you're ready to wake up and work on it right there.

02:13:09,155 --> 02:13:10,302
SPEAKER_1:  is the scale of...

02:13:10,658 --> 02:13:14,783
SPEAKER_1:  Is this in a scale, temporal scale of a single day or a couple of days, a week?

02:13:14,783 --> 02:13:18,142
SPEAKER_0:  So I can't talk about one day basically in isolation because

02:13:18,434 --> 02:13:22,270
SPEAKER_0:  It's a whole process. When I want to get productive in the problem, I feel like I need...

02:13:23,010 --> 02:13:25,950
SPEAKER_0:  span of a few days where I can really get in on that problem.

02:13:26,530 --> 02:13:30,846
SPEAKER_0:  and I don't want to be interrupted, and I'm going to just be completely obsessed with that problem.

02:13:31,170 --> 02:13:32,958
SPEAKER_0:  And that's where I do most of my good workouts.

02:13:34,082 --> 02:13:40,446
SPEAKER_1:  You've done a bunch of cool little projects in a very short amount of time very quickly, so that requires you just focusing on it.

02:13:40,866 --> 02:13:43,070
SPEAKER_0:  Yeah, basically I need to load my working memory with the problem.

02:13:43,394 --> 02:13:47,646
SPEAKER_0:  and I need to be productive because there's always like a huge fixed cost to approaching any problem.

02:13:48,162 --> 02:13:48,478
SPEAKER_0:  Uh.

02:13:49,026 --> 02:13:59,198
SPEAKER_0:  You know, like I was struggling with this, for example, at Tesla, because I want to work on like small side project. But okay, you first need to figure out, okay, I need to SSH into my cluster. I need to bring up a VS code editor so I can like work on this.

02:13:59,490 --> 02:14:00,094
SPEAKER_0:  I need to.

02:14:00,386 --> 02:14:07,006
SPEAKER_0:  I run into some stupid error because of some reason. Like you're not at a point where you can be just productive right away. You are facing barriers.

02:14:07,554 --> 02:14:09,758
SPEAKER_0:  And so it's about really.

02:14:10,402 --> 02:14:15,134
SPEAKER_0:  removing all of that barrier and you're able to go into the problem and you have the full problem loaded in your memory.

02:14:15,394 --> 02:14:19,294
SPEAKER_1:  And somehow avoiding distractions of all different forms like.

02:14:19,906 --> 02:14:20,958
SPEAKER_1:  news stories.

02:14:21,346 --> 02:14:29,566
SPEAKER_1:  emails, but also distractions from other interesting projects that you previously worked on or currently working on and so on. You just want to really focus your mind.

02:14:29,826 --> 02:14:37,694
SPEAKER_0:  And I mean, I can take some time off for distractions and in between, but I think it can't be too much. You know, most of your day is sort of like spent on that problem.

02:14:38,050 --> 02:14:39,294
SPEAKER_0:  And then, you know, I.

02:14:40,418 --> 02:14:46,238
SPEAKER_0:  drink coffee, I have my morning routine, I look at some news, Twitter, hacker news, Wall Street Journal, etc.

02:14:46,690 --> 02:14:47,815
SPEAKER_0:  So you.

02:14:47,815 --> 02:14:53,310
SPEAKER_1:  You wake up, you have some coffee. Are you trying to get to work as quickly as possible? Are you taking this diet of...

02:14:53,890 --> 02:14:55,934
SPEAKER_1:  of like what the hell's happening in the world.

02:14:56,066 --> 02:15:00,894
SPEAKER_0:  I am, I do find it interesting to know about the world. I don't know that it's...

02:15:01,154 --> 02:15:03,294
SPEAKER_0:  useful or good, but it is part of my routine right now.

02:15:03,618 --> 02:15:06,398
SPEAKER_0:  So I do read through a bunch of news articles and I want to be informed.

02:15:06,818 --> 02:15:07,614
SPEAKER_0:  and

02:15:08,322 --> 02:15:09,438
SPEAKER_0:  I'm suspicious of it.

02:15:09,954 --> 02:15:11,829
SPEAKER_0:  I'm suspicious of the practice, but currently that's where I am.

02:15:11,829 --> 02:15:13,854
SPEAKER_1:  Oh, you mean suspicious about the

02:15:14,114 --> 02:15:15,166
SPEAKER_1:  positive effect.

02:15:15,522 --> 02:15:18,897
SPEAKER_1:  of that practice on your productivity and your wealth.

02:15:18,897 --> 02:15:20,894
SPEAKER_0:  being my well-being psychologically

02:15:21,090 --> 02:15:24,030
SPEAKER_1:  and also on your ability to deeply understand the world because

02:15:24,450 --> 02:15:27,825
SPEAKER_1:  There's a bunch of sources of information. You're not really focused on.

02:15:27,825 --> 02:15:30,078
SPEAKER_0:  Yeah, it's a little distracting.

02:15:30,818 --> 02:15:33,086
SPEAKER_1:  In terms of a perfectly productive day.

02:15:33,314 --> 02:15:35,742
SPEAKER_1:  for how long of a stretch of time.

02:15:36,610 --> 02:15:39,294
SPEAKER_1:  In one session, do you try to work and focus on a thing?

02:15:39,874 --> 02:15:42,270
SPEAKER_1:  Couple hours is one hours at 30 minutes.

02:15:42,562 --> 02:15:43,198
SPEAKER_1:  minutes.

02:15:43,618 --> 02:15:46,782
SPEAKER_0:  I can probably go like a small few hours and then I need some breaks in between.

02:15:47,234 --> 02:15:48,222
SPEAKER_0:  for like food and stuff.

02:15:48,610 --> 02:15:49,406
SPEAKER_0:  and uh...

02:15:50,434 --> 02:16:00,734
SPEAKER_0:  Yeah, but I think it's still really hard to accumulate hours. I was using a tracker that told me exactly how much time I spent coding any one day. And even on a very productive day, I still spent only like six or eight hours.

02:16:01,666 --> 02:16:03,102
SPEAKER_0:  And it's just because there's so much padding.

02:16:03,586 --> 02:16:05,118
SPEAKER_0:  commute, talking to people.

02:16:06,434 --> 02:16:07,742
SPEAKER_0:  food, et cetera. There's like.

02:16:08,130 --> 02:16:08,830
SPEAKER_0:  cost of life.

02:16:09,378 --> 02:16:14,174
SPEAKER_0:  just living and sustaining and homeostasis and just maintaining yourself as a human.

02:16:14,402 --> 02:16:15,262
SPEAKER_0:  is very high.

02:16:15,906 --> 02:16:18,174
SPEAKER_1:  And there seems to be a desire.

02:16:18,786 --> 02:16:29,310
SPEAKER_1:  within the human mind to participate in society that creates that padding. Because the most productive days I've ever had is just completely from start to finish, just tuning out everything.

02:16:30,114 --> 02:16:30,974
SPEAKER_1:  and just sitting there.

02:16:31,298 --> 02:16:35,390
SPEAKER_1:  And then, and then you could do more than six and eight hours. Yeah, is there some wisdom about.

02:16:35,650 --> 02:16:37,150
SPEAKER_1:  what gives you strength to do like.

02:16:37,410 --> 02:16:37,758
SPEAKER_1:

02:16:38,018 --> 02:16:39,454
SPEAKER_1:  tough days of long focus.

02:16:40,962 --> 02:16:45,087
SPEAKER_0:  Yeah, just like whenever I get obsessed about a problem, something just needs to work, something just needs to exist.

02:16:45,087 --> 02:16:51,230
SPEAKER_1:  It needs to exist so you're able to deal with bugs and programming issues and technical issues.

02:16:51,522 --> 02:16:57,502
SPEAKER_1:  uh, design decisions that turn out to be the wrong ones. You're able to think through all of that given, given that you want to think to exist.

02:16:57,826 --> 02:17:03,806
SPEAKER_0:  Yeah, it needs to exist. And then I think to me also a big factor is, are other humans are going to appreciate it? Are they going to like it?

02:17:04,162 --> 02:17:06,558
SPEAKER_0:  That's a big part of my motivation, if I'm helping humans.

02:17:06,850 --> 02:17:08,862
SPEAKER_0:  and they seem happy, they say nice things.

02:17:09,090 --> 02:17:09,758
SPEAKER_0:  They

02:17:10,402 --> 02:17:13,598
SPEAKER_0:  tweet about it or whatever, that gives me pleasure because I'm doing something useful.

02:17:13,890 --> 02:17:14,846
SPEAKER_1:  So like you do.

02:17:15,234 --> 02:17:20,109
SPEAKER_1:  see yourself sharing it with the world. Like whether it's on GitHub or through blog posts or through videos. Yeah, what's

02:17:20,109 --> 02:17:25,278
SPEAKER_0:  about it. Like suppose I did all these things but did not share them. I don't think I would have the same amount of motivation that I can build up.

02:17:25,538 --> 02:17:28,254
SPEAKER_1:  you enjoy the feeling of other people.

02:17:29,026 --> 02:17:29,534
SPEAKER_1:  Um...

02:17:29,890 --> 02:17:33,278
SPEAKER_1:  gaining value and happiness from the stuff you've created. Yeah.

02:17:34,338 --> 02:17:35,390
SPEAKER_1:  What about diet?

02:17:36,162 --> 02:17:40,350
SPEAKER_1:  Is there, I saw you play with Intermittent Fast, New Fast, does that help? So it does with everything.

02:17:40,578 --> 02:17:43,294
SPEAKER_1:  We played with the things you played was been.

02:17:43,938 --> 02:17:47,198
SPEAKER_1:  most beneficial to your ability to mentally focus on a thing.

02:17:47,554 --> 02:17:50,014
SPEAKER_1:  and just mental productivity and happiness.

02:17:50,818 --> 02:17:51,943
SPEAKER_1:  You still fast? Yeah, so fast.

02:17:51,943 --> 02:17:57,790
SPEAKER_0:  I do intermittent fasting, but really what it means at the end of the day is I skip breakfast.

02:17:58,146 --> 02:18:00,798
SPEAKER_0:  18.6 roughly by default when I'm in my steady state.

02:18:01,154 --> 02:18:05,534
SPEAKER_0:  If I'm traveling or doing something else, I will break the rules. But in my steady state, I do 18.6.

02:18:05,890 --> 02:18:07,454
SPEAKER_0:  So I eat only from 12 to 6.

02:18:08,034 --> 02:18:10,270
SPEAKER_0:  Not a hard rule and I break it often, but that's my default.

02:18:10,882 --> 02:18:14,974
SPEAKER_0:  And then, yeah, I've done a bunch of random experiments, for the most part right now.

02:18:15,266 --> 02:18:18,558
SPEAKER_0:  where I've been for the last year and a half I want to say is I'm.

02:18:18,978 --> 02:18:25,374
SPEAKER_0:  Plant-based or plant-forward. I heard plant-forward, it sounds better. I don't actually know what the difference is, but it sounds better in my mind.

02:18:25,698 --> 02:18:27,966
SPEAKER_0:  but it just means I prefer plant-based food.

02:18:28,322 --> 02:18:31,294
SPEAKER_1:  and raw or cooked or I prefer cooked.

02:18:31,842 --> 02:18:32,766
SPEAKER_1:  and blend paste.

02:18:33,090 --> 02:18:34,206
SPEAKER_1:  So plan based.

02:18:35,170 --> 02:18:36,958
SPEAKER_1:  Forgive me, I don't actually know

02:18:37,314 --> 02:18:40,638
SPEAKER_1:  how wide the category of plan entails.

02:18:40,770 --> 02:18:50,750
SPEAKER_0:  Well, plant based just means that you're not a militant about it and you can flex. And you just prefer to eat plants and you're not making, you're not trying to influence other people.

02:18:50,978 --> 02:18:55,390
SPEAKER_0:  And if you come to someone's house party and they serve you a steak that they're really proud of, you will eat it.

02:18:55,586 --> 02:18:58,910
SPEAKER_1:  Yes. Right. That's beautiful. I mean, that's.

02:18:59,362 --> 02:19:02,494
SPEAKER_1:  I'm the flip side of that, but I'm very flexible.

02:19:02,754 --> 02:19:04,222
SPEAKER_1:  Have you tried doing one meal a day?

02:19:05,026 --> 02:19:05,694
SPEAKER_1:  I have.

02:19:05,890 --> 02:19:18,206
SPEAKER_0:  accidentally, not consistently, but I've accidentally had that. I don't like it. I think it makes me feel not good. It's too much of a hit. Yeah. So currently I have about two meals a day, 12 and 6.

02:19:18,594 --> 02:19:20,702
SPEAKER_1:  I do that nonstop. I'm doing it now.

02:19:20,930 --> 02:19:26,206
SPEAKER_1:  Do it one meal a day. Okay. It's interesting. It's an interesting feeling. Have you ever fasted longer than a day?

02:19:26,594 --> 02:19:29,694
SPEAKER_0:  Yeah, I've done a bunch of water fasts, because I was curious what happens.

02:19:29,858 --> 02:19:31,614
SPEAKER_1:  What happened? Anything interesting?

02:19:31,714 --> 02:19:38,206
SPEAKER_0:  Yeah, I would say so. I mean, you know, what's interesting is that you're hungry for two days and then starting day three or so, you're not hungry.

02:19:39,170 --> 02:19:42,334
SPEAKER_0:  It's like such a weird feeling because you haven't eaten in a few days and you're not hungry

02:19:42,818 --> 02:19:48,926
SPEAKER_1:  Isn't that weird? It's really weird. One of the many weird things about human biology, is figure something out. It finds.

02:19:49,314 --> 02:19:52,382
SPEAKER_1:  Finds another source of energy or something like that. через

02:19:52,706 --> 02:19:53,886
SPEAKER_1:  relaxes the system.

02:19:54,018 --> 02:19:59,710
SPEAKER_0:  I don't know how the body is like you're hungry, you're hungry and then it just gives up. It's like, okay, I guess we're fasting now. There's nothing.

02:19:59,970 --> 02:20:02,942
SPEAKER_0:  And then it just kind of like focuses on trying to make you not hungry.

02:20:03,234 --> 02:20:08,859
SPEAKER_0:  and not feel the damage of that and trying to give you some space to figure out the food situation.

02:20:08,859 --> 02:20:09,342
SPEAKER_1:  Ha!

02:20:09,826 --> 02:20:11,326
SPEAKER_1:  So are you still to this day?

02:20:11,746 --> 02:20:12,862
SPEAKER_1:  Most productive.

02:20:13,154 --> 02:20:13,950
SPEAKER_1:  uh, at night.

02:20:14,722 --> 02:20:18,334
SPEAKER_0:  I would say I am, but it is really hard to maintain my PhD schedule.

02:20:18,786 --> 02:20:19,262
SPEAKER_0:  Um...

02:20:19,714 --> 02:20:22,398
SPEAKER_0:  especially when I was, say, working at Tesla and so on. That's a non-starter.

02:20:22,914 --> 02:20:25,630
SPEAKER_0:  So, but even now, like, you know, people want to meet for...

02:20:26,594 --> 02:20:31,166
SPEAKER_0:  various events, society lives in a certain period of time and you sort of have to like...

02:20:31,458 --> 02:20:36,094
SPEAKER_1:  So it's hard to like do a social thing and then after that return and do work.

02:20:36,514 --> 02:20:37,694
SPEAKER_1:  Yeah, it's just really hard.

02:20:38,178 --> 02:20:39,102
SPEAKER_1:  Ryuva

02:20:39,618 --> 02:20:46,270
SPEAKER_1:  That's why I try when I do social thing that's right not to do too too much drinking so I can return and continue doing work

02:20:47,106 --> 02:20:48,318
SPEAKER_1:  Um, but it.

02:20:48,770 --> 02:20:50,942
SPEAKER_1:  Tesla is there, is there conversions?

02:20:51,234 --> 02:20:55,518
SPEAKER_1:  Tesla, but any company, is there a convergence in what you get out of this?

02:20:56,098 --> 02:20:57,054
SPEAKER_1:  Or is there more?

02:20:58,178 --> 02:21:00,254
SPEAKER_1:  Is that how humans behave when they collaborate?

02:21:00,706 --> 02:21:05,502
SPEAKER_1:  I need to learn about this. Yeah. Do they try to keep us consistent schedule where you're all awake at the same time?

02:21:05,730 --> 02:21:10,142
SPEAKER_0:  I'm going to do try to create a routine, and I try to create a steady state in which I'm...

02:21:10,370 --> 02:21:10,846
SPEAKER_0:  comfortable in.

02:21:11,202 --> 02:21:15,198
SPEAKER_0:  So I have a morning routine, I have a day routine, I try to keep things to a steady state.

02:21:15,554 --> 02:21:16,318
SPEAKER_0:  and um...

02:21:17,122 --> 02:21:20,094
SPEAKER_0:  things are predictable and then you can sort of just like, your body just sort of like.

02:21:20,322 --> 02:21:23,262
SPEAKER_0:  sticks to that and if you try to stress that a little too much it will create a

02:21:23,490 --> 02:21:26,750
SPEAKER_0:  when you're traveling and you're dealing with jet lag, you're not able to really ascend.

02:21:28,578 --> 02:21:29,246
SPEAKER_0:  where you need to go.

02:21:29,730 --> 02:21:33,086
SPEAKER_1:  Yeah, yeah, that's what you're humans with the habits and stuff.

02:21:33,410 --> 02:21:35,454
SPEAKER_1:  What are your thoughts on work-life balance?

02:21:35,970 --> 02:21:37,246
SPEAKER_1:  throughout a human lifetime.

02:21:38,690 --> 02:21:41,182
SPEAKER_1:  So testing part was known for sort of.

02:21:41,698 --> 02:21:43,006
SPEAKER_1:  pushing people to their limits.

02:21:44,162 --> 02:21:46,046
SPEAKER_1:  in terms of what they're able to do, in terms of...

02:21:46,530 --> 02:21:50,366
SPEAKER_1:  what they're trying to do in terms of how much they work, all that kind of stuff.

02:21:50,754 --> 02:21:51,070
SPEAKER_1:  Yeah.

02:21:51,586 --> 02:21:53,854
SPEAKER_0:  I mean, I will say test like it's all too much.

02:21:54,082 --> 02:21:57,502
SPEAKER_0:  bad rep for this because what's happening is Tesla, it's a bursty environment.

02:21:57,922 --> 02:21:58,430
SPEAKER_0:  Also.

02:21:58,754 --> 02:22:05,758
SPEAKER_0:  I would say the baseline, my only point of reference is Google, where I've interned three times and I saw what it's like inside Google and DeepMind.

02:22:06,178 --> 02:22:06,654
SPEAKER_0:  Rights including int.

02:22:06,882 --> 02:22:08,606
SPEAKER_0:  I would say the baseline is higher than that.

02:22:08,866 --> 02:22:12,190
SPEAKER_0:  but then there's a punctuated equilibrium where once in a while there's a fire.

02:22:12,514 --> 02:22:14,334
SPEAKER_0:  and people work really hard.

02:22:14,882 --> 02:22:16,542
SPEAKER_0:  And so it's spiky and bursty.

02:22:16,866 --> 02:22:26,366
SPEAKER_0:  And then all the stories get collected. About the bursts. And then it gives the appearance of like total insanity, but actually it's just a bit more intense environment. And there are fires and sprints.

02:22:27,010 --> 02:22:30,462
SPEAKER_0:  And so I think, definitely though, I would say.

02:22:30,882 --> 02:22:32,757
SPEAKER_0:  It's a more intense environment than something you would get.

02:22:32,757 --> 02:22:34,686
SPEAKER_1:  But you and your person forget all of that.

02:22:34,946 --> 02:22:36,926
SPEAKER_1:  just in your own personal life.

02:22:37,218 --> 02:22:38,558
SPEAKER_1:  What do you think about?

02:22:39,298 --> 02:22:43,230
SPEAKER_1:  the happiness of a human being, a brilliant person like yourself.

02:22:43,906 --> 02:22:44,350
SPEAKER_1:  above.

02:22:44,866 --> 02:22:48,318
SPEAKER_1:  finding a balance between work and life or is it such a thing?

02:22:48,546 --> 02:22:50,590
SPEAKER_1:  not a good thought experiment.

02:22:51,874 --> 02:22:52,382
SPEAKER_1:  Yeah, I think.

02:22:53,538 --> 02:22:58,110
SPEAKER_0:  think balance is good, but I also love to have sprints that are out of distribution.

02:22:58,690 --> 02:23:00,286
SPEAKER_0:  And that's when I think I've been pretty...

02:23:00,578 --> 02:23:01,054
SPEAKER_0:  Uh...

02:23:01,346 --> 02:23:02,750
SPEAKER_0:  creative and.

02:23:03,458 --> 02:23:03,838
SPEAKER_0:  as well.

02:23:04,258 --> 02:23:07,422
SPEAKER_1:  Sprints out of distribution means that most of the time

02:23:08,098 --> 02:23:09,470
SPEAKER_1:  You have a...

02:23:10,210 --> 02:23:12,071
SPEAKER_1:  quote unquote, balance. A balance

02:23:12,071 --> 02:23:15,390
SPEAKER_0:  most of the time. I like being obsessed with something once in a while.

02:23:15,938 --> 02:23:17,813
SPEAKER_1:  Once in a while is what? Once a week, once a month?

02:23:17,813 --> 02:23:20,254
SPEAKER_0:  once a year? Yeah probably like say once a month or something yeah.

02:23:20,546 --> 02:23:22,622
SPEAKER_1:  And that's when we get a new GitHub repo.

02:23:22,978 --> 02:23:26,750
SPEAKER_0:  That's when you like really care about a problem it must exist this will be awesome

02:23:27,010 --> 02:23:27,902
SPEAKER_0:  You're obsessed with it.

02:23:28,258 --> 02:23:34,110
SPEAKER_0:  and now you can't just do it on that day. You need to pay the fixed cost of getting into the groove and then you need to stay there for a while.

02:23:34,338 --> 02:23:41,310
SPEAKER_0:  and then society will come and they will try to mess with you and they will try to distract you. Yeah, the worst thing is like a person who's like, I just need five minutes of your time.

02:23:42,434 --> 02:23:42,910
SPEAKER_0:  This is.

02:23:43,490 --> 02:23:47,966
SPEAKER_0:  The cost of that is not five minutes. And society needs to change how it thinks about.

02:23:48,546 --> 02:23:49,662
SPEAKER_0:  Just five minutes of your time.

02:23:50,882 --> 02:23:56,318
SPEAKER_1:  It's never it's never just one minute. Just just 30. Just a quick deal. Why are you being so? Yeah.

02:23:56,578 --> 02:23:56,990
SPEAKER_1:  No.

02:23:58,306 --> 02:24:00,126
SPEAKER_1:  What's your computer setup?

02:24:00,962 --> 02:24:06,174
SPEAKER_1:  What was like the perfect G I use somebody that's flexible to no matter what laptop.

02:24:06,850 --> 02:24:07,742
SPEAKER_1:  screens.

02:24:08,066 --> 02:24:12,191
SPEAKER_1:  Yeah. Or do you prefer a certain setup that you.

02:24:12,191 --> 02:24:13,310
SPEAKER_0:  most productive.

02:24:13,698 --> 02:24:15,742
SPEAKER_0:  I guess the one that I'm familiar with is one.

02:24:15,970 --> 02:24:16,606
SPEAKER_0:  large screen.

02:24:16,834 --> 02:24:17,726
SPEAKER_0:  27 inch

02:24:18,146 --> 02:24:18,526
SPEAKER_0:  Um.

02:24:18,786 --> 02:24:21,310
SPEAKER_0:  and my laptop on the side. What operating system?

02:24:21,730 --> 02:24:23,038
SPEAKER_0:  I do max. That's my primary.

02:24:23,714 --> 02:24:24,862
SPEAKER_0:  for all tasks.

02:24:25,250 --> 02:24:30,206
SPEAKER_0:  I would say OS X, but when you're working on deep learning, everything is Linux. You're SSH'd into a cluster and you're working remotely.

02:24:30,882 --> 02:24:33,534
SPEAKER_1:  But what about the actual development like that using the IDE?

02:24:33,826 --> 02:24:37,214
SPEAKER_0:  You would use, I think a good way is you just run VS code.

02:24:37,698 --> 02:24:38,078
SPEAKER_0:  Um.

02:24:38,338 --> 02:24:39,998
SPEAKER_0:  My favorite editor right now on your Mac.

02:24:40,322 --> 02:24:43,294
SPEAKER_0:  but you have a remote folder through SSH.

02:24:43,554 --> 02:24:43,998
SPEAKER_0:  Um.

02:24:44,386 --> 02:24:47,390
SPEAKER_0:  So the actual files that you're manipulating are in the cluster somewhere else.

02:24:47,490 --> 02:24:49,598
SPEAKER_1:  So what's the best IDE?

02:24:50,594 --> 02:24:51,998
SPEAKER_1:  uh... vscode

02:24:52,290 --> 02:24:54,334
SPEAKER_1:  What else do people, so I use Emacs.

02:24:54,626 --> 02:24:57,310
SPEAKER_1:  Let's go.

02:24:57,538 --> 02:25:00,542
SPEAKER_1:  It may be cool. I don't know if it's maximum productivity.

02:25:00,834 --> 02:25:07,070
SPEAKER_1:  So what do you recommend in terms of editors? You work with a lot of software engineers, editors for...

02:25:08,066 --> 02:25:10,750
SPEAKER_1:  Python C++ machine learning applications.

02:25:11,330 --> 02:25:12,926
SPEAKER_0:  I think the current answer is VS code.

02:25:13,538 --> 02:25:15,454
SPEAKER_0:  Currently, I believe that's the best.

02:25:16,098 --> 02:25:20,254
SPEAKER_0:  IDE. It's got a huge amount of extensions. It has GitHub Copilot.

02:25:20,898 --> 02:25:21,854
SPEAKER_0:  integration.

02:25:22,082 --> 02:25:23,230
SPEAKER_0:  which I think is very valuable.

02:25:23,362 --> 02:25:29,534
SPEAKER_1:  What do you think about the copilot integration? I was actually, I got to talk a bunch with Guido Nrasim, and he, he was a criticize, he, it was not much of a copilot,

02:25:29,890 --> 02:25:34,174
SPEAKER_1:  Python and he loves Copa. He like programs a lot with it.

02:25:35,074 --> 02:25:35,806
SPEAKER_1:  Uh, do you?

02:25:36,642 --> 02:25:37,662
SPEAKER_0:  Yeah, use Copilot, I love it.

02:25:37,890 --> 02:25:40,158
SPEAKER_0:  and it's free for me, but I will pay for it.

02:25:40,738 --> 02:25:44,190
SPEAKER_0:  Yeah, I think it's very good. And the utility that I found with it was, is in, is in.

02:25:44,450 --> 02:25:46,462
SPEAKER_0:  I would say there's a learning curve and you need to...

02:25:46,978 --> 02:25:52,542
SPEAKER_0:  figure out when it's helpful and when to pay attention to its outputs and when it's not going to be helpful where you should not pay attention to it.

02:25:53,058 --> 02:25:56,318
SPEAKER_0:  Because if you're just reading at suggestions all the time, it's not a good way of interacting with it.

02:25:56,706 --> 02:25:58,590
SPEAKER_0:  But I think I was able to mold myself to it.

02:25:58,978 --> 02:26:03,358
SPEAKER_0:  I find it's very helpful, number one, in copy, paste, and replace some parts. Hello.

02:26:03,650 --> 02:26:04,318
SPEAKER_0:  I don't, um.

02:26:04,898 --> 02:26:07,166
SPEAKER_0:  The pattern is clear. It's really good at completing the pattern.

02:26:07,810 --> 02:26:11,006
SPEAKER_0:  And number two, sometimes it suggests APIs that I'm not aware of.

02:26:11,362 --> 02:26:13,726
SPEAKER_0:  So it tells you about something that you didn't know.

02:26:14,114 --> 02:26:29,543
SPEAKER_0:  And that's an opportunity to discover a new idea. It's an opportunity to see. I would never take Copilot code as given. I almost always copy paste into a Google search, and you see what this function is doing. And then you're like, oh, it's actually exactly what I need. Thank you, Copilot. So you learn something. So it's in part a search.

02:26:29,543 --> 02:26:30,526
SPEAKER_1:  engine apart.

02:26:31,074 --> 02:26:35,038
SPEAKER_1:  maybe getting the exact syntax correctly that once you see it.

02:26:35,522 --> 02:26:38,846
SPEAKER_1:  It's that NP-hard thing. Once you see it, you know.

02:26:38,946 --> 02:26:42,238
SPEAKER_0:  Yes, exactly. It's correct what you yourself struggle with. You can verify.

02:26:42,338 --> 02:26:44,963
SPEAKER_1:  You can verify efficiently, but you can't generate.

02:26:44,963 --> 02:26:48,574
SPEAKER_0:  And Copilot really, I mean, it's autopilot for programming.

02:26:49,570 --> 02:26:53,854
SPEAKER_0:  and currently is doing the link following, which is like the simple copy paste and sometimes suggest.

02:26:54,306 --> 02:26:56,542
SPEAKER_0:  but over time it's going to become more and more autonomous.

02:26:57,122 --> 02:27:02,142
SPEAKER_0:  And so the same thing will play out in not just coding, but actually across many, many different things probably.

02:27:02,434 --> 02:27:05,630
SPEAKER_1:  Coding is an important one, right? Writing programs.

02:27:06,050 --> 02:27:07,966
SPEAKER_1:  How do you see the future of that developing?

02:27:08,290 --> 02:27:13,950
SPEAKER_1:  the program synthesis, like being able to write programs that are more and more complicated. because right now

02:27:14,434 --> 02:27:15,966
SPEAKER_1:  It's human supervised.

02:27:16,450 --> 02:27:18,782
SPEAKER_1:  in interesting ways.

02:27:19,106 --> 02:27:21,118
SPEAKER_1:  It feels like the transition will be very painful.

02:27:21,986 --> 02:27:25,374
SPEAKER_0:  My mental model for it is the same thing will happen as with the autopilot.

02:27:26,018 --> 02:27:33,086
SPEAKER_0:  So currently he's doing lean following, he's doing some simple stuff. And eventually we'll be doing autonomy and people will have to intervene less and less.

02:27:33,186 --> 02:27:34,910
SPEAKER_1:  And those could be like, like.

02:27:35,362 --> 02:27:36,702
SPEAKER_1:  testing mechanisms.

02:27:37,474 --> 02:27:40,958
SPEAKER_1:  Like if it writes a function and that function looks pretty damn correct.

02:27:41,442 --> 02:27:42,622
SPEAKER_1:  But how do you know it's correct?

02:27:43,138 --> 02:27:48,382
SPEAKER_1:  because you're getting lazier and lazier as a programmer. Like your ability to, cause like little bugs.

02:27:48,706 --> 02:27:49,726
SPEAKER_1:  But I guess it won't make-

02:27:50,402 --> 02:27:55,646
SPEAKER_0:  No, it will. Copilot will make off by one subtle bugs. It has done that to me.

02:27:55,842 --> 02:27:57,470
SPEAKER_1:  But do you think future systems will?

02:27:57,858 --> 02:28:02,462
SPEAKER_1:  Or is it really the off by one is actually a fundamental challenger programming.

02:28:02,818 --> 02:28:06,334
SPEAKER_0:  In that case, it wasn't fundamental and I think things can improve, but...

02:28:06,850 --> 02:28:10,782
SPEAKER_0:  Yeah, I think humans have to supervise. I am nervous about people not supervising what comes out.

02:28:11,170 --> 02:28:14,782
SPEAKER_0:  and what happens to, for example, the proliferation of bugs in all of our systems.

02:28:15,394 --> 02:28:16,798
SPEAKER_0:  I'm nervous about that, but I think...

02:28:17,282 --> 02:28:21,022
SPEAKER_0:  There will probably be some other copilots for bug finding and stuff like that at some point.

02:28:21,314 --> 02:28:23,189
SPEAKER_0:  There'll be like a lot more automation for

02:28:23,189 --> 02:28:24,382
SPEAKER_1:  Oh man.

02:28:24,834 --> 02:28:27,390
SPEAKER_1:  It's like a program.

02:28:27,746 --> 02:28:30,110
SPEAKER_1:  a copilot that generates a compiler.

02:28:30,946 --> 02:28:35,678
SPEAKER_1:  one that does a linter, one that does like a type checker.

02:28:36,098 --> 02:28:36,702
SPEAKER_1:  Porukka card

02:28:36,962 --> 02:28:40,259
SPEAKER_0:  It's a committee of like a GPT sort of like...

02:28:40,259 --> 02:28:42,494
SPEAKER_1:  And then they'll be like a manager for the committee.

02:28:42,946 --> 02:28:46,846
SPEAKER_1:  And then there'll be somebody that says a new version of this is needed. We need to regenerate it.

02:28:47,522 --> 02:28:52,734
SPEAKER_0:  There were 10 GPTs, they were forwarded and gave 50 suggestions. Another one looked at it and picked the few that they liked.

02:28:53,026 --> 02:28:55,390
SPEAKER_0:  A bug one looked at it and it was like it's probably a bug.

02:28:55,682 --> 02:28:57,150
SPEAKER_0:  They got re-ranked by some other thing.

02:28:57,378 --> 02:28:59,102
SPEAKER_0:  and then a final ensemble.

02:28:59,394 --> 02:29:02,910
SPEAKER_0:  GPT comes in and is like, okay, given everything you guys have told me, this is probably the next token.

02:29:03,330 --> 02:29:03,998
SPEAKER_0:  Hahaha

02:29:04,130 --> 02:29:13,342
SPEAKER_1:  You know, the feeling is the number of programmers in the world has been growing and growing very quickly. Do you think it's possible that it'll actually level out and drop to like a very low number?

02:29:13,698 --> 02:29:15,614
SPEAKER_1:  with this kind of world, because then you'd be doing

02:29:15,906 --> 02:29:17,598
SPEAKER_1:  Software 2.0 programming.

02:29:18,434 --> 02:29:21,086
SPEAKER_1:  and you'll be doing this kind of.

02:29:21,762 --> 02:29:24,894
SPEAKER_1:  generation of copolytype systems programming.

02:29:25,218 --> 02:29:26,814
SPEAKER_1:  But you won't be doing the old school.

02:29:27,554 --> 02:29:29,470
SPEAKER_1:  software 1.0 program.

02:29:29,922 --> 02:29:32,958
SPEAKER_0:  I don't currently think that they're just going to replace human programmers.

02:29:33,218 --> 02:29:33,630
SPEAKER_0:  Um.

02:29:34,818 --> 02:29:38,943
SPEAKER_0:  I'm so hesitant saying stuff like this, right? Because this is going to be a-

02:29:38,943 --> 02:29:45,470
SPEAKER_1:  We're playing five years and it's going to show that like this is where we thought because I agree with you, but.

02:29:46,018 --> 02:29:47,710
SPEAKER_1:  I think we might be very surprised.

02:29:48,514 --> 02:29:49,374
SPEAKER_1:  Right? Like.

02:29:50,114 --> 02:29:51,166
SPEAKER_1:  What are the next?

02:29:52,482 --> 02:29:57,246
SPEAKER_1:  What's your sense of where we stand with language models? Does it feel like the beginning or the middle?

02:29:57,474 --> 02:29:59,102
SPEAKER_0:  or the end. 100%

02:29:59,490 --> 02:30:05,406
SPEAKER_0:  I think the big question in my mind is, for sure, GPT will be able to program quite well, competently and so on. How do you steer the system?

02:30:05,826 --> 02:30:08,766
SPEAKER_0:  you still have to provide some guidance to what you actually are looking for.

02:30:09,314 --> 02:30:11,870
SPEAKER_0:  And so how do you steer it and how do you say, how do you.

02:30:12,354 --> 02:30:13,822
SPEAKER_0:  to it? How do you um

02:30:14,786 --> 02:30:18,174
SPEAKER_0:  audit it and verify that what is done is correct. And how do you work with this?

02:30:18,466 --> 02:30:21,758
SPEAKER_0:  And it's as much not just an AI problem, but a UI UX problem.

02:30:22,434 --> 02:30:22,846
SPEAKER_0:  Um.

02:30:23,490 --> 02:30:26,686
SPEAKER_0:  So beautiful fertile ground for so much interesting work.

02:30:27,042 --> 02:30:28,894
SPEAKER_0:  for VS Code++ where

02:30:29,282 --> 02:30:31,678
SPEAKER_0:  You're not just, it's not just human programming anymore. It's amazing.

02:30:31,810 --> 02:30:35,070
SPEAKER_1:  Yeah, so you're interacting with the system, so not just one prompt.

02:30:35,746 --> 02:30:36,990
SPEAKER_1:  but it's iterative prompting.

02:30:37,282 --> 02:30:40,190
SPEAKER_1:  You're trying to figure out having a conversation with the system. Yeah.

02:30:40,418 --> 02:30:42,334
SPEAKER_1:  That actually, I mean to me that's super exciting.

02:30:42,818 --> 02:30:45,054
SPEAKER_1:  to have a conversation with the program I'm writing.

02:30:45,858 --> 02:30:51,070
SPEAKER_0:  Yeah, maybe at some point you're just conversing with it. It's like, OK, here's what I want to do. Actually, this variable.

02:30:51,810 --> 02:30:53,822
SPEAKER_0:  Maybe it's not even that low level as variable, but.

02:30:54,114 --> 02:30:55,358
SPEAKER_1:  You can also imagine like.

02:30:56,226 --> 02:30:58,851
SPEAKER_1:  Can you translate this to C++ and back to Python?

02:30:58,851 --> 02:31:01,101
SPEAKER_0:  You have an already kind of existence.

02:31:01,101 --> 02:31:04,030
SPEAKER_1:  like doing it as part of the program experience.

02:31:04,418 --> 02:31:07,038
SPEAKER_1:  I think I'd like to write this function in C++.

02:31:07,458 --> 02:31:15,870
SPEAKER_1:  Or like you just keep changing for different programs because they have different syntax. Maybe I want to convert this into a functional language. But most Koreans always just say,

02:31:16,226 --> 02:31:16,926
SPEAKER_1:  So like.

02:31:17,154 --> 02:31:19,070
SPEAKER_1:  you get to become multilingual.

02:31:19,298 --> 02:31:20,222
SPEAKER_1:  as a programmer.

02:31:20,578 --> 02:31:22,590
SPEAKER_1:  and dance back and forth efficiently. yeah

02:31:23,074 --> 02:31:29,022
SPEAKER_0:  I mean, I think the UI UX of it though is like still very hard to think through because it's not just about writing code on a page.

02:31:29,538 --> 02:31:37,918
SPEAKER_0:  You have an entire developer environment. You have a bunch of hardware on it. You have some environmental variables. You have some scripts that are running in a Chrome job. Like there's a lot going on to like.

02:31:38,274 --> 02:31:39,230
SPEAKER_0:  working with computers.

02:31:39,458 --> 02:31:41,470
SPEAKER_0:  and how do these systems.

02:31:42,146 --> 02:31:46,878
SPEAKER_0:  set up environment flags and work across multiple machines and set up screen sessions and automate.

02:31:47,170 --> 02:31:48,030
SPEAKER_0:  different processes like.

02:31:48,290 --> 02:31:51,390
SPEAKER_0:  how all that works and is auditable by humans and so on is like.

02:31:51,714 --> 02:31:52,702
SPEAKER_0:  Massive question at the moment.

02:31:53,314 --> 02:31:55,326
SPEAKER_1:  You've built archive sanity.

02:31:56,002 --> 02:31:56,958
SPEAKER_1:  What is our cut?

02:31:57,250 --> 02:31:59,998
SPEAKER_1:  and what is the future of academic research publishing?

02:32:00,322 --> 02:32:01,374
SPEAKER_1:  that you would like to see.

02:32:01,858 --> 02:32:04,670
SPEAKER_0:  So archive is this pre-print server. So if you have a paper.

02:32:05,026 --> 02:32:10,526
SPEAKER_0:  you can submit it for publication to journals or conferences and then wait six months and then maybe get a decision pass or fail.

02:32:10,914 --> 02:32:15,550
SPEAKER_0:  Or you can just upload it to archive. And then people can tweet about it three minutes later.

02:32:15,906 --> 02:32:20,135
SPEAKER_0:  And then everyone sees it, everyone reads it, and everyone can profit from it in their own way.

02:32:20,135 --> 02:32:21,534
SPEAKER_1:  You can cite it.

02:32:22,114 --> 02:32:24,670
SPEAKER_1:  and it has an official look to it. It feels like a pump.

02:32:25,154 --> 02:32:29,886
SPEAKER_1:  Like it feels like a publication process. It feels different than if you just put in a blog post.

02:32:30,370 --> 02:32:35,358
SPEAKER_0:  Oh yeah, yeah, I mean it's a paper and usually the bar is higher for something that you would expect on...

02:32:35,618 --> 02:32:38,243
SPEAKER_0:  archive as opposed to something you would see in a blog post.

02:32:38,243 --> 02:32:39,070
SPEAKER_1:  the culture.

02:32:39,618 --> 02:32:40,574
SPEAKER_1:  created the bar.

02:32:40,994 --> 02:32:44,830
SPEAKER_1:  because you could probably post a pretty crappy picture in the archive.

02:32:45,506 --> 02:32:48,798
SPEAKER_1:  So what's that make you feel like? What's that make you feel about peer review?

02:32:49,058 --> 02:32:50,942
SPEAKER_1:  So rigorous peer review.

02:32:51,202 --> 02:32:54,750
SPEAKER_1:  by two, three experts versus the peer review of.

02:32:55,426 --> 02:32:56,350
SPEAKER_1:  the community.

02:32:56,802 --> 02:32:58,078
SPEAKER_1:  Right as it's written. Yeah.

02:32:58,306 --> 02:33:02,174
SPEAKER_0:  Basically, I think the community is very, well, able to peer review things very quickly.

02:33:02,530 --> 02:33:02,974
SPEAKER_0:  on Twitter.

02:33:03,234 --> 02:33:03,550
SPEAKER_0:  You

02:33:03,938 --> 02:33:07,742
SPEAKER_0:  And I think maybe it just has to do something with AI machine learning field specifically though.

02:33:08,002 --> 02:33:13,246
SPEAKER_0:  I feel like things are more easily auditable, and the verification is easier.

02:33:13,602 --> 02:33:15,262
SPEAKER_0:  potentially than the verification somewhere else.

02:33:15,682 --> 02:33:16,606
SPEAKER_0:  So it's kind of like...

02:33:17,058 --> 02:33:31,134
SPEAKER_0:  You can think of these scientific publications as like little blockchains where everyone's building on each other's work and citing each other. And you sort of have AI, which is kind of like this much faster and loose blockchain, but then you have and any one individual entry is like very.

02:33:31,426 --> 02:33:32,222
SPEAKER_0:  Very cheap to make.

02:33:32,482 --> 02:33:35,198
SPEAKER_0:  And then you have other fields where maybe that model doesn't make as much sense.

02:33:35,618 --> 02:33:35,998
SPEAKER_0:  Um.

02:33:36,674 --> 02:33:44,318
SPEAKER_0:  And so I think in AI, at least things are pretty easily verifiable. And so that's why when people upload papers, they're a really good idea and so on. People can try it out.

02:33:44,642 --> 02:33:45,438
SPEAKER_0:  Like the next day.

02:33:45,858 --> 02:33:48,734
SPEAKER_0:  and they can be the final arbiter of whether it works or not on their problem.

02:33:49,058 --> 02:33:51,134
SPEAKER_0:  and the whole thing just moves significantly faster.

02:33:51,554 --> 02:33:52,542
SPEAKER_0:  So I kind of feel like.

02:33:52,802 --> 02:33:57,726
SPEAKER_0:  Academia still has a place, sorry, this like conference journal process still has a place, but it's sort of like an

02:33:58,018 --> 02:34:02,590
SPEAKER_0:  it lags behind, I think, and it's a bit more, maybe higher quality process.

02:34:02,946 --> 02:34:06,750
SPEAKER_0:  But it's not sort of the place where you will discover cutting edge work anymore.

02:34:07,074 --> 02:34:12,318
SPEAKER_0:  It used to be the case when I was starting my PhD that you go to conferences and journals and you discuss all the latest research.

02:34:12,546 --> 02:34:17,918
SPEAKER_0:  Now when you go to a conference or a journal, no one discusses anything that's there because it's already like three generations ago.

02:34:18,690 --> 02:34:19,815
SPEAKER_0:  Irrelevant.

02:34:19,815 --> 02:34:30,014
SPEAKER_1:  Which makes me sad about like DeepMind for example where they still publish in nature these big prestigious I mean there's still value I suppose to the prestige that comes with these big

02:34:30,306 --> 02:34:31,038
SPEAKER_1:  Venues but.

02:34:31,746 --> 02:34:35,550
SPEAKER_1:  The result is that they will announce some breakthrough performance.

02:34:36,034 --> 02:34:37,470
SPEAKER_1:  and it will take like a year.

02:34:37,858 --> 02:34:39,934
SPEAKER_1:  to actually publish the details. the

02:34:40,706 --> 02:34:42,270
SPEAKER_1:  and those details in.

02:34:42,658 --> 02:34:45,182
SPEAKER_1:  if they were published immediately it would inspire the community.

02:34:45,506 --> 02:34:47,381
SPEAKER_1:  to move in certain directions with that. This is what you do with pictures.

02:34:47,381 --> 02:34:51,966
SPEAKER_0:  out the rest of the community but I don't know to what extent that's part of their objective function also.

02:34:52,194 --> 02:34:52,670
SPEAKER_1:  That's true.

02:34:53,026 --> 02:34:56,606
SPEAKER_1:  So it's not just the prestige, a little bit of the delay is part of it.

02:34:56,994 --> 02:34:59,422
SPEAKER_0:  Yeah, they certainly deep mind specifically has been.

02:35:00,034 --> 02:35:01,598
SPEAKER_0:  working in the regime of having.

02:35:01,890 --> 02:35:06,558
SPEAKER_0:  slightly higher quality basically process and latency and publishing those papers that way.

02:35:07,202 --> 02:35:08,542
SPEAKER_1:  Another question from Reddit.

02:35:09,250 --> 02:35:12,190
SPEAKER_1:  Do you or have you suffered from imposter syndrome?

02:35:12,418 --> 02:35:14,462
SPEAKER_1:  being the director of AI Tesla.

02:35:15,074 --> 02:35:24,734
SPEAKER_1:  being this person when you're at Stanford, where the world looks at you as the expert in AI to teach the world about machine learning.

02:35:25,474 --> 02:35:28,926
SPEAKER_0:  When I was leaving Tesla after five years, I spent a ton of time in meeting rooms.

02:35:29,250 --> 02:35:37,150
SPEAKER_0:  uh... and you know i would read papers in the beginning when i joined us i was writing code and then i was writing less and less code and i was reading code and reading less and less code

02:35:37,730 --> 02:35:42,494
SPEAKER_0:  And so this is just a natural progression that happens, I think. And definitely, I would say, near the tail end.

02:35:42,722 --> 02:35:46,110
SPEAKER_0:  that's when it sort of like starts to hit you a bit more that you're supposed to be an expert, but actually

02:35:46,914 --> 02:35:51,230
SPEAKER_0:  source of truth is the code that people are writing, the GitHub and the actual code itself.

02:35:51,778 --> 02:35:53,758
SPEAKER_0:  And you're not as familiar with that as you used to be.

02:35:54,402 --> 02:35:56,958
SPEAKER_0:  And so I would say maybe there's some like insecurity there.

02:35:57,218 --> 02:35:58,686
SPEAKER_1:  Yeah, that's actually pretty profound.

02:35:59,074 --> 02:36:05,959
SPEAKER_1:  That a lot of the insecurity has to do with not writing the code in the computer science space like that because that is the truth that they're right there.

02:36:05,959 --> 02:36:09,566
SPEAKER_0:  Code is the source of truth, the papers and everything else. It's a high level summary.

02:36:09,922 --> 02:36:10,782
SPEAKER_0:  I don't...

02:36:11,362 --> 02:36:15,998
SPEAKER_0:  Yeah, just a high level summary, but at the end of the day, you have to read code. It's impossible to translate all that code into actual.

02:36:16,354 --> 02:36:17,982
SPEAKER_0:  you know, paper form.

02:36:18,402 --> 02:36:19,006
SPEAKER_0:  ASSO.

02:36:19,298 --> 02:36:22,622
SPEAKER_0:  when things come out, especially when they have a source code available, that's my favorite place to go.

02:36:23,202 --> 02:36:26,462
SPEAKER_1:  So like I said, you're one of the greatest teachers of machine learning.

02:36:26,690 --> 02:36:27,870
SPEAKER_1:  AI ever.

02:36:28,514 --> 02:36:31,358
SPEAKER_1:  from CS231n to today.

02:36:31,746 --> 02:36:35,294
SPEAKER_1:  What advice would you give to beginners interested in getting into machine learning?

02:36:36,514 --> 02:36:39,134
SPEAKER_0:  Beginners are often focused on like...

02:36:39,682 --> 02:36:42,974
SPEAKER_0:  what to do. And I think the focus should be more like how much you do.

02:36:43,426 --> 02:36:47,582
SPEAKER_0:  So I am kind of like believer on a high level in this 10,000 hours kind of concept where.

02:36:48,226 --> 02:36:53,982
SPEAKER_0:  You just kind of have to just pick the things where you can spend time and you care about and you're interested in. You literally have to put in 10,000 hours of work.

02:36:54,306 --> 02:36:54,782
SPEAKER_0:  Um.

02:36:55,074 --> 02:37:03,166
SPEAKER_0:  It doesn't even like matter as much like where you put it and you'll iterate and you'll improve and you'll waste some time. I don't know if there's a better way. You need to put in 10,000 hours.

02:37:03,618 --> 02:37:06,814
SPEAKER_0:  But I think it's actually really nice because I feel like there's some sense of determinism about.

02:37:07,234 --> 02:37:12,222
SPEAKER_0:  being an expert at a thing if you spend 10,000 hours. You can literally pick an arbitrary thing.

02:37:12,546 --> 02:37:16,894
SPEAKER_0:  And I think if you spend 10,000 hours of deliberate effort and work, you actually will become an expert at it.

02:37:17,762 --> 02:37:19,262
SPEAKER_0:  And so I think it's kind of like a...

02:37:19,618 --> 02:37:20,990
SPEAKER_0:  Nice thought.

02:37:21,506 --> 02:37:22,558
SPEAKER_0:  And so.

02:37:23,010 --> 02:37:25,822
SPEAKER_0:  Basically, I would focus more on like, are you spending 10,000 hours?

02:37:26,242 --> 02:37:27,367
SPEAKER_0:  That's what I focus on.

02:37:27,367 --> 02:37:33,406
SPEAKER_1:  So and then thinking about what kind of mechanisms maximize your likelihood of getting to 10,000 hours. So and then thinking about what kind of mechanisms maximize your likelihood of getting to 10,000

02:37:33,794 --> 02:37:34,334
SPEAKER_1:  for us.

02:37:34,754 --> 02:37:37,630
SPEAKER_1:  Silly humans means probably forming a daily habit.

02:37:38,114 --> 02:37:39,989
SPEAKER_1:  of like every single day actually doing the thing.

02:37:39,989 --> 02:37:40,894
SPEAKER_0:  Whatever helps you.

02:37:41,186 --> 02:37:47,902
SPEAKER_0:  So I do think to a large extent, it's a psychological problem for yourself. One other thing that I think is helpful for the psychology of it.

02:37:48,130 --> 02:37:51,806
SPEAKER_0:  As many times people compare themselves to others in the area, I think this is very harmful.

02:37:52,290 --> 02:37:55,742
SPEAKER_0:  only compare yourself to you from some time ago, like say a year ago.

02:37:56,130 --> 02:37:58,910
SPEAKER_0:  Are you better than you a year ago? Is the only way to think.

02:37:59,426 --> 02:37:59,870
SPEAKER_0:  Um.

02:38:00,162 --> 02:38:03,102
SPEAKER_0:  And I think then you can see your progress, and that's very motivating.

02:38:03,458 --> 02:38:07,006
SPEAKER_1:  That's so interesting that focus on the quantity of hours.

02:38:07,362 --> 02:38:08,702
SPEAKER_1:  I think a lot of people...

02:38:09,026 --> 02:38:10,942
SPEAKER_1:  in the beginner stage, but actually throughout.

02:38:11,330 --> 02:38:12,478
SPEAKER_1:  get paralyzed.

02:38:13,250 --> 02:38:16,254
SPEAKER_1:  by the choice, like which one.

02:38:16,642 --> 02:38:17,790
SPEAKER_1:  Do I pick this?

02:38:18,146 --> 02:38:22,325
SPEAKER_1:  path or this path. Yeah. I don't know if you get paralyzed by like which IDE to use.

02:38:22,325 --> 02:38:25,342
SPEAKER_0:  Well, they're worried. Yeah, they're worried about all these things, but the thing is

02:38:25,762 --> 02:38:31,166
SPEAKER_0:  some of the you will waste time doing something wrong. Yes. You will eventually figure out it's not right. You will accumulate scar tissue.

02:38:31,650 --> 02:38:37,502
SPEAKER_0:  And next time you'll grow stronger because next time you'll have the scar tissue and next time you'll learn from it. And now next time you-

02:38:37,762 --> 02:38:39,742
SPEAKER_0:  come to a similar situation you'll be like oh I...

02:38:40,770 --> 02:38:41,278
SPEAKER_0:  I messed up.

02:38:41,634 --> 02:38:44,862
SPEAKER_0:  I've spent a lot of time working on things that never materialize into anything.

02:38:45,186 --> 02:38:50,046
SPEAKER_0:  And I have all that scar tissue and I have some intuitions about what was useful, what wasn't useful, how things turned out.

02:38:50,498 --> 02:38:53,598
SPEAKER_0:  So all those mistakes were not dead work, you know?

02:38:53,986 --> 02:38:56,094
SPEAKER_0:  So I just think you should just focus on working.

02:38:56,610 --> 02:38:57,310
SPEAKER_0:  What have you done?

02:38:57,666 --> 02:38:58,462
SPEAKER_0:  What have you done last week?

02:38:58,562 --> 02:38:59,102
SPEAKER_1:  Hahaha

02:39:00,034 --> 02:39:04,638
SPEAKER_1:  That's a good question actually to ask for a lot of things that just machine learning.

02:39:04,994 --> 02:39:07,710
SPEAKER_1:  Um, it's a good way to cut the.

02:39:08,258 --> 02:39:11,358
SPEAKER_1:  I forgot what the term we used but the fluff, the blubber, whatever the...

02:39:11,938 --> 02:39:14,206
SPEAKER_1:  the inefficiencies in life.

02:39:14,658 --> 02:39:16,574
SPEAKER_1:  What do you love about teaching?

02:39:17,154 --> 02:39:18,910
SPEAKER_1:  You seem to find yourself.

02:39:19,650 --> 02:39:23,541
SPEAKER_1:  often draw to teaching. You're very good at it, but you're also drawn to it.

02:39:23,541 --> 02:39:26,782
SPEAKER_0:  I mean, I don't think I love teaching. I love happy humans.

02:39:27,106 --> 02:39:35,294
SPEAKER_0:  And happy humans like when I teach. I wouldn't say I hate teaching, I tolerate teaching, but it's not like the act of teaching that I like.

02:39:35,522 --> 02:39:36,478
SPEAKER_0:  It's that.

02:39:36,898 --> 02:39:38,302
SPEAKER_0:  you know, I have some.

02:39:38,530 --> 02:39:46,942
SPEAKER_0:  I have something, I'm actually okay at it. I'm okay at teaching and people appreciate it a lot. And so I'm just happy to try to be helpful.

02:39:47,202 --> 02:39:59,966
SPEAKER_0:  And teaching itself is not like the most, I mean, it can be really annoying, frustrating. I was working on a bunch of lectures just now. I was reminded back to my days of 231 and just how much work it is to create some of these materials to make them good.

02:40:00,418 --> 02:40:04,318
SPEAKER_0:  the amount of iteration and thought, and you go down blind alleys and just how much you change it.

02:40:04,866 --> 02:40:06,014
SPEAKER_0:  So creating something good.

02:40:06,434 --> 02:40:06,846
SPEAKER_0:  Um...

02:40:07,266 --> 02:40:09,086
SPEAKER_0:  in terms of educational values really hard.

02:40:09,730 --> 02:40:10,878
SPEAKER_0:  And it's not fun.

02:40:11,522 --> 02:40:15,262
SPEAKER_1:  It was difficult so people should definitely go watch in new stuff

02:40:15,522 --> 02:40:15,902
SPEAKER_1:

02:40:16,514 --> 02:40:20,542
SPEAKER_1:  There are lectures where you're actually building the thing, like from, like you said, the code is truth.

02:40:20,866 --> 02:40:21,822
SPEAKER_1:  So discussing.

02:40:22,210 --> 02:40:29,566
SPEAKER_1:  back propagation by building it by looking through and just the whole thing. So how difficult is that to prepare for? I think it's a really powerful way to teach.

02:40:30,082 --> 02:40:34,014
SPEAKER_1:  Did you have to prepare for that or are you just live thinking through it?

02:40:34,498 --> 02:40:38,174
SPEAKER_0:  I will typically do like say three takes and then I take like the better take.

02:40:38,530 --> 02:40:42,430
SPEAKER_0:  So I do multiple takes and I take some of the better takes and then I just build out a lecture that way.

02:40:42,786 --> 02:40:47,614
SPEAKER_0:  Sometimes I have to delete 30 minutes of content because it just went down the alley that I didn't like too much.

02:40:47,906 --> 02:40:49,406
SPEAKER_0:  There's a bunch of iteration.

02:40:49,698 --> 02:40:50,878
SPEAKER_0:  And it probably takes me...

02:40:51,266 --> 02:40:53,891
SPEAKER_0:  you know, somewhere around 10 hours to create one hour of content to give.

02:40:53,891 --> 02:40:54,430
SPEAKER_1:  One hour.

02:40:54,786 --> 02:40:58,622
SPEAKER_1:  It's interesting, I mean, is it difficult to go back to the basics?

02:40:58,978 --> 02:41:02,558
SPEAKER_1:  Do you draw a lot of wisdom from going back to the basics? yeah

02:41:02,754 --> 02:41:09,470
SPEAKER_0:  going back to back propagation loss functions, where they come from. And one thing I like about teaching a lot honestly, is it definitely strengthens your understanding.

02:41:10,082 --> 02:41:13,406
SPEAKER_0:  So it's not a purely altruistic activity, it's a way to learn.

02:41:13,794 --> 02:41:16,478
SPEAKER_0:  If you have to explain something to someone.

02:41:16,738 --> 02:41:18,366
SPEAKER_0:  you realize you have gaps in knowledge.

02:41:18,786 --> 02:41:20,094
SPEAKER_0:  And so.

02:41:20,578 --> 02:41:22,590
SPEAKER_0:  I even surprised myself in those lectures, like...

02:41:22,946 --> 02:41:26,302
SPEAKER_0:  Well, the result will obviously look at this and then the result doesn't look like it. And I'm like...

02:41:26,754 --> 02:41:28,629
SPEAKER_0:  Okay, I thought I understood this. Yeah.

02:41:28,629 --> 02:41:35,806
SPEAKER_1:  That's why it's really cool to literally code, you run it in the notebook and it gives you a result and you're like, Oh,

02:41:36,258 --> 02:41:39,633
SPEAKER_1:  Wow. And like actual numbers, actual input, actual.

02:41:39,633 --> 02:41:45,598
SPEAKER_0:  Yeah, it's not mathematical symbols, et cetera. The source of truth is the code. It's not slides. It's just like, let's build it.

02:41:45,986 --> 02:41:48,222
SPEAKER_1:  beautiful. You're a rare human in that sense.

02:41:48,514 --> 02:41:51,262
SPEAKER_1:  What advice would you give to researchers?

02:41:51,522 --> 02:41:52,862
SPEAKER_1:  trying to develop

02:41:53,506 --> 02:41:56,446
SPEAKER_1:  and publish idea that have a big impact in the world of AI.

02:41:56,898 --> 02:41:58,718
SPEAKER_1:  So maybe...

02:41:59,138 --> 02:42:01,502
SPEAKER_1:  undergrads, maybe early graduate students.

02:42:02,722 --> 02:42:09,310
SPEAKER_0:  I mean, I would say like they definitely have to be a little bit more strategic than I had to be as a PhD student because of the way AI is evolving.

02:42:09,730 --> 02:42:11,166
SPEAKER_0:  It's going the way of physics.

02:42:11,458 --> 02:42:15,966
SPEAKER_0:  where in physics you used to be able to do experiments on your bench top and everything was great and you could make

02:42:16,322 --> 02:42:16,766
SPEAKER_0:  Progress.

02:42:16,994 --> 02:42:20,062
SPEAKER_0:  and now you have to work in like LHC or like CERN.

02:42:21,122 --> 02:42:22,878
SPEAKER_0:  And so AI is going in that direction as well.

02:42:23,170 --> 02:42:25,150
SPEAKER_0:  So there's certain kinds of

02:42:25,506 --> 02:42:27,678
SPEAKER_0:  things that's just not possible to do on the bench top anymore.

02:42:28,194 --> 02:42:28,958
SPEAKER_0:  and uh...

02:42:29,890 --> 02:42:30,654
SPEAKER_0:  I think...

02:42:31,202 --> 02:42:32,574
SPEAKER_0:  That didn't used to be the case at the time.

02:42:32,674 --> 02:42:34,622
SPEAKER_1:  Do you still think that there's like...

02:42:35,810 --> 02:42:37,918
SPEAKER_1:  Gann type papers to be written.

02:42:38,562 --> 02:42:40,286
SPEAKER_1:  where like, like.

02:42:40,738 --> 02:42:44,113
SPEAKER_1:  very simple idea that requires just one computer to illustrate a simple

02:42:44,113 --> 02:42:47,614
SPEAKER_0:  I mean one example that's been very influential recently is diffusion models.

02:42:48,002 --> 02:42:51,358
SPEAKER_0:  The Fusion models are amazing. The Fusion models are six years old.

02:42:51,650 --> 02:42:53,630
SPEAKER_0:  for the longest time people who are kind of ignoring them.

02:42:53,858 --> 02:42:54,622
SPEAKER_0:  As far as I can tell.

02:42:55,010 --> 02:42:56,254
SPEAKER_0:  and they're an amazing

02:42:56,482 --> 02:42:58,750
SPEAKER_0:  generative model, especially in images.

02:42:58,978 --> 02:43:01,310
SPEAKER_0:  And so stable diffusion and so on, it's all diffusion based.

02:43:01,602 --> 02:43:02,622
SPEAKER_0:  The fusion is new.

02:43:02,850 --> 02:43:04,414
SPEAKER_0:  It was not there and came from...

02:43:05,026 --> 02:43:08,446
SPEAKER_0:  Well, it came from Google, but a researcher could have come up with it. In fact, some of the first.

02:43:09,442 --> 02:43:14,494
SPEAKER_0:  Actually, no, those came from Google as well. But a researcher could come up with that in an academic institution.

02:43:15,234 --> 02:43:17,630
SPEAKER_1:  Yeah, what do you find most fascinating about diffusion models?

02:43:17,890 --> 02:43:18,206
SPEAKER_1:  So.

02:43:18,850 --> 02:43:22,494
SPEAKER_1:  the from the societal impact of the technical architecture.

02:43:22,626 --> 02:43:24,350
SPEAKER_0:  What I like about the fusion is it works so well.

02:43:24,578 --> 02:43:32,510
SPEAKER_1:  Was that surprising to you? The amount of the variety, almost the novelty of the synthetic data is generating.

02:43:32,738 --> 02:43:34,942
SPEAKER_0:  Yes, so the stable diffusion images are.

02:43:35,234 --> 02:43:35,902
SPEAKER_0:  incredible.

02:43:36,194 --> 02:43:36,830
SPEAKER_0:  It's half.

02:43:37,378 --> 02:43:40,094
SPEAKER_0:  The speed of improvement in generating images has been insane.

02:43:40,514 --> 02:43:47,870
SPEAKER_0:  We went very quickly from generating like tiny digits to tiny faces and it all looked messed up and now we were stable diffusion. And that happened very quickly.

02:43:48,098 --> 02:43:51,230
SPEAKER_0:  There's a lot that academia can still contribute. For example,

02:43:51,842 --> 02:44:01,214
SPEAKER_0:  Flash attention is a very efficient kernel for running the attention operation inside the transformer that came from academic environment. It's a very clever way to structure the kernel.

02:44:01,890 --> 02:44:05,534
SPEAKER_0:  that's the calculation, so it doesn't materialize the attention matrix.

02:44:06,402 --> 02:44:06,782
SPEAKER_0:  Um.

02:44:07,138 --> 02:44:10,622
SPEAKER_0:  And so there's, I think there's still like lots of things to contribute, but you have to be just more strategic.

02:44:11,106 --> 02:44:13,022
SPEAKER_1:  Do you think neural networks can be made to reason?

02:44:14,434 --> 02:44:15,102
SPEAKER_1:  Ah, yes.

02:44:16,098 --> 02:44:17,310
SPEAKER_1:  Do you think there are any reason?

02:44:17,634 --> 02:44:17,950
SPEAKER_1:  Yes.

02:44:18,210 --> 02:44:19,454
SPEAKER_1:  What's your definition of reasoning?

02:44:20,322 --> 02:44:20,670
SPEAKER_1:  our our

02:44:21,090 --> 02:44:22,110
SPEAKER_0:  information processing.

02:44:22,498 --> 02:44:23,390
SPEAKER_0:

02:44:23,554 --> 02:44:23,998
SPEAKER_1:  haha

02:44:24,706 --> 02:44:28,350
SPEAKER_1:  So in a way that humans think through a problem and come up with novel ideas.

02:44:31,458 --> 02:44:32,862
SPEAKER_1:  It feels like reasoning.

02:44:34,018 --> 02:44:35,326
SPEAKER_1:  So the novelty.

02:44:37,442 --> 02:44:41,118
SPEAKER_1:  I don't want to say, but out of distribution ideas.

02:44:42,018 --> 02:44:42,814
SPEAKER_1:  You think it's possible?

02:44:43,330 --> 02:44:46,110
SPEAKER_0:  Yes, and I think we're seeing that already in the current neural nets.

02:44:46,402 --> 02:44:50,910
SPEAKER_0:  you're able to remix the training set information into true generalization in some sense.

02:44:51,522 --> 02:44:53,607
SPEAKER_1:  doesn't appear. It doesn't appear.

02:44:53,607 --> 02:44:57,406
SPEAKER_0:  in the training set. Like you're doing something interesting algorithmically. You're manipulating.

02:44:58,114 --> 02:45:00,350
SPEAKER_0:  you know, some symbols and you're coming up with some.

02:45:01,346 --> 02:45:02,750
SPEAKER_0:  correct a unique answer.

02:45:03,234 --> 02:45:03,838
SPEAKER_0:  and a new.

02:45:04,802 --> 02:45:06,142
SPEAKER_1:  What would, uh...

02:45:06,722 --> 02:45:09,950
SPEAKER_1:  Illustrate to you, holy shit, this thing is definitely thinking.

02:45:11,266 --> 02:45:16,926
SPEAKER_0:  To me, thinking or reasoning is just information processing and generalization, and I think the neural nets already do that.

02:45:18,018 --> 02:45:19,550
SPEAKER_1:  to being able to perceive the world.

02:45:19,778 --> 02:45:22,238
SPEAKER_1:  or perceive whatever the inputs are.

02:45:22,626 --> 02:45:22,942
SPEAKER_1:  and

02:45:23,266 --> 02:45:23,934
SPEAKER_1:  to make.

02:45:25,090 --> 02:45:28,995
SPEAKER_1:  predictions based on that or actions based on that. That's the reason.

02:45:28,995 --> 02:45:31,870
SPEAKER_0:  you're giving correct answers in novel settings.

02:45:32,930 --> 02:45:36,094
SPEAKER_0:  by manipulating information. We've learned the correct algorithm.

02:45:36,578 --> 02:45:39,038
SPEAKER_0:  you're not doing just some kind of a lookup table on their neighbor search.

02:45:40,578 --> 02:45:44,222
SPEAKER_1:  Let me ask you about AGI. What are some moonshot ideas you think?

02:45:44,514 --> 02:45:47,550
SPEAKER_1:  might make significant progress towards AGI.

02:45:47,970 --> 02:45:49,214
SPEAKER_1:  or maybe another ways.

02:45:49,602 --> 02:45:51,774
SPEAKER_1:  What are the big blockers that we're missing now?

02:45:52,418 --> 02:45:55,230
SPEAKER_0:  So basically I am fairly bullish on our ability to...

02:45:55,458 --> 02:45:56,446
SPEAKER_0:  Build AGI's.

02:45:56,770 --> 02:45:57,150
SPEAKER_0:  Uh.

02:45:57,442 --> 02:45:59,934
SPEAKER_0:  basically automated systems that.

02:46:00,290 --> 02:46:02,046
SPEAKER_0:  we can interact with and are very human-like.

02:46:02,402 --> 02:46:04,862
SPEAKER_0:  and we can interact with them in a digital realm or physical realm.

02:46:05,570 --> 02:46:06,750
SPEAKER_0:  Currently, it seems.

02:46:07,106 --> 02:46:08,830
SPEAKER_0:  Most of the models that sort of do these.

02:46:09,154 --> 02:46:10,846
SPEAKER_0:  or magical tasks are in a text realm.

02:46:11,746 --> 02:46:12,190
SPEAKER_0:  Uh huh.

02:46:12,898 --> 02:46:13,790
SPEAKER_0:  I think...

02:46:14,434 --> 02:46:20,094
SPEAKER_0:  As I mentioned, I'm suspicious that text realm is not enough to actually build full understanding of the world.

02:46:20,482 --> 02:46:24,446
SPEAKER_0:  I do actually think you need to go into pixels and understand the physical world and how it works.

02:46:24,930 --> 02:46:31,646
SPEAKER_0:  So I do think that we need to extend these models to consume images and videos and train on a lot more data that is multimodal in that way.

02:46:32,674 --> 02:46:35,299
SPEAKER_1:  Do you think you need to touch the world to understand it also?

02:46:35,299 --> 02:46:40,574
SPEAKER_0:  big open question I would say in my mind is if you also require the embodiment and the ability to

02:46:40,866 --> 02:46:44,094
SPEAKER_0:  sort of interact with the world, run experiments and...

02:46:44,610 --> 02:46:46,686
SPEAKER_0:  have a data of that form, then you need to go to Optimus.

02:46:47,234 --> 02:46:47,934
SPEAKER_0:  or something like that.

02:46:48,642 --> 02:46:51,070
SPEAKER_0:  And so I would say Optimus in some way is like a hedge.

02:46:51,458 --> 02:46:51,966
SPEAKER_0:  Um.

02:46:52,770 --> 02:46:54,206
SPEAKER_0:  in AGI because...

02:46:54,626 --> 02:46:55,614
SPEAKER_0:  It seems to me that...

02:46:56,066 --> 02:46:56,862
SPEAKER_0:  It's possible.

02:46:57,474 --> 02:46:59,614
SPEAKER_0:  that just having data from the internet is not enough.

02:47:00,322 --> 02:47:01,470
SPEAKER_0:  If that is the case, then...

02:47:01,890 --> 02:47:03,422
SPEAKER_0:  optimus may lead to AGI.

02:47:04,002 --> 02:47:07,198
SPEAKER_0:  because Optimus, to me, there's nothing beyond.

02:47:07,490 --> 02:47:11,038
SPEAKER_0:  You have like this humanoid form factor that can actually like do stuff in the world.

02:47:11,394 --> 02:47:14,078
SPEAKER_0:  you can have millions of them interacting with humans and so on.

02:47:14,562 --> 02:47:15,166
SPEAKER_0:  and uh...

02:47:15,522 --> 02:47:19,070
SPEAKER_0:  If that doesn't give rise to AGI at some point, like, I'm not sure what will.

02:47:19,618 --> 02:47:22,878
SPEAKER_0:  So from a completeness perspective, I think that's the.

02:47:23,138 --> 02:47:24,510
SPEAKER_0:  that's a really good platform.

02:47:24,770 --> 02:47:30,654
SPEAKER_0:  but it's a much harder platform because you are dealing with atoms and you need to actually build these things in.

02:47:31,106 --> 02:47:32,190
SPEAKER_0:  integrate them into society.

02:47:32,738 --> 02:47:34,206
SPEAKER_0:  So I think that path takes longer.

02:47:34,690 --> 02:47:35,998
SPEAKER_0:  but it's much more certain.

02:47:36,674 --> 02:47:40,798
SPEAKER_0:  And then there's a path of the internet and just like training these compression models effectively.

02:47:41,218 --> 02:47:43,838
SPEAKER_0:  on trying to compress all the Internet.

02:47:44,898 --> 02:47:47,550
SPEAKER_0:  And that might also give these agents as well.

02:47:48,162 --> 02:47:50,974
SPEAKER_1:  compress the internet, but also interact with the internet.

02:47:51,586 --> 02:47:53,438
SPEAKER_1:  So it's not obvious to me.

02:47:54,178 --> 02:47:58,622
SPEAKER_1:  In fact, I suspect you can reach AGI without ever entering the physical.

02:48:00,290 --> 02:48:01,982
SPEAKER_1:  and which is a little bit more.

02:48:02,946 --> 02:48:04,542
SPEAKER_1:  concerning because

02:48:05,442 --> 02:48:08,158
SPEAKER_1:  it might, that results in it happening faster.

02:48:08,962 --> 02:48:13,438
SPEAKER_1:  So it just feels like we're in like in boiling water. We won't know as it's happening.

02:48:14,242 --> 02:48:15,326
SPEAKER_1:  I would like to-

02:48:15,682 --> 02:48:16,190
SPEAKER_1:  I'm not.

02:48:16,482 --> 02:48:17,726
SPEAKER_1:  Afraid of AGI.

02:48:17,954 --> 02:48:22,270
SPEAKER_1:  I'm excited about it. There's always concerns, but I would like to know when it happens.

02:48:23,970 --> 02:48:27,166
SPEAKER_1:  Yeah. And have like hints about when it happens. Like.

02:48:27,458 --> 02:48:32,958
SPEAKER_1:  A year from now it will happen, that kind of thing. I just feel like in the digital realm, it just might happen.

02:48:33,410 --> 02:48:36,574
SPEAKER_0:  I think all we have available to us because no one has built AGI again.

02:48:36,866 --> 02:48:39,070
SPEAKER_0:  So all we have available to us is.

02:48:39,810 --> 02:48:45,950
SPEAKER_0:  Is there enough fertile ground on the periphery? I would say yes. And we have the progress so far, which has been very rapid.

02:48:46,466 --> 02:48:49,022
SPEAKER_0:  and there are next steps that are available. So this is step 7, or so

02:48:49,794 --> 02:48:53,790
SPEAKER_0:  I would say, yeah, it's quite likely that we'll be interacting with digital entities.

02:48:54,370 --> 02:48:55,262
SPEAKER_1:  How will you know?

02:48:55,554 --> 02:48:57,191
SPEAKER_1:  that somebody has built AGI.

02:48:57,191 --> 02:49:03,390
SPEAKER_0:  It's going to be a slow incremental transition. It's going to be product-based and focused. It's going to be GitHub Copilot getting better.

02:49:03,778 --> 02:49:05,982
SPEAKER_0:  and then GPT is helping you write.

02:49:06,498 --> 02:49:09,438
SPEAKER_0:  and then these oracles that you can go to with mathematical problems.

02:49:09,730 --> 02:49:10,718
SPEAKER_0:  I think we're on a...

02:49:11,074 --> 02:49:13,566
SPEAKER_0:  on the verge of being able to ask very complex.

02:49:14,242 --> 02:49:16,030
SPEAKER_0:  Questions in chemistry, physics, math?

02:49:16,322 --> 02:49:18,814
SPEAKER_0:  of these oracles and have them complete solutions.

02:49:19,842 --> 02:49:24,222
SPEAKER_1:  So AGI to use primarily focused on intelligence so consciousness doesn't enter.

02:49:25,314 --> 02:49:27,070
SPEAKER_1:  into it.

02:49:27,778 --> 02:49:36,126
SPEAKER_0:  So in my mind, consciousness is not a special thing you will figure out and bolt on. I think it's an emerging phenomenon of a large enough and complex enough.

02:49:36,738 --> 02:49:38,014
SPEAKER_0:  generative model sort of.

02:49:38,402 --> 02:49:39,454
SPEAKER_0:  So.

02:49:39,746 --> 02:49:41,662
SPEAKER_0:  if you have a complex enough world model.

02:49:42,274 --> 02:49:51,454
SPEAKER_0:  that understands the world, then it also understands its predicament in the world as being a language model which to me is a form of consciousness or self-awareness.

02:49:52,066 --> 02:49:59,998
SPEAKER_1:  So in order to understand the world deeply, you probably have to integrate yourself into the world. And in order to interact with humans and other living beings,

02:50:00,418 --> 02:50:02,686
SPEAKER_1:  Consciousness is a very useful tool.

02:50:02,818 --> 02:50:04,734
SPEAKER_0:  I think consciousness is like a modeling insight.

02:50:05,890 --> 02:50:13,727
SPEAKER_0:  modeling insight. Yeah, it's a, you have a powerful enough model of understanding the world that you actually understand that you are an entity in it. Yeah but there is

02:50:13,727 --> 02:50:15,262
SPEAKER_1:  There's also this...

02:50:15,490 --> 02:50:20,414
SPEAKER_1:  perhaps just the narrative we tell ourselves. It feels like something to experience the world.

02:50:20,866 --> 02:50:24,693
SPEAKER_1:  the hard problem of consciousness. That could be just the narrative that we tell ourselves.

02:50:24,693 --> 02:50:26,942
SPEAKER_0:  Yeah, I don't think we'll, I think it will emerge.

02:50:27,234 --> 02:50:29,694
SPEAKER_0:  I think it's going to be something very boring. Like.

02:50:30,082 --> 02:50:34,558
SPEAKER_0:  We'll be talking to these digital AIs. They will claim they're conscious. They will appear conscious.

02:50:35,042 --> 02:50:37,150
SPEAKER_0:  they will do all the things that you would expect of other humans.

02:50:37,506 --> 02:50:39,646
SPEAKER_0:  and it's going to just be a stalemate.

02:50:39,746 --> 02:50:41,982
SPEAKER_1:  I think there would be a lot of actual.

02:50:42,626 --> 02:50:44,254
SPEAKER_1:  fascinating ethical questions.

02:50:44,706 --> 02:50:46,942
SPEAKER_1:  like Supreme Court level questions.

02:50:47,618 --> 02:50:50,078
SPEAKER_1:  of whether you're allowed to turn off.

02:50:50,338 --> 02:50:52,734
SPEAKER_1:  A conscious AI, if you're allowed to.

02:50:52,994 --> 02:50:54,366
SPEAKER_1:  Build a conscious AI.

02:50:55,010 --> 02:50:55,326
SPEAKER_1:  on.

02:50:55,714 --> 02:50:57,886
SPEAKER_1:  maybe there would have to be the same kind of.

02:50:58,274 --> 02:50:59,550
SPEAKER_1:  the bass you have around.

02:51:00,450 --> 02:51:00,926
SPEAKER_1:  Um.

02:51:01,378 --> 02:51:04,510
SPEAKER_1:  Sorry to bring up a political topic, but you know, abortion.

02:51:04,962 --> 02:51:05,918
SPEAKER_1:  which is.

02:51:06,210 --> 02:51:07,838
SPEAKER_1:  the deeper question with abortion.

02:51:08,546 --> 02:51:10,846
SPEAKER_1:  Is what is life.

02:51:11,714 --> 02:51:13,342
SPEAKER_1:  And the deep question with AI is...

02:51:13,698 --> 02:51:16,990
SPEAKER_1:  Also, what is life and what is conscious? And I think-

02:51:17,346 --> 02:51:18,718
SPEAKER_1:  That'll be very fascinating.

02:51:19,970 --> 02:51:24,862
SPEAKER_1:  bring up, it might become illegal to build systems that are capable.

02:51:25,186 --> 02:51:26,014
SPEAKER_1:  Like.

02:51:26,530 --> 02:51:32,030
SPEAKER_1:  of such level of intelligence that consciousness would emerge and therefore the capacity to suffer would emerge.

02:51:32,258 --> 02:51:35,614
SPEAKER_1:  and a system that says, no, please don't kill me.

02:51:36,194 --> 02:51:43,902
SPEAKER_0:  Well, that's what the Lambda Chatbot already told this Google engineer, right? Like it was talking about not wanting to die or.

02:51:44,418 --> 02:51:44,766
SPEAKER_0:  So on.

02:51:44,962 --> 02:51:46,942
SPEAKER_1:  So that might become illegal to do that.

02:51:48,450 --> 02:51:49,022
SPEAKER_1:  Hehehe

02:51:49,282 --> 02:51:51,358
SPEAKER_1:  I, cause otherwise you might have a lot of

02:51:51,682 --> 02:51:53,566
SPEAKER_1:  a lot of creatures that don't want to die.

02:51:54,050 --> 02:51:54,878
SPEAKER_1:  And they will laugh.

02:51:55,138 --> 02:51:57,763
SPEAKER_0:  You can just spawn infinity of them on a cluster.

02:51:57,763 --> 02:52:08,286
SPEAKER_1:  And then that might lead to like horrible consequences, because then there might be a lot of people that secretly love murder, and they'll start practicing murder in those systems. And there's just-

02:52:08,610 --> 02:52:17,182
SPEAKER_1:  To me all of this stuff just brings a beautiful mirror to the human condition and human nature will get to explore it and that's what like the best

02:52:17,986 --> 02:52:22,078
SPEAKER_1:  of the Supreme Court, of all the different debates we have about ideas.

02:52:22,306 --> 02:52:25,022
SPEAKER_1:  what it means to be human. We get to ask those deep questions.

02:52:25,346 --> 02:52:29,918
SPEAKER_1:  that we've been asking throughout human history. There has always been the other in human history.

02:52:30,434 --> 02:52:30,814
SPEAKER_1:  Uh...

02:52:31,234 --> 02:52:37,406
SPEAKER_1:  We're the good guys and that's the bad guys and we're going to, you know, throughout human history, let's murder the bad guys.

02:52:37,890 --> 02:52:46,206
SPEAKER_1:  And the same will probably happen with robots. It'll be the other at first, and then we'll get to ask questions. What does it mean to be alive? What does it mean to be conscious? Yeah.

02:52:46,754 --> 02:52:50,750
SPEAKER_0:  And I think there's some canary in the coal mines, even with what we have today. And

02:52:51,010 --> 02:52:55,326
SPEAKER_0:  For example, there's these waifus that you can work with and some people are trying to like.

02:52:55,554 --> 02:52:57,822
SPEAKER_0:  This company is going to shut down, but this person really like...

02:52:58,082 --> 02:53:01,918
SPEAKER_0:  loved their waifu and is trying to port it somewhere else.

02:53:02,370 --> 02:53:03,422
SPEAKER_0:  It's not possible and like.

02:53:04,162 --> 02:53:05,182
SPEAKER_0:  I think like definitely.

02:53:05,762 --> 02:53:07,550
SPEAKER_0:  people will have feelings towards.

02:53:08,066 --> 02:53:08,542
SPEAKER_0:

02:53:08,802 --> 02:53:12,894
SPEAKER_0:  towards these systems because in some sense, they are like a mirror of humanity.

02:53:13,474 --> 02:53:14,078
SPEAKER_0:  because.

02:53:14,338 --> 02:53:18,110
SPEAKER_0:  They are sort of like a big average of humanity in the way that it's trained.

02:53:18,338 --> 02:53:19,198
SPEAKER_1:  it but we can.

02:53:19,522 --> 02:53:21,598
SPEAKER_1:  that average we can actually watch.

02:53:22,018 --> 02:53:26,910
SPEAKER_1:  It's nice to be able to interact with a big average of humanity and do like a search query on it.

02:53:28,034 --> 02:53:29,022
SPEAKER_0:  Yeah, it's very fascinating.

02:53:29,666 --> 02:53:37,182
SPEAKER_0:  And we can, of course, also shape it. It's not just a pure average. We can mess with the training data. We can mess with the objective. We can fine tune them in various ways.

02:53:37,538 --> 02:53:38,942
SPEAKER_0:  So we have some...

02:53:39,490 --> 02:53:41,054
SPEAKER_0:  you know, impact on what those.

02:53:41,570 --> 02:53:42,270
SPEAKER_0:  systems look like.

02:53:42,562 --> 02:53:44,542
SPEAKER_1:  if you want to achieve AGI.

02:53:45,186 --> 02:53:49,118
SPEAKER_1:  And you could have a conversation with her and ask her.

02:53:49,730 --> 02:53:53,726
SPEAKER_1:  Talk about anything, maybe ask her a question. What kind of stuff would you ask?

02:53:54,242 --> 02:53:56,190
SPEAKER_0:  I would have some practical questions in my mind like

02:53:56,450 --> 02:53:56,862
SPEAKER_0:  uh

02:53:57,474 --> 02:54:01,214
SPEAKER_0:  Do I or my loved ones really have to die? What can we do about that?

02:54:01,346 --> 02:54:02,302
SPEAKER_1:

02:54:02,882 --> 02:54:06,142
SPEAKER_1:  Do you think it will answer clearly or would it answer poetically?

02:54:07,394 --> 02:54:17,214
SPEAKER_0:  I would expect it to give solutions. I would expect it to be like, well, I've read all of these textbooks and I know all these things that you've produced. And it seems to me like here are the experiments that I think it would be useful to run next.

02:54:17,602 --> 02:54:19,454
SPEAKER_0:  And here's some gene therapies that I think would be helpful.

02:54:19,874 --> 02:54:22,206
SPEAKER_0:  And here are the kinds of experiments that you should run.

02:54:22,338 --> 02:54:24,702
SPEAKER_1:  Okay, let's go with the start experiment, okay?

02:54:25,314 --> 02:54:26,910
SPEAKER_1:  Imagine that...

02:54:27,490 --> 02:54:30,782
SPEAKER_1:  mortality is actually like a

02:54:31,362 --> 02:54:34,430
SPEAKER_1:  prerequisite for happiness. So if we become immortal,

02:54:34,786 --> 02:54:38,494
SPEAKER_1:  will actually become deeply unhappy. And the model is able to.

02:54:38,722 --> 02:54:39,230
SPEAKER_1:  know that.

02:54:39,714 --> 02:54:45,502
SPEAKER_1:  So what is this supposed to tell you, stupid human about it? Yes, you can become immortal, but you will become deeply unhappy.

02:54:46,210 --> 02:54:49,118
SPEAKER_1:  If the model is, if the AGI system.

02:54:49,922 --> 02:54:53,118
SPEAKER_1:  is trying to empathize with you human. What is this supposed to tell you?

02:54:53,570 --> 02:54:57,150
SPEAKER_1:  that yes, you don't have to die, but you're really not going to like it.

02:54:57,954 --> 02:55:00,478
SPEAKER_1:  Is it going to be deeply honest? Like there's a-

02:55:00,802 --> 02:55:03,070
SPEAKER_1:  Interstellar, what is it? The AI says like-

02:55:03,554 --> 02:55:05,918
SPEAKER_1:  humans want 90% honesty.

02:55:06,210 --> 02:55:06,558
SPEAKER_1:  haha

02:55:07,394 --> 02:55:11,998
SPEAKER_1:  Yeah. So like you have to pick how honest I want to answer these practical questions.

02:55:12,418 --> 02:55:17,054
SPEAKER_0:  I love AI Interstellar by the way. I think it's like such a sidekick to the entire story, but...

02:55:18,050 --> 02:55:19,646
SPEAKER_0:  At the same time, it's really interesting.

02:55:19,778 --> 02:55:20,734
SPEAKER_1:  It's kind of limited it.

02:55:21,090 --> 02:55:22,206
SPEAKER_1:  in certain ways, right?

02:55:22,338 --> 02:55:25,566
SPEAKER_0:  Yeah, it's limited, and I think that's totally fine, by the way. I don't think, uh...

02:55:25,954 --> 02:55:26,462
SPEAKER_0:  I think it's.

02:55:27,202 --> 02:55:30,270
SPEAKER_0:  find it plausible to have limited and imperfect AGI's.

02:55:32,322 --> 02:55:33,447
SPEAKER_1:  Is that the feature?

02:55:33,447 --> 02:55:35,838
SPEAKER_0:  As an example, like it has a

02:55:36,066 --> 02:55:37,950
SPEAKER_0:  fixed amount of compute on its physical body.

02:55:38,210 --> 02:55:40,926
SPEAKER_0:  And it might just be that even though you can have a...

02:55:41,378 --> 02:55:42,686
SPEAKER_0:  Super amazing mega brain.

02:55:42,946 --> 02:55:45,054
SPEAKER_0:  super intelligent AI, you also can have like...

02:55:45,410 --> 02:55:48,830
SPEAKER_0:  less intelligent AIs that you can deploy in a power efficient.

02:55:49,154 --> 02:55:51,326
SPEAKER_0:  way and then they're not perfect, they might make mistakes.

02:55:51,426 --> 02:55:54,686
SPEAKER_1:  No, I meant more like, say you had infinite compute.

02:55:55,298 --> 02:55:57,406
SPEAKER_1:  and it's still good to make mistakes sometimes.

02:55:58,146 --> 02:56:00,254
SPEAKER_1:  like in order to integrate yourself like a.

02:56:01,154 --> 02:56:04,638
SPEAKER_1:  What is it going back to good will hunting, a Robin Williams character.

02:56:05,186 --> 02:56:08,190
SPEAKER_1:  says like the human imperfections, that's the good stuff.

02:56:08,418 --> 02:56:14,910
SPEAKER_1:  Right? Isn't that the, like, we don't want perfect. We want flaws in part.

02:56:15,810 --> 02:56:20,126
SPEAKER_1:  to form connections with each other, because it feels like something you can attach your feelings to.

02:56:21,698 --> 02:56:22,526
SPEAKER_1:  the flaws.

02:56:22,754 --> 02:56:25,726
SPEAKER_1:  And in that same way, you want AI that's flawed.

02:56:26,114 --> 02:56:26,558
SPEAKER_1:  I don't know.

02:56:26,882 --> 02:56:29,150
SPEAKER_1:  I feel like perfection is cool. Then you're saying, okay, yeah.

02:56:29,794 --> 02:56:32,190
SPEAKER_1:  But that's not AGI. But see you AGI.

02:56:32,610 --> 02:56:39,678
SPEAKER_1:  We need to be intelligent enough to give answers to humans that humans don't understand. And I think perfect isn't something humans can't understand.

02:56:40,194 --> 02:56:44,798
SPEAKER_1:  because even science doesn't give perfect answers. There's always gaves and mysteries.

02:56:45,346 --> 02:56:46,398
SPEAKER_1:  I don't know.

02:56:47,010 --> 02:56:48,382
SPEAKER_1:  I don't know if humans want...

02:56:48,706 --> 02:56:49,214
SPEAKER_1:  Perfect.

02:56:50,050 --> 02:56:54,206
SPEAKER_0:  Yeah, I could imagine just having a conversation with this kind of oracle entity.

02:56:54,530 --> 02:56:55,454
SPEAKER_0:  as you'd imagine them.

02:56:55,778 --> 02:56:57,182
SPEAKER_0:  And yeah, maybe it can.

02:56:57,474 --> 02:56:58,110
SPEAKER_0:  tell you about.

02:56:58,882 --> 02:57:00,958
SPEAKER_0:  based on my analysis of human condition.

02:57:01,250 --> 02:57:01,630
SPEAKER_0:  Uh.

02:57:02,114 --> 02:57:04,734
SPEAKER_0:  You might not want this. And here are some of the things that might...

02:57:04,866 --> 02:57:05,918
SPEAKER_1:  But every.

02:57:06,178 --> 02:57:08,478
SPEAKER_1:  Every dumb human will say, yeah, yeah, yeah, yeah.

02:57:08,834 --> 02:57:09,342
SPEAKER_1:  Trust me.

02:57:09,858 --> 02:57:12,190
SPEAKER_1:  I can give me the truth. I can handle it.

02:57:12,354 --> 02:57:14,782
SPEAKER_0:  But that's the beauty, like people can choose, so.

02:57:15,042 --> 02:57:15,678
SPEAKER_1:  But then...

02:57:17,410 --> 02:57:21,630
SPEAKER_1:  The old marshmallow test with the kids and so on. I feel like too many people.

02:57:22,754 --> 02:57:25,438
SPEAKER_1:  Like, can't handle the truth.

02:57:25,858 --> 02:57:35,262
SPEAKER_1:  Probably including myself, like the deep truth of the human condition. I don't, I don't know if I can handle it. Like what, what if there's some dark stuff? What if we are an alien science experiment?

02:57:35,746 --> 02:57:38,110
SPEAKER_1:  And it realizes that what if it had, I mean.

02:57:38,914 --> 02:57:41,854
SPEAKER_0:  I mean, this is the matrix all over again.

02:57:42,466 --> 02:57:47,038
SPEAKER_1:  I don't know. I would, what would I talk about? I don't even, yeah.

02:57:47,298 --> 02:57:48,606
SPEAKER_1:  I

02:57:49,250 --> 02:57:54,110
SPEAKER_1:  Probably I will go with the safer scientific questions at first that have nothing to do with my own

02:57:54,690 --> 02:57:56,638
SPEAKER_1:  personal life and mortality.

02:57:57,058 --> 02:57:59,038
SPEAKER_1:  just like about physics and so on.

02:57:59,490 --> 02:58:00,126
SPEAKER_1:  Yeah.

02:58:00,674 --> 02:58:04,286
SPEAKER_1:  to build up like, let's see where it's at. Or maybe see if it has a sense of humor.

02:58:04,578 --> 02:58:05,566
SPEAKER_1:  That's another question.

02:58:06,082 --> 02:58:11,582
SPEAKER_1:  would it be able to, presumably in order to, if it understands humans deeply, would be able to generate.

02:58:12,354 --> 02:58:12,926
SPEAKER_1:  Uh...

02:58:13,890 --> 02:58:15,006
SPEAKER_1:  to generate humor.

02:58:15,394 --> 02:58:19,102
SPEAKER_0:  Yeah, I think that's actually a wonderful benchmark almost. Like, is it able?

02:58:19,394 --> 02:58:20,734
SPEAKER_0:  I think that's a really good point basically.

02:58:21,346 --> 02:58:26,718
SPEAKER_0:  to make you laugh. Yeah, if it's able to be like a very effective stand up comedian that is doing something very interesting computationally.

02:58:27,010 --> 02:58:28,606
SPEAKER_0:  I think being funny is extremely hard.

02:58:28,994 --> 02:58:29,438
SPEAKER_1:  Yeah.

02:58:30,434 --> 02:58:31,038
SPEAKER_1:  Because...

02:58:32,162 --> 02:58:33,502
SPEAKER_1:  It's hard in a way.

02:58:34,082 --> 02:58:36,414
SPEAKER_1:  like a touring test, the original.

02:58:36,674 --> 02:58:40,030
SPEAKER_1:  intent of the Turing test is hard because you have to convince humans.

02:58:40,354 --> 02:58:41,694
SPEAKER_1:  and there's nothing, that's why.

02:58:43,266 --> 02:58:47,614
SPEAKER_1:  That's what comedians talk about this. Like there's this is deeply honest.

02:58:48,002 --> 02:58:52,547
SPEAKER_1:  Cause if people can't help but laugh and if they don't laugh, that means you're not funny. They laugh.

02:58:52,547 --> 02:58:55,390
SPEAKER_0:  And you're showing you need a lot of knowledge to create.

02:58:55,682 --> 02:58:56,190
SPEAKER_0:  create humor.

02:58:56,578 --> 02:58:59,614
SPEAKER_0:  about the documentation, human condition and so on, then you need to be clever with it.

02:59:01,026 --> 02:59:05,502
SPEAKER_1:  You mentioned a few movies you tweeted movies that I've seen five plus times but

02:59:06,274 --> 02:59:12,926
SPEAKER_1:  I'm ready and willing to keep watching. Interstellar, Gladiator, Contact, Good Will, Hunting, The Matrix, Lord of the Rings.

02:59:13,282 --> 02:59:16,798
SPEAKER_1:  all three, Avatar, Fifth Elm, and so on, goes on. Terminator 2.

02:59:17,474 --> 02:59:23,294
SPEAKER_1:  Mean Girls, I'm not gonna ask about that one. I think her man. Mean Girls is great.

02:59:23,586 --> 02:59:27,006
SPEAKER_1:  What are some of the jump onto your memory that you love?

02:59:27,682 --> 02:59:30,334
SPEAKER_1:  NY. Like you mentioned the matrix.

02:59:30,946 --> 02:59:33,086
SPEAKER_1:  As a computer person, why do you love the Matrix?

02:59:34,466 --> 02:59:37,054
SPEAKER_0:  There's so many properties that make it beautiful and interesting.

02:59:37,282 --> 02:59:40,510
SPEAKER_0:  there's all these philosophical questions, but then there's also AGI's.

02:59:40,738 --> 02:59:41,886
SPEAKER_0:  and their simulation.

02:59:42,146 --> 02:59:44,798
SPEAKER_0:  and it's cool and there's the black.

02:59:45,346 --> 02:59:46,110
SPEAKER_0:  How you know, uh...

02:59:46,338 --> 02:59:52,126
SPEAKER_0:  The look of it, the feel of it. The look of it, the feel of it, the action, the bullet time. It was just like innovating in so many ways.

02:59:53,474 --> 02:59:56,702
SPEAKER_1:  And then good will hunting. Why do you like that one?

02:59:57,506 --> 02:59:58,238
SPEAKER_1:  Yeah, I just.

02:59:58,434 --> 02:59:59,646
SPEAKER_0:  I really like this.

03:00:00,226 --> 03:00:02,814
SPEAKER_0:  tortured genius sort of character who's like...

03:00:03,138 --> 03:00:05,086
SPEAKER_0:  grappling with whether or not he has like

03:00:05,410 --> 03:00:10,334
SPEAKER_0:  any responsibility or like what to do with this gift that he was given or like how to think about the whole thing.

03:00:10,818 --> 03:00:15,198
SPEAKER_1:  And there's also a dance between the genius and the personal.

03:00:15,746 --> 03:00:17,621
SPEAKER_1:  like what it means to love another human being.

03:00:17,621 --> 03:00:20,621
SPEAKER_0:  There's a lot of themes there. It's just a beautiful movie. And then the

03:00:20,621 --> 03:00:23,934
SPEAKER_1:  fatherly figure, the mentor and the psychiatrist.

03:00:24,322 --> 03:00:25,502
SPEAKER_0:  It like really like uh...

03:00:26,242 --> 03:00:28,862
SPEAKER_0:  it messes with you. You know, there's some movies that just like really mess with you.

03:00:29,314 --> 03:00:29,790
SPEAKER_0:  Uh...

03:00:30,242 --> 03:00:32,446
SPEAKER_1:  on a deep level. Do you relate to that movie at all?

03:00:33,218 --> 03:00:33,886
SPEAKER_1:  Mmmnghh

03:00:34,690 --> 03:00:39,582
SPEAKER_1:  It's not your fault Andre as I said. Lower the rings, that's self explanatory.

03:00:40,226 --> 03:00:42,014
SPEAKER_1:  Terminator 2, which is interesting.

03:00:42,786 --> 03:00:45,438
SPEAKER_1:  You rewatch that a lot. Is that better than Terminator 1?

03:00:46,146 --> 03:00:47,271
SPEAKER_1:  You like Arnold?

03:00:47,271 --> 03:00:48,830
SPEAKER_0:  I do like Terminator one as well.

03:00:49,314 --> 03:00:49,790
SPEAKER_0:  Oh.

03:00:50,114 --> 03:00:53,502
SPEAKER_0:  I like Terminator 2 a little bit more, but in terms of its surface properties.

03:00:53,794 --> 03:00:54,942
SPEAKER_1:  I love eye contact!

03:00:55,234 --> 03:00:58,014
SPEAKER_1:  Do you think Skynet is at all a possibility?

03:00:58,306 --> 03:00:59,038
SPEAKER_1:  uh... yes

03:01:00,130 --> 03:01:06,014
SPEAKER_1:  Like the actual sort of Thomas weapon system kind of thing. Do you worry about that?

03:01:06,274 --> 03:01:07,399
SPEAKER_1:  stuff I do worry

03:01:07,399 --> 03:01:08,510
SPEAKER_0:  being useful war.

03:01:09,474 --> 03:01:11,358
SPEAKER_0:  I 100% worry about it. And so the-

03:01:11,746 --> 03:01:18,494
SPEAKER_0:  I mean, some of these fears of AGI's and how this will plan out, I mean, these will be like very powerful entities probably at some point. And so.

03:01:18,914 --> 03:01:19,294
SPEAKER_0:  Um.

03:01:19,554 --> 03:01:21,630
SPEAKER_0:  For a long time there are going to be tools in the hands of humans.

03:01:22,018 --> 03:01:24,862
SPEAKER_0:  You know, people talk about like alignment of AGI's and how to make...

03:01:25,122 --> 03:01:27,294
SPEAKER_0:  The problem is like even humans are not aligned.

03:01:27,554 --> 03:01:28,158
SPEAKER_0:  also.

03:01:29,218 --> 03:01:32,222
SPEAKER_0:  how this will be used and what this is going to look like. How this, um—

03:01:32,770 --> 03:01:33,790
SPEAKER_0:  Yeah, it's troubling, so.

03:01:34,530 --> 03:01:37,790
SPEAKER_1:  Do you think it'll happen slowly enough that we'll be able to?

03:01:38,626 --> 03:01:40,286
SPEAKER_1:  as a human civilization.

03:01:40,578 --> 03:01:41,703
SPEAKER_1:  think through the problems.

03:01:41,703 --> 03:01:47,774
SPEAKER_0:  Yes, that's my hope is that it happens slowly enough and in an open enough way where a lot of people can see and participate in it.

03:01:48,194 --> 03:01:49,566
SPEAKER_0:  just figure out how to.

03:01:49,794 --> 03:01:51,669
SPEAKER_0:  deal with this transition, I think.

03:01:51,669 --> 03:02:01,694
SPEAKER_1:  interesting.

03:02:02,850 --> 03:02:09,822
SPEAKER_1:  When the systems are not so dangerous, they destroy human civilization, we deploy them and learn the lessons.

03:02:10,082 --> 03:02:11,294
SPEAKER_1:  and then we quickly.

03:02:11,810 --> 03:02:14,718
SPEAKER_1:  If it's too dangerous, we'll quickly, quickly, we might still deploy it.

03:02:15,266 --> 03:02:21,726
SPEAKER_1:  But you very quickly learn not to use them and so there'll be like this balance achieved. Humans are very clever as a species

03:02:21,954 --> 03:02:22,686
SPEAKER_1:  interesting.

03:02:23,042 --> 03:02:28,478
SPEAKER_1:  We exploit the resources as much as we can, but we don't, we avoid destroying ourselves, it seems like.

03:02:29,250 --> 03:02:31,838
SPEAKER_1:  Well, I don't know about that actually. I hope it continues.

03:02:31,938 --> 03:02:32,382
SPEAKER_0:  Um.

03:02:33,698 --> 03:02:39,646
SPEAKER_0:  I mean, I'm definitely concerned about nuclear weapons and so on, not just as a result of the recent conflict even before that.

03:02:40,130 --> 03:02:41,886
SPEAKER_0:  That's probably my number one.

03:02:42,146 --> 03:02:43,358
SPEAKER_0:  concern for humanity.

03:02:43,458 --> 03:02:44,830
SPEAKER_1:  So if humanity...

03:02:45,250 --> 03:02:46,782
SPEAKER_1:  destroys itself

03:02:47,650 --> 03:02:51,998
SPEAKER_1:  or destroys 90% of people, that would be because of nukes?

03:02:52,482 --> 03:02:53,022
SPEAKER_1:  I think so.

03:02:53,666 --> 03:02:58,942
SPEAKER_0:  And it's not even about full destruction. To me, it's bad enough if we reset society. That would be like terrible.

03:02:59,586 --> 03:03:01,374
SPEAKER_0:  It would be really bad and I can't believe we're like...

03:03:02,018 --> 03:03:03,422
SPEAKER_0:  so close to it.

03:03:03,586 --> 03:03:08,062
SPEAKER_1:  Yeah, it's like so crazy to me. It feels like we might be a few tweets away from something like that.

03:03:08,578 --> 03:03:11,262
SPEAKER_0:  Yep, basically it's extremely unnerving.

03:03:11,650 --> 03:03:13,406
SPEAKER_0:  and has been for me for a long time.

03:03:14,242 --> 03:03:15,326
SPEAKER_1:  It seems unstable.

03:03:15,906 --> 03:03:17,694
SPEAKER_1:  that world leaders.

03:03:18,594 --> 03:03:19,966
SPEAKER_1:  Just having a bad mood.

03:03:20,834 --> 03:03:22,046
SPEAKER_1:  in like...

03:03:22,914 --> 03:03:26,430
SPEAKER_1:  Take one step towards the bad direction and it escalates.

03:03:27,106 --> 03:03:28,158
SPEAKER_1:  and because of

03:03:28,674 --> 03:03:32,798
SPEAKER_1:  collection of bad moods it can escalate without being able to

03:03:33,090 --> 03:03:33,470
SPEAKER_1:  Stop.

03:03:34,658 --> 03:03:36,862
SPEAKER_0:  Yeah, it's just a huge amount of power.

03:03:37,186 --> 03:03:39,262
SPEAKER_0:  And then also with the proliferation.

03:03:39,554 --> 03:03:41,406
SPEAKER_0:  Basically, I don't actually really see.

03:03:41,922 --> 03:03:44,446
SPEAKER_0:  I don't actually know what the good outcomes are here.

03:03:44,834 --> 03:03:46,398
SPEAKER_0:  So I'm definitely worried about that a lot.

03:03:46,690 --> 03:03:50,078
SPEAKER_0:  And then AGI is not currently there, but I think at some point we'll...

03:03:50,466 --> 03:03:51,390
SPEAKER_0:  more and more become.

03:03:51,970 --> 03:03:52,894
SPEAKER_0:  Something like it

03:03:53,314 --> 03:03:57,694
SPEAKER_0:  The danger with AGI even is that I think it's even like slightly worse in the sense that

03:03:58,786 --> 03:04:00,798
SPEAKER_0:  there are good outcomes of AGI.

03:04:01,282 --> 03:04:06,302
SPEAKER_0:  and then the bad outcomes are like an epsilon away, like a tiny one away. And so I think.

03:04:06,626 --> 03:04:09,694
SPEAKER_0:  capitalism and humanity and so on will drive for the positive.

03:04:10,338 --> 03:04:15,774
SPEAKER_0:  ways of using that technology, but then if bad outcomes are just like a tiny, like flip a minus sign away.

03:04:16,194 --> 03:04:18,069
SPEAKER_0:  That's a really bad position to be in.

03:04:18,069 --> 03:04:22,782
SPEAKER_1:  A tiny perturbation of the system results in the destruction of the human species.

03:04:23,074 --> 03:04:23,838
SPEAKER_1:  So weird.

03:04:24,066 --> 03:04:24,958
SPEAKER_1:  line to walk.

03:04:25,218 --> 03:04:29,662
SPEAKER_0:  Yeah, I think in general, what's really weird about like the dynamics of humanity and this explosion we've talked about is just like

03:04:30,050 --> 03:04:36,126
SPEAKER_0:  the insane coupling afforded by technology and just the instability of the whole dynamical system.

03:04:36,418 --> 03:04:38,430
SPEAKER_0:  I think it just doesn't look good, honestly.

03:04:39,170 --> 03:04:44,158
SPEAKER_1:  Yes, that explosion could be destructive and constructive and the probabilities are non-zero in both.

03:04:44,578 --> 03:04:52,606
SPEAKER_0:  I mean, I have to, I do feel like I have to try to be optimistic and so on. Yes. I think even in this case, I still am predominantly optimistic, but there's definitely...

03:04:53,762 --> 03:04:56,926
SPEAKER_1:  Me too. Do you think we'll become a multi-planetary species?

03:04:58,722 --> 03:05:02,142
SPEAKER_0:  Probably yes, but I don't know if it's dominant feature of...

03:05:02,370 --> 03:05:03,166
SPEAKER_0:  future humanity.

03:05:03,874 --> 03:05:07,966
SPEAKER_0:  There might be some people on some planets and so on, but I'm not sure if it's like...

03:05:08,642 --> 03:05:11,614
SPEAKER_0:  Yeah, if it's like a major player in our culture and so on.

03:05:12,194 --> 03:05:16,414
SPEAKER_1:  We still have to solve the drivers of self destruction here on earth.

03:05:16,866 --> 03:05:19,518
SPEAKER_1:  So just having a backup on Mars is not going to solve the problem.

03:05:19,938 --> 03:05:26,366
SPEAKER_0:  So by the way, I love the backup on Mars. I think that's amazing. We should absolutely do that. Yes. And I'm so thankful. people over the years are thinking about when you get the

03:05:26,594 --> 03:05:27,742
SPEAKER_0:  Would you go to Mars?

03:05:28,418 --> 03:05:31,006
SPEAKER_0:  Personally, no. I do like Earth quite a lot.

03:05:31,618 --> 03:05:32,743
SPEAKER_0:  I'll go to Mars.

03:05:32,743 --> 03:05:35,070
SPEAKER_1:  I'll tweet at you from now.

03:05:35,394 --> 03:05:42,174
SPEAKER_0:  Maybe eventually I would, once it's safe enough, but I don't actually know if it's on my lifetime scale, unless I can extend it by a lot.

03:05:43,074 --> 03:05:52,062
SPEAKER_0:  I do think that, for example, a lot of people might disappear into virtual realities and stuff like that. That could be the major thrust of the cultural development of humanity.

03:05:52,482 --> 03:05:53,214
SPEAKER_0:  if it survives.

03:05:53,634 --> 03:05:56,958
SPEAKER_0:  So it might not be, it's just really hard to work in physical realm.

03:05:57,186 --> 03:05:57,918
SPEAKER_0:  and go out there.

03:05:58,498 --> 03:06:00,766
SPEAKER_0:  And I think ultimately all your experiences are in your

03:06:01,250 --> 03:06:05,054
SPEAKER_0:  brain. And so it's much easier to disappear into digital realm.

03:06:05,730 --> 03:06:08,798
SPEAKER_0:  and I think people will find them more compelling, easier, safer.

03:06:09,922 --> 03:06:11,047
SPEAKER_0:  more interesting.

03:06:11,047 --> 03:06:14,014
SPEAKER_1:  little bit captivated by virtual reality by the possible worlds.

03:06:14,306 --> 03:06:16,734
SPEAKER_1:  whether it's the metaverse or some other manifestation of that.

03:06:18,274 --> 03:06:20,382
SPEAKER_1:  Yeah, it's really interesting. It's a

03:06:21,410 --> 03:06:22,910
SPEAKER_1:  I'm interested just.

03:06:23,394 --> 03:06:26,046
SPEAKER_1:  talking a lot to Carmack. Where's the...

03:06:27,202 --> 03:06:29,118
SPEAKER_1:  Where's the thing that's currently preventing that?

03:06:30,114 --> 03:06:31,838
SPEAKER_0:  I mean, to be clear, I think what's interesting about.

03:06:32,098 --> 03:06:32,734
SPEAKER_0:  future is.

03:06:33,282 --> 03:06:33,630
SPEAKER_0:  Um.

03:06:33,858 --> 03:06:34,686
SPEAKER_0:  It's not that...

03:06:35,394 --> 03:06:36,158
SPEAKER_0:  I kind of feel like.

03:06:36,994 --> 03:06:38,750
SPEAKER_0:  the variance in the human condition grows.

03:06:39,106 --> 03:06:41,630
SPEAKER_0:  That's the primary thing that's changing. It's not as much the mean.

03:06:42,178 --> 03:06:47,838
SPEAKER_0:  of the distribution is like the variance of it. So there will probably be people on Mars and there will be people in VR and there will be people here on Earth.

03:06:48,098 --> 03:06:50,238
SPEAKER_0:  It's just like there will be so many more ways of being.

03:06:51,042 --> 03:06:54,174
SPEAKER_0:  And so I kind of feel like I see it as like a spreading out of a human experience.

03:06:54,658 --> 03:07:03,134
SPEAKER_1:  There's something about the internet that allows you to discover those little groups and then you gravitate to something about your biology, that likes that kind of world and that you find each other.

03:07:03,330 --> 03:07:08,205
SPEAKER_0:  and we'll have transhumanists and then we'll have the Amish and they're going to, everything is just going to coexist.

03:07:08,205 --> 03:07:11,134
SPEAKER_1:  about it because I've interacted with a bunch of internet communities.

03:07:11,618 --> 03:07:12,478
SPEAKER_1:  is aussi

03:07:12,930 --> 03:07:13,630
SPEAKER_1:  They don't know.

03:07:14,050 --> 03:07:15,038
SPEAKER_1:  about each other.

03:07:15,522 --> 03:07:20,926
SPEAKER_1:  Like you can have a very happy existence, just like having a very close-knit community and not knowing about each other.

03:07:21,250 --> 03:07:22,878
SPEAKER_1:  We even, you even sense this.

03:07:23,170 --> 03:07:24,574
SPEAKER_1:  just having traveled to Ukraine.

03:07:25,090 --> 03:07:28,830
SPEAKER_1:  They don't know so many things about America.

03:07:29,058 --> 03:07:32,574
SPEAKER_1:  When you travel across the world, I think you experience this.

03:07:32,898 --> 03:07:34,526
SPEAKER_1:  to their certain cultures they're like.

03:07:34,754 --> 03:07:36,190
SPEAKER_1:  They have their own thing going on. They don't.

03:07:36,706 --> 03:07:40,702
SPEAKER_1:  And so you can see that happening more and more and more and more in the future.

03:07:40,930 --> 03:07:42,334
SPEAKER_1:  We have little communities. Yeah.

03:07:42,690 --> 03:07:44,190
SPEAKER_0:  Yeah, I think so. That seems to be the...

03:07:44,450 --> 03:07:46,430
SPEAKER_0:  That seems to be how it's going right now.

03:07:46,786 --> 03:07:48,638
SPEAKER_0:  And I don't see that trend like really reversing.

03:07:48,866 --> 03:07:51,230
SPEAKER_0:  I think people are diverse and they're able to choose their own.

03:07:51,490 --> 03:07:52,446
SPEAKER_0:  like path and existence.

03:07:52,834 --> 03:07:54,110
SPEAKER_0:  and I sort of like celebrate that.

03:07:54,722 --> 03:07:55,198
SPEAKER_0:  Um,

03:07:55,394 --> 03:08:01,086
SPEAKER_1:  And so we spend so much time in the metaverse in the virtual reality or which community area

03:08:01,474 --> 03:08:04,126
SPEAKER_1:  Are you the physicalist?

03:08:04,962 --> 03:08:07,390
SPEAKER_1:  the physical reality enjoyer or.

03:08:07,810 --> 03:08:12,478
SPEAKER_1:  Do you see drawing a lot of pleasure and fulfillment in the digital world?

03:08:13,474 --> 03:08:17,022
SPEAKER_0:  Yeah, I think currently the virtual reality is not that compelling.

03:08:17,410 --> 03:08:20,958
SPEAKER_0:  I do think it can improve a lot, but I don't really know to what extent.

03:08:21,538 --> 03:08:26,718
SPEAKER_0:  Maybe there's actually even more exotic things you can think about with neural links or stuff like that. Thanks.

03:08:27,970 --> 03:08:28,478
SPEAKER_0:  um...

03:08:28,866 --> 03:08:34,398
SPEAKER_0:  Currently I kind of see myself as mostly a team human person. I love nature. Yeah. I love harmony. Die.

03:08:34,754 --> 03:08:35,230
SPEAKER_0:  I love.

03:08:35,682 --> 03:08:37,598
SPEAKER_0:  Humanity, I love emotions of humanity.

03:08:38,114 --> 03:08:38,526
SPEAKER_0:  Um,

03:08:38,946 --> 03:08:44,510
SPEAKER_0:  And I just want to be like in this like solar punk little utopia. That's my happy place. Yes.

03:08:44,770 --> 03:08:45,982
SPEAKER_0:  My happy place is like.

03:08:46,242 --> 03:08:50,686
SPEAKER_0:  people I love thinking about cool problems surrounded by a lush, beautiful, dynamic nature.

03:08:51,490 --> 03:08:54,174
SPEAKER_0:  and secretly high tech in places that count.

03:08:54,402 --> 03:09:00,094
SPEAKER_1:  places like that. So use technology to empower that love for other humans and nature.

03:09:00,546 --> 03:09:02,782
SPEAKER_0:  Yeah, I think a technology used like very sparingly.

03:09:03,106 --> 03:09:06,430
SPEAKER_0:  I don't love when it sort of gets in the way of humanity in many ways.

03:09:06,818 --> 03:09:07,902
SPEAKER_0:  I like.

03:09:08,226 --> 03:09:10,334
SPEAKER_0:  just people being humans in a way, we sort of like...

03:09:10,786 --> 03:09:13,054
SPEAKER_0:  Slightly evolved and prefer, I think, just by default.

03:09:13,314 --> 03:09:17,566
SPEAKER_1:  People kept asking me, because they know you love reading. Are there particular books?

03:09:18,402 --> 03:09:21,182
SPEAKER_1:  that you enjoyed, that had an impact on you.

03:09:21,986 --> 03:09:23,230
SPEAKER_1:  for silly or for...

03:09:23,458 --> 03:09:24,702
SPEAKER_1:  profound reasons.

03:09:24,930 --> 03:09:25,950
SPEAKER_1:  that you'll recommend.

03:09:27,106 --> 03:09:28,638
SPEAKER_1:  You mentioned a lot of question.

03:09:29,378 --> 03:09:37,278
SPEAKER_0:  Many, of course. I think in biology, as an example, the vital question is a good one. Anything by Nic Lane, really life-ascending, I would say, is like a bit more.

03:09:37,794 --> 03:09:38,334
SPEAKER_0:  potentially.

03:09:38,850 --> 03:09:40,830
SPEAKER_0:  representative is like a summary.

03:09:41,794 --> 03:09:43,742
SPEAKER_0:  of a lot of the things he's been talking about.

03:09:44,290 --> 03:09:45,950
SPEAKER_0:  I was very impacted by the selfish gene.

03:09:46,306 --> 03:09:51,614
SPEAKER_0:  I thought that was a really good book that helped me understand altruism as an example and where it comes from and just realizing that.

03:09:51,842 --> 03:09:54,942
SPEAKER_0:  the selection is in a level of genes was a huge insight for me at the time.

03:09:55,170 --> 03:09:56,862
SPEAKER_0:  and it sort of like cleared up a lot of things for me.

03:09:57,218 --> 03:09:58,046
SPEAKER_1:  What do you think about?

03:09:58,850 --> 03:10:02,558
SPEAKER_1:  The idea that ideas are the organisms, the means. Yes, love it.

03:10:02,658 --> 03:10:03,198
SPEAKER_0:  100%.

03:10:03,906 --> 03:10:04,606
SPEAKER_0:  yeah

03:10:04,802 --> 03:10:05,182
SPEAKER_1:  Dude.

03:10:05,794 --> 03:10:09,150
SPEAKER_1:  Are you able to walk around with that notion for a while that

03:10:10,082 --> 03:10:13,457
SPEAKER_1:  that there is an evolutionary kind of process with ideas as well.

03:10:13,457 --> 03:10:17,918
SPEAKER_0:  Absolutely, yes. There's memes just like jeans and they compete and they live in our brains.

03:10:18,434 --> 03:10:19,006
SPEAKER_0:  It's beautiful.

03:10:19,394 --> 03:10:23,870
SPEAKER_1:  Are we silly humans thinking that we're the organisms? Is it possible that the primary...

03:10:24,450 --> 03:10:26,046
SPEAKER_1:  Organisms are the ideas.

03:10:27,202 --> 03:10:33,493
SPEAKER_0:  Yeah, I would say like the ideas kind of live in the software of like our civilization in the in the binds and so on.

03:10:33,493 --> 03:10:40,382
SPEAKER_1:  We think as humans that the hardware is the fundamental thing. I human is a hardware entity.

03:10:41,026 --> 03:10:42,238
SPEAKER_1:  but it could be the software.

03:10:42,818 --> 03:10:43,454
SPEAKER_1:  Right? Yeah.

03:10:44,738 --> 03:10:48,638
SPEAKER_0:  Yeah, I would say there needs to be some grounding at some point to a physical reality.

03:10:48,866 --> 03:10:50,334
SPEAKER_1:  Yeah, but if we clone an Andre...

03:10:52,514 --> 03:10:53,854
SPEAKER_1:  The software is the thing.

03:10:54,658 --> 03:10:58,910
SPEAKER_1:  Like, is this thing that makes that thing special, right? Yeah, I guess I- you're right.

03:10:59,394 --> 03:11:04,126
SPEAKER_1:  But then cloning might be exceptionally difficult. Like there might be a deep integration between the software and the hardware.

03:11:04,482 --> 03:11:06,142
SPEAKER_1:  in ways we don't quite understand.

03:11:06,306 --> 03:11:13,758
SPEAKER_0:  Well from the other side of view, what makes me special is more like the gang of genes that are writing in my chromosomes, I suppose, right? Like they're the...

03:11:13,986 --> 03:11:15,838
SPEAKER_0:  replicating unit I suppose and

03:11:16,066 --> 03:11:18,878
SPEAKER_1:  No, but that's just the thing that makes you special.

03:11:19,330 --> 03:11:19,838
SPEAKER_1:  Sure.

03:11:20,066 --> 03:11:20,510
SPEAKER_1:  Wow.

03:11:22,178 --> 03:11:23,390
SPEAKER_1:  The reality is...

03:11:23,874 --> 03:11:27,166
SPEAKER_1:  What makes you special is your ability to survive.

03:11:28,450 --> 03:11:32,958
SPEAKER_1:  based on the software that runs on the hardware that was built by the genes.

03:11:33,218 --> 03:11:36,606
SPEAKER_1:  So the software is the thing that makes you survive, not the hardware.

03:11:37,378 --> 03:11:38,503
SPEAKER_1:  I guess it's a little bit of both.

03:11:38,503 --> 03:11:40,126
SPEAKER_0:  It's just like a second layer.

03:11:40,354 --> 03:11:44,007
SPEAKER_0:  It's a new second layer that hasn't been there before the brain. They both coexist.

03:11:44,007 --> 03:11:48,190
SPEAKER_1:  but there's also layers of the software. I mean, it's not, it's a...

03:11:48,578 --> 03:11:51,806
SPEAKER_1:  So abstraction on top of abstractions.

03:11:52,002 --> 03:11:54,078
SPEAKER_0:  But okay, so selfish gene.

03:11:54,594 --> 03:11:55,230
SPEAKER_0:  and the claim.

03:11:55,586 --> 03:12:00,222
SPEAKER_0:  I would say sometimes books are like not sufficient. I like to reach for textbooks sometimes.

03:12:00,802 --> 03:12:01,214
SPEAKER_0:  Um.

03:12:01,570 --> 03:12:04,894
SPEAKER_0:  I kind of feel like books are for too much of a general consumption sometimes.

03:12:05,122 --> 03:12:06,462
SPEAKER_0:  And they just kind of like...

03:12:06,882 --> 03:12:11,518
SPEAKER_0:  They're too high up in the level of abstraction and it's not good enough. So I like textbooks, I like this cell.

03:12:12,130 --> 03:12:13,630
SPEAKER_0:  I think the sound was pretty cool.

03:12:14,562 --> 03:12:20,798
SPEAKER_0:  That's why I like the writing of Nick Lane, because he's pretty willing to step one level down.

03:12:21,218 --> 03:12:22,558
SPEAKER_0:  and he doesn't...

03:12:23,234 --> 03:12:24,734
SPEAKER_0:  Yeah, he's sort of, he's willing to go there.

03:12:25,730 --> 03:12:31,102
SPEAKER_0:  but he's also willing to sort of be throughout the stack. So he'll go down to a lot of detail, but then he will come back up and.

03:12:31,394 --> 03:12:32,222
SPEAKER_0:  I think he has a...

03:12:32,642 --> 03:12:33,950
SPEAKER_1:  Yeah, basically, I really appreciate that.

03:12:34,722 --> 03:12:37,342
SPEAKER_1:  That's why I love college, early college, even high school.

03:12:37,698 --> 03:12:39,390
SPEAKER_1:  just textbooks on the basics.

03:12:39,650 --> 03:12:39,998
SPEAKER_1:  Yeah.

03:12:40,482 --> 03:12:44,414
SPEAKER_1:  Of computer science and mathematics of, of biology, of chemistry. Yes.

03:12:44,706 --> 03:12:46,174
SPEAKER_1:  Those are they condense down.

03:12:46,690 --> 03:12:47,358
SPEAKER_1:  Like, uh...

03:12:48,066 --> 03:12:53,054
SPEAKER_1:  Uh, it's sufficiently general that you can understand the both the philosophy and the details, but also like you get

03:12:53,634 --> 03:12:54,942
SPEAKER_1:  homework problems.

03:12:55,170 --> 03:12:57,886
SPEAKER_1:  You get to play with it as much as you would if you were in.

03:12:58,818 --> 03:12:59,966
SPEAKER_1:  programming stuff. Yeah.

03:13:00,066 --> 03:13:09,086
SPEAKER_0:  And then I'm also suspicious of textbooks, honestly, because as an example in deep learning, there's no like amazing textbooks and the field is changing very quickly. I imagine the same is true and say.

03:13:09,346 --> 03:13:11,134
SPEAKER_0:  synthetic biology and so on.

03:13:11,394 --> 03:13:21,438
SPEAKER_0:  these books like The Cell are kind of outdated. They're still high level. Like what is the actual real source of truth? It's people in wet labs working with cells, you know, sequencing genomes and...

03:13:22,882 --> 03:13:34,334
SPEAKER_0:  Yeah, actually working with it. And I don't have that much exposure to that or what that looks like. So I still don't fully, I'm reading through the cell and it's kind of interesting and I'm learning, but it's still not sufficient, I would say, in terms of understanding.

03:13:34,754 --> 03:13:38,078
SPEAKER_1:  Well, it's a clean summarization of the mainstream narrative.

03:13:38,786 --> 03:13:40,286
SPEAKER_1:  Yeah. To learn that.

03:13:40,578 --> 03:13:42,855
SPEAKER_1:  before you break out. Yeah, I think of it.

03:13:42,855 --> 03:13:43,934
SPEAKER_0:  Where's the cutting edge? Ed

03:13:44,194 --> 03:13:48,158
SPEAKER_0:  What is the actual process of working with these cells and growing them and incubating them?

03:13:48,386 --> 03:13:54,238
SPEAKER_0:  It's kind of like a massive cooking recipes of making sure yourself slows and proliferate and then you're sequencing and running experiments and

03:13:54,850 --> 03:13:57,566
SPEAKER_0:  Just how that works, I think is kind of like the source of truth of

03:13:57,794 --> 03:14:00,094
SPEAKER_0:  at the end of the day what's really useful in terms of.

03:14:00,386 --> 03:14:01,502
SPEAKER_0:  creating therapies and so on.

03:14:01,954 --> 03:14:04,606
SPEAKER_1:  I wonder what in the future AI textbooks will be.

03:14:04,930 --> 03:14:08,894
SPEAKER_1:  Because you know, there's artificial intelligence, modern approach actually haven't read.

03:14:09,154 --> 03:14:10,750
SPEAKER_1:  if it's come out the recent version.

03:14:10,978 --> 03:14:12,990
SPEAKER_1:  the recent, there's been a recent addition.

03:14:13,410 --> 03:14:15,230
SPEAKER_1:  I also saw there's a science of deep learning.

03:14:15,874 --> 03:14:19,806
SPEAKER_1:  I'm waiting for textbooks that worth recommending, worth reading. Yeah, it's just tricky.

03:14:20,258 --> 03:14:21,470
SPEAKER_1:  because it's like papers.

03:14:22,210 --> 03:14:23,335
SPEAKER_1:  and code code.

03:14:23,335 --> 03:14:25,470
SPEAKER_0:  Honestly, I think papers are quite good.

03:14:25,762 --> 03:14:31,299
SPEAKER_0:  I especially like the appendix of any paper as well. It's like the most detail you can have.

03:14:31,299 --> 03:14:31,806
SPEAKER_1:

03:14:33,154 --> 03:14:39,181
SPEAKER_1:  It doesn't have to be cohesive to connect it to anything else. You just described me a very specific way you saw the particular thing. Yeah.

03:14:39,181 --> 03:14:45,502
SPEAKER_0:  Many times papers can be actually quite readable, not always, but sometimes the introduction and the abstract is readable even for someone outside of the field.

03:14:45,986 --> 03:14:47,486
SPEAKER_0:  This is not always true.

03:14:47,746 --> 03:14:52,126
SPEAKER_0:  And sometimes I think unfortunately scientists use complex terms even when it's not necessary.

03:14:52,610 --> 03:14:53,566
SPEAKER_0:  I think that's harmful.

03:14:54,050 --> 03:14:55,614
SPEAKER_0:  I think there's no reason for that.

03:14:55,810 --> 03:14:56,510
SPEAKER_1:  and papers.

03:14:56,834 --> 03:14:59,774
SPEAKER_1:  Sometimes they're longer than they need to be in the parts that.

03:15:00,578 --> 03:15:01,534
SPEAKER_1:  Don't matter.

03:15:02,018 --> 03:15:06,366
SPEAKER_1:  Appendix should be long, but then the paper itself, you know, look at Einstein make it simple

03:15:07,106 --> 03:15:13,182
SPEAKER_0:  But certainly I've come across papers I would say in say like synthetic biology or something that I thought were quite readable for the abstract and the introduction

03:15:13,410 --> 03:15:17,822
SPEAKER_0:  and then you're reading the rest of it and you don't fully understand but you kind of are getting a gist and I think it's cool.

03:15:19,010 --> 03:15:19,646
SPEAKER_1:

03:15:20,194 --> 03:15:21,598
SPEAKER_1:  What are advice?

03:15:21,890 --> 03:15:28,222
SPEAKER_1:  You give advice to folks interested in machine learning and research, but in general, life advice to a young person. High school.

03:15:29,122 --> 03:15:34,334
SPEAKER_1:  early college about how to have a career they can be proud of or a life they can be proud of.

03:15:35,650 --> 03:15:43,806
SPEAKER_0:  Yeah, I think I'm very hesitant to give general advice. I think it's really hard. I've mentioned like some of the stuff I've mentioned is fairly general, I think, like focus on just the amount of work you're spending.

03:15:44,130 --> 03:15:44,798
SPEAKER_0:  unlike a thing.

03:15:45,506 --> 03:15:49,726
SPEAKER_0:  compare yourself only to yourself not to others. That's good. I think those are fairly general.

03:15:49,954 --> 03:15:50,846
SPEAKER_0:  How do you pick the thing?

03:15:51,970 --> 03:15:54,910
SPEAKER_0:  You just have like a deep interest in something.

03:15:55,394 --> 03:15:57,150
SPEAKER_0:  or try to find the Art Max.

03:15:57,410 --> 03:15:58,535
SPEAKER_0:  over the things that you're interested in.

03:15:58,535 --> 03:16:02,430
SPEAKER_1:  Armax at that moment and stick with it. How do you not get distracted?

03:16:02,658 --> 03:16:04,414
SPEAKER_1:  and switch to another thing.

03:16:05,026 --> 03:16:06,151
SPEAKER_1:  You can, if you like.

03:16:06,151 --> 03:16:06,558
SPEAKER_0:  anyone alike?

03:16:07,490 --> 03:16:10,174
SPEAKER_1:  Well, if you do an argmax repeatedly

03:16:10,434 --> 03:16:12,309
SPEAKER_1:  every week, every month. Converge.

03:16:12,309 --> 03:16:17,918
SPEAKER_0:  That's the problem. Yeah, you can low pass filter yourself in terms of what has consistently been true for you.

03:16:18,594 --> 03:16:19,102
SPEAKER_0:  Um

03:16:19,362 --> 03:16:20,958
SPEAKER_0:  But yeah, I definitely see how.

03:16:21,634 --> 03:16:25,726
SPEAKER_0:  it can be hard, but I would say like, you're going to work the hardest on the thing that you care about the most.

03:16:25,954 --> 03:16:28,638
SPEAKER_0:  So low pass filter yourself and really introspect.

03:16:29,026 --> 03:16:34,270
SPEAKER_0:  in your past, what are the things that gave you energy and what are the things that took energy away from you? Concrete examples.

03:16:34,626 --> 03:16:37,918
SPEAKER_0:  And usually from those concrete examples, sometimes patterns can emerge.

03:16:38,594 --> 03:16:41,141
SPEAKER_0:  I like it when things look like this when I'm in this position.

03:16:41,141 --> 03:16:45,502
SPEAKER_1:  So that's not necessarily the field but the kind of stuff you're doing in a particular field. So for you

03:16:45,986 --> 03:16:48,062
SPEAKER_1:  It seems like you were energized by...

03:16:48,418 --> 03:16:50,334
SPEAKER_1:  Implementing stuff building actual things

03:16:50,658 --> 03:16:52,382
SPEAKER_0:  Yeah, being low level, learning.

03:16:52,610 --> 03:16:58,014
SPEAKER_0:  and then also communicating so that others can go through the same realizations and shortening that gap.

03:16:58,466 --> 03:16:58,878
SPEAKER_0:  Um.

03:16:59,266 --> 03:17:08,126
SPEAKER_0:  because I usually have to do way too much work to understand a thing. And then I'm like, okay, this is actually like, okay, I think I get it. And like, why was it so much work? It should have been much less work.

03:17:08,418 --> 03:17:12,062
SPEAKER_0:  And that gives me a lot of frustration. And that's why I sometimes go teach.

03:17:12,610 --> 03:17:14,910
SPEAKER_1:  So aside from the teaching you're doing now.

03:17:15,170 --> 03:17:16,478
SPEAKER_1:  putting our videos

03:17:17,058 --> 03:17:18,974
SPEAKER_1:  aside from a potential

03:17:19,682 --> 03:17:20,862
SPEAKER_1:  Godfather Part 2.

03:17:21,602 --> 03:17:28,510
SPEAKER_1:  uh, with the AGI at Tesla and beyond. Uh, what does the future of Fondre Capati hold? Have you figured that out yet or no?

03:17:28,770 --> 03:17:29,342
SPEAKER_1:  I mean...

03:17:29,794 --> 03:17:31,998
SPEAKER_1:  as you see through the fog of war.

03:17:32,610 --> 03:17:33,854
SPEAKER_1:  that is all of our future.

03:17:34,594 --> 03:17:36,222
SPEAKER_1:  Um, do you, do you.

03:17:36,482 --> 03:17:39,902
SPEAKER_1:  start seeing what that possible future could look like.

03:17:40,994 --> 03:17:45,406
SPEAKER_0:  The consistent thing I've been always interested in for me at least is AI.

03:17:46,530 --> 03:17:50,494
SPEAKER_0:  That's probably what I'm spending the rest of my life on because I just care about it a lot.

03:17:50,850 --> 03:17:54,238
SPEAKER_0:  And I actually care about many other problems as well, like say aging.

03:17:54,498 --> 03:17:55,966
SPEAKER_0:  which I basically view as disease.

03:17:56,258 --> 03:17:56,990
SPEAKER_0:  and um

03:17:57,794 --> 03:18:01,246
SPEAKER_0:  I care about that as well, but I don't think it's a good idea to go after it.

03:18:01,474 --> 03:18:04,830
SPEAKER_0:  specifically. I don't actually think that humans will be able to...

03:18:05,058 --> 03:18:05,790
SPEAKER_0:  Come up with the answer.

03:18:06,242 --> 03:18:08,510
SPEAKER_0:  I think the correct thing to do is to ignore those problems.

03:18:08,802 --> 03:18:11,454
SPEAKER_0:  and you solve AI and then use that to solve everything else.

03:18:11,842 --> 03:18:14,302
SPEAKER_0:  And I think there's a chance that this will work. I think it's a very high chance.

03:18:14,818 --> 03:18:16,574
SPEAKER_0:  And that's kind of like the...

03:18:17,250 --> 03:18:18,366
SPEAKER_0:  The way I'm betting, at least.

03:18:18,466 --> 03:18:21,214
SPEAKER_1:  So when you think about AI, are you interested in?

03:18:21,762 --> 03:18:24,638
SPEAKER_1:  all kinds of applications, all kinds of domains.

03:18:25,186 --> 03:18:29,918
SPEAKER_1:  and any domain you focus on will allow you to get insights of the big problem of AGI.

03:18:30,018 --> 03:18:31,646
SPEAKER_0:  Yeah, for me it's the ultimate meta problem.

03:18:31,906 --> 03:18:37,630
SPEAKER_0:  I don't want to work on any one specific problem, there's too many problems. So how can you work on all problems simultaneously? To solve the meta problem.

03:18:37,858 --> 03:18:40,158
SPEAKER_0:  which to me is just intelligence and how do you...

03:18:40,770 --> 03:18:41,694
SPEAKER_0:  automated.

03:18:42,338 --> 03:18:45,822
SPEAKER_1:  Is there cool small projects like Archive Sanity and...

03:18:46,434 --> 03:18:47,902
SPEAKER_1:  and so on that you're thinking about.

03:18:48,322 --> 03:18:48,702
SPEAKER_1:  Don't.

03:18:48,962 --> 03:18:52,414
SPEAKER_1:  that the world, the ML world can anticipate.

03:18:53,186 --> 03:18:56,510
SPEAKER_0:  There's always some fun side projects. Arc of Sanity is one.

03:18:56,738 --> 03:18:59,870
SPEAKER_0:  Basically, there's way too many archive papers, how can I organize it?

03:19:00,130 --> 03:19:01,854
SPEAKER_0:  and recommend papers and so on.

03:19:02,434 --> 03:19:05,059
SPEAKER_0:  I transcribed all of your podcasts.

03:19:05,059 --> 03:19:07,038
SPEAKER_1:  Where did you learn from that experience?

03:19:07,458 --> 03:19:08,542
SPEAKER_1:  transcribing.

03:19:08,962 --> 03:19:12,798
SPEAKER_1:  the process of like you like consuming audio books and podcasts and so on.

03:19:13,026 --> 03:19:15,134
SPEAKER_1:  Here's a process that achieves.

03:19:15,778 --> 03:19:16,222
SPEAKER_1:  Um...

03:19:16,482 --> 03:19:19,614
SPEAKER_1:  closer to human level performance on annotation. Yeah...

03:19:19,778 --> 03:19:22,430
SPEAKER_0:  I definitely was surprised at that transcription.

03:19:22,818 --> 03:19:24,862
SPEAKER_0:  with opening eyes whisper was working so well.

03:19:25,154 --> 03:19:27,774
SPEAKER_0:  compared to what I'm familiar with from Siri and like...

03:19:28,034 --> 03:19:29,150
SPEAKER_0:  few other systems, I guess.

03:19:29,570 --> 03:19:30,718
SPEAKER_0:  It works so well and.

03:19:31,714 --> 03:19:36,318
SPEAKER_0:  That's what gave me some energy to try it out and I thought it could be fun to run them podcasts.

03:19:36,738 --> 03:19:37,150
SPEAKER_0:  It's kind of.

03:19:37,602 --> 03:19:38,974
SPEAKER_0:  Not obviously why.

03:19:39,650 --> 03:19:49,246
SPEAKER_0:  Whisper is so much better compared to anything else because I feel like there should be a lot of incentive for a lot of companies to produce transcription systems and that they've done so over a long time. Whisper is not a super exotic model, it's a transformer.

03:19:49,666 --> 03:19:50,014
SPEAKER_0:

03:19:50,242 --> 03:19:51,934
SPEAKER_0:  It takes smell spectrograms and...

03:19:52,322 --> 03:19:55,614
SPEAKER_0:  you know, just outputs tokens of text. It's not crazy.

03:19:55,842 --> 03:19:57,438
SPEAKER_0:  to model and everything has been around for a long time.

03:19:58,498 --> 03:20:00,373
SPEAKER_0:  I'm not actually 100% sure why this game is.

03:20:00,373 --> 03:20:09,406
SPEAKER_1:  It's not obvious to me either. It makes me feel like I'm missing something. I'm missing something. Yeah, because there is a huge, even at Google and so on YouTube.

03:20:09,730 --> 03:20:15,262
SPEAKER_1:  transcription. Yeah, it's unclear, but some of it is also integrating into a bigger system.

03:20:16,546 --> 03:20:20,414
SPEAKER_1:  that is so the user interface house deployed and all that kind of stuff maybe

03:20:20,674 --> 03:20:27,550
SPEAKER_1:  running it as an independent thing is much easier, like an order of magnitude easier than deploying a large integrated system.

03:20:27,778 --> 03:20:29,630
SPEAKER_1:  like YouTube transcription or...

03:20:30,274 --> 03:20:34,206
SPEAKER_1:  Um, anything like meetings, like zoom has trans transcription.

03:20:34,946 --> 03:20:36,158
SPEAKER_1:  That's kinda crappy.

03:20:36,546 --> 03:20:40,158
SPEAKER_1:  but creating interface where it detects different individual speakers

03:20:40,546 --> 03:20:42,206
SPEAKER_1:  It's able to.

03:20:43,202 --> 03:20:44,446
SPEAKER_1:  display it in.

03:20:44,738 --> 03:20:48,542
SPEAKER_1:  compelling ways, run in real time, all that kind of stuff. Maybe that's difficult.

03:20:49,762 --> 03:20:52,126
SPEAKER_1:  That's the only explanation I have because like, um...

03:20:53,090 --> 03:20:55,134
SPEAKER_1:  I'm currently paying quite a bit.

03:20:55,490 --> 03:20:59,230
SPEAKER_1:  for human transcription, human caption annotation.

03:20:59,714 --> 03:21:00,894
SPEAKER_1:  And like, it seems like...

03:21:01,730 --> 03:21:03,806
SPEAKER_1:  There's a huge incentive to automate that.

03:21:04,322 --> 03:21:05,447
SPEAKER_1:  It's very confusing. Correct.

03:21:05,447 --> 03:21:08,094
SPEAKER_0:  I mean, I don't know if you looked at some of the whisper transcripts, but they're-

03:21:08,482 --> 03:21:08,958
SPEAKER_0:  Quite good.

03:21:09,090 --> 03:21:13,022
SPEAKER_1:  They're good. And especially in tricky cases. Yeah, I've seen.

03:21:13,634 --> 03:21:18,750
SPEAKER_1:  uh... whispers performance unlike super tricky cases and does incredibly well so

03:21:19,138 --> 03:21:21,342
SPEAKER_1:  I don't know, a podcast is pretty simple. I don't know, a podcast is pretty simple.

03:21:21,634 --> 03:21:23,102
SPEAKER_1:  high quality audio.

03:21:23,458 --> 03:21:28,222
SPEAKER_1:  and you're speaking usually pretty clearly. So I don't know it uh

03:21:28,706 --> 03:21:31,582
SPEAKER_1:  I don't know what OpenAI's plans are either.

03:21:31,874 --> 03:21:33,342
SPEAKER_0:  but yeah, there's always like fun.

03:21:33,570 --> 03:21:47,870
SPEAKER_0:  Fun projects basically. And stable diffusion also is opening up a huge amount of experimentation, I would say in the visual realm, and generating images and videos and movies, ultimately. Yeah, videos now. And so that's going to be pretty crazy. That's going to almost certainly work.

03:21:48,322 --> 03:21:50,078
SPEAKER_0:  and is going to be a really interesting one.

03:21:50,530 --> 03:21:52,542
SPEAKER_0:  cost of content creation is going to fall to zero.

03:21:52,962 --> 03:21:57,310
SPEAKER_0:  You used to need a painter for a few months to paint a thing, and now it's going to be speak to your phone.

03:21:57,602 --> 03:21:58,654
SPEAKER_0:  to get your video.

03:21:59,170 --> 03:22:01,918
SPEAKER_1:  So if Hollywood will start using that to generate scenes.

03:22:03,746 --> 03:22:08,958
SPEAKER_1:  which completely opens up, yeah, so you can make a movie like Avatar.

03:22:09,442 --> 03:22:11,998
SPEAKER_1:  eventually for under a million dollars.

03:22:12,386 --> 03:22:14,398
SPEAKER_0:  much less maybe just by talking to your phone.

03:22:14,626 --> 03:22:16,766
SPEAKER_0:  I mean, I know it sounds kind of crazy.

03:22:17,698 --> 03:22:24,926
SPEAKER_1:  And then there'd be some voting mechanism. Like how do you have a, like, would there be a show on Netflix that's generated completely, uh, automatedly.

03:22:25,506 --> 03:22:26,631
SPEAKER_1:  semi-impotentially.

03:22:26,631 --> 03:22:27,038
SPEAKER_0:  Yeah.

03:22:27,394 --> 03:22:32,254
SPEAKER_0:  And what does it look like also when you can generate it on demand? And it's...

03:22:32,514 --> 03:22:33,630
SPEAKER_0:  and there's infinity of it.

03:22:33,986 --> 03:22:35,774
SPEAKER_1:  Yeah. All right so a lot of you guys are probably probably

03:22:36,034 --> 03:22:36,958
SPEAKER_1:  Oh man.

03:22:38,178 --> 03:22:40,606
SPEAKER_1:  all the synthetic art I mean it's humbling because

03:22:40,898 --> 03:22:45,694
SPEAKER_1:  We treat ourselves as special for being able to generate art and ideas and all that kind of stuff.

03:22:46,338 --> 03:22:49,822
SPEAKER_1:  if that can be done in an automated way by AI.

03:22:49,922 --> 03:22:50,270
SPEAKER_0:  Yeah.

03:22:50,946 --> 03:22:57,182
SPEAKER_0:  I think it's fascinating to me how these, the predictions of AI and what it's going to look like and what it's going to be capable of are completely inverted and wrong.

03:22:57,634 --> 03:23:00,926
SPEAKER_0:  And sci-fi of 50s and 60s was just like totally not right.

03:23:01,346 --> 03:23:08,030
SPEAKER_0:  They imagine AI as like super calculating theory improvers and we're getting things that can talk to you about emotions. They can do art.

03:23:08,930 --> 03:23:10,014
SPEAKER_0:  It's just like weird.

03:23:10,274 --> 03:23:11,582
SPEAKER_1:  Are you excited about that future?

03:23:11,906 --> 03:23:12,926
SPEAKER_1:  just, yeah, it's...

03:23:13,154 --> 03:23:13,502
SPEAKER_1:  Like.

03:23:13,954 --> 03:23:15,102
SPEAKER_1:  hybrid systems.

03:23:15,842 --> 03:23:19,070
SPEAKER_1:  heterogeneous systems of humans and AIs talking about emotions.

03:23:19,554 --> 03:23:25,470
SPEAKER_1:  Netflix and children AI system that's where the Netflix thing you watch is also generated by AI.

03:23:26,082 --> 03:23:29,054
SPEAKER_0:  I think it's going to be interesting for sure. the

03:23:29,346 --> 03:23:32,894
SPEAKER_0:  And I think I'm cautiously optimistic, but it's not obvious.

03:23:33,698 --> 03:23:36,094
SPEAKER_1:  Well the sad thing is your brain and mine.

03:23:36,482 --> 03:23:37,118
SPEAKER_1:  developed.

03:23:37,346 --> 03:23:38,078
SPEAKER_1:  in a time.

03:23:38,882 --> 03:23:43,454
SPEAKER_1:  where before Twitter, before the internet. I wasCh

03:23:43,682 --> 03:23:46,942
SPEAKER_1:  people that are born inside of it might have a different experience.

03:23:47,266 --> 03:23:50,910
SPEAKER_1:  Like I, maybe you can still resist it.

03:23:51,426 --> 03:23:53,301
SPEAKER_1:  and the people born now.

03:23:53,301 --> 03:23:53,950
SPEAKER_0:  I will not.

03:23:54,754 --> 03:23:56,638
SPEAKER_0:  Well, I do feel like humans are extremely malleable.

03:23:56,802 --> 03:23:57,118
SPEAKER_1:  Yeah.

03:23:58,370 --> 03:23:58,942
SPEAKER_1:  And uh...

03:23:59,298 --> 03:23:59,870
SPEAKER_1:  You're probably right.

03:24:01,026 --> 03:24:02,814
SPEAKER_1:  What is the meaning of life, Andre?

03:24:04,930 --> 03:24:06,334
SPEAKER_1:  We talked about sort of...

03:24:07,362 --> 03:24:13,342
SPEAKER_1:  The universe having a conversation with us humans or with the systems we create to try to answer.

03:24:14,082 --> 03:24:15,006
SPEAKER_1:  for the universe to

03:24:15,490 --> 03:24:19,038
SPEAKER_1:  for the creator of the universe to notice us. We're trying to create systems.

03:24:19,522 --> 03:24:20,638
SPEAKER_1:  They're loud enough.

03:24:21,410 --> 03:24:22,558
SPEAKER_1:  just answer back.

03:24:23,778 --> 03:24:26,718
SPEAKER_0:  I don't know if that's the meaning of life, that's like meaning of life for some people.

03:24:26,978 --> 03:24:28,510
SPEAKER_0:  The first level answer I would say is.

03:24:28,738 --> 03:24:32,606
SPEAKER_0:  Anyone can choose their own meaning of life because we are a conscious entity and it's beautiful.

03:24:32,866 --> 03:24:33,630
SPEAKER_0:  Number one.

03:24:34,018 --> 03:24:34,750
SPEAKER_0:  But uh...

03:24:35,298 --> 03:24:41,342
SPEAKER_0:  I do think that like a deeper meaning of life as someone who's interested is along the lines of like, what the hell is all this?

03:24:42,178 --> 03:24:43,006
SPEAKER_0:  and like why.

03:24:43,362 --> 03:24:43,838
SPEAKER_0:  end.

03:24:44,290 --> 03:24:47,870
SPEAKER_0:  if you look into fundamental physics and the quantum field theory in the standard model

03:24:48,194 --> 03:24:50,814
SPEAKER_0:  They're like very complicated.

03:24:51,778 --> 03:24:55,006
SPEAKER_0:  there's this like 19 free parameters of our universe.

03:24:55,458 --> 03:24:57,342
SPEAKER_0:  and like what's going on with all this stuff.

03:24:57,698 --> 03:24:59,486
SPEAKER_0:  and was it here and can I hack it?

03:24:59,714 --> 03:25:02,622
SPEAKER_0:  Can I work with it? Is there a message for me? Am I supposed to create a message?

03:25:03,362 --> 03:25:05,246
SPEAKER_0:  And so I think there's some fundamental answers there.

03:25:05,634 --> 03:25:07,070
SPEAKER_0:  But I think there's actually even like...

03:25:07,714 --> 03:25:10,878
SPEAKER_0:  You can't actually really make dent in those without more time.

03:25:11,234 --> 03:25:12,606
SPEAKER_0:  And so to me also there's a big.

03:25:13,122 --> 03:25:15,166
SPEAKER_0:  question around just getting more time honestly.

03:25:15,970 --> 03:25:17,822
SPEAKER_0:  Yeah, that's kind of like what I think about quite a bit as well.

03:25:18,146 --> 03:25:18,910
SPEAKER_1:  So kind of.

03:25:19,490 --> 03:25:20,158
SPEAKER_1:  The ultimate.

03:25:21,282 --> 03:25:23,966
SPEAKER_1:  or at least first way to sneak up to the why.

03:25:24,578 --> 03:25:26,366
SPEAKER_1:  question is to try to escape.

03:25:27,650 --> 03:25:28,126
SPEAKER_1:  uh...

03:25:28,610 --> 03:25:29,278
SPEAKER_1:  the system.

03:25:29,538 --> 03:25:31,422
SPEAKER_1:  the universe. And then for that.

03:25:31,906 --> 03:25:37,854
SPEAKER_1:  You sort of backtrack and say, okay, for that, that's going to take a very long time. So the why question.

03:25:38,146 --> 03:25:41,182
SPEAKER_1:  boils down from an engineering perspective to how do we extend.

03:25:41,282 --> 03:25:44,574
SPEAKER_0:  Yeah, I think that's the question number one, practically speaking, because you can't...

03:25:44,802 --> 03:25:48,638
SPEAKER_0:  you're not going to calculate the answer to the deeper questions in time you have.

03:25:49,026 --> 03:25:53,406
SPEAKER_1:  and that could be extending your own lifetime or extending just the lifetime of human civilization.

03:25:53,634 --> 03:25:56,510
SPEAKER_0:  of whoever wants to. Not many people might not want that.

03:25:56,930 --> 03:25:59,742
SPEAKER_0:  But I think people who do want that, I think...

03:26:00,418 --> 03:26:01,470
SPEAKER_0:  I think it's probably possible.

03:26:01,922 --> 03:26:03,838
SPEAKER_0:  And I don't think I don't know that people

03:26:04,546 --> 03:26:10,878
SPEAKER_0:  fully realized this. I kind of feel like people think of death as an inevitability, but at the end of the day this is a physical system.

03:26:11,170 --> 03:26:12,030
SPEAKER_0:  Somethings go wrong.

03:26:12,546 --> 03:26:13,982
SPEAKER_0:  It makes sense why.

03:26:14,402 --> 03:26:16,350
SPEAKER_0:  things like this happen evolutionarily speaking.

03:26:16,802 --> 03:26:17,886
SPEAKER_0:  and there's.

03:26:18,114 --> 03:26:20,830
SPEAKER_0:  most certainly interventions that mitigate it.

03:26:21,122 --> 03:26:24,190
SPEAKER_1:  That'd be interesting if death is eventually looked at as

03:26:24,898 --> 03:26:27,806
SPEAKER_1:  as a fascinating thing that used to happen.

03:26:28,130 --> 03:26:28,798
SPEAKER_1:  to humans.

03:26:28,962 --> 03:26:30,814
SPEAKER_0:  I don't think it's unlikely, I think it's...

03:26:31,170 --> 03:26:31,838
SPEAKER_0:  I think it's likely.

03:26:33,378 --> 03:26:39,038
SPEAKER_1:  And it's up to our imagination to try to predict what the world without death looks like.

03:26:39,746 --> 03:26:40,830
SPEAKER_1:  Yeah, it's hard to...

03:26:41,090 --> 03:26:43,070
SPEAKER_1:  I think the values will completely change.

03:26:44,482 --> 03:26:45,086
SPEAKER_1:  could be.

03:26:45,346 --> 03:26:48,638
SPEAKER_0:  I don't really buy all these ideas that, oh, without...

03:26:48,866 --> 03:26:51,806
SPEAKER_0:  There's no meaning. There's nothing as

03:26:52,418 --> 03:26:54,622
SPEAKER_0:  I don't intuitively buy all those arguments.

03:26:54,946 --> 03:26:59,678
SPEAKER_0:  I think there's plenty of meaning, plenty of things to learn. They're interesting, exciting. I want to know. I think there's plenty of meaning, plenty of things to learn.

03:27:00,354 --> 03:27:01,918
SPEAKER_0:  I want to improve the condition.

03:27:02,530 --> 03:27:02,846
SPEAKER_0:  off.

03:27:03,586 --> 03:27:05,214
SPEAKER_0:  of the humans and organisms that are alive.

03:27:05,698 --> 03:27:07,806
SPEAKER_1:  at the way we find meaning might change.

03:27:08,130 --> 03:27:11,358
SPEAKER_1:  There is a lot of humans, probably including myself, that...

03:27:11,586 --> 03:27:14,014
SPEAKER_1:  finds meaning in the finiteness of things.

03:27:14,626 --> 03:27:16,766
SPEAKER_1:  but that doesn't mean that's the only source of meaning. Yeah,

03:27:17,378 --> 03:27:18,814
SPEAKER_0:  I do think many people will...

03:27:19,074 --> 03:27:20,670
SPEAKER_0:  we'll go with that, which I think is great.

03:27:21,154 --> 03:27:23,390
SPEAKER_0:  I love the idea that people can just choose their own adventure.

03:27:24,098 --> 03:27:26,526
SPEAKER_0:  Like you are born as a conscious free entity.

03:27:26,818 --> 03:27:27,902
SPEAKER_0:  By default, I'd like to think.

03:27:28,162 --> 03:27:29,118
SPEAKER_0:  Yeah.

03:27:29,538 --> 03:27:33,182
SPEAKER_0:  you have your unalienable rights for life.

03:27:33,410 --> 03:27:34,654
SPEAKER_0:  in the pursuit of happiness.

03:27:34,818 --> 03:27:36,382
SPEAKER_1:  I don't know if you have that.

03:27:36,834 --> 03:27:39,550
SPEAKER_1:  And the nature the landscape of happiness.

03:27:39,714 --> 03:27:41,278
SPEAKER_0:  and you can choose your own adventure mostly.

03:27:41,538 --> 03:27:42,846
SPEAKER_0:  And that's not...

03:27:43,010 --> 03:27:45,822
SPEAKER_1:  Fully true, but I still am pretty sure I'm an NPC.

03:27:46,082 --> 03:27:49,630
SPEAKER_1:  but an NPC can't know it's an NPC.

03:27:51,458 --> 03:27:53,854
SPEAKER_1:  There could be different degrees and levels of consciousness.

03:27:54,306 --> 03:27:55,646
SPEAKER_1:  I don't think there's a more...

03:27:56,322 --> 03:28:03,934
SPEAKER_1:  beautiful way tended. Andre, you're an incredible person. I'm really honored you would talk with me. Everything you've done for the machine learning world.

03:28:04,162 --> 03:28:05,182
SPEAKER_1:  for the AI world.

03:28:06,210 --> 03:28:09,406
SPEAKER_1:  to just inspire people to educate millions of people. that's been

03:28:09,794 --> 03:28:14,782
SPEAKER_1:  It's been great and I can't wait to see what you do next. It's been an honor, man. Thank you so much for talking to me. Awesome, thank you.

03:28:15,778 --> 03:28:17,534
SPEAKER_1:  Thanks for listening to this conversation.

03:28:17,890 --> 03:28:19,070
SPEAKER_1:  with Andre Kapathy.

03:28:19,362 --> 03:28:23,230
SPEAKER_1:  to support this podcast, please check out our sponsors in the description.

03:28:23,650 --> 03:28:24,190
SPEAKER_1:  And now...

03:28:24,546 --> 03:28:26,398
SPEAKER_1:  Let me leave you with some words from.

03:28:26,722 --> 03:28:27,870
SPEAKER_1:  Samuel Carlin.

03:28:28,642 --> 03:28:29,950
SPEAKER_1:  The purpose of models.

03:28:30,178 --> 03:28:31,454
SPEAKER_1:  is not to fit the data.

03:28:32,194 --> 03:28:33,182
SPEAKER_1:  But this sharpen...

03:28:33,506 --> 03:28:34,366
SPEAKER_1:  the questions.

03:28:35,554 --> 03:28:37,310
SPEAKER_1:  Thanks for listening and hope to see you.

03:28:37,698 --> 03:28:38,206
SPEAKER_1:  next time.
