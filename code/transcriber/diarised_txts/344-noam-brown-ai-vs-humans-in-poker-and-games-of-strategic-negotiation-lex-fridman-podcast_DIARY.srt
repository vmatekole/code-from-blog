00:00:00,098 --> 00:00:04,286
SPEAKER_0:  A lot of people were saying like, oh, this whole idea of game theory, it's just nonsense.soft? chewing rotating phone

00:00:04,514 --> 00:00:09,118
SPEAKER_0:  If you really want to make money, you've got to look into the other person's eyes and read their soul and figure out...

00:00:09,346 --> 00:00:10,206
SPEAKER_0:  what cards they have.

00:00:10,722 --> 00:00:15,550
SPEAKER_0:  But what happened was where we played our bot against four top heads up no limit hold'em

00:00:15,810 --> 00:00:16,510
SPEAKER_0:  Poker players.

00:00:17,026 --> 00:00:18,014
SPEAKER_0:  and the bot.

00:00:18,242 --> 00:00:24,798
SPEAKER_0:  wasn't trying to adapt to them, it wasn't trying to exploit them, it wasn't trying to do these mind games, it was just trying to approximate the Nash equilibrium.

00:00:25,250 --> 00:00:26,014
SPEAKER_0:  and it crushed them.

00:00:28,738 --> 00:00:30,654
SPEAKER_1:  The following is a conversation with No Brown.

00:00:31,106 --> 00:00:32,478
SPEAKER_1:  Research scientists at FAIR.

00:00:32,770 --> 00:00:35,326
SPEAKER_1:  Facebook AI research group at MetaAI.

00:00:35,682 --> 00:00:38,078
SPEAKER_1:  He co-created the first AI system.

00:00:38,338 --> 00:00:42,078
SPEAKER_1:  that achieved superhuman level performance in no limit Texas hold them.

00:00:42,434 --> 00:00:43,966
SPEAKER_1:  both heads up and multiplayer.

00:00:44,578 --> 00:00:45,118
SPEAKER_1:  And now...

00:00:45,442 --> 00:00:46,078
SPEAKER_1:  recently.

00:00:46,402 --> 00:00:51,070
SPEAKER_1:  He co-created an AI system that can strategically out-negotiate humans.

00:00:51,362 --> 00:00:52,638
SPEAKER_1:  using natural language.

00:00:52,866 --> 00:00:55,038
SPEAKER_1:  in a popular board game called Diplomacy.

00:00:55,362 --> 00:00:56,350
SPEAKER_1:  which is a war game.

00:00:56,578 --> 00:00:58,558
SPEAKER_1:  that emphasizes negotiation.

00:00:59,714 --> 00:01:01,406
SPEAKER_1:  This is the Lex Friedman podcast.

00:01:01,634 --> 00:01:02,238
SPEAKER_1:  to support it.

00:01:02,466 --> 00:01:04,542
SPEAKER_1:  Please check out our sponsors in the description.

00:01:04,898 --> 00:01:05,438
SPEAKER_1:  And now...

00:01:05,730 --> 00:01:06,462
SPEAKER_1:  Dear friends.

00:01:06,850 --> 00:01:07,486
SPEAKER_1:  Here's Gnome.

00:01:07,938 --> 00:01:08,286
SPEAKER_1:  crowd.

00:01:09,410 --> 00:01:14,878
SPEAKER_1:  You've been a lead on three amazing AI projects. So we've got Labratus that's solved.

00:01:15,138 --> 00:01:21,182
SPEAKER_1:  or at least achieved human level performance on No Limit Texas Hold'em poker with two players. Heads up!

00:01:22,178 --> 00:01:22,558
SPEAKER_1:  You got it.

00:01:23,170 --> 00:01:23,838
SPEAKER_1:  Pluribus.

00:01:24,098 --> 00:01:26,430
SPEAKER_1:  That's solved No Limit Texas Hold'em poker with.

00:01:26,658 --> 00:01:27,550
SPEAKER_1:  Six players.

00:01:28,162 --> 00:01:31,518
SPEAKER_1:  And just now you have Cicero, these are all names of systems.

00:01:32,002 --> 00:01:34,526
SPEAKER_1:  that solved or achieved human level.

00:01:34,914 --> 00:01:37,246
SPEAKER_1:  Performance on the Game of Diplomacy.

00:01:37,730 --> 00:01:39,486
SPEAKER_1:  which for people who don't know.

00:01:40,034 --> 00:01:41,822
SPEAKER_1:  is a popular strategy board game.

00:01:42,178 --> 00:01:45,054
SPEAKER_1:  It was loved by JFK, John F. Kennedy.

00:01:45,314 --> 00:01:46,686
SPEAKER_1:  and Henry Kissinger.

00:01:47,042 --> 00:01:48,318
SPEAKER_1:  and many other.

00:01:49,154 --> 00:01:49,566
SPEAKER_1:  Big.

00:01:49,826 --> 00:01:50,686
SPEAKER_1:  famous people.

00:01:51,170 --> 00:01:52,414
SPEAKER_1:  in the decade since.

00:01:52,738 --> 00:01:54,430
SPEAKER_1:  Let's talk about poker and diplomacy today.

00:01:54,850 --> 00:01:56,126
SPEAKER_1:  First poker. What?

00:01:56,642 --> 00:01:58,558
SPEAKER_1:  is the game of no limit Texas Hold'em.

00:01:59,202 --> 00:02:00,350
SPEAKER_1:  and how's it different from chess?

00:02:00,450 --> 00:02:01,918
SPEAKER_0:  Well, no limit to Texas Hold'em Poker.

00:02:02,338 --> 00:02:04,350
SPEAKER_0:  is the most popular variant of poker in the world.

00:02:05,186 --> 00:02:10,110
SPEAKER_0:  So you go to a casino, you sit down at the poker table, the game that you're playing is No Limit Texas Hold'em.

00:02:11,106 --> 00:02:14,110
SPEAKER_0:  If you watch movies about poker like Casino Royale or Rounders.

00:02:14,562 --> 00:02:16,606
SPEAKER_0:  The game that they're playing is No Limit Texas Hold'em Poker.

00:02:17,442 --> 00:02:25,534
SPEAKER_0:  Now, it's very different from limit hold them in that you can bet any amount of chips that you want. The stakes escalate really quickly.

00:02:26,082 --> 00:02:28,318
SPEAKER_0:  You start out with like one or two dollars in the pot.

00:02:28,674 --> 00:02:30,398
SPEAKER_0:  And then by the end of the hand, you've got like...

00:02:30,914 --> 00:02:32,094
SPEAKER_0:  Thousand dollars in there maybe.

00:02:32,482 --> 00:02:36,126
SPEAKER_1:  to the option to increase the number very aggressively very quickly as always there.

00:02:36,354 --> 00:02:36,670
SPEAKER_0:  Right.

00:02:36,930 --> 00:02:39,614
SPEAKER_0:  The no limit aspect is there's no limit to how much you can bet.

00:02:40,002 --> 00:02:40,542
SPEAKER_0:  You know, you can...

00:02:40,834 --> 00:02:41,726
SPEAKER_0:  In limit hold on.

00:02:42,274 --> 00:02:48,830
SPEAKER_0:  there's like $2 in the potty, you can only bet like $2. But if you got $10,000 in front of you, you're always welcome to it.

00:02:49,154 --> 00:02:50,279
SPEAKER_0:  put $10,000 into the...

00:02:50,279 --> 00:02:55,134
SPEAKER_1:  So I got a chance to hang out with Phil Hellmuth who plays all these different variants of poker?

00:02:55,810 --> 00:02:57,470
SPEAKER_1:  and correct me if I'm wrong, but.

00:02:58,178 --> 00:03:00,734
SPEAKER_1:  It seems like No Limit Rewards crazy.

00:03:01,378 --> 00:03:04,766
SPEAKER_1:  versus the other ones rewards more kind of calculated strategy.

00:03:05,282 --> 00:03:06,974
SPEAKER_1:  or no, because you're sort of.

00:03:07,362 --> 00:03:08,126
SPEAKER_1:  Looking from an.

00:03:08,738 --> 00:03:10,942
SPEAKER_1:  from an analytic perspective is.

00:03:11,170 --> 00:03:14,878
SPEAKER_1:  is strategy also rewarded in no limit taxes hold them.

00:03:15,010 --> 00:03:19,614
SPEAKER_0:  I think both variants reward strategy, but I think what's different about NoLimit Hold'em

00:03:20,226 --> 00:03:26,142
SPEAKER_0:  is it's much easier to get jumpy. You go in there thinking you're going to lose, you're going to...

00:03:26,402 --> 00:03:27,102
SPEAKER_0:  Play for like.

00:03:27,746 --> 00:03:32,222
SPEAKER_0:  a hundred dollars or something and suddenly there's like you know a thousand dollars in the pot a lot of people can't handle that

00:03:32,578 --> 00:03:33,566
SPEAKER_0:  Can you define jumpy?

00:03:33,922 --> 00:03:39,134
SPEAKER_0:  When you're playing poker, you always want to choose the action that's going to maximize your expected value.

00:03:39,522 --> 00:03:43,134
SPEAKER_0:  It's kind of like with investing, right? If you're ever in a situation where you're...

00:03:43,394 --> 00:03:44,734
SPEAKER_0:  the amount of money that's at stake.

00:03:45,090 --> 00:03:46,078
SPEAKER_0:  is.

00:03:46,914 --> 00:03:48,702
SPEAKER_0:  is going to have a material impact on your life?

00:03:49,250 --> 00:03:51,390
SPEAKER_0:  then you're going to play in a more risk averse style.

00:03:51,906 --> 00:03:53,278
SPEAKER_0:  You know, if somebody makes a huge bet...

00:03:53,794 --> 00:03:56,798
SPEAKER_0:  You're gonna if you're playing element hold them and somebody makes a huge bet

00:03:57,602 --> 00:04:01,374
SPEAKER_0:  there might come a point where you're like, this is too much money for me to handle. Like I can't.

00:04:01,666 --> 00:04:02,526
SPEAKER_0:  risk this amount.

00:04:03,106 --> 00:04:04,798
SPEAKER_0:  And that's what throws a lot of people off.

00:04:05,602 --> 00:04:06,174
SPEAKER_0:  So.

00:04:06,754 --> 00:04:09,246
SPEAKER_0:  That's the big difference I think between no limit and limit.

00:04:09,922 --> 00:04:13,566
SPEAKER_1:  What about on the action side when you're actually making that big bet?

00:04:14,178 --> 00:04:17,918
SPEAKER_1:  That's what I mean by crazy. I was trying to refer to the technical.

00:04:18,658 --> 00:04:22,302
SPEAKER_1:  the technical term of crazy, meaning use the.

00:04:22,530 --> 00:04:23,742
SPEAKER_1:  Big jump in the bet.

00:04:24,514 --> 00:04:27,358
SPEAKER_1:  to completely throw off the other person in terms of...

00:04:27,874 --> 00:04:29,310
SPEAKER_1:  their ability to reason.

00:04:29,634 --> 00:04:30,238
SPEAKER_1:  optimally.

00:04:30,338 --> 00:04:31,486
SPEAKER_0:  I think that's right. I think that's right.

00:04:32,290 --> 00:04:33,950
SPEAKER_0:  one of the key strategies in poker.

00:04:34,498 --> 00:04:35,230
SPEAKER_0:  is to

00:04:35,458 --> 00:04:37,502
SPEAKER_0:  put the other person into an uncomfortable position.

00:04:38,050 --> 00:04:40,382
SPEAKER_0:  And if you're doing that, then you're playing poker.

00:04:40,898 --> 00:04:43,134
SPEAKER_0:  And there's a lot of opportunities to do that at No Limit Hold'em.

00:04:43,586 --> 00:04:44,798
SPEAKER_0:  You know, you can have like...

00:04:45,154 --> 00:04:47,326
SPEAKER_0:  $50 in there, you throw in a $1,000 bet.

00:04:47,906 --> 00:04:48,702
SPEAKER_0:  And um...

00:04:49,090 --> 00:04:50,590
SPEAKER_0:  You know, that's sometimes if you do it right.

00:04:51,042 --> 00:04:52,862
SPEAKER_0:  it puts the other person in a really tough spot.

00:04:53,218 --> 00:04:56,094
SPEAKER_0:  Now it's also possible that you make huge mistakes that way.

00:04:56,386 --> 00:04:59,422
SPEAKER_0:  And so it's really easy to lose a lot of money and no limit hold them if you don't know what you're doing.

00:05:00,034 --> 00:05:00,414
SPEAKER_0:  Um.

00:05:00,770 --> 00:05:02,398
SPEAKER_0:  But there's a lot of upside potential to.

00:05:02,594 --> 00:05:07,774
SPEAKER_1:  So when you build systems, AI systems that play these games, we'll talk about poker, we'll talk about diplomacy.

00:05:08,610 --> 00:05:09,598
SPEAKER_1:  Are you, uh...

00:05:10,050 --> 00:05:13,598
SPEAKER_1:  Are you drawn in in part by the beauty of the game itself, AI aside?

00:05:13,986 --> 00:05:17,022
SPEAKER_1:  or is it to you primarily a fascinating?

00:05:17,954 --> 00:05:19,710
SPEAKER_1:  problem set for the AI to solve.

00:05:20,162 --> 00:05:22,302
SPEAKER_0:  I'm drawn in by the beauty of the game. Goodnight

00:05:22,946 --> 00:05:26,014
SPEAKER_0:  I started playing poker when I was in high school and

00:05:26,626 --> 00:05:28,638
SPEAKER_0:  The idea to me that there is...

00:05:28,930 --> 00:05:31,934
SPEAKER_0:  a correct, an objectively correct way of playing poker.

00:05:32,450 --> 00:05:34,814
SPEAKER_0:  And if you could figure out what that is, then you're...

00:05:35,426 --> 00:05:37,598
SPEAKER_0:  You're making unlimited money basically.

00:05:38,274 --> 00:05:40,126
SPEAKER_0:  That's like a really fascinating concept to me.

00:05:41,186 --> 00:05:43,518
SPEAKER_0:  And so I was fascinated by the strategy of poker.

00:05:44,194 --> 00:05:45,918
SPEAKER_0:  even when I was like 16 years old.

00:05:46,274 --> 00:05:48,899
SPEAKER_0:  It wasn't until much later that I actually worked on poker AIs.

00:05:48,899 --> 00:05:51,230
SPEAKER_1:  So there was a sense that you can solve poker.

00:05:52,258 --> 00:05:55,294
SPEAKER_1:  In the way you can solve chess, for example, or checkers.

00:05:55,938 --> 00:05:57,214
SPEAKER_1:  I believe checkers got sold, right?

00:05:57,474 --> 00:05:58,599
SPEAKER_1:  Yeah, checkers, checkers.

00:05:58,599 --> 00:06:01,599
SPEAKER_0:  completely solved. Optimal strategy. Optimal strategy.

00:06:01,599 --> 00:06:05,086
SPEAKER_1:  Yeah, and so in that same way you could technically solve chess.

00:06:05,826 --> 00:06:08,510
SPEAKER_0:  You could solve chess, you could solve poker. You could solve poker.

00:06:08,866 --> 00:06:11,806
SPEAKER_0:  So this gets into the concept of an Nash equilibrium.

00:06:12,450 --> 00:06:13,918
SPEAKER_0:  So it is a Nash equilibrium.

00:06:14,946 --> 00:06:15,326
SPEAKER_0:  So.

00:06:16,418 --> 00:06:18,654
SPEAKER_0:  in any finite 2-player zero-sum game.

00:06:19,074 --> 00:06:20,478
SPEAKER_0:  There is an optimal strategy.

00:06:20,802 --> 00:06:25,566
SPEAKER_0:  that if you play it, you are guaranteed to not lose an expectation no matter what your opponent does.

00:06:26,626 --> 00:06:27,262
SPEAKER_0:  And this is.

00:06:27,522 --> 00:06:29,214
SPEAKER_0:  kind of a radical concept to a lot of people.

00:06:29,602 --> 00:06:34,142
SPEAKER_0:  But it's true in chess. It's true in poker. It's true in any finite two-player zero-sum game

00:06:34,914 --> 00:06:38,110
SPEAKER_0:  And to give some intuition for this, you can think of rock, paper, scissors.

00:06:39,010 --> 00:06:40,062
SPEAKER_0:  and rock, paper, scissors.

00:06:40,482 --> 00:06:44,126
SPEAKER_0:  If you randomly choose between throwing rock, paper, and scissors with equal probability,

00:06:44,546 --> 00:06:46,238
SPEAKER_0:  then no matter what your opponent does...

00:06:46,722 --> 00:06:50,014
SPEAKER_0:  you are not going to lose an expectation. you are not going to lose an expectation in the long run.

00:06:51,138 --> 00:06:51,518
SPEAKER_0:  No.

00:06:51,938 --> 00:06:55,966
SPEAKER_0:  The same is true for poker. There exists some strategy, some really complicated strategy.

00:06:56,322 --> 00:06:59,454
SPEAKER_0:  that if you play that, you are guaranteed to not lose money.

00:06:59,842 --> 00:07:01,502
SPEAKER_0:  in the long run. And I should say this is for two player.

00:07:01,890 --> 00:07:03,015
SPEAKER_0:  Six player poker is a different story.

00:07:03,015 --> 00:07:05,022
SPEAKER_1:  Yeah, it's a beautiful giant mess.

00:07:05,442 --> 00:07:07,742
SPEAKER_1:  when you say in expectation.

00:07:08,194 --> 00:07:12,350
SPEAKER_1:  You're guaranteed not to lose in expectation. What does in expectation mean?

00:07:12,706 --> 00:07:20,190
SPEAKER_0:  Poker is a very high variance game, so you're gonna have hands where you win, you're gonna have hands where you lose, even if you're playing the perfect strategy, you can't guarantee you're gonna win every single hand.

00:07:20,706 --> 00:07:22,110
SPEAKER_0:  But if you play for long enough...

00:07:22,498 --> 00:07:25,502
SPEAKER_0:  then you are guaranteed to at least break even and.

00:07:25,730 --> 00:07:26,622
SPEAKER_0:  in practice probably one.

00:07:27,650 --> 00:07:30,558
SPEAKER_1:  So that's an expectation. The size of your stack.

00:07:30,786 --> 00:07:31,838
SPEAKER_1:  generally speaking.

00:07:32,290 --> 00:07:34,142
SPEAKER_1:  Now that doesn't include anything about

00:07:34,786 --> 00:07:41,694
SPEAKER_1:  the fact that you can go broke, it doesn't include any of those kinds of normal, real world limitations. You're talking in a theoretical world.

00:07:42,466 --> 00:07:47,486
SPEAKER_1:  What about the zero sum aspect? How big of a constraint is that? How big of a constraint is finite?

00:07:48,546 --> 00:07:48,990
SPEAKER_0:  So.

00:07:49,378 --> 00:07:49,982
SPEAKER_0:  Finites in-

00:07:50,210 --> 00:07:54,014
SPEAKER_0:  not a huge constraint. So I mean most games that you play are finite in size.

00:07:54,306 --> 00:07:59,134
SPEAKER_0:  It's also true actually that there exists this like perfect strategy in many infinite games as well.

00:07:59,458 --> 00:08:01,086
SPEAKER_0:  Technically the game has to be compact.

00:08:01,474 --> 00:08:01,918
SPEAKER_0:  Um...

00:08:02,370 --> 00:08:06,398
SPEAKER_0:  There are like some edge cases where you don't have a Nash Equilibrium in a two player zero sum game.

00:08:06,818 --> 00:08:08,158
SPEAKER_0:  So you can think of a game where you're like.

00:08:08,610 --> 00:08:10,750
SPEAKER_0:  You know, if we're playing a game where whoever names the bigger number...

00:08:11,042 --> 00:08:11,582
SPEAKER_0:  is the winner.

00:08:11,842 --> 00:08:15,646
SPEAKER_0:  There's no Nash equilibrium to that game. 17. Yeah, exactly. 18.

00:08:16,066 --> 00:08:17,054
SPEAKER_1:  You win again.

00:08:17,410 --> 00:08:18,366
SPEAKER_1:  You're good at this.

00:08:18,914 --> 00:08:19,550
SPEAKER_1:  I played a lot of games.

00:08:20,002 --> 00:08:20,414
SPEAKER_1:  Yeah

00:08:20,898 --> 00:08:23,902
SPEAKER_1:  Okay, so that's and then the zero sum aspect.

00:08:24,098 --> 00:08:25,726
SPEAKER_0:  Zero sum aspects.

00:08:26,210 --> 00:08:28,126
SPEAKER_0:  So there exists a Nash equilibrium.

00:08:28,418 --> 00:08:32,862
SPEAKER_0:  in non-two-player zero-sum games as well. And by the way, just to clarify what I mean by two-player zero-sum.

00:08:33,282 --> 00:08:36,670
SPEAKER_0:  I mean, there's two players, and whatever one player wins, the other player loses.

00:08:37,026 --> 00:08:40,222
SPEAKER_0:  So if we're playing poker and I win $50, that means that you're losing $50.

00:08:41,218 --> 00:08:41,566
SPEAKER_0:  Now

00:08:42,210 --> 00:08:43,966
SPEAKER_0:  Outside of two-player zero-sum games.

00:08:44,610 --> 00:08:46,526
SPEAKER_0:  there still exists dash equilibria.

00:08:46,914 --> 00:08:47,774
SPEAKER_0:  but they're not as meaningful.

00:08:48,546 --> 00:08:50,462
SPEAKER_0:  because you can think of a game like Risk.

00:08:51,458 --> 00:08:55,518
SPEAKER_0:  If everybody else on the board decides to team up against you and take you out.

00:08:56,002 --> 00:08:58,014
SPEAKER_0:  There's no perfect strategy you can play that's gonna...

00:08:58,242 --> 00:09:00,126
SPEAKER_0:  guarantee that you win there. There's just nothing you can do.

00:09:00,706 --> 00:09:03,678
SPEAKER_0:  So outside of two-player zero-sum games, there's no guarantee.

00:09:04,386 --> 00:09:06,110
SPEAKER_0:  You're going to win by playing a Nash Equilibrium.

00:09:07,234 --> 00:09:08,798
SPEAKER_1:  Have you ever tried to model in?

00:09:09,026 --> 00:09:10,782
SPEAKER_1:  the other aspects of the game.

00:09:11,426 --> 00:09:12,222
SPEAKER_1:  Which is like...

00:09:12,706 --> 00:09:14,942
SPEAKER_1:  the pleasure you draw from playing the game.

00:09:15,426 --> 00:09:17,886
SPEAKER_1:  And then if you're a professional poker player.

00:09:18,562 --> 00:09:20,734
SPEAKER_1:  if you're exciting even if you lose.

00:09:22,114 --> 00:09:26,974
SPEAKER_1:  the money you would get from the attention you get to the sponsors and all that kind of stuff

00:09:27,394 --> 00:09:27,742
SPEAKER_1:  Z

00:09:28,386 --> 00:09:29,918
SPEAKER_1:  That'd be a fun thing to model.

00:09:30,210 --> 00:09:30,814
SPEAKER_1:  to model in.

00:09:31,074 --> 00:09:34,814
SPEAKER_1:  I was like, make it sort of super complex to include the human factor.

00:09:35,138 --> 00:09:36,670
SPEAKER_1:  in its full complexity.

00:09:36,962 --> 00:09:39,230
SPEAKER_0:  I think you bring up a couple good points there, so I think...

00:09:39,618 --> 00:09:46,110
SPEAKER_0:  A lot of professional poker players, I mean, they get a huge amount of money, not from actually playing poker, but from the sponsorships and...

00:09:46,562 --> 00:09:49,214
SPEAKER_0:  having a personality that people want to tune in and watch.

00:09:49,570 --> 00:09:52,798
SPEAKER_0:  That's a big way to make a name for yourself in poker.

00:09:53,378 --> 00:09:58,334
SPEAKER_1:  I just wonder from an AI perspective if you create and we'll talk about this more maybe

00:09:58,754 --> 00:09:59,422
SPEAKER_1:  AI.

00:09:59,746 --> 00:10:00,830
SPEAKER_1:  system that also.

00:10:01,090 --> 00:10:02,590
SPEAKER_1:  talks trash and all that kind of stuff.

00:10:03,394 --> 00:10:06,334
SPEAKER_1:  that becomes part of the function to maximize. It's not just.

00:10:06,946 --> 00:10:12,830
SPEAKER_1:  optimal poker play maybe sometimes you want to be chaotic maybe sometimes you want to be suboptimal you lose

00:10:13,474 --> 00:10:13,886
SPEAKER_1:  Um.

00:10:14,210 --> 00:10:17,950
SPEAKER_1:  the chaos and maybe sometimes you want to be overly aggressive.

00:10:18,306 --> 00:10:18,814
SPEAKER_1:  because people.

00:10:19,394 --> 00:10:20,190
SPEAKER_1:  the audience.

00:10:20,418 --> 00:10:21,022
SPEAKER_1:  Loves that.

00:10:21,858 --> 00:10:22,750
SPEAKER_1:  That'd be fascinating.

00:10:23,010 --> 00:10:27,966
SPEAKER_0:  I think what you're getting at here is that there's a difference between making an AI that wins a game and an AI that's fun to play with.

00:10:28,866 --> 00:10:32,862
SPEAKER_1:  Yeah. And fun to watch. So those are all different things. Fun to play with and fun to watch.

00:10:33,250 --> 00:10:35,006
SPEAKER_0:  Yeah, and I think, you know, I've...

00:10:35,522 --> 00:10:42,334
SPEAKER_0:  I've heard talks from game designers and they say people that work on AI for actual recreational games that people play.

00:10:42,882 --> 00:10:47,966
SPEAKER_0:  And they say, yeah, there's a big difference between trying to make an AI that actually wins. And you look at a game like Civilization.

00:10:48,482 --> 00:10:48,926
SPEAKER_0:  Um.

00:10:49,378 --> 00:10:51,550
SPEAKER_0:  The way that the AIs play is not...

00:10:52,162 --> 00:10:56,766
SPEAKER_0:  optimal for trying to win. They're playing a different game. They're trying to have personalities. They're trying to.

00:10:56,994 --> 00:10:58,174
SPEAKER_0:  Be fun and engaging.

00:10:58,530 --> 00:10:59,390
SPEAKER_0:  Um, and

00:10:59,714 --> 00:11:00,670
SPEAKER_0:  That makes for a better game.

00:11:00,994 --> 00:11:06,910
SPEAKER_1:  And we also talk about NPCs. I just talked to Todd Howard, who is the creator of Fallout and the Elder Scrolls series.

00:11:07,650 --> 00:11:08,190
SPEAKER_1:  Um...

00:11:08,802 --> 00:11:10,366
SPEAKER_1:  Starfield, the new game coming out.

00:11:11,522 --> 00:11:14,334
SPEAKER_1:  creator what I think is the greatest game of all time, which is skyrim.

00:11:14,754 --> 00:11:20,318
SPEAKER_1:  and the NPCs there, the AI that governs that whole game is very interesting, but the NPCs also are super interesting.

00:11:20,802 --> 00:11:21,150
SPEAKER_1:  and

00:11:21,506 --> 00:11:24,222
SPEAKER_1:  considering what language models might do.

00:11:24,866 --> 00:11:26,942
SPEAKER_1:  NPCs in an open world.

00:11:27,330 --> 00:11:28,990
SPEAKER_1:  RPG, Role Playing Game.

00:11:29,826 --> 00:11:30,686
SPEAKER_1:  It's super exciting.

00:11:31,042 --> 00:11:32,062
SPEAKER_0:  Yeah, honestly, I'm...

00:11:32,642 --> 00:11:36,030
SPEAKER_0:  I think this is one of the first applications where we're going to see real...

00:11:36,418 --> 00:11:38,494
SPEAKER_0:  consumer interaction with large language models.

00:11:38,914 --> 00:11:39,358
SPEAKER_0:  Um.

00:11:39,874 --> 00:11:44,350
SPEAKER_0:  I guess Elder Scrolls 6 is in development now. They're probably like pretty close to finishing it.

00:11:45,122 --> 00:11:47,806
SPEAKER_0:  I would not be surprised at all if Elder Scrolls 7

00:11:48,194 --> 00:11:50,069
SPEAKER_0:  was using large language models for their NPS.

00:11:50,069 --> 00:11:54,129
SPEAKER_1:  They're not there. I mean, I'm not saying anything. I'm not saying anything.

00:11:54,129 --> 00:11:55,646
SPEAKER_0:  This is me speculating, not you.

00:11:55,874 --> 00:12:01,790
SPEAKER_1:  No, but there's the just releasing the star feel good. They do one game at a time. Yeah. and so.

00:12:02,050 --> 00:12:03,038
SPEAKER_1:  Whatever it is.

00:12:03,362 --> 00:12:14,110
SPEAKER_1:  Whenever the date is, I don't know what the date is, calm down, but it would be, I don't know, like 2024, 25, 26. So it's actually very possible that it would include language models.

00:12:14,274 --> 00:12:16,094
SPEAKER_0:  I was listening to this.

00:12:16,322 --> 00:12:18,750
SPEAKER_0:  this talk by a gaming executive.

00:12:19,010 --> 00:12:20,158
SPEAKER_0:  when I was in grad school.

00:12:20,834 --> 00:12:22,718
SPEAKER_0:  And one of the questions that

00:12:23,074 --> 00:12:27,070
SPEAKER_0:  A person in the audience asked is, why are all these games so focused on fighting and killing?

00:12:27,778 --> 00:12:31,966
SPEAKER_0:  And the person responded that it's just so much harder to make an AI that can

00:12:32,642 --> 00:12:35,742
SPEAKER_0:  talk with you and cooperate with you than it is to make an AI that can fight you.

00:12:36,610 --> 00:12:37,118
SPEAKER_0:  and

00:12:37,602 --> 00:12:43,550
SPEAKER_0:  I think once this technology develops further and you can have a, you can reach a point where like not every single line of dialogue has to be scripted.

00:12:44,194 --> 00:12:47,134
SPEAKER_0:  It unlocks a lot of potential for new kinds of games, like much more.

00:12:48,290 --> 00:12:51,262
SPEAKER_0:  interactions that are not so focused on fighting and I'm really looking forward to that.

00:12:51,842 --> 00:12:58,814
SPEAKER_1:  It might not be positive, it might be just drama. So you'll be in like a Call of Duty game instead of doing the shooting. You'll just be hanging out.

00:12:59,202 --> 00:13:01,694
SPEAKER_1:  and like arguing with an AI about like.

00:13:02,210 --> 00:13:07,230
SPEAKER_1:  Like passive aggressive and then you won't be able to sleep that night. You have to return and continue the argument

00:13:07,778 --> 00:13:10,462
SPEAKER_1:  that you were emotionally hurt.

00:13:12,674 --> 00:13:14,814
SPEAKER_1:  I mean, yeah, I think that's actually an exciting world.

00:13:15,106 --> 00:13:21,566
SPEAKER_1:  Whatever is the drama, the chaos that we love, the push and pull of human connection, I think it's possible to do that in the video game world.

00:13:22,242 --> 00:13:28,318
SPEAKER_1:  And I think you could be messier and make more mistakes in a video game world, which is why it would be a nice place.

00:13:28,898 --> 00:13:30,590
SPEAKER_1:  And also it doesn't have.

00:13:31,170 --> 00:13:38,910
SPEAKER_1:  as deep of a real psychological impact because inside video games it's kind of understood that you're in a not a real world.

00:13:39,458 --> 00:13:39,998
SPEAKER_1:  So whatever.

00:13:40,354 --> 00:13:41,694
SPEAKER_1:  crazy stuff he does.

00:13:42,050 --> 00:13:43,614
SPEAKER_1:  we have some flexibility to play.

00:13:43,906 --> 00:13:45,918
SPEAKER_1:  Just like with the game of diplomacy, it's a game.

00:13:46,402 --> 00:13:51,774
SPEAKER_1:  This is not real geopolitics, not real war. It's a game, so you can have a little bit of fun.

00:13:52,418 --> 00:13:58,142
SPEAKER_1:  uh... a little bit of chaos okay back to the secret uh... how do we find the Nash Equilibrium

00:13:59,202 --> 00:14:01,374
SPEAKER_0:  Alright, so there's different ways to find a Nash Equilibrium.

00:14:01,794 --> 00:14:02,622
SPEAKER_0:  So.

00:14:03,746 --> 00:14:06,430
SPEAKER_0:  The way that we do it is with this process called self-play.

00:14:07,330 --> 00:14:08,766
SPEAKER_0:  Basically, we have this algorithm.

00:14:09,218 --> 00:14:11,326
SPEAKER_0:  that starts by playing totally randomly.

00:14:11,810 --> 00:14:12,830
SPEAKER_0:  and it learns.

00:14:13,346 --> 00:14:14,878
SPEAKER_0:  how to play the game by playing against itself.

00:14:15,714 --> 00:14:16,222
SPEAKER_0:  So.

00:14:16,770 --> 00:14:17,694
SPEAKER_0:  It will

00:14:18,018 --> 00:14:22,302
SPEAKER_0:  start playing the game totally randomly and then it you know if it's playing poker it'll eventually like

00:14:22,754 --> 00:14:25,566
SPEAKER_0:  get to the end of the game and make $50.

00:14:26,242 --> 00:14:29,310
SPEAKER_0:  and then it will like review all the decisions that it made along the way.

00:14:29,634 --> 00:14:30,270
SPEAKER_0:  and say it.

00:14:30,498 --> 00:14:33,246
SPEAKER_0:  What would have happened if I had chosen this other action instead?

00:14:33,922 --> 00:14:35,838
SPEAKER_0:  you know, if I had raised here instead of called.

00:14:36,418 --> 00:14:37,918
SPEAKER_0:  What would the other player have done?

00:14:38,498 --> 00:14:42,142
SPEAKER_0:  And because it's playing against a copy of itself, it's able to do that counterfactual reasoning.

00:14:42,658 --> 00:14:43,934
SPEAKER_0:  So I can say, okay, well.

00:14:44,354 --> 00:14:47,742
SPEAKER_0:  If I took this action and the other person takes this action and then I take this action.

00:14:48,034 --> 00:14:50,430
SPEAKER_0:  and eventually I make $150 instead of 50.

00:14:51,522 --> 00:14:52,094
SPEAKER_0:  And so.

00:14:52,610 --> 00:14:53,598
SPEAKER_0:  It updates.

00:14:53,954 --> 00:14:55,518
SPEAKER_0:  the regret value for that action.

00:14:56,226 --> 00:14:59,742
SPEAKER_0:  Regret is basically like how much does it regret having not played that action in the past?

00:15:00,578 --> 00:15:01,054
SPEAKER_0:  and

00:15:01,314 --> 00:15:03,390
SPEAKER_0:  when it encounters that same situation again.

00:15:03,810 --> 00:15:06,654
SPEAKER_0:  it's going to pick actions that have higher regret with higher probability.

00:15:07,746 --> 00:15:08,094
SPEAKER_0:  Now

00:15:09,090 --> 00:15:14,046
SPEAKER_0:  It'll just keep simulating the games this way. It'll keep, you know, accumulating regrets for different situations.

00:15:14,434 --> 00:15:16,254
SPEAKER_0:  Um, and in the long run.

00:15:16,802 --> 00:15:20,350
SPEAKER_0:  If you pick actions that have higher probability in the correct way,

00:15:21,186 --> 00:15:23,454
SPEAKER_0:  proven to converge to a Nash equilibrium.

00:15:24,738 --> 00:15:28,350
SPEAKER_1:  even for super complex games, even for imperfect information games.

00:15:28,610 --> 00:15:32,862
SPEAKER_0:  It's true for all games. It's true for chess, it's true for poker. It's particularly useful for poker.

00:15:33,570 --> 00:15:36,195
SPEAKER_1:  So this is the method of counterfactual regret minimization.

00:15:36,195 --> 00:15:37,695
SPEAKER_0:  This is counterfactual regret.

00:15:37,695 --> 00:15:40,158
SPEAKER_1:  That doesn't have to do with self-play, it has to do with just...

00:15:40,386 --> 00:15:43,998
SPEAKER_1:  and if you follow this kind of process, hopefully or not.

00:15:44,578 --> 00:15:45,630
SPEAKER_1:  you'll be able to...

00:15:46,114 --> 00:15:47,902
SPEAKER_1:  arrive at an optimal set of actions.

00:15:48,386 --> 00:15:51,262
SPEAKER_0:  So this counterfactual regret minimization is a kind of self-play.

00:15:51,490 --> 00:15:51,902
SPEAKER_0:  It's a.

00:15:52,130 --> 00:15:55,070
SPEAKER_0:  principled kind of self-play that's proven to converge to Nash Equilibria.

00:15:55,394 --> 00:15:57,310
SPEAKER_0:  even in in-private information games.

00:15:57,730 --> 00:16:02,014
SPEAKER_0:  Now you can have other forms of self-play and people use other forms of self-play for perfect information games.

00:16:02,434 --> 00:16:02,942
SPEAKER_0:  Um...

00:16:03,330 --> 00:16:06,974
SPEAKER_0:  where you have more flexibility the algorithm doesn't have to be as theoretically sound.

00:16:07,394 --> 00:16:10,398
SPEAKER_0:  in order to converge to that class of games because there's

00:16:10,754 --> 00:16:11,710
SPEAKER_0:  It's a simpler setting.

00:16:11,842 --> 00:16:13,470
SPEAKER_1:  Sure. So I kind of.

00:16:13,762 --> 00:16:21,726
SPEAKER_1:  In my brain, the word self-play has mapped to neural networks, but we're speaking something bigger than just neural networks. It could be anything.

00:16:22,114 --> 00:16:26,239
SPEAKER_1:  The self-playing mechanism is just the mechanism of a system playing itself.

00:16:26,239 --> 00:16:29,502
SPEAKER_0:  Exactly, yeah. Self-play is not tied specifically to neural nets. It's a...

00:16:29,826 --> 00:16:31,390
SPEAKER_0:  kind of reinforcement learning basically.

00:16:32,034 --> 00:16:33,022
SPEAKER_0:  And I would also say.

00:16:33,378 --> 00:16:34,878
SPEAKER_0:  this process of like.

00:16:35,170 --> 00:16:37,150
SPEAKER_0:  trying to reason, oh, what would the value have been if I had

00:16:37,474 --> 00:16:38,846
SPEAKER_0:  taking this other action instead.

00:16:39,298 --> 00:16:41,790
SPEAKER_0:  This is very similar to how humans learn to play a game like poker.

00:16:42,306 --> 00:16:44,350
SPEAKER_0:  Right? Like, you probably played poker before and...

00:16:44,642 --> 00:16:46,398
SPEAKER_0:  With your friends you probably ask like, oh.

00:16:46,754 --> 00:16:48,190
SPEAKER_0:  Would you have called me if I raised there?

00:16:49,058 --> 00:16:50,046
SPEAKER_0:  You know, and that's, that's.

00:16:50,530 --> 00:16:54,238
SPEAKER_0:  a person trying to do the same kind of like learning from a counterfactual that the AI is doing.

00:16:54,914 --> 00:16:58,078
SPEAKER_1:  Okay, and if you do that at scale, you're gonna be able to learn.

00:16:58,402 --> 00:16:59,454
SPEAKER_1:  an optimal policy.

00:17:00,258 --> 00:17:01,534
SPEAKER_0:  Now where the neural nets come in...

00:17:01,794 --> 00:17:04,318
SPEAKER_0:  I said like, okay, if it's in that situation again.

00:17:04,674 --> 00:17:07,166
SPEAKER_0:  then it will choose the action that has high regret.

00:17:07,522 --> 00:17:09,982
SPEAKER_0:  Now the problem is that poker is such a huge game.

00:17:10,434 --> 00:17:15,550
SPEAKER_0:  I think at No Limits Texas Hold'em, the version that we were playing has 10 to the 161 different decision points.

00:17:15,906 --> 00:17:18,782
SPEAKER_0:  which is more than the number of atoms in the universe squared. That's heads up.

00:17:19,042 --> 00:17:19,550
SPEAKER_0:  That's heads up.

00:17:20,258 --> 00:17:21,383
SPEAKER_1:  turn to the 161

00:17:21,383 --> 00:17:25,566
SPEAKER_0:  Yeah, I mean it depends on the number of chips that you have, the stacks and everything, but like the version that we were playing.

00:17:25,794 --> 00:17:26,782
SPEAKER_0:  Let's test the 161.

00:17:27,106 --> 00:17:29,758
SPEAKER_1:  which I assume would be a somewhat simplified version anyway.

00:17:30,466 --> 00:17:30,782
SPEAKER_1:

00:17:32,354 --> 00:17:35,358
SPEAKER_1:  I bet there's some like step function you had for like bets.

00:17:36,130 --> 00:17:39,678
SPEAKER_0:  Oh no no no that's that's I'm saying like we played the full game you can bet whatever amount you

00:17:39,906 --> 00:17:43,646
SPEAKER_0:  The bot maybe was constrained in what it considered for bed sizes.

00:17:43,906 --> 00:17:45,662
SPEAKER_0:  the person on the other side can bet whatever they wanted.

00:17:45,794 --> 00:17:46,526
SPEAKER_1:  Yeah, I mean.

00:17:47,010 --> 00:17:49,630
SPEAKER_1:  161 plus or minus 10 doesn't matter.

00:17:49,922 --> 00:17:50,238
SPEAKER_1:  Yeah.

00:17:50,754 --> 00:17:54,590
SPEAKER_0:  Um, and so the way neural nets help out here is.

00:17:54,978 --> 00:18:00,766
SPEAKER_0:  You know, you don't have to run into the same exact situation because that's never going to happen again. The odds of you running into the same exact situation are pretty slim.

00:18:01,282 --> 00:18:06,782
SPEAKER_0:  But if you run into a similar situation, then you can generalize from other states that you've been in that kind of look like that one.

00:18:07,042 --> 00:18:08,030
SPEAKER_0:  And you can say like, well...

00:18:08,290 --> 00:18:11,934
SPEAKER_0:  These other situations, I had high regret for this action. So maybe I should play that action here.

00:18:12,194 --> 00:18:12,542
SPEAKER_0:  as well.

00:18:12,706 --> 00:18:14,462
SPEAKER_1:  which is the more complex game.

00:18:15,074 --> 00:18:15,710
SPEAKER_1:  Cheers.

00:18:16,386 --> 00:18:17,054
SPEAKER_1:  or poker.

00:18:17,506 --> 00:18:19,134
SPEAKER_1:  or go or poker, you know?

00:18:19,266 --> 00:18:19,710
SPEAKER_0:  That is a-

00:18:19,970 --> 00:18:22,247
SPEAKER_0:  controversial question. Okay. I'm gonna.

00:18:22,247 --> 00:18:27,006
SPEAKER_1:  It's like somebody screaming on reddit right now. It depends on which subreddit you're on. Is it chess or is it poker?

00:18:27,266 --> 00:18:33,022
SPEAKER_0:  I'm sure David Silver is going to get really angry at me. I'm going to say poker actually, and I think for a couple reasons.

00:18:33,538 --> 00:18:34,663
SPEAKER_0:  um... they're not here to defend

00:18:34,663 --> 00:18:35,678
SPEAKER_1:  themselves.

00:18:36,290 --> 00:18:36,894
SPEAKER_0:  So.

00:18:37,314 --> 00:18:39,998
SPEAKER_0:  First of all, you have the imperfect information aspect. And so,

00:18:40,514 --> 00:18:41,470
SPEAKER_0:  It's um...

00:18:42,754 --> 00:18:45,694
SPEAKER_0:  We can go into that, but once you introduce him to have

00:18:46,146 --> 00:18:46,526
SPEAKER_0:  Uh

00:18:47,170 --> 00:18:48,638
SPEAKER_0:  things get much more complicated.

00:18:49,122 --> 00:18:50,174
SPEAKER_1:  So we should say.

00:18:50,978 --> 00:18:52,158
SPEAKER_1:  Maybe you can describe.

00:18:52,706 --> 00:18:55,486
SPEAKER_1:  what is seen to the players, what is not seen.

00:18:56,066 --> 00:18:57,662
SPEAKER_1:  uh... in the game of Texas Hold'em

00:18:57,986 --> 00:19:01,054
SPEAKER_0:  Yeah, so Texas Fold-Um, you get two cards face down.

00:19:01,282 --> 00:19:02,206
SPEAKER_0:  that only you see.

00:19:02,658 --> 00:19:07,358
SPEAKER_0:  And so that's the hidden information of the game. The other players also all get two cards facedown that only they see.

00:19:08,002 --> 00:19:12,350
SPEAKER_0:  And so you have to kind of, as you're playing, reason about like, okay, what do they think I have?

00:19:12,674 --> 00:19:13,470
SPEAKER_0:  What do they have?

00:19:13,698 --> 00:19:16,478
SPEAKER_0:  What do they think I think they have? That kind of stuff and-

00:19:16,962 --> 00:19:17,470
SPEAKER_0:  Um...

00:19:17,922 --> 00:19:20,510
SPEAKER_0:  That's kind of where bluffing comes into play, right? Because...

00:19:20,738 --> 00:19:22,142
SPEAKER_0:  The fact that you can bluff.

00:19:22,658 --> 00:19:25,406
SPEAKER_0:  The fact that you can bet with a bad hand and still win.

00:19:25,762 --> 00:19:27,230
SPEAKER_0:  is because they don't know what your cards are.

00:19:28,258 --> 00:19:33,118
SPEAKER_0:  And that's the key difference between a perfect information game like poker, sorry, like chess and go.

00:19:33,634 --> 00:19:35,582
SPEAKER_0:  and in preparation games like

00:19:36,066 --> 00:19:36,734
SPEAKER_1:  This is what.

00:19:37,122 --> 00:19:38,270
SPEAKER_1:  trash talk looks like.

00:19:40,290 --> 00:19:41,630
SPEAKER_1:  The implied statement is.

00:19:42,018 --> 00:19:48,958
SPEAKER_1:  The game I solved is much tougher. But yeah, so when you're playing, I'm just gonna do random questions here. So what?

00:19:49,282 --> 00:19:50,366
SPEAKER_1:  when you're playing your opponent.

00:19:51,266 --> 00:19:52,926
SPEAKER_1:  under imperfect information.

00:19:54,530 --> 00:19:58,686
SPEAKER_1:  Is there some degree to which you're trying to estimate the range of hands that they have?

00:20:00,194 --> 00:20:03,870
SPEAKER_1:  or is that not part of the algorithm? What are the different approaches?

00:20:04,386 --> 00:20:05,918
SPEAKER_1:  to the imperfect information game.

00:20:06,754 --> 00:20:07,102
SPEAKER_0:  So.

00:20:07,426 --> 00:20:10,878
SPEAKER_0:  The key thing to understand about why imperfect information makes things difficult.

00:20:11,138 --> 00:20:14,334
SPEAKER_0:  is that you have to worry not just about which actions to play.

00:20:14,754 --> 00:20:16,734
SPEAKER_0:  but the probability that you're going to play those actions.

00:20:17,474 --> 00:20:18,526
SPEAKER_0:  So you think about...

00:20:19,522 --> 00:20:23,198
SPEAKER_0:  Rock, paper, scissors for example. Rock, paper, scissors is an improved information game.

00:20:23,842 --> 00:20:26,343
SPEAKER_0:  because you don't know what I'm about to.

00:20:26,343 --> 00:20:28,734
SPEAKER_1:  I do, but yeah, usually not.

00:20:29,186 --> 00:20:31,966
SPEAKER_0:  And so you can't just say like, I'm just going to throw a rock every single time.

00:20:32,290 --> 00:20:33,918
SPEAKER_0:  because the other person's gonna figure that out.

00:20:34,146 --> 00:20:36,670
SPEAKER_0:  and notice a pattern and then suddenly you're going to start losing.

00:20:37,282 --> 00:20:41,310
SPEAKER_0:  And so you don't just have to figure out which action to play, you have to figure out the probability that you play it.

00:20:42,050 --> 00:20:42,494
SPEAKER_0:  and

00:20:42,754 --> 00:20:43,614
SPEAKER_0:  really importantly.

00:20:43,970 --> 00:20:45,246
SPEAKER_0:  the value of an action.

00:20:45,506 --> 00:20:46,942
SPEAKER_0:  on the probability that you're going to play it.

00:20:47,618 --> 00:20:48,830
SPEAKER_0:  So if you're playing...

00:20:49,186 --> 00:20:51,390
SPEAKER_0:  rock every single time, that value is really low.

00:20:51,938 --> 00:20:52,798
SPEAKER_0:  But if you're

00:20:53,442 --> 00:20:56,734
SPEAKER_0:  Never playing rock. You play rock like 1% of the time and suddenly the

00:20:57,122 --> 00:20:58,526
SPEAKER_0:  The other person's probably gonna be throwing...

00:20:58,882 --> 00:20:59,358
SPEAKER_0:  Scissors.

00:20:59,810 --> 00:21:02,302
SPEAKER_0:  And when you throw a rock, the value of that action is going to be really high.

00:21:03,330 --> 00:21:05,694
SPEAKER_0:  Now you take that to poker, what that means is...

00:21:06,594 --> 00:21:08,702
SPEAKER_0:  the value of bluffing, for example.

00:21:09,122 --> 00:21:12,926
SPEAKER_0:  If you're the kind of person that never bluffs and you have this reputation as somebody that never bluffs

00:21:13,282 --> 00:21:14,238
SPEAKER_0:  and suddenly you bluff?

00:21:14,498 --> 00:21:16,350
SPEAKER_0:  there's a really good chance that that bluff is gonna work.

00:21:16,578 --> 00:21:17,470
SPEAKER_0:  and you're gonna make a lot of money.

00:21:18,178 --> 00:21:23,902
SPEAKER_0:  On the other hand, if you got a reputation, like if they've seen you play for a long time and they see, oh, you're the kind of person that's bluffing all the time.

00:21:24,738 --> 00:21:27,582
SPEAKER_0:  When you bluff, they're not gonna buy it and they're gonna call you down, you're gonna lose a lot.

00:21:28,962 --> 00:21:29,438
SPEAKER_0:  And that.

00:21:30,338 --> 00:21:32,862
SPEAKER_0:  finding that balance of how often you should be bluffing.

00:21:33,250 --> 00:21:33,854
SPEAKER_0:  is

00:21:34,402 --> 00:21:36,382
SPEAKER_0:  the key challenge of a game of poker.

00:21:37,250 --> 00:21:40,062
SPEAKER_0:  And you contrast that with a game like chess.

00:21:41,090 --> 00:21:43,486
SPEAKER_0:  It doesn't matter if you're opening with the Queen's Gambit.

00:21:43,938 --> 00:21:45,662
SPEAKER_0:  10% of the time or 100% of the time.

00:21:46,178 --> 00:21:47,934
SPEAKER_0:  the value, the expected value is the same.

00:21:49,634 --> 00:21:50,270
SPEAKER_0:  So, um.

00:21:50,594 --> 00:21:52,734
SPEAKER_0:  So that's why we need these algorithms.

00:21:53,346 --> 00:21:54,686
SPEAKER_0:  that understand.

00:21:55,074 --> 00:21:59,230
SPEAKER_0:  Not just we have to figure out what actions are good, but the probabilities. We need to get the exact probabilities correct.

00:21:59,650 --> 00:22:01,822
SPEAKER_0:  And that's actually when we created the bot, Lebratis.

00:22:02,306 --> 00:22:05,566
SPEAKER_0:  Lebrotis means balanced because the algorithm that we designed

00:22:06,178 --> 00:22:08,414
SPEAKER_0:  was designed to find that right balance of.

00:22:08,674 --> 00:22:09,886
SPEAKER_0:  how often it should play each action.

00:22:10,946 --> 00:22:15,166
SPEAKER_1:  the balance of how often in the key sort of branching is the bluff or not the bluff.

00:22:16,098 --> 00:22:20,702
SPEAKER_1:  Is that a good crude simplification of the major decision in poker?

00:22:21,090 --> 00:22:23,614
SPEAKER_0:  It's a good simplification. I think that's like the main tension.

00:22:23,906 --> 00:22:25,822
SPEAKER_0:  But it's not just.

00:22:26,306 --> 00:22:30,750
SPEAKER_0:  how often to bluff or not to bluff. It's like, how often should you bet in general? How often should you...

00:22:31,330 --> 00:22:32,606
SPEAKER_0:  What kind of bet should you make?

00:22:32,930 --> 00:22:34,974
SPEAKER_0:  Should you bet big or should you bet small?

00:22:35,234 --> 00:22:37,246
SPEAKER_0:  And with which hands?

00:22:37,730 --> 00:22:40,094
SPEAKER_0:  And so this is where the idea of a range comes from.

00:22:40,738 --> 00:22:41,246
SPEAKER_0:  because...

00:22:41,538 --> 00:22:44,062
SPEAKER_0:  when you're bluffing with a particular hand in a particular spot.

00:22:44,898 --> 00:22:50,366
SPEAKER_0:  You don't want there to be a pattern for the other person to pick up on. You don't want them to figure out, oh, whenever this person is in this spot.

00:22:50,722 --> 00:22:51,486
SPEAKER_0:  They're always bluffing.

00:22:51,874 --> 00:22:53,278
SPEAKER_0:  And so you have to reason about.

00:22:53,602 --> 00:22:54,014
SPEAKER_0:  Okay.

00:22:54,306 --> 00:22:55,422
SPEAKER_0:  Would I also?

00:22:55,810 --> 00:22:57,406
SPEAKER_0:  bet with a good hand in this spot.

00:22:58,274 --> 00:22:59,294
SPEAKER_0:  You want to be unpredictable.

00:22:59,778 --> 00:23:00,766
SPEAKER_0:  So you have to think about.

00:23:00,994 --> 00:23:04,158
SPEAKER_0:  What would I do if I had this different set of cards?

00:23:04,290 --> 00:23:13,022
SPEAKER_1:  Is there explicit estimation of like a theory of mind that the other person has about you or is that just a emergent thing that happens?

00:23:14,434 --> 00:23:15,518
SPEAKER_0:  The way that the...

00:23:15,810 --> 00:23:16,254
SPEAKER_0:  bots.

00:23:16,482 --> 00:23:20,990
SPEAKER_0:  handle it that are really successful, they have an explicit theory of mind. So they're explicitly reasoning.

00:23:21,442 --> 00:23:21,854
SPEAKER_0:  about.

00:23:22,274 --> 00:23:22,846
SPEAKER_0:  What are?

00:23:23,682 --> 00:23:25,790
SPEAKER_0:  What's the common knowledge belief? What is-

00:23:26,018 --> 00:23:28,574
SPEAKER_0:  What do you think I have? What do I think you have?

00:23:28,834 --> 00:23:30,398
SPEAKER_0:  What do you think? I think you have.

00:23:30,786 --> 00:23:32,661
SPEAKER_0:  It's explicitly reasoning about that.

00:23:32,661 --> 00:23:34,878
SPEAKER_1:  Is there multiple U's there? So,

00:23:35,650 --> 00:23:39,998
SPEAKER_1:  Maybe that's jumping ahead to six players, but is there a stickiness to the person?

00:23:40,610 --> 00:23:43,326
SPEAKER_1:  to certain iterative game you're playing the same person

00:23:45,186 --> 00:23:45,886
SPEAKER_1:  There is a s-

00:23:46,146 --> 00:23:49,182
SPEAKER_1:  There's a stickiness to that, right? You're gathering information as you play.

00:23:49,762 --> 00:23:51,614
SPEAKER_1:  It's not every, every, uh,

00:23:52,002 --> 00:23:54,238
SPEAKER_1:  Every hand is a new hand. Is there, um...

00:23:54,786 --> 00:23:56,446
SPEAKER_1:  a continuation in terms of.

00:23:56,674 --> 00:23:58,750
SPEAKER_1:  estimating what kind of player I'm facing here.

00:23:59,362 --> 00:24:00,478
SPEAKER_0:  That's a good question. so

00:24:01,346 --> 00:24:07,614
SPEAKER_0:  You could approach the game that way. The way that the bots do it, they don't, and the way that humans approach it also, experts human players.

00:24:07,970 --> 00:24:10,142
SPEAKER_0:  The way they approach it is to basically assume...

00:24:10,594 --> 00:24:11,774
SPEAKER_0:  that you know.

00:24:12,098 --> 00:24:12,798
SPEAKER_0:  My strategy.

00:24:13,346 --> 00:24:13,790
SPEAKER_0:  So.

00:24:14,818 --> 00:24:19,966
SPEAKER_0:  I'm going to try to pick a strategy where even if I were to play it for 10,000 hands and you could figure out exactly what it was.

00:24:20,226 --> 00:24:24,062
SPEAKER_0:  you still wouldn't be able to beat it. Basically what that means is I'm trying to approximate the Nash Equilibrium.

00:24:24,418 --> 00:24:27,614
SPEAKER_0:  I'm trying to be perfectly balanced because if I'm playing the Nash Equilibrium...

00:24:27,970 --> 00:24:30,398
SPEAKER_0:  even if you know what my strategy is.

00:24:30,754 --> 00:24:32,574
SPEAKER_0:  Like I said, I'm still unbeatable in expectation.

00:24:33,218 --> 00:24:35,358
SPEAKER_0:  So that's what the bot aims for.

00:24:35,746 --> 00:24:39,774
SPEAKER_0:  And that's actually what a lot of expert poker players aim for as well. To start by playing...

00:24:40,034 --> 00:24:40,958
SPEAKER_0:  The Nash Equilibrium.

00:24:41,186 --> 00:24:43,422
SPEAKER_0:  and then maybe if they spot weaknesses in the way you're playing.

00:24:43,714 --> 00:24:45,726
SPEAKER_0:  then they can deviate a little bit to take advantage of that.

00:24:47,362 --> 00:24:49,886
SPEAKER_1:  They aim to be unbeatable in expectation. OK.

00:24:50,722 --> 00:24:55,134
SPEAKER_1:  So who's the greatest poker player of all time and why is it Phil Hellmuth? So this is for Phil.

00:24:55,618 --> 00:24:58,110
SPEAKER_1:  uh... so he's known uh...

00:24:59,266 --> 00:25:00,574
SPEAKER_1:  at least in part for.

00:25:00,866 --> 00:25:02,462
SPEAKER_1:  maybe playing suboptimally.

00:25:02,722 --> 00:25:04,350
SPEAKER_1:  and he still wins a lot.

00:25:04,834 --> 00:25:05,886
SPEAKER_1:  It's a bit chaotic.

00:25:06,178 --> 00:25:07,038
SPEAKER_1:  So maybe...

00:25:07,746 --> 00:25:13,726
SPEAKER_1:  Can you speak from an AI perspective about the genius of his madness, or the madness of his genius?

00:25:14,466 --> 00:25:16,574
SPEAKER_1:  So playing suboptimally, playing chaotically.

00:25:18,530 --> 00:25:19,038
SPEAKER_1:  Um...

00:25:19,394 --> 00:25:21,086
SPEAKER_1:  as a way to make you hard to.

00:25:21,314 --> 00:25:23,102
SPEAKER_1:  down about what your strategy is.

00:25:24,258 --> 00:25:24,606
SPEAKER_0:  Okay.

00:25:25,090 --> 00:25:26,910
SPEAKER_0:  The thing that I should explain first of all with like...

00:25:27,362 --> 00:25:28,350
SPEAKER_0:  Nash Equilibrium.

00:25:28,610 --> 00:25:29,886
SPEAKER_0:  It doesn't mean that it's predictable.

00:25:30,114 --> 00:25:32,254
SPEAKER_0:  The whole point of it is that you're trying to be unpredictable.

00:25:32,898 --> 00:25:33,342
SPEAKER_0:  Now

00:25:33,762 --> 00:25:36,542
SPEAKER_0:  I think when somebody like Phil Helmuth might be really successful.

00:25:36,962 --> 00:25:39,742
SPEAKER_0:  is not in being unpredictable, but in being able to...

00:25:40,226 --> 00:25:40,734
SPEAKER_0:  Um.

00:25:41,538 --> 00:25:45,022
SPEAKER_0:  take advantage of the other player and figure out where they're being predictable.

00:25:45,698 --> 00:25:46,238
SPEAKER_0:  or

00:25:46,690 --> 00:25:48,830
SPEAKER_0:  guiding the other player into thinking.

00:25:49,282 --> 00:25:51,102
SPEAKER_0:  that you have certain weaknesses.

00:25:51,362 --> 00:25:55,422
SPEAKER_0:  and then understanding how they're going to change their behavior, they're going to deviate from

00:25:55,810 --> 00:25:57,470
SPEAKER_0:  Nash equilibrium style of play.

00:25:57,698 --> 00:26:00,542
SPEAKER_0:  to try to take advantage of those perceived weaknesses and then counter exploit them.

00:26:00,930 --> 00:26:02,366
SPEAKER_0:  So you kind of get into the mind games there.

00:26:02,754 --> 00:26:03,742
SPEAKER_1:  So you think about.

00:26:04,034 --> 00:26:07,326
SPEAKER_1:  at least has a poker as a dance between two agents.

00:26:07,714 --> 00:26:10,174
SPEAKER_1:  I guess you're playing the cards or you're playing the player.

00:26:10,530 --> 00:26:12,670
SPEAKER_0:  So this gets down to a big argument.

00:26:12,898 --> 00:26:17,374
SPEAKER_0:  in the poker community and the academic community. For a long time there was this debate of like

00:26:17,794 --> 00:26:20,414
SPEAKER_0:  What's called GTO, Game Theory Optimal Poker.

00:26:20,802 --> 00:26:21,982
SPEAKER_0:  or exploitative play.

00:26:22,818 --> 00:26:23,742
SPEAKER_0:  And um

00:26:24,034 --> 00:26:26,910
SPEAKER_0:  up until about like 2017 when we did the Lebronis match.

00:26:27,138 --> 00:26:30,942
SPEAKER_0:  I think actually exploitative play had the advantage. A lot of people were saying like...

00:26:31,170 --> 00:26:33,886
SPEAKER_0:  Oh, this whole idea of game theory, it's just nonsense and-

00:26:34,114 --> 00:26:38,686
SPEAKER_0:  If you really want to make money, you've got to look into the other person's eyes and read their soul and figure out...

00:26:38,946 --> 00:26:39,774
SPEAKER_0:  what cards they have.

00:26:40,322 --> 00:26:44,318
SPEAKER_0:  But what happened was people started adopting the Game Theory Optimal Strategy.

00:26:44,770 --> 00:26:45,150
SPEAKER_0:  Um.

00:26:45,474 --> 00:26:46,494
SPEAKER_0:  and they were making good money.

00:26:46,882 --> 00:26:48,670
SPEAKER_0:  and they weren't trying to.

00:26:48,930 --> 00:26:51,230
SPEAKER_0:  adapt so much to the other player, they were just trying to play...

00:26:51,554 --> 00:26:52,318
SPEAKER_0:  the Nash Equilibrium.

00:26:52,770 --> 00:26:56,158
SPEAKER_0:  And then what really solidified it, I think, was the Labrador's match.

00:26:56,962 --> 00:27:00,158
SPEAKER_0:  where we played our bot against four top heads up, no limit. Hold em.

00:27:00,418 --> 00:27:01,118
SPEAKER_0:  Poker players.

00:27:01,634 --> 00:27:02,590
SPEAKER_0:  and the bot.

00:27:02,850 --> 00:27:09,406
SPEAKER_0:  wasn't trying to adapt to them, it wasn't trying to exploit them, it wasn't trying to do these mind games, it was just trying to approximate the Nash equilibrium.

00:27:09,858 --> 00:27:10,654
SPEAKER_0:  and it crushed them.

00:27:11,554 --> 00:27:12,158
SPEAKER_0:  I think.

00:27:12,642 --> 00:27:13,054
SPEAKER_0:  You know.

00:27:13,890 --> 00:27:17,022
SPEAKER_0:  We were playing for $50, $100 blinds.

00:27:17,314 --> 00:27:20,062
SPEAKER_0:  And over the course of about 120,000 hands, it made...

00:27:20,514 --> 00:27:21,639
SPEAKER_0:  close to $2 million.

00:27:21,639 --> 00:27:23,139
SPEAKER_1:  120,000 hands.

00:27:23,139 --> 00:27:29,139
SPEAKER_0:  20,000 hands. Against humans. Yeah. And this was fake money to be clear. So there was real money at stake. There was $200,000.

00:27:29,139 --> 00:27:31,230
SPEAKER_1:  All money is fake, but um.

00:27:31,778 --> 00:27:33,822
SPEAKER_1:  That's a different conversation.

00:27:34,178 --> 00:27:34,590
SPEAKER_1:  Um.

00:27:34,946 --> 00:27:35,934
SPEAKER_1:  we give it meaning.

00:27:36,386 --> 00:27:42,366
SPEAKER_1:  It's a phenomena that gets meaning from our complex psychology as a human civilization.

00:27:42,914 --> 00:27:52,510
SPEAKER_1:  It's emerging from the collective intelligence of the human species, but that's not what you mean. You mean like there's literally, you can't buy stuff with it. Okay. Can you actually step back and-

00:27:52,738 --> 00:27:54,270
SPEAKER_1:  take me through that.

00:27:54,722 --> 00:27:55,294
SPEAKER_1:  competition.

00:27:55,938 --> 00:27:57,246
SPEAKER_0:  Yeah, okay, so.

00:27:58,114 --> 00:28:01,982
SPEAKER_0:  When I was in grad school, there was this thing called the annual computer poker competition.

00:28:02,242 --> 00:28:03,102
SPEAKER_0:  where every year.

00:28:03,522 --> 00:28:08,862
SPEAKER_0:  All the different research labs that were working on AI for poker would get together, they would make a bot, they would play them against each other.

00:28:09,762 --> 00:28:12,894
SPEAKER_0:  And we made a bot that actually won the

00:28:13,154 --> 00:28:15,742
SPEAKER_0:  the 2014 competition, the 2016 competition.

00:28:16,258 --> 00:28:19,006
SPEAKER_0:  And so we decided we're going to take this bot, build on it.

00:28:19,362 --> 00:28:19,806
SPEAKER_0:  and

00:28:20,386 --> 00:28:24,414
SPEAKER_0:  play against real top professional, heads up, no limit, Texas Hold'em poker players.

00:28:25,090 --> 00:28:25,598
SPEAKER_0:  So.

00:28:25,890 --> 00:28:28,126
SPEAKER_0:  We invited four of the world's best.

00:28:28,418 --> 00:28:29,982
SPEAKER_0:  uh... players in this specialty

00:28:30,658 --> 00:28:34,654
SPEAKER_0:  and we challenge them to 120,000 hands of poker over the course of 20 days.

00:28:35,746 --> 00:28:37,790
SPEAKER_0:  And we had $200,000.

00:28:38,082 --> 00:28:39,806
SPEAKER_0:  $200,000 in prize money at stake.

00:28:40,290 --> 00:28:43,902
SPEAKER_0:  where it would basically be divided among them depending on how well they did relative to each other.

00:28:44,770 --> 00:28:46,974
SPEAKER_0:  So we wanted to have some incentive for them to play their best.

00:28:47,842 --> 00:28:49,022
SPEAKER_1:  Did you have a confidence?

00:28:49,410 --> 00:28:52,158
SPEAKER_1:  2014, 16, that this is even possible.

00:28:52,674 --> 00:28:53,822
SPEAKER_1:  How much doubt was there?

00:28:53,922 --> 00:29:01,694
SPEAKER_0:  So we did a competition actually in 2015 where we also played against professional poker players and the bot lost by a pretty sizable margin actually.

00:29:02,146 --> 00:29:05,566
SPEAKER_0:  Now there were some big improvements from 2015 to 2017.

00:29:05,890 --> 00:29:11,358
SPEAKER_1:  And so can you speak to the improvements? Is it computational nature? Or is it the algorithm, the methods?

00:29:11,458 --> 00:29:12,670
SPEAKER_0:  It was really an algorithmic approach.

00:29:13,026 --> 00:29:14,270
SPEAKER_0:  that was the difference. So we go

00:29:14,946 --> 00:29:17,502
SPEAKER_0:  2015 it was much more focused on

00:29:18,114 --> 00:29:18,974
SPEAKER_0:  trying to

00:29:19,394 --> 00:29:24,382
SPEAKER_0:  come up with a strategy upfront, like trying to solve the entire game of poker, like, and then just have

00:29:24,610 --> 00:29:27,262
SPEAKER_0:  a lookup table where you're saying like, oh, I'm in this situation.

00:29:27,586 --> 00:29:28,414
SPEAKER_0:  What's the strategy?

00:29:28,994 --> 00:29:32,510
SPEAKER_0:  The approach that we took in 2017 was much more search-based.

00:29:32,866 --> 00:29:34,398
SPEAKER_0:  It was trying to say, okay, well...

00:29:34,786 --> 00:29:36,158
SPEAKER_0:  Let me in real time.

00:29:36,450 --> 00:29:37,182
SPEAKER_0:  try to compute.

00:29:37,506 --> 00:29:40,510
SPEAKER_0:  a much better strategy than what I had precomputed.

00:29:40,866 --> 00:29:42,741
SPEAKER_0:  by playing against myself during self-play.

00:29:42,741 --> 00:29:44,446
SPEAKER_1:  What is the search space?

00:29:44,738 --> 00:29:45,278
SPEAKER_1:  for watching.

00:29:45,986 --> 00:29:46,366
SPEAKER_1:  Poker.

00:29:47,042 --> 00:29:48,158
SPEAKER_1:  What are you searching over?

00:29:49,794 --> 00:29:52,638
SPEAKER_1:  What's that look like? There's different actions like raising.

00:29:52,930 --> 00:29:53,566
SPEAKER_1:  Calling.

00:29:53,954 --> 00:29:55,358
SPEAKER_1:  Yeah, what are the actions?

00:29:56,386 --> 00:29:56,926
SPEAKER_1:  Um...

00:29:57,666 --> 00:29:59,198
SPEAKER_1:  Is it just a search over actions?

00:29:59,650 --> 00:30:00,062
SPEAKER_0:  So.

00:30:00,770 --> 00:30:03,774
SPEAKER_0:  In a game like Chess, the search is like, okay, I'm in this.

00:30:04,098 --> 00:30:07,902
SPEAKER_0:  position and I can like, you know, move these different pieces and see where things end up.

00:30:08,354 --> 00:30:10,270
SPEAKER_0:  In poker, what you're searching over is...

00:30:10,722 --> 00:30:12,766
SPEAKER_0:  the actions you can take for your hand.

00:30:13,026 --> 00:30:14,750
SPEAKER_0:  the probabilities that you take those actions.

00:30:14,978 --> 00:30:18,462
SPEAKER_0:  And then also the probabilities that you take other actions with other hands that you might have.

00:30:19,682 --> 00:30:21,566
SPEAKER_0:  And that's kind of like a

00:30:22,114 --> 00:30:24,478
SPEAKER_0:  to wrap your head around like why are you searching over

00:30:24,706 --> 00:30:25,630
SPEAKER_0:  These like other.

00:30:25,922 --> 00:30:26,622
SPEAKER_0:  Hands that you-

00:30:27,138 --> 00:30:29,598
SPEAKER_0:  might have and like trying to figure out what you would do with those hands.

00:30:29,986 --> 00:30:30,462
SPEAKER_0:  Um

00:30:30,754 --> 00:30:33,118
SPEAKER_0:  And the idea is, again, you want to...

00:30:33,634 --> 00:30:36,222
SPEAKER_0:  You want to always be balanced and unpredictable.

00:30:36,866 --> 00:30:38,942
SPEAKER_0:  And so if you're a search algorithm is saying like.

00:30:39,266 --> 00:30:41,054
SPEAKER_0:  Oh, I want to raise with this hand.

00:30:41,314 --> 00:30:41,662
SPEAKER_0:  Well,

00:30:41,986 --> 00:30:47,198
SPEAKER_0:  in order to know whether that's a good action. Like let's say it's a bluff. You know, let's say you have a bad hand and you're saying like, oh, I think I should.

00:30:47,554 --> 00:30:49,886
SPEAKER_0:  be betting here with this really bad hand and bluffing.

00:30:50,338 --> 00:30:52,990
SPEAKER_0:  Well that's only a good action if you're also...

00:30:53,826 --> 00:30:56,606
SPEAKER_0:  betting with a strong hand. Otherwise, it's an obvious bluff.

00:30:57,090 --> 00:30:58,270
SPEAKER_1:  So if your action...

00:30:58,658 --> 00:31:00,478
SPEAKER_1:  in some sense maximizes your own break.

00:31:00,770 --> 00:31:01,758
SPEAKER_1:  unpredictability.

00:31:02,146 --> 00:31:06,782
SPEAKER_1:  So that action could be mapped by your opponent to a lot of different hands, then that's a good action.

00:31:07,266 --> 00:31:08,734
SPEAKER_0:  Basically what you wanna do is...

00:31:09,090 --> 00:31:10,398
SPEAKER_0:  put your opponent into a tough spot.

00:31:11,106 --> 00:31:15,582
SPEAKER_0:  So you want them to always have some doubt, like should I call here? Should I fold here?

00:31:15,906 --> 00:31:17,086
SPEAKER_0:  And if you are...

00:31:17,378 --> 00:31:26,590
SPEAKER_0:  raising in the appropriate balance between bluffs and good hands, then you're putting them into that tough spot. And so that's what we're trying to do. We're always trying to search for a strategy that would put the opponent into a difficult position.

00:31:26,882 --> 00:31:27,422
SPEAKER_1:  Can you?

00:31:27,714 --> 00:31:33,086
SPEAKER_1:  give a metric that you're trying to maximize or minimize. Does this have to do with the regret thing that we're talking about?

00:31:33,346 --> 00:31:34,046
SPEAKER_1:  in terms of.

00:31:34,562 --> 00:31:36,702
SPEAKER_1:  putting your opponent in a maximally tough spot.

00:31:37,378 --> 00:31:40,126
SPEAKER_0:  Yeah, ultimately what you're trying to maximize is your expected

00:31:40,354 --> 00:31:45,054
SPEAKER_0:  winning, so like your expected value, the amount of money that you're going to walk away from assuming that your opponent's

00:31:45,314 --> 00:31:46,910
SPEAKER_0:  was playing optimally in response.

00:31:47,490 --> 00:31:49,790
SPEAKER_0:  So you're gonna assume that your opponent is...

00:31:50,370 --> 00:31:51,998
SPEAKER_0:  is also playing like.

00:31:52,226 --> 00:31:54,718
SPEAKER_0:  as well as possible a Nash equilibrium approach.

00:31:55,138 --> 00:31:58,750
SPEAKER_0:  Because if they're not, then you're just gonna make more money, right? You know.

00:31:59,074 --> 00:32:02,526
SPEAKER_0:  anything that deviates, like by definition, the Nash equilibrium is.

00:32:02,914 --> 00:32:04,030
SPEAKER_0:  the strategy that

00:32:04,354 --> 00:32:05,822
SPEAKER_0:  does the best in expectation.

00:32:06,306 --> 00:32:08,414
SPEAKER_0:  And so if you're deviating from that, then you're just.

00:32:08,738 --> 00:32:12,094
SPEAKER_0:  they're gonna lose money and since it's a two-player zero-sum game, that means you're gonna make money.

00:32:12,450 --> 00:32:15,550
SPEAKER_1:  So there's not an explicit like objective function that.

00:32:16,258 --> 00:32:19,678
SPEAKER_1:  maximizes the toughness of the spot they're put in. You're always.

00:32:20,418 --> 00:32:23,678
SPEAKER_1:  This is from a self-play reinforcement learning perspective.

00:32:24,130 --> 00:32:25,822
SPEAKER_1:  You're just trying to maximize winnings.

00:32:26,146 --> 00:32:27,678
SPEAKER_1:  and the rest is implicit.

00:32:27,810 --> 00:32:31,582
SPEAKER_0:  That's right, yeah. So what we're actually trying to maximize is the expected value.

00:32:31,842 --> 00:32:34,270
SPEAKER_0:  given that the opponent is playing optimally in response to us.

00:32:34,530 --> 00:32:36,478
SPEAKER_0:  Now in practice what that ends up looking like.

00:32:36,738 --> 00:32:41,726
SPEAKER_0:  is it's putting the opponent into difficult situations where there's no obvious decision to be made.

00:32:41,922 --> 00:32:43,486
SPEAKER_1:  So the system doesn't know.

00:32:43,714 --> 00:32:54,430
SPEAKER_1:  anything about the difficulty of the situation? Not at all, doesn't care. Okay, all right. My head was getting excited whenever I was making the opponent sweat. Okay, so you're in 2015, you didn't do as well.

00:32:55,298 --> 00:32:58,142
SPEAKER_1:  So what's the journey from that to a system that...

00:32:58,434 --> 00:32:59,838
SPEAKER_1:  and your mind could have a chance.

00:33:00,418 --> 00:33:01,662
SPEAKER_0:  So 2015 we...

00:33:02,338 --> 00:33:03,998
SPEAKER_0:  God, we beat pretty badly.

00:33:04,450 --> 00:33:07,582
SPEAKER_0:  And we actually learned a lot from that competition. In particular,

00:33:08,034 --> 00:33:12,990
SPEAKER_0:  You know, what became clear to me is that the way the humans were approaching the game was very different from how

00:33:13,282 --> 00:33:14,398
SPEAKER_0:  The bot was approaching the game.

00:33:15,010 --> 00:33:16,094
SPEAKER_0:  The bot would

00:33:16,482 --> 00:33:22,974
SPEAKER_0:  not be doing search, it would just be trying to compute, you know, it would do like months of self play. It would just be playing against itself for months.

00:33:23,234 --> 00:33:25,502
SPEAKER_0:  But then when it's actually playing the game, it would just act instantly.

00:33:26,050 --> 00:33:30,174
SPEAKER_0:  And the humans when they're in a tough spot, they would sit there and think

00:33:31,202 --> 00:33:33,310
SPEAKER_0:  sometimes even like five minutes about.

00:33:33,570 --> 00:33:34,846
SPEAKER_0:  whether they're going to call or fold a hand.

00:33:35,458 --> 00:33:35,934
SPEAKER_0:  Um...

00:33:36,482 --> 00:33:37,854
SPEAKER_0:  and it became clear to me

00:33:38,562 --> 00:33:39,070
SPEAKER_0:  That's

00:33:39,746 --> 00:33:43,134
SPEAKER_0:  there's a good chance that that's what's missing from our bot. So I actually did some.

00:33:43,618 --> 00:33:47,198
SPEAKER_0:  initial experiments to try to figure out how much of a difference this is actually make.

00:33:47,490 --> 00:33:48,862
SPEAKER_0:  and the difference was huge.

00:33:49,026 --> 00:33:51,651
SPEAKER_1:  as a signal to the human player how long you took.

00:33:51,651 --> 00:33:55,454
SPEAKER_0:  No, no, no. I'm not saying that there were any timing tells. I was saying when the human.

00:33:55,810 --> 00:33:57,886
SPEAKER_0:  Like the bot would always act instantly. It wouldn't-

00:33:58,274 --> 00:34:00,542
SPEAKER_0:  try to come up with a better strategy in real time.

00:34:00,962 --> 00:34:01,406
SPEAKER_0:  Um.

00:34:01,698 --> 00:34:03,998
SPEAKER_0:  over what it had pre-computed during training.

00:34:04,450 --> 00:34:07,326
SPEAKER_0:  Whereas the human, like they have all this intuition about how to play.

00:34:07,650 --> 00:34:09,246
SPEAKER_0:  but they're also in real time.

00:34:09,666 --> 00:34:10,590
SPEAKER_0:  Leveraging.

00:34:11,074 --> 00:34:13,630
SPEAKER_0:  their ability to think, to search, to plan.

00:34:14,018 --> 00:34:17,393
SPEAKER_0:  and coming up with an even better strategy than what their intuition would say.

00:34:17,393 --> 00:34:18,814
SPEAKER_1:  So you're saying that there's.

00:34:19,042 --> 00:34:21,758
SPEAKER_1:  you're doing that's what you mean by you're doing search also.

00:34:22,306 --> 00:34:23,518
SPEAKER_1:  You have a- You have a-

00:34:23,874 --> 00:34:24,862
SPEAKER_1:  Intuition.

00:34:25,314 --> 00:34:28,350
SPEAKER_1:  and search on top of that looking for a better solution.

00:34:29,058 --> 00:34:30,462
SPEAKER_0:  That's what I mean by search.

00:34:30,946 --> 00:34:31,358
SPEAKER_0:  Um.

00:34:31,618 --> 00:34:34,174
SPEAKER_0:  instead of acting instantly, you know, a neural net.

00:34:34,882 --> 00:34:38,686
SPEAKER_0:  usually gives you a response in like 100 milliseconds or something, depends on the size of the net.

00:34:39,138 --> 00:34:42,206
SPEAKER_0:  But if you can leverage extra computational resources...

00:34:42,946 --> 00:34:44,926
SPEAKER_0:  you can possibly get a much better.

00:34:45,282 --> 00:34:45,694
SPEAKER_0:  outcome.

00:34:46,434 --> 00:34:47,038
SPEAKER_0:  and

00:34:47,330 --> 00:34:50,398
SPEAKER_0:  So we did some experiments in small-scale versions of Poker.

00:34:50,914 --> 00:34:51,422
SPEAKER_0:  and

00:34:51,714 --> 00:34:53,150
SPEAKER_0:  What we found was that...

00:34:53,666 --> 00:34:54,462
SPEAKER_0:  If you

00:34:55,650 --> 00:34:57,726
SPEAKER_0:  do a little bit of search, even just a little bit.

00:34:58,402 --> 00:35:00,190
SPEAKER_0:  It was the equivalent of making.

00:35:00,578 --> 00:35:01,246
SPEAKER_0:  your

00:35:01,570 --> 00:35:02,046
SPEAKER_0:  You know?

00:35:02,274 --> 00:35:05,054
SPEAKER_0:  your pre computed strategy like you can kind of think it as your neural net.

00:35:05,346 --> 00:35:06,174
SPEAKER_0:  a thousand times bigger.

00:35:07,010 --> 00:35:08,094
SPEAKER_0:  with just a little bit of search.

00:35:08,514 --> 00:35:09,822
SPEAKER_0:  and it just like blew away.

00:35:10,082 --> 00:35:13,310
SPEAKER_0:  all of the research that we had been working on and trying to like scale up.

00:35:13,698 --> 00:35:15,358
SPEAKER_0:  this like pre-computed solution.

00:35:15,842 --> 00:35:16,510
SPEAKER_0:  It was.

00:35:17,186 --> 00:35:19,262
SPEAKER_0:  by the benefit that we got from search.

00:35:19,938 --> 00:35:24,094
SPEAKER_1:  Can you just linger on what you mean by search here? You're searching over a space of...

00:35:24,610 --> 00:35:25,470
SPEAKER_1:  actions

00:35:26,146 --> 00:35:28,574
SPEAKER_1:  for your hand and for other hands.

00:35:29,090 --> 00:35:31,358
SPEAKER_1:  How are you selecting the other hands to search over?

00:35:32,162 --> 00:35:32,670
SPEAKER_1:  So

00:35:32,770 --> 00:35:33,790
SPEAKER_0:  Yeah. Randomly.

00:35:34,050 --> 00:35:37,374
SPEAKER_0:  No, it's all the other hands that you could have. So when you're playing No Limit Text, just hold on.

00:35:37,666 --> 00:35:41,054
SPEAKER_0:  you've got two face down cards. And so that's 52 choose 2.

00:35:41,378 --> 00:35:47,902
SPEAKER_0:  1326 different combinations. Now that's actually a little bit lower because there's face-up cards in the middle and so you can eliminate those as well.

00:35:48,354 --> 00:35:51,454
SPEAKER_0:  But you're looking at like around a thousand different possible hands that you can have.

00:35:52,034 --> 00:35:54,110
SPEAKER_0:  And so when we're doing, when the bot's doing search.

00:35:54,498 --> 00:35:55,838
SPEAKER_0:  It's thinking explicitly.

00:35:56,162 --> 00:35:56,734
SPEAKER_0:  There are these.

00:35:56,994 --> 00:35:59,998
SPEAKER_0:  thousand different hands that I could have. There are these thousand different hands that you could have.

00:36:00,674 --> 00:36:04,478
SPEAKER_0:  Let me try to figure out what would be a better strategy than what I've precomputed.

00:36:04,898 --> 00:36:05,342
SPEAKER_0:  for.

00:36:05,602 --> 00:36:06,974
SPEAKER_0:  these hands and your hands.

00:36:07,810 --> 00:36:09,022
SPEAKER_1:  Okay, so.

00:36:09,826 --> 00:36:11,006
SPEAKER_1:  That's search.

00:36:11,458 --> 00:36:13,214
SPEAKER_1:  How do you fuse that?

00:36:13,986 --> 00:36:18,462
SPEAKER_1:  with what the neural net is telling you or what the train system is telling you

00:36:19,042 --> 00:36:19,710
SPEAKER_0:  Yeah, so.

00:36:20,450 --> 00:36:21,502
SPEAKER_0:  You kind of like...

00:36:21,954 --> 00:36:24,734
SPEAKER_0:  where the train system comes in is the value.

00:36:25,154 --> 00:36:26,430
SPEAKER_0:  at the end. There's

00:36:26,914 --> 00:36:27,390
SPEAKER_0:  Um.

00:36:27,938 --> 00:36:29,374
SPEAKER_0:  You only look so far ahead.

00:36:29,858 --> 00:36:33,694
SPEAKER_0:  you look like maybe one round ahead. So if you're on the flop, looking to the start of the turn.

00:36:34,338 --> 00:36:34,750
SPEAKER_0:  Um.

00:36:35,650 --> 00:36:37,598
SPEAKER_0:  And at that point you can use.

00:36:37,826 --> 00:36:39,422
SPEAKER_0:  the pre-computed solution to figure out.

00:36:39,842 --> 00:36:42,750
SPEAKER_0:  what are what's the value here of like of this strategy

00:36:43,938 --> 00:36:46,494
SPEAKER_1:  Is it of a single action essentially in that spot?

00:36:46,914 --> 00:36:49,022
SPEAKER_1:  you're getting a value or is it?

00:36:49,282 --> 00:36:51,678
SPEAKER_1:  the value of the entire series of actions.

00:36:52,258 --> 00:36:53,054
SPEAKER_0:  Well, it's kind of both.

00:36:53,282 --> 00:36:55,998
SPEAKER_0:  because you're trying to maximize the value for.

00:36:56,930 --> 00:37:00,766
SPEAKER_0:  hand that you have, but in the process in order to maximize the value of the hand that you have.

00:37:01,154 --> 00:37:02,078
SPEAKER_0:  You have to figure out.

00:37:02,402 --> 00:37:04,158
SPEAKER_0:  What would I be doing with all these other hands as well?

00:37:04,578 --> 00:37:08,030
SPEAKER_1:  But are you in the search or was going to the end?

00:37:08,258 --> 00:37:08,702
SPEAKER_1:  the game.

00:37:09,282 --> 00:37:10,686
SPEAKER_0:  In Labrador's we did.

00:37:11,170 --> 00:37:13,758
SPEAKER_0:  So we only use search starting on the turn

00:37:14,274 --> 00:37:16,766
SPEAKER_0:  and then we searched all the way to the end of the game.

00:37:17,090 --> 00:37:17,982
SPEAKER_1:  to turn the river.

00:37:19,138 --> 00:37:20,638
SPEAKER_1:  Can we take it?

00:37:20,994 --> 00:37:24,734
SPEAKER_0:  Yeah, there's four rounds of poker. So there's the preflop.

00:37:25,090 --> 00:37:26,334
SPEAKER_0:  flop the turn in the river.

00:37:26,850 --> 00:37:29,278
SPEAKER_0:  And so we would start doing search.

00:37:29,698 --> 00:37:30,526
SPEAKER_0:  halfway through the game.

00:37:30,978 --> 00:37:34,142
SPEAKER_0:  Now the first half of the game, that was all precomputed, it would just act instantly.

00:37:34,530 --> 00:37:36,862
SPEAKER_0:  And then when it got to the halfway point.

00:37:37,122 --> 00:37:38,878
SPEAKER_0:  then it would always search to the end of the game.

00:37:39,106 --> 00:37:42,942
SPEAKER_0:  Now we later improved this so it wouldn't have to search all the way to the end of the game. It would actually search.

00:37:43,266 --> 00:37:44,894
SPEAKER_0:  Just a few moves ahead.

00:37:45,538 --> 00:37:50,910
SPEAKER_0:  But that came later and that drastically reduced the amount of computational resources that we needed.

00:37:51,362 --> 00:37:56,222
SPEAKER_1:  But the moves, because you can keep betting on top of each other. That's what you mean by moves. So like that's where.

00:37:56,706 --> 00:37:57,918
SPEAKER_1:  You don't just get one bet.

00:37:58,882 --> 00:38:02,046
SPEAKER_1:  per turner poker. You can have multiple arbitrary number of bets.

00:38:02,178 --> 00:38:04,062
SPEAKER_0:  Right, right. I'm trying to think like...

00:38:04,610 --> 00:38:08,158
SPEAKER_0:  I'm gonna bet and then what are you gonna do in response? Are you gonna raise me? Are you gonna call?

00:38:08,386 --> 00:38:09,790
SPEAKER_0:  And then if you raise, what should I do?

00:38:10,178 --> 00:38:10,590
SPEAKER_0:  So it's.

00:38:10,850 --> 00:38:15,390
SPEAKER_0:  reasoning about that whole process up until the end of the game in the case of Labrador's.

00:38:15,586 --> 00:38:19,422
SPEAKER_1:  So for the broadest, what's the most number of re-raises have you ever seen?

00:38:20,578 --> 00:38:26,398
SPEAKER_0:  You probably cap out at like five or something because at that point you're basically all in.

00:38:26,722 --> 00:38:28,254
SPEAKER_1:  I mean, is there like a...

00:38:28,866 --> 00:38:30,782
SPEAKER_1:  interesting patterns like that that you've seen.

00:38:31,202 --> 00:38:34,942
SPEAKER_1:  that the game does like you'll have like AlphaZero doing way more sacrifices than

00:38:35,234 --> 00:38:36,414
SPEAKER_1:  humans usually do.

00:38:36,930 --> 00:38:38,238
SPEAKER_1:  Is there something like...

00:38:38,882 --> 00:38:42,494
SPEAKER_1:  The Brattice was constantly re-raising or something like that that you noticed.

00:38:43,042 --> 00:38:46,270
SPEAKER_0:  There was something really interesting that we observed with Labradorus.

00:38:46,498 --> 00:38:47,070
SPEAKER_0:  So.

00:38:47,650 --> 00:38:49,086
SPEAKER_0:  Humans when they're playing poker.

00:38:49,378 --> 00:38:52,190
SPEAKER_0:  they usually size their bets relative to the size of the pot.

00:38:52,514 --> 00:38:54,910
SPEAKER_0:  So, you know, if the pot has a hundred dollars in there.

00:38:55,234 --> 00:39:00,094
SPEAKER_0:  Maybe you bet like $75 or somewhere around that, somewhere between like $50 and $100.

00:39:00,834 --> 00:39:06,014
SPEAKER_0:  And with Lebronis, we gave it the option to basically bet whatever it wanted. It was actually...

00:39:06,466 --> 00:39:09,150
SPEAKER_0:  really easy for us to say like, oh, if you want, you can bet like 10 times the pot.

00:39:09,602 --> 00:39:11,742
SPEAKER_0:  And we didn't think it would actually do that. It was just like...

00:39:12,194 --> 00:39:13,246
SPEAKER_0:  Why not give it the option?

00:39:13,890 --> 00:39:16,350
SPEAKER_0:  And then during the competition, it actually started doing this.

00:39:16,706 --> 00:39:20,542
SPEAKER_0:  And by the way, this is like a very last minute decision on our part to add this option. And so we.

00:39:20,834 --> 00:39:24,190
SPEAKER_0:  did not, we did not think the bot would do this and.

00:39:24,482 --> 00:39:29,694
SPEAKER_0:  I was actually kind of worried when it did start to do this. Like, oh, is this a problem? Like humans don't do this. Like, is it screwing up?

00:39:30,274 --> 00:39:33,950
SPEAKER_0:  But it would put the humans into really difficult spots when it would do that.

00:39:34,786 --> 00:39:35,294
SPEAKER_0:  because...

00:39:35,778 --> 00:39:39,774
SPEAKER_0:  You know, you can imagine like you have the second best hand that's possible given the board.

00:39:40,162 --> 00:39:43,262
SPEAKER_0:  and you're thinking like, oh, you're in a really great spot here. And suddenly the bot.

00:39:43,522 --> 00:39:46,238
SPEAKER_0:  but it's $20,000 into a $1,000 pot.

00:39:46,690 --> 00:39:48,414
SPEAKER_0:  and it's basically saying like.

00:39:48,642 --> 00:39:49,790
SPEAKER_0:  I have the best hand.

00:39:50,306 --> 00:39:51,166
SPEAKER_0:  Or I'm bluffing.

00:39:51,874 --> 00:39:54,142
SPEAKER_0:  and you having the second best hand like.

00:39:54,402 --> 00:39:55,870
SPEAKER_0:  Now you get a really tough choice to make.

00:39:56,482 --> 00:40:00,158
SPEAKER_0:  And so the humans would sometimes think like five or 10 minutes about like.

00:40:00,514 --> 00:40:02,622
SPEAKER_0:  What do you do? Should I call? Should I fold?

00:40:03,106 --> 00:40:03,934
SPEAKER_0:  And um

00:40:04,514 --> 00:40:09,214
SPEAKER_0:  And when I saw the humans really struggling with that decision, that's when I realized, oh, actually, this is maybe a good thing to do after all.

00:40:09,794 --> 00:40:11,806
SPEAKER_1:  And of course, the system is a no.

00:40:12,130 --> 00:40:12,862
SPEAKER_1:  making.

00:40:13,442 --> 00:40:16,094
SPEAKER_1:  Again, like we said, that is putting them in a tough spot.

00:40:16,898 --> 00:40:21,054
SPEAKER_1:  It's just that's part of the game theory optimal.

00:40:21,282 --> 00:40:25,246
SPEAKER_0:  Right, from the bots perspective, it's just doing the thing that's going to make it the most money.

00:40:25,826 --> 00:40:31,518
SPEAKER_0:  Um, and the fact that it's putting the humans in a difficult spot, like that's just, um, you know, a side effect of that.

00:40:32,194 --> 00:40:33,438
SPEAKER_0:  And this was, I think...

00:40:34,178 --> 00:40:37,310
SPEAKER_0:  The one thing, I mean, there were a few things that the humans walked away from, but this was-

00:40:37,762 --> 00:40:38,174
SPEAKER_0:  day.

00:40:38,466 --> 00:40:41,374
SPEAKER_0:  The number one thing that the humans walked away from the competition saying like

00:40:41,698 --> 00:40:42,846
SPEAKER_0:  We need to start doing this.

00:40:43,074 --> 00:40:45,918
SPEAKER_0:  And now these over bets, what are called over bets.

00:40:46,242 --> 00:40:48,510
SPEAKER_0:  have become really common in high level poker play.

00:40:48,770 --> 00:40:49,790
SPEAKER_1:  Have you ever talked to like...

00:40:50,082 --> 00:40:51,838
SPEAKER_1:  somebody like Danny on the ground know about this.

00:40:52,162 --> 00:40:53,566
SPEAKER_1:  He seems to be a student of the game.

00:40:54,114 --> 00:40:57,566
SPEAKER_0:  I did actually have a conversation with Daniel DeGrawne once.

00:40:58,082 --> 00:40:59,422
SPEAKER_0:  I was visiting the Isle of Man.

00:40:59,650 --> 00:41:02,014
SPEAKER_0:  to talk to PokerStars about AI.

00:41:02,498 --> 00:41:03,070
SPEAKER_0:  Um...

00:41:03,330 --> 00:41:07,134
SPEAKER_0:  And Daniel and Grandi was there. We had dinner together with some other people.

00:41:07,586 --> 00:41:11,102
SPEAKER_0:  And yeah, he was really interested in it. He mentioned that he was like.

00:41:11,426 --> 00:41:14,238
SPEAKER_0:  excited about learning from these AIs.

00:41:14,466 --> 00:41:16,254
SPEAKER_1:  So he wasn't scared, he was excited.

00:41:16,354 --> 00:41:17,566
SPEAKER_0:  He was excited and uh...

00:41:17,986 --> 00:41:22,430
SPEAKER_0:  And he honestly, he wanted to play against the bot. He thought he had a decent chance of beating it.

00:41:22,818 --> 00:41:24,542
SPEAKER_0:  I think he, you know.

00:41:25,314 --> 00:41:27,934
SPEAKER_0:  This is like several years ago when I think it was like.

00:41:28,322 --> 00:41:32,414
SPEAKER_0:  not as clear to everybody that the AIs were taking over.

00:41:32,674 --> 00:41:35,294
SPEAKER_0:  I think now people recognize that if you're playing against...

00:41:35,618 --> 00:41:38,357
SPEAKER_0:  a bot, there's like no chance that you have in a game.

00:41:38,357 --> 00:41:40,638
SPEAKER_1:  So consistently the bots will win.

00:41:41,154 --> 00:41:42,878
SPEAKER_1:  The bots have heads up.

00:41:43,650 --> 00:41:47,518
SPEAKER_1:  and in other variants too, so multi six player.

00:41:48,226 --> 00:41:48,862
SPEAKER_1:  Texas Hold'em.

00:41:49,250 --> 00:41:51,102
SPEAKER_1:  No limit access hold, don't miss the bots win.

00:41:51,842 --> 00:41:56,670
SPEAKER_0:  Yeah, that's the case. So I think there is some debate about like, is it true for every single variant of poker? I think...

00:41:57,282 --> 00:42:02,942
SPEAKER_0:  I think for every single variant of poker, if somebody really put in the effort, they could make an AI that would beat all humans at it.

00:42:03,330 --> 00:42:03,806
SPEAKER_0:  Um...

00:42:04,386 --> 00:42:04,894
SPEAKER_0:  Weave.

00:42:05,218 --> 00:42:08,542
SPEAKER_0:  focused on the most popular variants. So heads up, no limit, Texas hold them.

00:42:08,898 --> 00:42:10,494
SPEAKER_0:  And then we followed that up with...

00:42:10,818 --> 00:42:12,638
SPEAKER_0:  with a six player poker as well.

00:42:13,026 --> 00:42:14,974
SPEAKER_0:  where he managed to make about that beat.

00:42:15,266 --> 00:42:18,110
SPEAKER_0:  expert human players. And I think even there now.

00:42:18,434 --> 00:42:21,059
SPEAKER_0:  it's pretty clear that humans don't stand a chance.

00:42:21,059 --> 00:42:23,070
SPEAKER_1:  I would love to hook up an AI system that...

00:42:23,842 --> 00:42:25,150
SPEAKER_1:  looks at EEG.

00:42:25,858 --> 00:42:27,454
SPEAKER_1:  Like how, like actually.

00:42:27,778 --> 00:42:30,718
SPEAKER_1:  tries to optimize the toughness of the spot, it puts a human in.

00:42:31,330 --> 00:42:35,134
SPEAKER_1:  I would love to see how different is that from the Game Theory optimal.

00:42:35,586 --> 00:42:38,974
SPEAKER_1:  So you try to maximize the heart rate of the human player.

00:42:40,194 --> 00:42:41,342
SPEAKER_1:  Like, the freaking out.

00:42:41,634 --> 00:42:44,702
SPEAKER_1:  over a long period of time. I wonder if there's going to be...

00:42:45,538 --> 00:42:47,294
SPEAKER_1:  different strategies that emerge.

00:42:47,778 --> 00:42:51,326
SPEAKER_1:  they're close in terms of effectiveness because something tells me

00:42:51,938 --> 00:42:52,574
SPEAKER_1:  You could still.

00:42:52,802 --> 00:42:53,342
SPEAKER_1:  B.

00:42:54,018 --> 00:42:57,662
SPEAKER_1:  achieve superhuman level performance by just making people sweat.

00:42:58,562 --> 00:42:59,294
SPEAKER_0:  I feel like that

00:42:59,938 --> 00:43:02,302
SPEAKER_0:  There's a good chance that that is the case, yeah. If you're-

00:43:02,594 --> 00:43:03,582
SPEAKER_0:  able to see like.

00:43:03,842 --> 00:43:11,166
SPEAKER_0:  It's like a decent proxy for score, right? And this is actually like the common poker wisdom where they're teaching.

00:43:11,394 --> 00:43:12,510
SPEAKER_0:  players before they're robots.

00:43:12,898 --> 00:43:15,518
SPEAKER_0:  and they were trying to teach people how to play poker. They would say like,

00:43:15,746 --> 00:43:18,270
SPEAKER_0:  The key to the game is to put your opponent to difficult spots.

00:43:18,658 --> 00:43:22,179
SPEAKER_0:  It's a good estimate for if you're making the right decision.

00:43:22,179 --> 00:43:25,342
SPEAKER_1:  What else can you say about the fundamental role of search?

00:43:25,954 --> 00:43:26,910
SPEAKER_1:  in poker.

00:43:27,298 --> 00:43:30,014
SPEAKER_1:  and maybe if you can also relate it to chess and go.

00:43:30,306 --> 00:43:31,294
SPEAKER_1:  in these games.

00:43:31,874 --> 00:43:32,222
SPEAKER_1:  Um.

00:43:33,154 --> 00:43:35,646
SPEAKER_1:  What's the role of search to solve in these games?

00:43:37,058 --> 00:43:46,910
SPEAKER_0:  Yeah, I think a lot of people under... This is true for the general public, and I think it's true for the AI community. A lot of people underestimate the importance of search for these kinds of game AI results.

00:43:47,522 --> 00:43:48,030
SPEAKER_0:  Um...

00:43:48,322 --> 00:43:49,982
SPEAKER_0:  An example of this is.

00:43:50,914 --> 00:43:53,374
SPEAKER_0:  TD Gammon that came out in 1992. This was the first review of external

00:43:53,634 --> 00:43:56,798
SPEAKER_0:  the first real instance of a neural net being used in a game AI.

00:43:57,186 --> 00:44:00,062
SPEAKER_0:  landmark achievement. It was actually the inspiration for AlphaZero.

00:44:00,578 --> 00:44:04,190
SPEAKER_0:  and it used search, it used to apply search to figure out its next move.

00:44:05,026 --> 00:44:05,406
SPEAKER_0:  You got—

00:44:05,762 --> 00:44:06,782
SPEAKER_0:  Deep Blue there.

00:44:07,490 --> 00:44:09,758
SPEAKER_0:  It was very heavily focused on search.

00:44:09,986 --> 00:44:13,054
SPEAKER_0:  looking many, many moves ahead, farther than any human could.

00:44:13,346 --> 00:44:15,006
SPEAKER_0:  And that was key for Wyatt1.

00:44:15,714 --> 00:44:17,342
SPEAKER_0:  and then even with something like AlphaGo.

00:44:18,114 --> 00:44:19,838
SPEAKER_0:  I mean, AlphaGo is.

00:44:20,194 --> 00:44:21,566
SPEAKER_0:  commonly hailed as

00:44:21,922 --> 00:44:24,446
SPEAKER_0:  a landmark achievement for neural nets, and it is.

00:44:24,802 --> 00:44:28,638
SPEAKER_0:  But there's also this huge component of search, Monte Carlo tree search to AlphaGo.

00:44:29,090 --> 00:44:31,166
SPEAKER_0:  that was key, absolutely essential.

00:44:31,490 --> 00:44:33,182
SPEAKER_0:  for the AI to be able to beat top humans.

00:44:34,050 --> 00:44:34,494
SPEAKER_0:  Um...

00:44:35,298 --> 00:44:37,502
SPEAKER_0:  I think a good example of this is you look at.

00:44:37,826 --> 00:44:41,374
SPEAKER_0:  the latest versions of AlphaGo, like it was called AlphaZero.

00:44:42,306 --> 00:44:42,814
SPEAKER_0:  Um...

00:44:43,394 --> 00:44:45,694
SPEAKER_0:  And there's this metric called Elo rating where you can.

00:44:46,082 --> 00:44:48,798
SPEAKER_0:  compare different humans and you can compare bots to humans.

00:44:49,346 --> 00:44:49,758
SPEAKER_0:  Now.

00:44:50,178 --> 00:44:54,878
SPEAKER_0:  A top human player is around 3600 elo, maybe a little bit higher now.

00:44:55,426 --> 00:44:57,086
SPEAKER_0:  Alpha Zero, the strongest version.

00:44:57,346 --> 00:44:58,910
SPEAKER_0:  is around 5200 ELO.

00:44:59,938 --> 00:45:02,398
SPEAKER_0:  But if you take out the search that's being done.

00:45:03,458 --> 00:45:05,598
SPEAKER_0:  test time and but by the way what I mean by search is

00:45:06,050 --> 00:45:08,766
SPEAKER_0:  The planning ahead, the thinking of like, oh, if I move my...

00:45:09,154 --> 00:45:11,230
SPEAKER_0:  If I place this stone here and then he does this.

00:45:11,522 --> 00:45:14,558
SPEAKER_0:  And then you look like five moves ahead and you see like what the board state looks like.

00:45:15,042 --> 00:45:15,486
SPEAKER_0:  Um...

00:45:15,874 --> 00:45:16,894
SPEAKER_0:  That's what I mean by search.

00:45:17,250 --> 00:45:19,486
SPEAKER_0:  If you take out the search that's done during the game...

00:45:19,810 --> 00:45:21,694
SPEAKER_0:  The ELO rating drops to around 3000.

00:45:22,818 --> 00:45:23,678
SPEAKER_0:  So even today.

00:45:24,354 --> 00:45:25,982
SPEAKER_0:  what, seven years after AlphaGo.

00:45:27,010 --> 00:45:29,790
SPEAKER_0:  If you take out the Monte Carlo tree search that's being done.

00:45:30,242 --> 00:45:30,558
SPEAKER_0:  at.

00:45:30,882 --> 00:45:31,966
SPEAKER_0:  when playing against a human.

00:45:32,642 --> 00:45:33,694
SPEAKER_0:  The bots are not superhuman.

00:45:34,050 --> 00:45:34,814
SPEAKER_0:  Nobody

00:45:35,330 --> 00:45:36,542
SPEAKER_0:  has made a raw neural net.

00:45:37,026 --> 00:45:38,110
SPEAKER_0:  that is superhuman in Go.

00:45:39,362 --> 00:45:42,302
SPEAKER_1:  It's worth lingering on, that's quite profound.

00:45:43,170 --> 00:45:46,782
SPEAKER_1:  So without search that just means looking at the next move.

00:45:47,650 --> 00:45:48,510
SPEAKER_1:  and saying...

00:45:48,802 --> 00:45:52,542
SPEAKER_1:  this is the best move. so having a function that estimates accurately.

00:45:52,962 --> 00:45:54,142
SPEAKER_1:  what the best move is.

00:45:54,338 --> 00:45:55,422
SPEAKER_0:  That's right. Without search.

00:45:55,970 --> 00:46:03,070
SPEAKER_0:  Yeah, and all these bots, they have the, what's called a policy network where it will tell you, this is what the neural net thinks is the next best move.

00:46:03,554 --> 00:46:04,030
SPEAKER_0:  Um...

00:46:04,866 --> 00:46:05,342
SPEAKER_0:  and

00:46:05,858 --> 00:46:10,430
SPEAKER_0:  It's kind of like the intuition that a human has. You know, the human looks at the board and...

00:46:10,722 --> 00:46:11,390
SPEAKER_0:  and any

00:46:11,874 --> 00:46:16,830
SPEAKER_0:  uh go or chess master will be able to tell you like oh instantly here's what i think the right move is

00:46:17,282 --> 00:46:19,326
SPEAKER_0:  And the bot is able to do the same thing.

00:46:19,778 --> 00:46:20,510
SPEAKER_0:  But just like.

00:46:20,802 --> 00:46:21,534
SPEAKER_0:  How a human...

00:46:22,050 --> 00:46:24,766
SPEAKER_0:  Grandmaster can make a better decision if they have more time to think.

00:46:25,218 --> 00:46:27,262
SPEAKER_0:  when you add on this Monte Carlo tree search.

00:46:27,682 --> 00:46:29,822
SPEAKER_0:  the bot is able to make a better decision.

00:46:30,498 --> 00:46:35,102
SPEAKER_1:  Yeah, I mean, of course a human is doing something like searching their brain, but it's not.

00:46:36,802 --> 00:46:41,406
SPEAKER_1:  I hesitate to draw a hard line, but it's not like Monte Carlo tree search.

00:46:41,730 --> 00:46:42,750
SPEAKER_1:  It's more like...

00:46:43,618 --> 00:46:45,566
SPEAKER_1:  sequential language model.

00:46:45,826 --> 00:46:46,654
SPEAKER_1:  generation.

00:46:46,882 --> 00:46:48,286
SPEAKER_1:  So it's like a different, it's a.

00:46:48,610 --> 00:46:50,334
SPEAKER_1:  The neural network is doing the searching.

00:46:51,394 --> 00:46:53,534
SPEAKER_1:  I wonder what the human brain is doing in terms of searching.

00:46:53,794 --> 00:46:57,022
SPEAKER_1:  because you're doing that computation, like humanist computing.

00:46:57,250 --> 00:46:58,686
SPEAKER_1:  They have intuition, they've got-

00:47:00,066 --> 00:47:02,654
SPEAKER_1:  They have a really strong ability to estimate, you know.

00:47:02,978 --> 00:47:07,646
SPEAKER_1:  amongst the top players of what is good and not position without calculating all the details.

00:47:08,354 --> 00:47:10,014
SPEAKER_1:  but they're still doing search in their head.

00:47:10,274 --> 00:47:14,654
SPEAKER_1:  but it's a different kind of search. Have you ever thought about like, what is the difference between the human and the animal?

00:47:15,554 --> 00:47:17,086
SPEAKER_1:  the search that the human is.

00:47:17,410 --> 00:47:18,654
SPEAKER_1:  Performing versus what?

00:47:19,714 --> 00:47:20,606
SPEAKER_1:  computers are doing.

00:47:21,186 --> 00:47:23,774
SPEAKER_0:  I have thought a lot about that and I think it's a really important question.

00:47:24,226 --> 00:47:29,022
SPEAKER_0:  So the AI in Alpha and Alpha's in AlphaGo or any of these Go AIs.

00:47:29,314 --> 00:47:33,022
SPEAKER_0:  they're all doing Monte Carlo tree search, which is a particular kind of search.

00:47:33,378 --> 00:47:40,318
SPEAKER_0:  It's actually a symbolic tabular search. It uses the neural net to guide its search, but it isn't actually like...

00:47:40,706 --> 00:47:41,918
SPEAKER_0:  full on neural net.

00:47:42,786 --> 00:47:43,134
SPEAKER_0:  Now.

00:47:43,458 --> 00:47:44,766
SPEAKER_0:  That kind of search is.

00:47:45,186 --> 00:47:49,534
SPEAKER_0:  very successful in these kinds of like perfect information board games like chess and go.

00:47:50,114 --> 00:47:52,574
SPEAKER_0:  But if you take it to a game like poker, for example, it doesn't work.

00:47:52,866 --> 00:47:53,598
SPEAKER_0:  It can't.

00:47:53,954 --> 00:47:56,478
SPEAKER_0:  It can't understand the concept of hidden information.

00:47:56,706 --> 00:47:59,326
SPEAKER_0:  It doesn't understand the balance that you have to strike between like

00:47:59,554 --> 00:48:01,758
SPEAKER_0:  the amount that you're raising versus the amount that you're calling.

00:48:02,210 --> 00:48:04,190
SPEAKER_0:  and in every one of these games.

00:48:04,610 --> 00:48:06,078
SPEAKER_0:  you see a different kind of search.

00:48:06,690 --> 00:48:11,006
SPEAKER_0:  And the human brain is able to plan for all these different games in a very general way.

00:48:11,586 --> 00:48:15,998
SPEAKER_0:  Now, I think that's one thing that we're missing from AI today, and I think it's a really important missing piece.

00:48:16,290 --> 00:48:17,246
SPEAKER_0:  the ability to

00:48:17,762 --> 00:48:19,614
SPEAKER_0:  plan and reason more generally.

00:48:19,970 --> 00:48:23,070
SPEAKER_0:  across a wide variety of different settings.

00:48:23,842 --> 00:48:26,046
SPEAKER_1:  in a way worth general reasoning.

00:48:26,402 --> 00:48:28,615
SPEAKER_1:  makes you better at each one of the games.

00:48:28,615 --> 00:48:32,062
SPEAKER_0:  Not worse. Yeah, so you can kind of think of it as like neural nets today.

00:48:32,866 --> 00:48:35,454
SPEAKER_0:  They'll give you like transformers for example, or super general.

00:48:36,162 --> 00:48:37,406
SPEAKER_0:  but you know, they'll give you...

00:48:37,634 --> 00:48:38,782
SPEAKER_0:  It'll output an answer.

00:48:39,202 --> 00:48:45,214
SPEAKER_0:  in like 100 milliseconds. And if you tell it like, oh, you've got five minutes to give me a decision, feel free to take more time to make a better decision.

00:48:45,634 --> 00:48:46,622
SPEAKER_0:  It's not gonna know what to do with that.

00:48:47,394 --> 00:48:47,870
SPEAKER_0:  Um...

00:48:48,546 --> 00:48:52,478
SPEAKER_0:  But a human, if you're playing a game like chess, they're gonna give you a very different answer depending on if-

00:48:52,834 --> 00:48:55,422
SPEAKER_0:  you say, oh, you've got 100 milliseconds or you've got five minutes.

00:48:57,506 --> 00:49:06,014
SPEAKER_1:  Yeah, there, I mean, people have started using the right transformers, the language models, like the, in an iterative way that does improve the answer or like showing the work.

00:49:06,306 --> 00:49:06,782
SPEAKER_1:  This

00:49:07,074 --> 00:49:07,710
SPEAKER_0:  kind of idea.

00:49:08,130 --> 00:49:15,518
SPEAKER_0:  Yeah, they got this thing called chain of thought reasoning. And that's, I think, super promising, right? Yeah. And I think, I think it's a good step in the right direction.

00:49:15,874 --> 00:49:21,854
SPEAKER_0:  I would kind of like say it's similar to Monte Carlo rollouts in a game like chess. There's a kind of search that you can do.

00:49:22,274 --> 00:49:25,086
SPEAKER_0:  where you're saying like, I'm gonna roll out my intuition and see like,

00:49:25,442 --> 00:49:26,878
SPEAKER_0:  without really thinking.

00:49:27,234 --> 00:49:29,982
SPEAKER_0:  you know, what are the better decisions I can make farther down the path?

00:49:30,434 --> 00:49:33,790
SPEAKER_0:  What would I do if I just acted according to intuition for the next 10 moves?

00:49:34,242 --> 00:49:34,750
SPEAKER_0:  Um...

00:49:35,394 --> 00:49:36,062
SPEAKER_0:  And that.

00:49:36,322 --> 00:49:38,078
SPEAKER_0:  gets you an improvement but I think that there's much

00:49:38,530 --> 00:49:40,382
SPEAKER_0:  uh... much richer kinds of of

00:49:40,610 --> 00:49:41,598
SPEAKER_0:  planning that we could do.

00:49:42,210 --> 00:49:45,502
SPEAKER_1:  So when the broadest actually beat the poker players, what did that feel like?

00:49:46,018 --> 00:49:48,350
SPEAKER_1:  What was that? I mean, actually on that day.

00:49:49,090 --> 00:49:51,582
SPEAKER_1:  What were you feeling like? Were you nervous?

00:49:53,186 --> 00:49:58,110
SPEAKER_1:  I mean, poker was one of the games that you thought like is not going to be solvable because of the human factor, so...

00:49:59,042 --> 00:50:01,374
SPEAKER_1:  at least in the narratives we tell ourselves.

00:50:02,050 --> 00:50:04,510
SPEAKER_1:  the human factor is so fundamental to the game of poker.

00:50:05,314 --> 00:50:08,158
SPEAKER_0:  Yeah, the Lebronis competition was super stressful for me.

00:50:08,546 --> 00:50:09,790
SPEAKER_0:  Also, I mean...

00:50:10,466 --> 00:50:14,270
SPEAKER_0:  I was working on this basically continuously for a year leading up to the competition.

00:50:14,498 --> 00:50:19,646
SPEAKER_0:  I mean, for me, it became like very clear, like, okay, this is the search technique. This is the approach that we need.

00:50:19,874 --> 00:50:22,462
SPEAKER_0:  And then I spent a year working on this pretty much like nonstop.

00:50:22,626 --> 00:50:26,654
SPEAKER_1:  Oh, can we actually get into details like what programming languages is it written in?

00:50:26,914 --> 00:50:27,998
SPEAKER_1:  What's some interesting-

00:50:28,514 --> 00:50:30,846
SPEAKER_1:  implementation details that are like...

00:50:31,522 --> 00:50:32,862
SPEAKER_1:  fun slash painful.

00:50:33,346 --> 00:50:34,942
SPEAKER_0:  Yeah, so one of the interesting things about.

00:50:35,234 --> 00:50:45,214
SPEAKER_0:  Lebron is that we had no idea what the bar was to actually beat top humans Yeah, we could play against like our prior bots and that kind of gives us some sense of like are we making progress? Are we going in the right direction?

00:50:45,602 --> 00:50:50,654
SPEAKER_0:  But we had no idea what the bar actually was. And so we threw a huge amount of resources.

00:50:50,882 --> 00:50:52,574
SPEAKER_0:  at trying to make the strongest bot possible.

00:50:53,058 --> 00:50:57,758
SPEAKER_0:  So we used C++, it was parallelized, we were using I think like a thousand CPUs.

00:50:58,114 --> 00:51:00,574
SPEAKER_0:  Maybe more, actually.

00:51:00,962 --> 00:51:06,691
SPEAKER_0:  You know, today that sounds like nothing, but for a grad student back in 2016, that was a huge amount of resources.

00:51:06,691 --> 00:51:10,878
SPEAKER_1:  still a lot for even any grad student today. It's still tough to get.

00:51:12,002 --> 00:51:14,846
SPEAKER_1:  or even to allow yourself to think in that.

00:51:15,170 --> 00:51:18,654
SPEAKER_1:  in terms of scale at CMU at MIT, anything like that.

00:51:18,882 --> 00:51:21,438
SPEAKER_0:  Yeah, and you know talking about terabytes of memory.

00:51:21,826 --> 00:51:26,078
SPEAKER_0:  So it was a very parallelized and it had to be very fast too because

00:51:26,434 --> 00:51:28,190
SPEAKER_0:  the more games that you could simulate.

00:51:28,514 --> 00:51:29,886
SPEAKER_0:  uh... the stronger the bot would be

00:51:30,178 --> 00:51:32,862
SPEAKER_1:  So is there some like John Carmack style?

00:51:33,538 --> 00:51:34,046
SPEAKER_1:  Like.

00:51:34,466 --> 00:51:38,622
SPEAKER_1:  Efficiencies yet to come up with like an efficient way to represent the hand.

00:51:39,138 --> 00:51:40,030
SPEAKER_1:  all that kind of stuff.

00:51:40,194 --> 00:51:45,086
SPEAKER_0:  There are all sorts of optimizations that I had to make to try to get this thing to run as fast as possible. They were like.

00:51:45,314 --> 00:51:52,062
SPEAKER_0:  How do you minimize the latency? How do you package things together so that you minimize the amount of communication between the different nodes?

00:51:52,482 --> 00:51:52,990
SPEAKER_0:  Um...

00:51:53,538 --> 00:51:56,030
SPEAKER_0:  How do you optimize the algorithm so that you can...

00:51:56,418 --> 00:51:59,486
SPEAKER_0:  you know, try to squeeze out more and more from the game that you're actually playing.

00:51:59,810 --> 00:52:01,950
SPEAKER_0:  all these kinds of different decisions that I...

00:52:02,210 --> 00:52:03,102
SPEAKER_1:  you know, had to make.

00:52:03,746 --> 00:52:07,070
SPEAKER_1:  Just a fun question. What ID did you use?

00:52:07,682 --> 00:52:09,438
SPEAKER_1:  What for C++?

00:52:10,114 --> 00:52:11,239
SPEAKER_1:  I think I used...

00:52:11,239 --> 00:52:12,542
SPEAKER_0:  It's a visual studio actually.

00:52:12,802 --> 00:52:15,422
SPEAKER_1:  Okay. Is that still carried through to today?

00:52:15,650 --> 00:52:17,525
SPEAKER_0:  VS Code is what I use today.

00:52:17,525 --> 00:52:21,854
SPEAKER_1:  It's the community basically conversion on. Okay, cool. You got this.

00:52:22,626 --> 00:52:24,638
SPEAKER_1:  super optimized C++ system.

00:52:25,410 --> 00:52:27,454
SPEAKER_1:  and then you show up to the day of competition.

00:52:28,226 --> 00:52:28,542
SPEAKER_1:  Yeah.

00:52:28,866 --> 00:52:30,814
SPEAKER_1: ...humans versus machine.

00:52:31,554 --> 00:52:32,126
SPEAKER_1:  Um...

00:52:32,514 --> 00:52:33,758
SPEAKER_1:  How did it feel throughout the day?

00:52:34,530 --> 00:52:35,198
SPEAKER_0:  super stressful.

00:52:35,490 --> 00:52:35,902
SPEAKER_0:  Um...

00:52:36,450 --> 00:52:36,894
SPEAKER_0:  I mean.

00:52:37,186 --> 00:52:42,494
SPEAKER_0:  I thought going into it that we had like a 50-50 chance. Basically, I thought if they play...

00:52:42,850 --> 00:52:45,310
SPEAKER_0:  In a totally normal style, I think we'll squeak out a win.

00:52:45,858 --> 00:52:46,174
SPEAKER_0:  but.

00:52:46,562 --> 00:52:49,278
SPEAKER_0:  there's always a chance that they can find some weakness in the bot.

00:52:49,858 --> 00:52:55,806
SPEAKER_0:  and if they do and we're playing like for 20 days 120 000 hands of poker they have a lot of time to find weaknesses in the system

00:52:56,386 --> 00:52:57,310
SPEAKER_0:  And if they do...

00:52:57,794 --> 00:53:01,086
SPEAKER_0:  we're gonna get crushed. And that's actually what happened in the previous competition.

00:53:01,506 --> 00:53:05,214
SPEAKER_0:  The humans, you know, they started out it wasn't like a they were winning from the start

00:53:05,602 --> 00:53:07,902
SPEAKER_0:  but then they found these weaknesses that they could take advantage of.

00:53:08,194 --> 00:53:10,654
SPEAKER_0:  And for the next, you know, like 10 days, they were just...

00:53:11,010 --> 00:53:12,670
SPEAKER_0:  crushing the bots, stealing money from it.

00:53:13,058 --> 00:53:14,590
SPEAKER_1:  What were the weaknesses they found?

00:53:15,074 --> 00:53:19,038
SPEAKER_1:  Maybe over betting was effective, that kind of stuff. So certain betting strategies worked.

00:53:19,746 --> 00:53:25,246
SPEAKER_0:  What they found is, yeah, overbetting, like betting certain amounts, the bot would have a lot of trouble dealing with those sizes.

00:53:25,506 --> 00:53:26,558
SPEAKER_0:  And then also.

00:53:27,042 --> 00:53:27,486
SPEAKER_0:  Um.

00:53:27,874 --> 00:53:28,798
SPEAKER_0:  when the bot

00:53:29,378 --> 00:53:31,614
SPEAKER_0:  got into really difficult all-in situations.

00:53:31,874 --> 00:53:36,190
SPEAKER_0:  It wasn't able to because it wasn't doing search. It had to

00:53:36,642 --> 00:53:39,742
SPEAKER_0:  clump different hands together and it wouldn't it would treat them identically.

00:53:39,970 --> 00:53:42,526
SPEAKER_0:  And so it wouldn't be able to distinguish.

00:53:42,818 --> 00:53:43,486
SPEAKER_0:  You know, like...

00:53:43,874 --> 00:53:52,574
SPEAKER_0:  having a king high flush versus an ace high flush. And in some situations that really matters a lot. And so they could put the bot into those situations and then the bot would just bleed money.

00:53:53,058 --> 00:53:57,054
SPEAKER_1:  Clever humans. Okay, so I didn't realize it was over 20 days.

00:53:57,634 --> 00:53:58,142
SPEAKER_1:  So.

00:53:58,434 --> 00:53:59,006
SPEAKER_1:  Um...

00:54:00,322 --> 00:54:02,654
SPEAKER_1:  What were the humans like over those 20 days?

00:54:03,234 --> 00:54:04,222
SPEAKER_1:  And what was the bot like?

00:54:04,578 --> 00:54:08,894
SPEAKER_0:  So we had set up the competition, you know, like I said, there was $200,000 in prize money.

00:54:09,218 --> 00:54:11,102
SPEAKER_0:  and they would get paid.

00:54:11,394 --> 00:54:14,014
SPEAKER_0:  a fraction of that depending on how well they did relative to each other.

00:54:14,466 --> 00:54:17,950
SPEAKER_0:  So I was kind of hoping that they wouldn't work together to try to find weaknesses in the bot.

00:54:19,010 --> 00:54:22,462
SPEAKER_0:  they enter the competition with their like number one objective being to beat the bot

00:54:22,786 --> 00:54:26,334
SPEAKER_0:  and they didn't care about like individual glory. They were like, we're all going to work as a team.

00:54:26,594 --> 00:54:29,118
SPEAKER_0:  to try to take down this bot. Yeah.

00:54:29,538 --> 00:54:31,806
SPEAKER_0:  they immediately started comparing notes. what they would do.

00:54:32,226 --> 00:54:34,014
SPEAKER_0:  is they would coordinate.

00:54:34,402 --> 00:54:38,942
SPEAKER_0:  looking at different parts of the strategy to try to find out weaknesses.

00:54:39,362 --> 00:54:39,774
SPEAKER_0:  Um.

00:54:40,514 --> 00:54:41,662
SPEAKER_0:  And then at the end of the day...

00:54:41,954 --> 00:54:44,702
SPEAKER_0:  We actually sent them a log of all the hands that were played and what

00:54:45,186 --> 00:54:47,902
SPEAKER_0:  cards the bot had on each of those hands. Oh wow.

00:54:48,130 --> 00:54:52,670
SPEAKER_0:  That's gutsy. Yeah, it was honestly, I'm not sure why we did that in retrospect, but um.

00:54:52,994 --> 00:54:55,070
SPEAKER_0:  I mean, I'm glad we did it because we ended up winning anyway, but...

00:54:55,362 --> 00:54:59,710
SPEAKER_0:  That if you've ever played poker before like that is golden information. I mean to know

00:55:00,162 --> 00:55:02,910
SPEAKER_0:  Usually when you play poker, you see about a third of the hands to showdown.

00:55:03,394 --> 00:55:04,862
SPEAKER_0:  and to just hand them.

00:55:05,314 --> 00:55:08,030
SPEAKER_0:  all the cards that the bot had on every single hand.

00:55:08,546 --> 00:55:09,726
SPEAKER_0:  That was, um...

00:55:10,370 --> 00:55:13,438
SPEAKER_0:  just a gold mine for them. And so then they would review the hands.

00:55:13,698 --> 00:55:18,302
SPEAKER_0:  and try to see like, okay, could they find patterns in the bot? The weakness is, and could they then?

00:55:18,562 --> 00:55:21,566
SPEAKER_0:  then they would coordinate and study together and try to figure out okay now

00:55:21,858 --> 00:55:26,782
SPEAKER_0:  This person's gonna explore this part of the strategy for weaknesses. This person's gonna explore this part of the strategy for weaknesses.

00:55:28,098 --> 00:55:31,774
SPEAKER_1:  It's a kind of psychological warfare showing in the hands. Yeah.

00:55:32,034 --> 00:55:34,398
SPEAKER_1:  I mean, I'm sure you didn't think of it that way, but like

00:55:34,626 --> 00:55:38,526
SPEAKER_1:  Doing that means you're confident in the possibility to win.

00:55:38,722 --> 00:55:41,758
SPEAKER_0:  Well, that's one way of putting it. I wasn't super confident. Yeah.

00:55:42,018 --> 00:55:42,494
SPEAKER_0:  So.

00:55:43,170 --> 00:55:46,846
SPEAKER_0:  You know, going in, like I said, I think I had like 50, 50 odds on us winning the-

00:55:47,458 --> 00:55:49,214
SPEAKER_0:  When we actually when we announced the competition

00:55:49,602 --> 00:55:57,118
SPEAKER_0:  the poker community decided to gamble on who would win and their initial odds against us were like four to one. They were really convinced that the humans were gonna-

00:55:57,794 --> 00:55:58,302
SPEAKER_0:  lot of win.

00:55:58,914 --> 00:55:59,390
SPEAKER_0:  Um...

00:55:59,842 --> 00:56:02,430
SPEAKER_0:  The bot ended up winning for three days straight.

00:56:03,010 --> 00:56:06,238
SPEAKER_0:  And even then after three days, the betting odds were still just 50-50.

00:56:06,818 --> 00:56:07,198
SPEAKER_0:  Um.

00:56:08,418 --> 00:56:09,630
SPEAKER_0:  And then at that point...

00:56:10,274 --> 00:56:11,966
SPEAKER_0:  It started to look like the humans.

00:56:12,322 --> 00:56:16,414
SPEAKER_0:  were coming back, they started to like, you know, but poker is a very high variance game.

00:56:16,898 --> 00:56:17,310
SPEAKER_0:  Um.

00:56:17,570 --> 00:56:18,142
SPEAKER_0:  and

00:56:18,434 --> 00:56:22,206
SPEAKER_0:  I think what happened is like they thought that they spotted some weaknesses that weren't actually there.

00:56:22,626 --> 00:56:26,654
SPEAKER_0:  And then around day eight, it was just very clear that they were getting absolutely crushed.

00:56:27,298 --> 00:56:27,710
SPEAKER_0:  Um.

00:56:28,162 --> 00:56:31,614
SPEAKER_0:  And from that point, for a while there, I was super stressed out.

00:56:31,970 --> 00:56:41,502
SPEAKER_0:  thinking like, oh my God, the humans are coming back and they've found weaknesses and now we're just gonna lose the whole thing. But no, it ended up going in the other direction and the bot ended up crushing them in the long-

00:56:42,850 --> 00:56:44,766
SPEAKER_1:  How did it feel at the end?

00:56:45,634 --> 00:56:47,038
SPEAKER_1:  As a human being, what did-

00:56:47,714 --> 00:56:48,926
SPEAKER_1:  as a person who loves.

00:56:49,410 --> 00:56:51,198
SPEAKER_1:  appreciates the beauty of the game of poker.

00:56:51,586 --> 00:56:53,406
SPEAKER_1:  and the person who appreciates

00:56:54,146 --> 00:56:55,230
SPEAKER_1:  The beauty of AI.

00:56:55,490 --> 00:56:56,286
SPEAKER_1:  Is there?

00:56:56,994 --> 00:56:58,558
SPEAKER_1:  Did you feel a certain kind of way about it?

00:56:59,330 --> 00:57:01,822
SPEAKER_0:  I felt a lot of things, man.

00:57:02,594 --> 00:57:05,630
SPEAKER_0:  I mean, at that point in my life, I had spent five years working on this project.

00:57:05,954 --> 00:57:06,718
SPEAKER_0:  and um

00:57:07,394 --> 00:57:12,574
SPEAKER_0:  It was a huge sense of accomplishment. I mean, to spend five years working on something and finally see it succeed.

00:57:13,058 --> 00:57:13,598
SPEAKER_0:  Um.

00:57:13,826 --> 00:57:15,390
SPEAKER_0:  Yeah, I wouldn't trade that for anything in the world.

00:57:16,066 --> 00:57:18,974
SPEAKER_1:  Yeah, because that's a real benchmark. It's not like a-

00:57:19,586 --> 00:57:23,294
SPEAKER_1:  getting some percent accuracy in a data set.

00:57:23,554 --> 00:57:24,702
SPEAKER_1:  This is like real.

00:57:25,026 --> 00:57:25,982
SPEAKER_1:  This is real world.

00:57:26,338 --> 00:57:30,398
SPEAKER_1:  It's just a game, but it's also a game that means a lot to a lot of people.

00:57:30,722 --> 00:57:34,910
SPEAKER_1:  and this is humans doing their best to beat the machine. So this is a real benchmark.

00:57:35,234 --> 00:57:36,254
SPEAKER_1:  unlike anything else.

00:57:36,450 --> 00:57:37,662
SPEAKER_0:  Yeah, and I mean this is...

00:57:38,178 --> 00:57:42,782
SPEAKER_0:  This is what I had been dreaming about since I was like 16 playing poker, you know, with my friends in high school.

00:57:43,234 --> 00:57:45,150
SPEAKER_0:  the idea that you could find a strategy.

00:57:45,826 --> 00:57:48,830
SPEAKER_0:  uh... uh... you know approximately the national will be able to beat

00:57:49,154 --> 00:57:50,622
SPEAKER_0:  all the poker players in the world with it.

00:57:51,106 --> 00:57:52,670
SPEAKER_0:  You know, so to actually see that.

00:57:52,930 --> 00:57:54,654
SPEAKER_0:  come to fruition and be realized?

00:57:55,074 --> 00:57:56,190
SPEAKER_0:  Uh, that was...

00:57:56,930 --> 00:57:57,726
SPEAKER_0:  It's kind of magical.

00:57:58,466 --> 00:58:02,142
SPEAKER_1:  Yeah, especially money is on the line too. It's different than chess.

00:58:02,978 --> 00:58:04,958
SPEAKER_1:  And that aspect, like people get.

00:58:05,794 --> 00:58:07,838
SPEAKER_1:  That's why you want to look at betting Marcus.

00:58:08,162 --> 00:58:09,630
SPEAKER_1:  if you want to actually understand.

00:58:09,890 --> 00:58:15,102
SPEAKER_1:  what people really think. In the same sense, poker, it's really high stakes because it's money.

00:58:15,746 --> 00:58:18,654
SPEAKER_1:  to solve that game. That's an amazing accomplishment.

00:58:18,946 --> 00:58:21,182
SPEAKER_1:  So the leap from that to...

00:58:21,762 --> 00:58:23,902
SPEAKER_1:  Multiway 6 player poker

00:58:24,386 --> 00:58:24,958
SPEAKER_1:  What's...

00:58:25,282 --> 00:58:26,590
SPEAKER_1:  How difficult is that jump?

00:58:27,554 --> 00:58:30,206
SPEAKER_1:  And what are some interesting differences between heads up poker and-

00:58:30,658 --> 00:58:31,710
SPEAKER_1:  and multiway poker.

00:58:32,546 --> 00:58:33,630
SPEAKER_0:  Yeah, so I mentioned.

00:58:33,890 --> 00:58:36,254
SPEAKER_0:  You know, Nash equilibrium in two-player zero-sum games.

00:58:37,154 --> 00:58:41,374
SPEAKER_0:  If you play that strategy, you are guaranteed to not lose an expectation no matter what your opponent does.

00:58:41,954 --> 00:58:45,118
SPEAKER_0:  Now once you go to 6 player poker, you're no longer playing a 2 player zero sum game.

00:58:45,506 --> 00:58:48,926
SPEAKER_0:  And so there was a lot of debate among the academic community and among the poker community.

00:58:49,282 --> 00:58:51,294
SPEAKER_0:  about how well these techniques would extend.

00:58:51,522 --> 00:58:53,790
SPEAKER_0:  beyond just two player heads up poker.

00:58:54,658 --> 00:58:55,038
SPEAKER_0:  Now

00:58:56,034 --> 00:58:57,758
SPEAKER_0:  What I had come to realize is that...

00:58:58,274 --> 00:58:58,846
SPEAKER_0:  Um...

00:58:59,362 --> 00:59:04,734
SPEAKER_0:  the techniques actually I thought really would extend to six-player poker because even though in theory

00:59:05,186 --> 00:59:05,886
SPEAKER_0:  They don't.

00:59:06,178 --> 00:59:08,414
SPEAKER_0:  give you these guarantees outside of two players, here are some of the games!

00:59:08,866 --> 00:59:10,974
SPEAKER_0:  In practice, it still gives you a really strong strategy.

00:59:11,874 --> 00:59:12,254
SPEAKER_0:  Now.

00:59:12,738 --> 00:59:18,622
SPEAKER_0:  There were a lot of complications that would come up with 6-player poker, besides the game theoretic aspect, I mean for one...

00:59:18,946 --> 00:59:19,710
SPEAKER_0:  The game is just.

00:59:19,970 --> 00:59:20,958
SPEAKER_0:  exponentially larger.

00:59:21,666 --> 00:59:22,142
SPEAKER_0:  Um...

00:59:22,658 --> 00:59:24,382
SPEAKER_0:  So the main thing that allowed us.

00:59:24,610 --> 00:59:26,238
SPEAKER_0:  go from two player to six player?

00:59:26,626 --> 00:59:28,702
SPEAKER_0:  was the idea of Deaf Limited Search.

00:59:29,410 --> 00:59:32,990
SPEAKER_0:  So I said before, like, you know, we would do search, we would plan out.

00:59:33,282 --> 00:59:36,734
SPEAKER_0:  The bot would plan out what it's going to do next and for the next several moves.

00:59:37,154 --> 00:59:39,102
SPEAKER_0:  And in the broadest that search was done.

00:59:39,458 --> 00:59:40,830
SPEAKER_0:  extending all the way to the end of the game.

00:59:41,442 --> 00:59:42,430
SPEAKER_0:  So we have to start.

00:59:42,882 --> 00:59:43,390
SPEAKER_0:  Um...

00:59:44,098 --> 00:59:45,534
SPEAKER_0:  It from from the.

00:59:45,954 --> 00:59:48,734
SPEAKER_0:  turn onwards, like looking maybe 10 moves ahead.

00:59:49,250 --> 00:59:50,814
SPEAKER_0:  it would have to figure out.

00:59:51,426 --> 00:59:52,926
SPEAKER_0:  what it was doing for all those moves.

00:59:53,666 --> 00:59:59,006
SPEAKER_0:  Now when you get to six player poker, it can't do that exhaustive search anymore because the game is just way too large.

00:59:59,618 --> 01:00:01,150
SPEAKER_0:  Um, but by only

01:00:01,634 --> 01:00:04,190
SPEAKER_0:  having to look a few moves ahead and then stopping there.

01:00:04,450 --> 01:00:06,590
SPEAKER_0:  and substituting a value estimate of like...

01:00:06,818 --> 01:00:08,446
SPEAKER_0:  How good is that strategy at that point?

01:00:08,770 --> 01:00:09,982
SPEAKER_0:  then we're able to...

01:00:10,210 --> 01:00:12,030
SPEAKER_0:  do a much more scalable form of search.

01:00:14,402 --> 01:00:16,734
SPEAKER_1:  Is there something cool we're looking at the paper right now?

01:00:17,122 --> 01:00:19,742
SPEAKER_1:  Is there something cool in the paper in terms of graphics?

01:00:20,290 --> 01:00:22,398
SPEAKER_1:  a GameTree Traversal via Monte Carlo.

01:00:22,530 --> 01:00:24,414
SPEAKER_0:  i think if you go down a bit uh...

01:00:25,698 --> 01:00:28,990
SPEAKER_1:  Figure 1, an example of equilibrium selection problem.

01:00:29,282 --> 01:00:30,654
SPEAKER_1:  Ooh, so yeah, uh...

01:00:31,042 --> 01:00:32,574
SPEAKER_1:  What do we know about equilibria?

01:00:32,898 --> 01:00:34,270
SPEAKER_1:  when there's multiple players.

01:00:34,882 --> 01:00:35,326
SPEAKER_0:  So.

01:00:35,906 --> 01:00:37,694
SPEAKER_0:  When you go outside of two players, you're a sum.

01:00:38,210 --> 01:00:41,342
SPEAKER_0:  So a Nash equilibrium is a set of strategies, like one strategy for each player.

01:00:41,762 --> 01:00:45,246
SPEAKER_0:  where no player has an incentive to switch to a different strategy.

01:00:46,242 --> 01:00:49,086
SPEAKER_0:  And so you can kind of think of it as like imagine

01:00:49,378 --> 01:00:50,942
SPEAKER_0:  you have a game where there's a ring.

01:00:51,490 --> 01:00:53,534
SPEAKER_0:  That's actually the visual here. You got a ring.

01:00:54,082 --> 01:00:57,598
SPEAKER_0:  and the object of the game is to be as far away from the other players as possible.

01:00:58,882 --> 01:00:59,198
SPEAKER_0:  There's.

01:00:59,714 --> 01:01:03,742
SPEAKER_0:  A Nash equilibrium is for all the players to be spaced equally apart around this ring.

01:01:04,642 --> 01:01:07,934
SPEAKER_0:  But there's infinitely many different Nash equilibria, right? There's infinitely many ways.

01:01:08,162 --> 01:01:10,110
SPEAKER_0:  to space four dots along a ring.

01:01:11,138 --> 01:01:11,678
SPEAKER_0:  and

01:01:12,066 --> 01:01:15,518
SPEAKER_0:  if every single player independently computes a Nash Equilibrium

01:01:16,258 --> 01:01:19,646
SPEAKER_0:  then there's no guarantee that the joint strategy they're all playing

01:01:20,162 --> 01:01:22,526
SPEAKER_0:  is going to be a Nash equilibrium.

01:01:22,754 --> 01:01:23,070
SPEAKER_0:  DERP

01:01:23,330 --> 01:01:26,110
SPEAKER_0:  They're just gonna be like random dots scattered along this ring rather than...

01:01:26,338 --> 01:01:28,606
SPEAKER_0:  four coordinated dots being equally spaced apart.

01:01:28,802 --> 01:01:31,582
SPEAKER_1:  Is it possible to sort of optimally do this kind of selection?

01:01:32,962 --> 01:01:34,846
SPEAKER_1:  to do the selection of.

01:01:35,170 --> 01:01:37,278
SPEAKER_1:  of the equilibrium you're chasing.

01:01:37,666 --> 01:01:39,774
SPEAKER_1:  So is there like a meta problem to be solved here?

01:01:40,290 --> 01:01:43,230
SPEAKER_0:  So the meta problem is in some sense, how do you...

01:01:43,906 --> 01:01:46,718
SPEAKER_0:  How do you understand the Nash equilibria that the other players are going to play?

01:01:47,554 --> 01:01:47,966
SPEAKER_0:  Um.

01:01:48,546 --> 01:01:49,086
SPEAKER_0:  and

01:01:49,826 --> 01:01:52,862
SPEAKER_0:  And even if you do that, again, there's no guarantee that you're going to win.

01:01:53,186 --> 01:01:53,630
SPEAKER_0:  So.

01:01:54,434 --> 01:01:55,518
SPEAKER_0:  You know, if you're playing...

01:01:56,290 --> 01:01:56,958
SPEAKER_0:  If you're

01:01:57,282 --> 01:02:00,670
SPEAKER_0:  playing risk like I said and all the other players side team up against you

01:02:00,994 --> 01:02:03,006
SPEAKER_0:  You're gonna lose. Mass equilibrium doesn't help you there.

01:02:03,970 --> 01:02:04,510
SPEAKER_0:  and

01:02:05,026 --> 01:02:09,310
SPEAKER_0:  So there is this big debate about whether Nash equilibrium and all these techniques that compute it

01:02:09,634 --> 01:02:12,478
SPEAKER_0:  are even useful once you go outside of two players' in-game

01:02:13,090 --> 01:02:13,534
SPEAKER_0:  Now.

01:02:13,890 --> 01:02:16,670
SPEAKER_0:  I think for many games, there is a valid criticism here.

01:02:17,026 --> 01:02:19,486
SPEAKER_0:  And I think when we talk about, when we got to something like diplomacy.

01:02:19,842 --> 01:02:20,862
SPEAKER_0:  We run into this issue.

01:02:21,634 --> 01:02:22,174
SPEAKER_0:  The

01:02:22,882 --> 01:02:25,438
SPEAKER_0:  approach of trying to approximate a Nash equilibrium.

01:02:26,018 --> 01:02:27,198
SPEAKER_0:  doesn't really work anymore.

01:02:27,874 --> 01:02:29,758
SPEAKER_0:  But it turns out that in 6 player poker...

01:02:30,146 --> 01:02:33,022
SPEAKER_0:  Because six player poker is such an adversarial game

01:02:33,602 --> 01:02:34,078
SPEAKER_0:  Um.

01:02:34,402 --> 01:02:34,718
SPEAKER_0:  where.

01:02:34,978 --> 01:02:37,246
SPEAKER_0:  none of the players really try to work with each other.

01:02:38,178 --> 01:02:41,758
SPEAKER_0:  the techniques that were used into Player Poker to try to approximate an equilibrium.

01:02:42,018 --> 01:02:45,393
SPEAKER_0:  those still end up working in practice in six player poker as well.

01:02:45,393 --> 01:02:46,174
SPEAKER_1:  There's some.

01:02:47,234 --> 01:02:53,054
SPEAKER_1:  Deep way in which six player poker is just a bunch of heads up poker like games in one.

01:02:53,698 --> 01:02:56,830
SPEAKER_1:  It's like embedded in it. So the competitiveness.

01:02:57,570 --> 01:02:57,950
SPEAKER_1:  Um.

01:02:58,306 --> 01:03:01,214
SPEAKER_1:  is more fundamental to poker than the cooperation.

01:03:01,634 --> 01:03:09,150
SPEAKER_0:  Right, yeah, poker is just such an adversarial game, there's no real cooperation. In fact, you're not even allowed to cooperate in poker. It's considered collusion, it's against the rules.

01:03:09,730 --> 01:03:10,302
SPEAKER_0:  Um...

01:03:11,330 --> 01:03:15,838
SPEAKER_0:  And so for that reason, the techniques end up working really well. And I think that's true more broadly.

01:03:16,130 --> 01:03:18,046
SPEAKER_0:  and extremely adverse serial games in general.

01:03:18,338 --> 01:03:21,854
SPEAKER_1:  but that's sort of in practice versus being able to prove something.

01:03:22,402 --> 01:03:28,478
SPEAKER_0:  That's right, nobody has a proof that that's the case. And it could be that six player poker belongs to some class of games.

01:03:28,802 --> 01:03:29,214
SPEAKER_0:  where

01:03:29,858 --> 01:03:32,830
SPEAKER_0:  Approximating an ash equilibrium through self play

01:03:33,442 --> 01:03:34,366
SPEAKER_0:  Proveably works well.

01:03:34,850 --> 01:03:35,262
SPEAKER_0:  Um.

01:03:35,490 --> 01:03:40,542
SPEAKER_0:  And you know, there are other classes of games beyond just two players, where this is proven to work well.

01:03:40,994 --> 01:03:43,454
SPEAKER_0:  So there are these kinds of games called potential games, which...

01:03:43,714 --> 01:03:46,174
SPEAKER_0:  I won't go into, it's kind of like a complicated concept, but...

01:03:46,530 --> 01:03:47,038
SPEAKER_0:  Um

01:03:47,394 --> 01:03:48,094
SPEAKER_0:  There are...

01:03:48,706 --> 01:03:49,950
SPEAKER_0:  classes of games where...

01:03:51,010 --> 01:03:54,334
SPEAKER_0:  this approach to approximating an Nash equilibrium is proven to work well.

01:03:54,850 --> 01:04:00,126
SPEAKER_0:  Now, 6-player poker is not known to belong to one of those classes, but it is possible that there is some classic games where it's not.

01:04:00,386 --> 01:04:03,486
SPEAKER_0:  It either provably performs well or provably performs not that badly.

01:04:04,258 --> 01:04:04,798
SPEAKER_1:  So.

01:04:05,634 --> 01:04:08,862
SPEAKER_1:  What are some interesting things about player bus that was.

01:04:09,090 --> 01:04:11,358
SPEAKER_1:  able to achieve human level performance on this.

01:04:11,810 --> 01:04:13,342
SPEAKER_1:  or superhuman level performance.

01:04:13,698 --> 01:04:15,070
SPEAKER_1:  on the 6 player version of poker.

01:04:16,226 --> 01:04:21,278
SPEAKER_0:  Personally, I think the most interesting thing about Pluribus is that it was so much cheaper.

01:04:21,602 --> 01:04:23,742
SPEAKER_0:  than Labrador. I mean, Labrador.

01:04:24,354 --> 01:04:30,334
SPEAKER_0:  If you had to put a price tag on the computational resources that went into it, I would say the final training run took about $100,000.

01:04:31,170 --> 01:04:32,510
SPEAKER_0:  You go to Pluribus.

01:04:32,994 --> 01:04:33,982
SPEAKER_0:  The final training run.

01:04:34,434 --> 01:04:37,438
SPEAKER_0:  would cost like less than $150 on AWS.

01:04:37,954 --> 01:04:41,310
SPEAKER_1:  Is this normalized to computational inflation?

01:04:41,634 --> 01:04:44,574
SPEAKER_1:  So meaning, is this just, does this.

01:04:45,154 --> 01:04:46,366
SPEAKER_1:  just have to do with the fact that.

01:04:46,594 --> 01:04:48,382
SPEAKER_1:  Pluribus was trained like a year later.

01:04:49,122 --> 01:04:54,846
SPEAKER_0:  No, no, no. It's not. It's, I mean, first of all, like, yeah, computing resources are getting cheaper every day and like

01:04:55,682 --> 01:04:59,742
SPEAKER_0:  You're not going to see a thousand-fold decrease in the computational resources over two years.

01:05:00,130 --> 01:05:04,638
SPEAKER_0:  or even anywhere close to that. The real improvement was algorithmic improvements.

01:05:04,866 --> 01:05:05,726
SPEAKER_0:  And in particular...

01:05:06,114 --> 01:05:07,614
SPEAKER_0:  the ability to do depth-limited search.

01:05:08,482 --> 01:05:11,934
SPEAKER_1:  So does depth limited search also work for Lebrados?

01:05:12,930 --> 01:05:13,694
SPEAKER_0:  Yes, so.

01:05:13,986 --> 01:05:17,694
SPEAKER_0:  Where this depth limited search came from is, you know, I developed this technique and

01:05:18,210 --> 01:05:20,734
SPEAKER_0:  um, ran it on to player poker first.

01:05:21,090 --> 01:05:21,822
SPEAKER_0:  And that's it.

01:05:22,274 --> 01:05:25,854
SPEAKER_0:  Reduce the computational resources needed to make an AI that was superhuman.

01:05:26,306 --> 01:05:28,542
SPEAKER_0:  from, you know, $100,000 for Labratus.

01:05:28,866 --> 01:05:29,374
SPEAKER_0:  to.

01:05:29,698 --> 01:05:31,134
SPEAKER_0:  something you could train on your laptop.

01:05:31,618 --> 01:05:32,606
SPEAKER_1:  What do you learn from that?

01:05:34,114 --> 01:05:34,590
SPEAKER_1:  Um...

01:05:34,850 --> 01:05:35,774
SPEAKER_1:  from that discovery.

01:05:36,994 --> 01:05:39,966
SPEAKER_0:  What I would take away from that is that algorithmic improvements really do matter.

01:05:40,194 --> 01:05:42,462
SPEAKER_1:  How would you describe the more general?

01:05:42,978 --> 01:05:44,638
SPEAKER_1:  case of limited dev search.

01:05:45,282 --> 01:05:49,822
SPEAKER_1:  So it's basically constraining the scale, temporal, or in some other way.

01:05:50,146 --> 01:05:52,574
SPEAKER_1:  the computation you're doing in some clever way.

01:05:53,378 --> 01:05:54,014
SPEAKER_1:  like with

01:05:54,594 --> 01:05:58,142
SPEAKER_1:  Like how else can you significantly constrain computation?

01:05:59,650 --> 01:06:00,990
SPEAKER_0:  Well, I think the idea is that...

01:06:01,410 --> 01:06:08,254
SPEAKER_0:  We want to be able to leverage search as much as possible. And the way that we were doing it in Labradus required us to search all the way to the end of the game.

01:06:08,642 --> 01:06:12,190
SPEAKER_0:  Now if you're playing a game like chess, the idea that you're gonna search always to the end of the game

01:06:12,674 --> 01:06:17,278
SPEAKER_0:  is kind of unimaginable, right? Like there's just so many situations where you just won't be able to use search in that

01:06:17,506 --> 01:06:20,222
SPEAKER_0:  or the cost would be prohibitive.

01:06:20,994 --> 01:06:21,598
SPEAKER_0:  and

01:06:22,338 --> 01:06:25,502
SPEAKER_0:  This technique allowed us to leverage search.

01:06:25,858 --> 01:06:29,150
SPEAKER_0:  and without having to pay such a huge computational cost for it.

01:06:29,538 --> 01:06:30,430
SPEAKER_0:  and be able to apply it.

01:06:30,690 --> 01:06:31,550
SPEAKER_0:  more broadly.

01:06:31,810 --> 01:06:36,414
SPEAKER_1:  So to what degree did you use neural nets for Lebrados and Pleuribus?

01:06:36,674 --> 01:06:39,710
SPEAKER_1:  and more generally what role do you need that's have to play

01:06:40,354 --> 01:06:41,182
SPEAKER_1:  in, um...

01:06:41,794 --> 01:06:43,966
SPEAKER_1:  and superhuman level performance in poker.

01:06:44,706 --> 01:06:47,582
SPEAKER_0:  So we actually did not use neural nets at all for Lebrotis.

01:06:47,810 --> 01:06:48,574
SPEAKER_0:  or Pluribus.

01:06:49,218 --> 01:06:49,790
SPEAKER_0:  and

01:06:50,210 --> 01:06:51,806
SPEAKER_0:  A lot of people found this surprising.

01:06:52,034 --> 01:06:54,270
SPEAKER_0:  Back in 2017, I think they found it surprising today.

01:06:54,722 --> 01:06:55,230
SPEAKER_0:  Um...

01:06:55,458 --> 01:06:56,734
SPEAKER_0:  that we were able to do this.

01:06:57,186 --> 01:06:58,878
SPEAKER_0:  without using any neural nets.

01:06:59,746 --> 01:07:02,750
SPEAKER_0:  And I think the reason for that, I mean, I think neural nets are...

01:07:03,170 --> 01:07:03,518
SPEAKER_0:  Um.

01:07:03,746 --> 01:07:04,702
SPEAKER_0:  incredibly powerful.

01:07:04,930 --> 01:07:05,278
SPEAKER_0:  and

01:07:05,538 --> 01:07:07,870
SPEAKER_0:  the techniques that are used today, even for poker AIs.

01:07:08,258 --> 01:07:09,054
SPEAKER_0:  Do rely.

01:07:09,282 --> 01:07:10,814
SPEAKER_0:  quite heavily on neural nets.

01:07:11,266 --> 01:07:15,390
SPEAKER_0:  Um, but it wasn't the main challenge for poker. Like I think what neural nets.

01:07:15,842 --> 01:07:16,862
SPEAKER_0:  are really good for?

01:07:17,314 --> 01:07:18,590
SPEAKER_0:  If you're in a situation where

01:07:18,882 --> 01:07:19,934
SPEAKER_0:  Finding features.

01:07:20,482 --> 01:07:21,566
SPEAKER_0:  for a value function.

01:07:22,050 --> 01:07:25,758
SPEAKER_0:  is really difficult, then neural nets are really powerful. And this was the problem in Go.

01:07:26,178 --> 01:07:27,422
SPEAKER_0:  Right, like the problem in Go...

01:07:28,002 --> 01:07:28,542
SPEAKER_0:  was that.

01:07:28,770 --> 01:07:30,366
SPEAKER_0:  Or the final problem in Go at least.

01:07:30,626 --> 01:07:32,478
SPEAKER_0:  was that nobody had a good way of

01:07:32,706 --> 01:07:33,598
SPEAKER_0:  Looking at a board?

01:07:33,890 --> 01:07:36,734
SPEAKER_0:  and figuring out who was winning or losing and describing.

01:07:36,994 --> 01:07:38,302
SPEAKER_0:  through a simple algorithm.

01:07:38,562 --> 01:07:39,454
SPEAKER_0:  who was winning or losing.

01:07:40,322 --> 01:07:41,662
SPEAKER_0:  And so they are neural nets.

01:07:41,890 --> 01:07:46,622
SPEAKER_0:  were super helpful because you could just feed in a ton of different board positions into this neural net.

01:07:46,946 --> 01:07:49,278
SPEAKER_0:  and it would be able to predict then who was winning or losing.

01:07:49,858 --> 01:07:50,686
SPEAKER_0:  But in poker...

01:07:51,234 --> 01:07:54,366
SPEAKER_0:  The features weren't the challenge. The challenge was...

01:07:54,722 --> 01:07:55,518
SPEAKER_0:  How do you?

01:07:55,842 --> 01:07:57,214
SPEAKER_0:  design a scalable algorithm?

01:07:57,698 --> 01:08:00,894
SPEAKER_0:  that would allow you to find this balance strategy.

01:08:01,346 --> 01:08:04,766
SPEAKER_0:  that would understand that you have to bluff with the right probability.

01:08:06,114 --> 01:08:09,886
SPEAKER_1:  Can that be somehow incorporated into the value function?

01:08:10,498 --> 01:08:10,974
SPEAKER_1:  this.

01:08:11,874 --> 01:08:14,334
SPEAKER_1:  the complexity of poker that you've described.

01:08:14,882 --> 01:08:21,374
SPEAKER_0:  Yeah, so the way the value functions work in poker, like the latest and greatest poker AIs, they do use neural nets for the value function.

01:08:22,050 --> 01:08:26,942
SPEAKER_0:  The way it's done is very different from how it's done in a game like Chess or Go because

01:08:27,522 --> 01:08:30,622
SPEAKER_0:  In poker, you have to reason about beliefs.

01:08:31,170 --> 01:08:32,638
SPEAKER_0:  And so the value.

01:08:33,090 --> 01:08:33,822
SPEAKER_0:  of a state.

01:08:34,370 --> 01:08:36,702
SPEAKER_0:  depends on the beliefs that players have.

01:08:37,026 --> 01:08:37,438
SPEAKER_0:  about.

01:08:37,794 --> 01:08:38,814
SPEAKER_0:  what the different cards are.

01:08:39,330 --> 01:08:41,118
SPEAKER_0:  Like if you have pocket aces...

01:08:41,730 --> 01:08:42,206
SPEAKER_0:  Then...

01:08:42,722 --> 01:08:45,566
SPEAKER_0:  Whether that's a really, really good hand or just an okay hand.

01:08:45,890 --> 01:08:48,222
SPEAKER_0:  depends on whether you know I have pocket aces.

01:08:48,482 --> 01:08:50,270
SPEAKER_0:  where like if you know that I have pocket aces.

01:08:50,658 --> 01:08:51,102
SPEAKER_0:  then.

01:08:51,458 --> 01:08:53,086
SPEAKER_0:  if I bet you're gonna fold immediately.

01:08:53,474 --> 01:08:55,422
SPEAKER_0:  But if you think that I have a really bad hand...

01:08:55,714 --> 01:08:57,918
SPEAKER_0:  then I could bet with pocket aces and make a ton of money.

01:08:58,370 --> 01:08:58,846
SPEAKER_0:  So.

01:08:59,938 --> 01:09:02,046
SPEAKER_0:  the value function in poker these days.

01:09:02,818 --> 01:09:04,414
SPEAKER_0:  takes the beliefs as an input.

01:09:05,122 --> 01:09:08,030
SPEAKER_0:  which is very different from how chess and Go AIs work.

01:09:09,218 --> 01:09:11,326
SPEAKER_1:  So as a person who appreciates the game...

01:09:12,066 --> 01:09:12,446
SPEAKER_1:  Uh...

01:09:13,922 --> 01:09:15,806
SPEAKER_1:  Who do you think is the greatest poker player of all time?

01:09:16,962 --> 01:09:18,398
SPEAKER_1:  That's a tough question.

01:09:18,626 --> 01:09:24,382
SPEAKER_1:  Can AI help answer that question? Can you actually analyze the quality of play?

01:09:24,962 --> 01:09:26,590
SPEAKER_1:  so the air chest engines can.

01:09:27,330 --> 01:09:27,870
SPEAKER_1:  in.

01:09:28,258 --> 01:09:29,950
SPEAKER_1:  Give estimates of the quality of play.

01:09:31,522 --> 01:09:31,902
SPEAKER_1:  Uh...

01:09:33,282 --> 01:09:36,894
SPEAKER_1:  I wonder if there's a, is there an Elo rating type of system for poker?

01:09:37,730 --> 01:09:39,966
SPEAKER_1:  I suppose you could, but there's just not enough.

01:09:41,218 --> 01:09:43,134
SPEAKER_1:  You would have to play a lot of games, right?

01:09:43,714 --> 01:09:48,798
SPEAKER_1:  a very large number of games, like more than you would in chess. The deterministic game makes it easier to estimate ELO.

01:09:49,762 --> 01:09:52,478
SPEAKER_0:  I think. Is much harder to estimate.

01:09:52,706 --> 01:09:54,846
SPEAKER_0:  Something like Elo rating in poker. I think it's doable.

01:09:55,362 --> 01:09:57,438
SPEAKER_0:  The problem is that the game is very high variance.

01:09:57,698 --> 01:09:58,206
SPEAKER_0:  So you could.

01:09:58,434 --> 01:09:58,910
SPEAKER_0:  play.

01:09:59,170 --> 01:10:00,670
SPEAKER_0:  You could be profitable in poker.

01:10:00,994 --> 01:10:01,662
SPEAKER_0:  for a year.

01:10:02,114 --> 01:10:04,926
SPEAKER_0:  and you can actually be a bad player, just because the variance is so high.

01:10:05,314 --> 01:10:05,822
SPEAKER_0:  when you've got.

01:10:06,274 --> 01:10:08,286
SPEAKER_0:  professional poker players that would lose for a year.

01:10:09,058 --> 01:10:11,038
SPEAKER_0:  just because they're on a really bad.

01:10:11,298 --> 01:10:11,838
SPEAKER_0:  Bad streak.

01:10:12,354 --> 01:10:15,998
SPEAKER_1:  Yeah, so for ELO you have to have a nice clean way of saying

01:10:16,834 --> 01:10:18,462
SPEAKER_1:  Player A played Player B.

01:10:19,266 --> 01:10:22,334
SPEAKER_1:  and A beats B that says something, that's a signal.

01:10:22,754 --> 01:10:24,629
SPEAKER_1:  In poker, that's a very noisy signal.

01:10:24,629 --> 01:10:28,990
SPEAKER_0:  It's a very noisy signal. There is a signal there, and so you could do this calculation.

01:10:29,378 --> 01:10:30,462
SPEAKER_0:  it would just be much harder.

01:10:31,010 --> 01:10:35,774
SPEAKER_0:  Um, but the same way that AIs have now taken over chess and

01:10:36,098 --> 01:10:40,190
SPEAKER_0:  You know, all the top professional chess players train with, with AIs.

01:10:40,514 --> 01:10:41,534
SPEAKER_0:  The same is true for poker.

01:10:41,922 --> 01:10:43,326
SPEAKER_0:  The game has become

01:10:43,650 --> 01:10:45,950
SPEAKER_0:  a very computational

01:10:46,274 --> 01:10:49,406
SPEAKER_0:  people train with AIs to try to find out where they're making mistakes.

01:10:49,730 --> 01:10:52,126
SPEAKER_0:  try to learn from the AIs to improve their strategy.

01:10:52,514 --> 01:10:53,598
SPEAKER_0:  Uh, so.

01:10:54,530 --> 01:10:55,038
SPEAKER_0:  Now

01:10:55,490 --> 01:11:00,766
SPEAKER_0:  Yeah, so the game has been revolutionized in the past five years by the development of AI in this sport.

01:11:00,898 --> 01:11:05,023
SPEAKER_1:  The skill with which you avoided the question of the greatest of all time was impressive.

01:11:05,023 --> 01:11:08,734
SPEAKER_0:  So my feeling is that it's a difficult it's a difficult question because

01:11:09,058 --> 01:11:10,238
SPEAKER_0:  just like in chess.

01:11:10,754 --> 01:11:14,334
SPEAKER_0:  where you can't really compare Magnus Carlsen today to Garry Kasparov.

01:11:14,722 --> 01:11:17,118
SPEAKER_0:  because the game has evolved so much.

01:11:17,666 --> 01:11:18,046
SPEAKER_0:  Um.

01:11:18,370 --> 01:11:19,870
SPEAKER_0:  the poker players today.

01:11:20,642 --> 01:11:23,870
SPEAKER_0:  so far beyond the skills of like...

01:11:24,098 --> 01:11:25,310
SPEAKER_0:  people that were playing even.

01:11:25,730 --> 01:11:26,686
SPEAKER_0:  10 or 20 years ago.

01:11:27,202 --> 01:11:32,382
SPEAKER_0:  So you look at the kinds of all-stars that were on ESPN at the height of the poker boom.

01:11:33,346 --> 01:11:36,126
SPEAKER_0:  Pretty much all those players are actually not that good at the game.

01:11:36,354 --> 01:11:38,046
SPEAKER_0:  today. At least the

01:11:38,434 --> 01:11:45,406
SPEAKER_0:  strategy aspect. I mean, they might still be good at like reading the player at the other side of the table and trying to figure out like are they bluffing or not.

01:11:45,730 --> 01:11:47,070
SPEAKER_0:  but in terms of the actual like.

01:11:47,458 --> 01:11:48,798
SPEAKER_0:  computational strategy of the game.

01:11:49,122 --> 01:11:52,254
SPEAKER_0:  A lot of them have really struggled to keep up with that development.

01:11:52,834 --> 01:11:53,214
SPEAKER_0:  No.

01:11:54,018 --> 01:11:57,534
SPEAKER_0:  So for that reason, I'll give an answer and I'm gonna say Daniel Lagranio.

01:11:58,274 --> 01:12:00,798
SPEAKER_0:  who you actually had on the podcast recently. So it was a great episode.

01:12:01,442 --> 01:12:01,822
SPEAKER_0:  Yeah.

01:12:01,954 --> 01:12:04,579
SPEAKER_1:  and Phil's gonna hate it.

01:12:04,579 --> 01:12:08,190
SPEAKER_0:  this so much. And I'm going to give him credit.

01:12:08,610 --> 01:12:09,310
SPEAKER_0:  because...

01:12:09,570 --> 01:12:13,918
SPEAKER_0:  He is one of the few like old school really strong players that have kept up.

01:12:14,242 --> 01:12:15,367
SPEAKER_0:  with the development of AI.

01:12:15,367 --> 01:12:16,318
SPEAKER_1:  he is trying to.

01:12:16,706 --> 01:12:18,974
SPEAKER_1:  he's constantly studying the game theory optimal.

01:12:19,106 --> 01:12:20,318
SPEAKER_0:  way of playing. Exactly.

01:12:20,930 --> 01:12:26,558
SPEAKER_0:  And I think a lot of the old school poker players have just kind of given up on that aspect, and I gotta give them a grand new credit.

01:12:26,978 --> 01:12:28,094
SPEAKER_0:  for keeping up.

01:12:28,546 --> 01:12:30,718
SPEAKER_0:  with all the developments that are happening in the sport.

01:12:31,394 --> 01:12:34,238
SPEAKER_1:  Yeah, it's fascinating to watch. It's fascinating to watch where it's headed.

01:12:34,818 --> 01:12:35,262
SPEAKER_1:  Um.

01:12:35,938 --> 01:12:38,238
SPEAKER_1:  Yeah, so there you go. Some love for Daniel.

01:12:39,298 --> 01:12:41,246
SPEAKER_1:  Quick pause, bath and break? Yep, let's do it.

01:12:42,210 --> 01:12:44,542
SPEAKER_1:  Let's go from poker to diplomacy.

01:12:45,474 --> 01:12:48,222
SPEAKER_1:  what is at a high level the game of diplomacy.

01:12:49,250 --> 01:12:56,606
SPEAKER_0:  Yeah, so I talked a lot about two players, zero-sum games. And what's interesting about diplomacy is that it's very different from...

01:12:56,994 --> 01:12:58,462
SPEAKER_0:  These like adversarial.

01:12:58,786 --> 01:13:02,398
SPEAKER_0:  uh... games like chess go poker even starcraft and do it up

01:13:02,850 --> 01:13:05,502
SPEAKER_0:  Diplomacy has a much bigger cooperative element to it.

01:13:05,922 --> 01:13:07,326
SPEAKER_0:  It's a seven player game.

01:13:07,586 --> 01:13:09,758
SPEAKER_0:  It was actually created in the 50s.

01:13:10,242 --> 01:13:11,710
SPEAKER_0:  and it takes place.

01:13:12,290 --> 01:13:15,966
SPEAKER_0:  uh... before world war one it's like a map of europe with seven great powers

01:13:16,386 --> 01:13:18,654
SPEAKER_0:  Um, and they're all trying to.

01:13:19,106 --> 01:13:22,270
SPEAKER_0:  form alliances with each other. There's a lot of negotiation going on.

01:13:22,882 --> 01:13:26,174
SPEAKER_0:  And so the whole focus of the game is on

01:13:27,490 --> 01:13:30,206
SPEAKER_0:  Forming alliances with the other players to take on the other players.

01:13:30,338 --> 01:13:32,446
SPEAKER_1:  England, Germany, Russia, Turkey.

01:13:32,802 --> 01:13:35,454
SPEAKER_1:  Austria, Hungary, Italy and France.

01:13:35,586 --> 01:13:36,094
SPEAKER_0:  That's right.

01:13:36,962 --> 01:13:38,238
SPEAKER_0:  So the way the game works...

01:13:38,722 --> 01:13:39,294
SPEAKER_0:  is

01:13:40,130 --> 01:13:43,518
SPEAKER_0:  On each turn, you spend about, you know, five to 15 minutes.

01:13:43,874 --> 01:13:45,822
SPEAKER_0:  talking to the other players in privates.

01:13:46,242 --> 01:13:46,878
SPEAKER_0:  and

01:13:47,138 --> 01:13:50,334
SPEAKER_0:  You make all sorts of deals with them. You say, like, hey, let's work together.

01:13:50,594 --> 01:13:51,102
SPEAKER_0:  Um...

01:13:51,394 --> 01:13:57,438
SPEAKER_0:  you know, let's team up against this other player, because the only way that you can make progress is by working with somebody else against the others.

01:13:57,986 --> 01:13:58,462
SPEAKER_0:  Um

01:13:58,786 --> 01:14:01,022
SPEAKER_0:  And then after that negotiation period is done.

01:14:01,314 --> 01:14:04,190
SPEAKER_0:  All the players simultaneously submit their

01:14:04,514 --> 01:14:05,022
SPEAKER_0:  Moves?

01:14:05,474 --> 01:14:07,134
SPEAKER_0:  and they're all executed at the same time.

01:14:07,842 --> 01:14:10,942
SPEAKER_0:  And so you can tell people like, Hey, I'm going to support you this turn.

01:14:11,298 --> 01:14:12,958
SPEAKER_0:  But then you don't follow through with it.

01:14:13,314 --> 01:14:16,862
SPEAKER_0:  And they're only going to figure that out once they see the moves being read off.

01:14:17,026 --> 01:14:19,582
SPEAKER_1:  How much of it is natural language, like written?

01:14:20,194 --> 01:14:22,494
SPEAKER_1:  actual text how much is like a

01:14:22,818 --> 01:14:25,534
SPEAKER_1:  you're actually saying phrases that are structured.

01:14:25,986 --> 01:14:27,646
SPEAKER_0:  So there's different ways to play the game.

01:14:27,970 --> 01:14:31,006
SPEAKER_0:  You can play it in person, and in that case, it's all natural language.

01:14:31,394 --> 01:14:36,382
SPEAKER_0:  Free forum communication. There's no constraints on the kinds of deals that you can make the kinds of things that you can discuss

01:14:36,866 --> 01:14:37,342
SPEAKER_0:  Um.

01:14:37,698 --> 01:14:39,390
SPEAKER_0:  You can also play it online so you can.

01:14:39,618 --> 01:14:41,534
SPEAKER_0:  you know, send long emails back and forth.

01:14:41,922 --> 01:14:43,998
SPEAKER_0:  Um, you can play it like.

01:14:44,354 --> 01:14:45,982
SPEAKER_0:  live online or over voice chat.

01:14:46,306 --> 01:14:49,374
SPEAKER_0:  But the focus, the important thing to understand is that

01:14:49,634 --> 01:14:52,190
SPEAKER_0:  This is unstructured communication. You can say whatever you want.

01:14:52,642 --> 01:14:54,398
SPEAKER_0:  You can make any sorts of deals that you want.

01:14:54,690 --> 01:14:56,030
SPEAKER_0:  and everything is done.

01:14:56,258 --> 01:15:01,214
SPEAKER_0:  privately so it's not like you're all around the board together having a conversation.

01:15:01,634 --> 01:15:06,590
SPEAKER_0:  You're grabbing somebody going off into a corner and conspiring behind everybody else's back about what you're planning.

01:15:07,010 --> 01:15:07,710
SPEAKER_1:  And uh...

01:15:07,970 --> 01:15:12,382
SPEAKER_1:  There's no limit in theory to the conversation you can have directly with one person.

01:15:12,578 --> 01:15:22,590
SPEAKER_0:  That's right. You can make all sorts of... You can talk about anything. You can say like, hey, let's have a long-term alliance against this guy. You can say like, hey, can you support me this turn in return? I'll do this other thing for you next turn.

01:15:23,042 --> 01:15:24,990
SPEAKER_0:  or you know, yeah, just.

01:15:25,218 --> 01:15:29,630
SPEAKER_0:  You can talk about like what you talked about with somebody else and gossip about like what they're planning.

01:15:30,338 --> 01:15:34,526
SPEAKER_0:  The way that I would describe the game is that it's kind of like a mix between risk

01:15:34,914 --> 01:15:36,926
SPEAKER_0:  poker and the tv show survivor

01:15:37,634 --> 01:15:40,286
SPEAKER_0:  There's like this big element of like trying to.

01:15:40,706 --> 01:15:41,246
SPEAKER_0:  Um...

01:15:41,570 --> 01:15:43,230
SPEAKER_0:  Yeah, there's a big social element.

01:15:43,682 --> 01:15:48,638
SPEAKER_0:  And the best way that I would describe the game is that it's really a game about people rather than the pieces.

01:15:49,890 --> 01:15:50,526
SPEAKER_1:  So...

01:15:50,946 --> 01:15:53,150
SPEAKER_1:  risk because it is a map it's kind of

01:15:53,410 --> 01:15:54,718
SPEAKER_1:  Wargame-like.

01:15:56,162 --> 01:16:00,382
SPEAKER_1:  uh... poker because there's a game theory component that's very kind of strategic

01:16:00,802 --> 01:16:03,614
SPEAKER_1:  So you could convert it into an artificial intelligence problem.

01:16:04,194 --> 01:16:07,486
SPEAKER_1:  and then survivor because of the social component. Strong social component.

01:16:07,906 --> 01:16:09,598
SPEAKER_1:  I saw that somebody said online that...

01:16:09,890 --> 01:16:11,774
SPEAKER_1:  the internet version of the game.

01:16:12,322 --> 01:16:13,726
SPEAKER_1:  has this quality of...

01:16:14,146 --> 01:16:16,734
SPEAKER_1:  that it's easier to almost to do like role playing.

01:16:17,570 --> 01:16:19,134
SPEAKER_1:  as opposed to being yourself.

01:16:19,554 --> 01:16:25,246
SPEAKER_1:  You can actually like be the, like really imagine yourself as the leader of France or Russia and so on.

01:16:25,538 --> 01:16:26,334
SPEAKER_1:  Like really.

01:16:26,882 --> 01:16:31,198
SPEAKER_1:  pretend to be that person. It's actually fun to really lean into being that.

01:16:31,746 --> 01:16:32,414
SPEAKER_1:  that leader.

01:16:32,834 --> 01:16:35,390
SPEAKER_0:  Yeah, so some players do go this route where they just like...

01:16:35,810 --> 01:16:39,326
SPEAKER_0:  of view it as a strategy game but also a role-playing game where they can like act out like

01:16:39,650 --> 01:16:43,775
SPEAKER_0:  What would I be like if I was, you know, a leader of France in 1900?

01:16:43,775 --> 01:16:46,206
SPEAKER_1:  forfeit right away. No, I'm just kidding. I'm just kidding.

01:16:46,658 --> 01:16:47,166
SPEAKER_1:  Heh!

01:16:47,458 --> 01:16:49,758
SPEAKER_1:  And they sometimes use like the old timey language.

01:16:50,338 --> 01:16:51,326
SPEAKER_1:  to like...

01:16:51,842 --> 01:16:59,006
SPEAKER_1:  or how they imagine the elites would talk at that time. Anyway, so what are the different turns of the game? Like what are the rounds?

01:16:59,682 --> 01:17:01,918
SPEAKER_0:  Yeah, so on every turn you got like...

01:17:02,562 --> 01:17:05,982
SPEAKER_0:  a bunch of different units that you start out with. So you start out controlling like.

01:17:06,466 --> 01:17:12,510
SPEAKER_0:  just a few units and the object of the game is to gain control of the majority of the map. If you're able to do that, then you've won the game.

01:17:13,154 --> 01:17:16,510
SPEAKER_0:  But like I said, the only way that you're able to do that is by working with other players.

01:17:16,930 --> 01:17:21,406
SPEAKER_0:  So on every turn, you can issue a move order. So for each of your units, you can move.

01:17:21,634 --> 01:17:23,262
SPEAKER_0:  them to an adjacent territory.

01:17:24,002 --> 01:17:25,982
SPEAKER_0:  or you can keep them where they are.

01:17:26,370 --> 01:17:28,062
SPEAKER_0:  or you can support a move.

01:17:28,578 --> 01:17:29,278
SPEAKER_0:  or a hold.

01:17:29,666 --> 01:17:30,526
SPEAKER_0:  of different units.

01:17:30,946 --> 01:17:31,454
SPEAKER_0:  So.

01:17:31,586 --> 01:17:34,142
SPEAKER_1:  What are the territories? How is the map divided up?

01:17:34,306 --> 01:17:38,846
SPEAKER_0:  It's kind of like risk where the map is divided up into like 50 different territories.

01:17:39,266 --> 01:17:39,710
SPEAKER_0:  Um.

01:17:40,258 --> 01:17:40,670
SPEAKER_0:  Now-

01:17:40,930 --> 01:17:48,318
SPEAKER_0:  You can enter a territory if you're moving into that territory with more supports than the person that's in there or the person that's trying to move in there.

01:17:48,962 --> 01:17:51,070
SPEAKER_0:  So if you're moving in and there's somebody already there.

01:17:51,394 --> 01:17:52,030
SPEAKER_0:  then

01:17:52,290 --> 01:17:56,414
SPEAKER_0:  If neither of you have support, it's a one versus one and you'll bounce back and neither of you will make progress.

01:17:56,898 --> 01:17:59,806
SPEAKER_0:  if you have a unit that's supporting that move into the territory.

01:18:00,130 --> 01:18:03,454
SPEAKER_0:  then it's a two versus one and you'll kick them out and they'll have to retreat somewhere.

01:18:04,034 --> 01:18:08,510
SPEAKER_0:  What does support mean? Support is like, it's an action that you can issue in the game. So you can say...

01:18:08,738 --> 01:18:12,510
SPEAKER_0:  this unit, you get you right down, this unit is supporting this other unit into this territory.

01:18:13,154 --> 01:18:15,902
SPEAKER_1:  Are these units from opposing forces?

01:18:16,098 --> 01:18:22,078
SPEAKER_0:  It could be, they could be. And this is where the interesting aspect of the game comes in because you can support your own units into territory.

01:18:22,370 --> 01:18:26,942
SPEAKER_0:  but you can also support other people's units into territories. And so that's what the negotiations.

01:18:27,202 --> 01:18:28,350
SPEAKER_0:  really revolve around.

01:18:28,802 --> 01:18:31,614
SPEAKER_1:  but you don't have to do the thing you say you're going to do.

01:18:32,162 --> 01:18:35,390
SPEAKER_1:  And this, yeah. So you can say I'm going to support you, but then.

01:18:35,746 --> 01:18:36,734
SPEAKER_1:  backstab the person.

01:18:37,474 --> 01:18:40,606
SPEAKER_1:  That's absolutely right. And that tension is, is core to the game.

01:18:41,026 --> 01:18:42,814
SPEAKER_0:  That tension is absolutely core to the game.

01:18:43,458 --> 01:18:45,566
SPEAKER_0:  The fact that you can make all sorts of...

01:18:45,826 --> 01:18:51,550
SPEAKER_0:  promises but you have to reason about the fact that like they might not trust you if you say you're gonna do something

01:18:51,906 --> 01:18:53,278
SPEAKER_0:  or they might be lying to you.

01:18:53,698 --> 01:18:55,550
SPEAKER_0:  when they say they're going to support you.

01:18:56,354 --> 01:18:57,758
SPEAKER_1:  So maybe just.

01:18:58,562 --> 01:19:00,958
SPEAKER_1:  to jump back, what's the history of the game in general?

01:19:01,378 --> 01:19:03,230
SPEAKER_1:  Is it true that Henry Kissinger loved the game?

01:19:03,842 --> 01:19:11,358
SPEAKER_1:  and JFK and all those, I've heard like a bunch of different people that, or is that just one of those things that the cool kids say they do, but they don't actually play.

01:19:11,554 --> 01:19:13,982
SPEAKER_0:  So the game was created in the 50s.

01:19:14,626 --> 01:19:20,734
SPEAKER_0:  And from what I understand, it was JFK's, it was played in like the JFK White House, Henry Kissinger's favorite game.

01:19:21,090 --> 01:19:24,465
SPEAKER_0:  I don't know if it's true, but that's definitely what I've heard. Since their

01:19:24,465 --> 01:19:26,110
SPEAKER_1:  they went with World War I.

01:19:27,106 --> 01:19:29,118
SPEAKER_1:  when it was created after World War II.

01:19:30,274 --> 01:19:32,222
SPEAKER_0:  The story that I've heard for the creation of the game.

01:19:32,610 --> 01:19:34,302
SPEAKER_0:  is it was created by

01:19:34,722 --> 01:19:37,854
SPEAKER_0:  um... somebody that had looked at the history of

01:19:38,178 --> 01:19:40,126
SPEAKER_0:  of the twentieth century and they saw

01:19:40,578 --> 01:19:42,878
SPEAKER_0:  World War I as a failure of diplomacy.

01:19:43,330 --> 01:19:43,678
SPEAKER_0:  So.

01:19:44,450 --> 01:19:48,030
SPEAKER_0:  You know, they saw the fact that this war broke out as like...

01:19:48,258 --> 01:19:54,878
SPEAKER_0:  the diplomats of all these countries really failed to prevent a war, and he wanted to create a game that would basically teach people about diplomacy.

01:19:55,682 --> 01:19:56,190
SPEAKER_0:  Um.

01:19:56,866 --> 01:19:57,438
SPEAKER_0:  and

01:19:58,018 --> 01:20:03,070
SPEAKER_0:  It's really fascinating that in his ideal version of the game of diplomacy, nobody actually wins the game.

01:20:03,682 --> 01:20:07,966
SPEAKER_0:  because the whole point is that if somebody is about to win, then the other players should be able to work together.

01:20:08,258 --> 01:20:09,726
SPEAKER_0:  to stop that person from winning.

01:20:10,274 --> 01:20:14,046
SPEAKER_0:  And so the ideal version of the game is just one where nobody actually wins.

01:20:14,402 --> 01:20:17,182
SPEAKER_0:  It kind of has a nice, wholesome take home message then that...

01:20:17,538 --> 01:20:19,582
SPEAKER_0:  You know, war is ultimately futile.

01:20:19,842 --> 01:20:20,446
SPEAKER_0:  and uh...

01:20:21,506 --> 01:20:23,582
SPEAKER_1:  and that optimal.

01:20:24,514 --> 01:20:27,806
SPEAKER_1:  that feudal optimal could be achieved through great diplomacy.

01:20:28,898 --> 01:20:35,806
SPEAKER_1:  So is there some asymmetry in terms of which is more powerful, Russia versus Germany versus

01:20:36,418 --> 01:20:37,502
SPEAKER_1:  of France and so on.

01:20:38,178 --> 01:20:48,990
SPEAKER_0:  So I think the general consensus is that France is the strongest power in the game. But the beautiful thing about diplomacy is that it's self-balancing, right? So it's the fact that France has an inherited advantage from the beginning.

01:20:49,346 --> 01:20:50,142
SPEAKER_0:  means that the other.

01:20:50,370 --> 01:20:50,910
SPEAKER_0:  players.

01:20:51,234 --> 01:20:52,574
SPEAKER_0:  are less likely to work with it.

01:20:52,994 --> 01:20:57,918
SPEAKER_1:  I saw that Russia has four units or four of something that the others have three of something.

01:20:58,018 --> 01:21:01,758
SPEAKER_0:  That's true, yeah, so Russia starts off with four units while all the other players start with three.

01:21:02,082 --> 01:21:05,598
SPEAKER_0:  But Russia is also in a much more vulnerable position because they have to like...

01:21:06,018 --> 01:21:07,893
SPEAKER_0:  They have a lot more neighbors as well

01:21:07,893 --> 01:21:12,734
SPEAKER_1:  Got it, larger territory, more, yeah, right, more border to defend.

01:21:13,090 --> 01:21:13,534
SPEAKER_1:  Okay.

01:21:13,762 --> 01:21:14,814
SPEAKER_1:  uh... what else is

01:21:15,554 --> 01:21:17,470
SPEAKER_1:  What else is important to know about the rules?

01:21:17,730 --> 01:21:21,694
SPEAKER_1:  So there's how many rounds are there like is this iterative game?

01:21:22,146 --> 01:21:25,214
SPEAKER_1:  Is a finite, you just keep going indefinitely?

01:21:25,442 --> 01:21:27,806
SPEAKER_0:  Usually the game lasts, uh, I would say about

01:21:28,706 --> 01:21:29,918
SPEAKER_0:  15 or 20 turns.

01:21:30,434 --> 01:21:33,214
SPEAKER_0:  There's in theory no limit, it could last longer.

01:21:33,570 --> 01:21:39,838
SPEAKER_0:  But at some point, I mean, if you're playing a house game with friends, at some point, you just get tired and you all agree like, okay, we're going to end the game here and call it a draw.

01:21:40,322 --> 01:21:40,862
SPEAKER_0:  Um...

01:21:41,378 --> 01:21:44,670
SPEAKER_0:  If you're playing online, there's usually set limits on when the game will actually end.

01:21:45,026 --> 01:21:47,390
SPEAKER_1:  And what's the end? What's the termination condition?

01:21:47,682 --> 01:21:47,998
SPEAKER_1:  Thank you.

01:21:48,354 --> 01:21:49,982
SPEAKER_1:  This one.

01:21:50,722 --> 01:21:52,222
SPEAKER_1:  country have to conquer everything else?

01:21:52,802 --> 01:21:59,582
SPEAKER_0:  So if somebody is able to actually gain control of a majority of the map, then they've won the game, and that is a solo victory as it's called.

01:21:59,970 --> 01:22:04,286
SPEAKER_0:  Now that pretty rarely happens, especially with strong players, because like I said, the game is designed.

01:22:04,706 --> 01:22:09,086
SPEAKER_0:  to incentivize the other players to put a stop to that and all work together to stop the superpower.

01:22:09,666 --> 01:22:10,174
SPEAKER_0:  Um...

01:22:10,658 --> 01:22:12,190
SPEAKER_0:  Usually what ends up happening is that

01:22:12,514 --> 01:22:14,366
SPEAKER_0:  you know, all the players agreed to a draw.

01:22:14,658 --> 01:22:19,646
SPEAKER_0:  and then the the score the the win is divided among the the remaining players.

01:22:20,098 --> 01:22:23,294
SPEAKER_0:  There's a lot of different scoring systems, the one that we used in our research.

01:22:23,586 --> 01:22:24,766
SPEAKER_0:  Basically,

01:22:25,090 --> 01:22:25,438
SPEAKER_0:  Um.

01:22:25,794 --> 01:22:31,614
SPEAKER_0:  gives a score relative to how much control you have of the map. So the more that you control, the higher your score.

01:22:32,642 --> 01:22:34,974
SPEAKER_1:  What's the history of using this game?

01:22:35,618 --> 01:22:37,310
SPEAKER_1:  as a benchmark for AI research.

01:22:37,826 --> 01:22:39,102
SPEAKER_1:  Do people use it?

01:22:39,746 --> 01:22:40,510
SPEAKER_0:  Yeah, so-

01:22:40,994 --> 01:22:44,254
SPEAKER_0:  People have been working on AI for diplomacy since about the 80s.

01:22:44,578 --> 01:22:45,054
SPEAKER_0:  Um.

01:22:45,506 --> 01:22:47,870
SPEAKER_0:  There was some really exciting research back then, but...

01:22:48,450 --> 01:22:59,134
SPEAKER_0:  The approach that was taken was very different from what we see today. I mean, the research in the 80s was a very rule-based approach, kind of a heuristic approach. It was very in line with the kind of research that was being done in the 80s.

01:22:59,554 --> 01:23:03,390
SPEAKER_0:  you know, basically trying to encode human knowledge into the strategy of the AI.

01:23:04,066 --> 01:23:04,510
SPEAKER_0:  Um.

01:23:04,866 --> 01:23:05,278
SPEAKER_0:  and

01:23:05,570 --> 01:23:07,966
SPEAKER_0:  You know, it's understandable. I mean, the game is so incredibly...

01:23:08,194 --> 01:23:08,830
SPEAKER_0:  different and

01:23:09,154 --> 01:23:10,846
SPEAKER_0:  so much more complicated.

01:23:11,106 --> 01:23:12,318
SPEAKER_0:  than the kinds of games that

01:23:12,962 --> 01:23:14,526
SPEAKER_0:  people working on like chess and go.

01:23:14,978 --> 01:23:16,030
SPEAKER_0:  and poker that

01:23:16,706 --> 01:23:18,590
SPEAKER_0:  It was honestly even hard to like.

01:23:19,010 --> 01:23:19,454
SPEAKER_0:  Start.

01:23:20,130 --> 01:23:22,846
SPEAKER_0:  getting making any progress in diplomacy. Can you just.

01:23:23,042 --> 01:23:23,614
SPEAKER_1:  formulate.

01:23:23,874 --> 01:23:26,366
SPEAKER_1:  What is the problem from an AI perspective?

01:23:26,690 --> 01:23:29,118
SPEAKER_1:  And why is it hard? Why is it a challenging game to solve?

01:23:29,730 --> 01:23:34,398
SPEAKER_0:  So there's a lot of aspects in diplomacy that make it a huge challenge. First of all,

01:23:34,850 --> 01:23:38,494
SPEAKER_0:  you have the natural language components. And I think this really is what makes it.

01:23:39,298 --> 01:23:41,470
SPEAKER_0:  are really the most difficult.

01:23:41,922 --> 01:23:43,582
SPEAKER_0:  uh... game among the major benchmarks

01:23:43,906 --> 01:23:44,542
SPEAKER_0:  The fact that.

01:23:45,378 --> 01:23:47,582
SPEAKER_0:  You have to, it's not about.

01:23:47,874 --> 01:23:49,086
SPEAKER_0:  Moving pieces on the board?

01:23:49,762 --> 01:23:52,350
SPEAKER_0:  your action space is basically all the different

01:23:52,994 --> 01:23:55,742
SPEAKER_0:  sentences that you could communicate to somebody else in this game.

01:23:56,162 --> 01:23:56,894
SPEAKER_0:  and um

01:23:57,474 --> 01:23:59,902
SPEAKER_1:  Is there, can we just like link on that? So

01:24:00,610 --> 01:24:01,790
SPEAKER_1:  is part of it.

01:24:02,210 --> 01:24:04,126
SPEAKER_1:  Like the ambiguity in the language?

01:24:04,930 --> 01:24:07,166
SPEAKER_1:  If it was like very strict.

01:24:07,970 --> 01:24:12,126
SPEAKER_1:  If you narrowed the set of possible sentences, it could do it. Would that simplify the game significantly?

01:24:12,738 --> 01:24:13,534
SPEAKER_0:  The real-

01:24:14,402 --> 01:24:15,518
SPEAKER_0:  Difficulty is the-

01:24:15,906 --> 01:24:17,630
SPEAKER_0:  of things that you can talk about.

01:24:18,114 --> 01:24:18,558
SPEAKER_0:  Um.

01:24:18,914 --> 01:24:24,990
SPEAKER_0:  You could have natural language in other games, like Settlers of Catan, for example. You could have a natural language, Settlers of Catan AI.

01:24:25,442 --> 01:24:28,254
SPEAKER_0:  But the things that you're gonna talk about are basically like, am I trading you?

01:24:28,546 --> 01:24:30,206
SPEAKER_0:  Two sheep for a wood or three sheep for a wood?

01:24:30,690 --> 01:24:31,198
SPEAKER_0:  Um...

01:24:31,650 --> 01:24:33,214
SPEAKER_0:  Whereas in a game like Diplomacy...

01:24:33,634 --> 01:24:36,254
SPEAKER_0:  the breadth of conversations that you're going to have are like

01:24:36,546 --> 01:24:41,566
SPEAKER_0:  you know, am I going to support you? Are you going to support me in return? Which units are going to do what?

01:24:41,826 --> 01:24:44,350
SPEAKER_0:  What did this other person say promise you?

01:24:44,610 --> 01:24:48,446
SPEAKER_0:  They're lying because they told this other person that they're going to do this instead.

01:24:48,738 --> 01:24:51,262
SPEAKER_0:  If you help me out this turn then in the future I'll...

01:24:51,842 --> 01:24:53,310
SPEAKER_0:  do these things that will help you out.

01:24:53,826 --> 01:24:54,206
SPEAKER_0:  Um.

01:24:54,818 --> 01:24:55,262
SPEAKER_0:  the

01:24:56,130 --> 01:24:59,454
SPEAKER_0:  depth and breadth of these conversations is really...

01:24:59,810 --> 01:25:00,382
SPEAKER_0:  complicated.

01:25:01,058 --> 01:25:02,846
SPEAKER_0:  and it's all being done in natural language.

01:25:03,234 --> 01:25:03,710
SPEAKER_0:  Um.

01:25:04,130 --> 01:25:08,894
SPEAKER_0:  Now you could approach it, and we actually consider doing this, like having a simplified language.

01:25:09,282 --> 01:25:11,966
SPEAKER_0:  to make this complexity smaller.

01:25:13,122 --> 01:25:14,142
SPEAKER_0:  Ultimately we thought

01:25:14,818 --> 01:25:17,886
SPEAKER_0:  the most impactful way of doing this research would be to.

01:25:18,210 --> 01:25:20,958
SPEAKER_0:  head on.

01:25:21,378 --> 01:25:23,710
SPEAKER_0:  and just try to go for the full game upfront.

01:25:24,386 --> 01:25:27,070
SPEAKER_1:  Just looking at sample games and with the conversations.

01:25:27,458 --> 01:25:31,838
SPEAKER_1:  look like greetings England this should prove to be a fun game since all the

01:25:32,130 --> 01:25:34,622
SPEAKER_1:  private press is going to be made public at the end.

01:25:35,234 --> 01:25:41,609
SPEAKER_1:  At the least, it will be interesting to see if the press changes because of that. Anyway, good. So there's like a-

01:25:41,609 --> 01:25:47,678
SPEAKER_0:  Yeah, that's just kind of like the generic greetings at the beginning of the game I think that the meat comes a little bit later when you're starting to talk about like

01:25:48,066 --> 01:25:49,502
SPEAKER_0:  specific strategy and stuff.

01:25:50,146 --> 01:25:54,238
SPEAKER_1:  I agree there are a lot of advantages to the two of us keeping in touch.

01:25:54,658 --> 01:25:55,838
SPEAKER_1:  and our nation's.

01:25:56,194 --> 01:25:59,838
SPEAKER_1:  make strong natural allies in the middle game. So that kind of stuff.

01:26:00,290 --> 01:26:02,302
SPEAKER_1:  making friends making enemies

01:26:02,466 --> 01:26:05,694
SPEAKER_0:  Yeah, or like if you look at the next line, so the person's saying like, I've heard...

01:26:06,082 --> 01:26:11,230
SPEAKER_0:  Bits about a lapanto and an octopus opening and basically telling Austria like hey just a heads up

01:26:11,586 --> 01:26:14,526
SPEAKER_0:  You know, I've heard these whispers about like what might be going on behind your back.

01:26:14,978 --> 01:26:15,326
SPEAKER_1:  Yeah.

01:26:15,714 --> 01:26:16,446
SPEAKER_1:  That's it, basalt.

01:26:16,770 --> 01:26:19,614
SPEAKER_1:  There's all kinds of complexities in that.

01:26:20,386 --> 01:26:22,270
SPEAKER_1:  in the language of that.

01:26:22,498 --> 01:26:27,646
SPEAKER_1:  Like to interpret what the heck that means. It's hard for us humans, but for AI it's even harder.

01:26:27,970 --> 01:26:31,646
SPEAKER_1:  You have to understand at every level the semantics of that.

01:26:31,810 --> 01:26:36,382
SPEAKER_0:  Right, I mean, there's the complexity and understanding. When somebody is saying this to me, what does that mean?

01:26:36,738 --> 01:26:38,430
SPEAKER_0:  And then there's also the complexity of like.

01:26:38,690 --> 01:26:46,174
SPEAKER_0:  Should I be telling this person this? Like, I've overheard these whispers. Should I be telling this person that, like, hey, you might be getting attacked by this other power?

01:26:46,754 --> 01:26:47,838
SPEAKER_1:  Okay, so...

01:26:50,050 --> 01:26:53,470
SPEAKER_1:  What? How are we supposed to think about? Okay, so that's the natural language.

01:26:54,210 --> 01:26:56,926
SPEAKER_1:  How do you even begin trying to solve this game? It seems like.

01:26:57,634 --> 01:26:59,870
SPEAKER_1:  This seems like the touring test on steroids.

01:27:00,130 --> 01:27:05,118
SPEAKER_0:  Yeah, and I mean, there's the natural language aspect. And then even besides the natural language aspect, you also have.

01:27:05,570 --> 01:27:08,798
SPEAKER_0:  the the cooperative elements of the game and i think this is actually

01:27:09,410 --> 01:27:13,726
SPEAKER_0:  Something that I find really interesting if you look at all the previous game AI

01:27:14,018 --> 01:27:14,942
SPEAKER_0:  Breakthroughs?

01:27:15,266 --> 01:27:20,094
SPEAKER_0:  They've all happened in these purely adversarial games where you don't actually need to understand how humans play the game.

01:27:20,514 --> 01:27:22,590
SPEAKER_0:  It's all just AI versus AI.

01:27:22,882 --> 01:27:23,902
SPEAKER_0:  Right, like you look at.

01:27:24,226 --> 01:27:24,542
SPEAKER_0:  uh

01:27:25,122 --> 01:27:27,294
SPEAKER_0:  Checkers, chess, go, poker.

01:27:27,746 --> 01:27:29,566
SPEAKER_0:  Starcraft Dota 2

01:27:30,018 --> 01:27:33,502
SPEAKER_0:  Like in some of those cases, they leveraged human data, but they never needed to.

01:27:33,890 --> 01:27:34,686
SPEAKER_0:  They were always.

01:27:35,138 --> 01:27:36,254
SPEAKER_0:  just trying to...

01:27:36,642 --> 01:27:37,918
SPEAKER_0:  have a scalable algorithm.

01:27:38,594 --> 01:27:38,942
SPEAKER_0:  that.

01:27:39,234 --> 01:27:42,718
SPEAKER_0:  then they could throw a lot of computational resources out, a lot of memory out.

01:27:43,106 --> 01:27:44,958
SPEAKER_0:  and then eventually it would converge.

01:27:45,346 --> 01:27:48,062
SPEAKER_0:  to an approximation of a Nash equilibrium.

01:27:48,962 --> 01:27:49,918
SPEAKER_0:  perfect strategy.

01:27:50,178 --> 01:27:55,294
SPEAKER_0:  that in the two players, your some game guarantees that they're going to be able to not lose to any opponents.

01:27:55,874 --> 01:27:57,246
SPEAKER_1:  so you can't leverage self-play.

01:27:57,506 --> 01:27:58,206
SPEAKER_1:  to solve this game.

01:27:58,338 --> 01:28:01,406
SPEAKER_0:  You can leverage self-play, but it's no longer sufficient.

01:28:01,826 --> 01:28:02,951
SPEAKER_0:  to beat humans.

01:28:02,951 --> 01:28:05,022
SPEAKER_1:  How do you integrate the human into the loop of this?

01:28:05,314 --> 01:28:06,494
SPEAKER_0:  So what you have to do...

01:28:06,882 --> 01:28:08,414
SPEAKER_0:  is incorporate human data.

01:28:09,282 --> 01:28:14,814
SPEAKER_0:  And to kind of give you some intuition for why this is the case, like imagine you're playing a negotiation game like like diplomacy

01:28:15,330 --> 01:28:17,886
SPEAKER_0:  but you're training completely from scratch.

01:28:18,626 --> 01:28:22,526
SPEAKER_0:  without any human data. The AI is not going to suddenly like-

01:28:22,818 --> 01:28:26,974
SPEAKER_0:  figure out how to communicate in English. It's going to figure out some weird robot language.

01:28:27,330 --> 01:28:28,638
SPEAKER_0:  that only it will understand.

01:28:28,866 --> 01:28:31,518
SPEAKER_0:  And then when you stick that in a game with six other humans

01:28:32,034 --> 01:28:32,542
SPEAKER_0:  They're gonna.

01:28:33,058 --> 01:28:36,478
SPEAKER_0:  this person's talking gibberish and they're just going to ally with each other and team up against the bot.

01:28:37,282 --> 01:28:39,294
SPEAKER_0:  or not even team up against the bot, but just not work with the bot.

01:28:39,906 --> 01:28:42,270
SPEAKER_0:  And so in order to be able to play this game.

01:28:42,626 --> 01:28:45,982
SPEAKER_0:  With humans, it has to understand the human way of playing the game.

01:28:46,306 --> 01:28:48,094
SPEAKER_0:  not this machine way of playing the game.

01:28:48,706 --> 01:28:49,022
SPEAKER_1:  Yeah.

01:28:49,730 --> 01:28:51,006
SPEAKER_1:  Yeah, that's fascinating.

01:28:52,322 --> 01:28:55,454
SPEAKER_1:  That's a nuanced thing to understand because the

01:28:56,834 --> 01:29:01,470
SPEAKER_1:  A chess playing program doesn't need to play like a human to be a human. Exactly.

01:29:01,762 --> 01:29:04,478
SPEAKER_1:  But here you have to play like a human in order to beat them.

01:29:04,674 --> 01:29:08,638
SPEAKER_0:  Or at least you have to understand how humans play the game so that you can understand how to work with them.

01:29:08,962 --> 01:29:10,974
SPEAKER_0:  if they have certain expectations about

01:29:11,330 --> 01:29:12,926
SPEAKER_0:  What does it mean to be a good ally?

01:29:13,186 --> 01:29:17,118
SPEAKER_0:  What does it mean to have a reciprocal relationship when we're working together?

01:29:17,538 --> 01:29:19,774
SPEAKER_0:  You have to abide by those conventions.

01:29:20,034 --> 01:29:22,430
SPEAKER_0:  And if you don't, they're just going to work with somebody else instead.

01:29:23,170 --> 01:29:24,030
SPEAKER_1:  Do you think of this?

01:29:24,546 --> 01:29:25,886
SPEAKER_1:  as a clean.

01:29:26,370 --> 01:29:30,142
SPEAKER_1:  in some deep sense of the spirit of the touring test as formulated by Alan Turing.

01:29:30,498 --> 01:29:31,326
SPEAKER_1:  Is it?

01:29:31,810 --> 01:29:34,910
SPEAKER_1:  In some sense, this is what the Turing test actually looks like.

01:29:36,450 --> 01:29:37,694
SPEAKER_1:  So because of...

01:29:38,050 --> 01:29:41,534
SPEAKER_1:  Open ended natural language conversation seems like.

01:29:42,338 --> 01:29:43,774
SPEAKER_1:  very difficult to evaluate.

01:29:44,098 --> 01:29:44,958
SPEAKER_1:  Like here.

01:29:45,218 --> 01:29:49,630
SPEAKER_1:  at a high stakes where humans are trying to win a game. That seems like how you actually.

01:29:50,370 --> 01:29:51,486
SPEAKER_1:  perform the towing test.

01:29:52,002 --> 01:29:58,846
SPEAKER_0:  I think it's different from the Turing test. Like the way that the Turing test is formulated, it's about trying to distinguish a human from a machine.

01:29:59,138 --> 01:30:01,054
SPEAKER_0:  and seeing, oh, could the machine...

01:30:01,474 --> 01:30:05,598
SPEAKER_0:  successfully pass as a human in this adversarial setting where the

01:30:06,082 --> 01:30:08,478
SPEAKER_0:  player is trying to figure out whether it's a machine or a human.

01:30:08,994 --> 01:30:10,174
SPEAKER_0:  Whereas in diplomacy...

01:30:10,562 --> 01:30:14,078
SPEAKER_0:  It's not about trying to figure out whether this player is a human or a machine.

01:30:14,434 --> 01:30:16,830
SPEAKER_0:  It's ultimately about whether I can work with this.

01:30:17,122 --> 01:30:19,518
SPEAKER_0:  player, regardless of whether they are a human or machine.

01:30:19,906 --> 01:30:21,182
SPEAKER_0:  and can the machine.

01:30:21,442 --> 01:30:22,814
SPEAKER_0:  do that better than a human can.

01:30:24,066 --> 01:30:25,054
SPEAKER_1:  Yeah.

01:30:25,314 --> 01:30:27,582
SPEAKER_1:  think about that but that just feels like

01:30:28,514 --> 01:30:30,014
SPEAKER_1:  the implied.

01:30:30,402 --> 01:30:33,342
SPEAKER_1:  Requirement for that is for the machine to be human-like.

01:30:34,530 --> 01:30:38,430
SPEAKER_0:  I think that's true, that if you're going to play in this human game...

01:30:39,170 --> 01:30:41,470
SPEAKER_0:  you have to somehow adapt to the...

01:30:41,730 --> 01:30:43,006
SPEAKER_0:  to the human surroundings.

01:30:43,394 --> 01:30:44,638
SPEAKER_0:  and the human play style.

01:30:44,738 --> 01:30:46,462
SPEAKER_1:  and to when you have to adapt.

01:30:47,298 --> 01:30:49,150
SPEAKER_1:  So you can't, if you're the outsider.

01:30:50,114 --> 01:30:53,502
SPEAKER_1:  If you're not human-like, I feel like that's a losing strategy.

01:30:53,858 --> 01:30:55,422
SPEAKER_0:  I think that's correct, yeah.

01:30:56,322 --> 01:30:56,958
SPEAKER_1:  So, okay.

01:30:57,378 --> 01:30:58,046
SPEAKER_1:  Uh...

01:30:59,394 --> 01:31:01,854
SPEAKER_1:  What what are the complexities here? What was your approach to it?

01:31:02,690 --> 01:31:06,942
SPEAKER_0:  Before I get to that, one thing I should explain, like why we decided to work on diplomacy.

01:31:07,394 --> 01:31:07,934
SPEAKER_0:  So.

01:31:08,322 --> 01:31:10,462
SPEAKER_0:  Basically what happened is in 2019,

01:31:11,042 --> 01:31:14,878
SPEAKER_0:  I was wrapping up the work on six player poker on Fluribus.

01:31:15,362 --> 01:31:17,470
SPEAKER_0:  and was trying to think about what to work on next.

01:31:17,954 --> 01:31:18,462
SPEAKER_0:  and

01:31:19,074 --> 01:31:22,910
SPEAKER_0:  I had been seeing like all these other breakthroughs happening in AI. I mean like 2019.

01:31:23,202 --> 01:31:27,358
SPEAKER_0:  You have StarCraft, you have AlphaStar beating humans in StarCraft, you've got

01:31:27,682 --> 01:31:29,726
SPEAKER_0:  the Dota 2 stuff happening at OpenAI.

01:31:30,050 --> 01:31:31,070
SPEAKER_0:  You have GPT.

01:31:31,490 --> 01:31:33,950
SPEAKER_0:  or gpd3 come i think it was gpd2 at the time

01:31:34,562 --> 01:31:38,558
SPEAKER_0:  And it became clear that AI was progressing really, really rapidly.

01:31:39,490 --> 01:31:40,094
SPEAKER_0:  and

01:31:40,770 --> 01:31:44,190
SPEAKER_0:  People were throwing out these other games about what should be the next

01:31:44,418 --> 01:31:46,334
SPEAKER_0:  challenge for multi-agent AI.

01:31:46,882 --> 01:31:49,822
SPEAKER_0:  and I just felt like we had to aim bigger.

01:31:50,082 --> 01:31:50,494
SPEAKER_0:  Um.

01:31:51,586 --> 01:31:52,382
SPEAKER_0:  If you look at a game like

01:31:52,674 --> 01:31:54,046
SPEAKER_0:  chess or a game like Go.

01:31:54,562 --> 01:31:56,734
SPEAKER_0:  They took decades for researchers to...

01:31:57,346 --> 01:32:02,174
SPEAKER_0:  to ultimately reach superhuman performance at. I mean, like, chess took four years of AI research.

01:32:02,594 --> 01:32:03,870
SPEAKER_0:  Go take another 20 years.

01:32:04,322 --> 01:32:04,862
SPEAKER_0:  Um...

01:32:05,442 --> 01:32:05,886
SPEAKER_0:  and

01:32:06,754 --> 01:32:08,478
SPEAKER_0:  We thought that diplomacy.

01:32:08,834 --> 01:32:12,414
SPEAKER_0:  would be this incredibly difficult challenge that could easily take a decade.

01:32:12,962 --> 01:32:14,686
SPEAKER_0:  to make an AI that could play competently.

01:32:15,266 --> 01:32:17,886
SPEAKER_0:  Um, but we felt like that was, that was a goal worth aiming for.

01:32:18,786 --> 01:32:19,230
SPEAKER_0:  Um.

01:32:20,258 --> 01:32:23,614
SPEAKER_0:  And so honestly, I was kind of reluctant to work on it first because I thought it was like

01:32:24,386 --> 01:32:26,590
SPEAKER_0:  too far out of the realm of possibility, but...

01:32:26,850 --> 01:32:33,950
SPEAKER_0:  I was talking to a coworker of mine, Adam Lerner, and he was basically saying, like, eh, why not aim for it? We'll learn some interesting things along the way, and maybe it'll be possible.

01:32:34,498 --> 01:32:36,638
SPEAKER_0:  And so we decided to go for it.

01:32:37,186 --> 01:32:39,614
SPEAKER_0:  I think it was the right choice considering.

01:32:40,226 --> 01:32:42,302
SPEAKER_0:  just how much progress there was.

01:32:42,530 --> 01:32:45,406
SPEAKER_0:  in AI and that progress has continued in the years since.

01:32:46,050 --> 01:32:47,550
SPEAKER_1:  Winning in diplomacy.

01:32:48,066 --> 01:32:49,886
SPEAKER_1:  What does that really look like?

01:32:50,242 --> 01:32:50,974
SPEAKER_1:  It means...

01:32:51,458 --> 01:32:52,510
SPEAKER_1:  talking to...

01:32:53,026 --> 01:32:54,334
SPEAKER_1:  six other players.

01:32:54,594 --> 01:32:55,934
SPEAKER_1:  six other entities.

01:32:56,162 --> 01:32:56,830
SPEAKER_1:  agents.

01:32:57,506 --> 01:32:58,558
SPEAKER_1:  and convincing.

01:32:59,714 --> 01:33:03,102
SPEAKER_1:  and convincing them of stuff that you want them to be convinced of.

01:33:03,650 --> 01:33:05,790
SPEAKER_1:  Like what exactly I'm trying to get like.

01:33:06,178 --> 01:33:08,254
SPEAKER_1:  to deeply understand what the problem is.

01:33:09,474 --> 01:33:10,238
SPEAKER_0:  Ultimately

01:33:11,042 --> 01:33:11,774
SPEAKER_0:  The problem is...

01:33:12,290 --> 01:33:16,862
SPEAKER_0:  It's simple to quantify, right? So you're going to play this game with humans and-

01:33:17,186 --> 01:33:25,694
SPEAKER_0:  You want your score on average to be as high as possible. You know, if you can say like, I am winning more than any human alive.

01:33:26,114 --> 01:33:28,382
SPEAKER_0:  then you're a champion diplomacy player.

01:33:29,090 --> 01:33:29,470
SPEAKER_0:  Um.

01:33:29,986 --> 01:33:35,166
SPEAKER_0:  Now ultimately we didn't reach that, we got to human level performance, we actually, so we played about 40 games.

01:33:35,522 --> 01:33:36,734
SPEAKER_0:  with real humans.

01:33:36,994 --> 01:33:37,534
SPEAKER_0:  Online.

01:33:37,954 --> 01:33:41,694
SPEAKER_0:  The bot came in second out of all players that played five or more games.

01:33:42,018 --> 01:33:42,814
SPEAKER_0:  And, um...

01:33:43,330 --> 01:33:45,054
SPEAKER_0:  So not like number one, but...

01:33:45,634 --> 01:33:46,759
SPEAKER_0:  way way higher than- slowly i just

01:33:46,759 --> 01:33:52,009
SPEAKER_1:  What was the expertise level? Are they beginners? Are they intermediate players, advanced players?

01:33:52,009 --> 01:33:54,430
SPEAKER_0:  Give a sense. That's a great question. And so I think.

01:33:55,170 --> 01:33:56,830
SPEAKER_0:  This kind of goes into how do you measure.

01:33:57,090 --> 01:34:00,958
SPEAKER_0:  performance in diplomacy and I would argue that when you're measuring performance in a game like this

01:34:01,378 --> 01:34:02,942
SPEAKER_0:  You don't actually want to measure it.

01:34:03,170 --> 01:34:04,926
SPEAKER_0:  in games with all expert players.

01:34:05,538 --> 01:34:07,870
SPEAKER_0:  It's kind of like if you're developing a self-driving car.

01:34:08,578 --> 01:34:10,494
SPEAKER_0:  You don't want to measure that car.

01:34:10,722 --> 01:34:12,894
SPEAKER_0:  on the road with a bunch of expert stunt drivers.

01:34:13,154 --> 01:34:16,990
SPEAKER_0:  You want to put it on a road of like an actual American city and see

01:34:17,250 --> 01:34:18,174
SPEAKER_0:  Is this car?

01:34:18,626 --> 01:34:20,862
SPEAKER_0:  crashing less often than an expert driver.

01:34:21,954 --> 01:34:23,870
SPEAKER_0:  So that's the metric that we've used.

01:34:24,130 --> 01:34:26,110
SPEAKER_0:  We're saying like, we're gonna stick this game.

01:34:26,498 --> 01:34:28,190
SPEAKER_0:  We're gonna stick this bot in games with.

01:34:28,514 --> 01:34:30,046
SPEAKER_0:  a wide variety of skill levels.

01:34:30,466 --> 01:34:32,286
SPEAKER_0:  And then are we doing better?

01:34:32,514 --> 01:34:35,934
SPEAKER_0:  than a strong or expert human player would in the same situation.

01:34:36,866 --> 01:34:41,694
SPEAKER_1:  That's quite brilliant, because I played a lot of sports in my life, like as a tennis judo, whatever.

01:34:42,242 --> 01:34:42,910
SPEAKER_1:  And it's.

01:34:43,586 --> 01:34:46,654
SPEAKER_1:  Somehow almost easier to go against experts almost always.

01:34:46,946 --> 01:34:49,310
SPEAKER_1:  I think they're more predictable.

01:34:49,634 --> 01:34:50,878
SPEAKER_1:  and the quality of play.

01:34:51,874 --> 01:34:56,510
SPEAKER_1:  that the space of strategies you're operating under is narrower against experts.

01:34:56,802 --> 01:35:01,662
SPEAKER_1:  It's more fun. It's really frustrating to go against beginners. Also, because beginners talk trash to you.

01:35:02,018 --> 01:35:09,214
SPEAKER_1:  when they somehow do beat you. So that's a human thing that AI doesn't have to be worried about that. They have the variance in strategies.

01:35:09,442 --> 01:35:13,502
SPEAKER_1:  It's greater, especially with natural language. It's just all over the place then.

01:35:14,018 --> 01:35:15,870
SPEAKER_0:  Yeah, and honestly when you look

01:35:16,226 --> 01:35:16,830
SPEAKER_0:  at

01:35:17,378 --> 01:35:19,678
SPEAKER_0:  what makes a good human diplomacy player.

01:35:20,194 --> 01:35:26,590
SPEAKER_0:  Obviously they're able to handle themselves in games with other expert humans, but where they really shine is when they're playing with these weak players.

01:35:26,818 --> 01:35:28,702
SPEAKER_0:  and they know how to take advantage.

01:35:28,994 --> 01:35:33,022
SPEAKER_0:  of the fact that they're a weak player, that they won't be able to pull off a stab as well.

01:35:33,346 --> 01:35:33,758
SPEAKER_0:  or that.

01:35:34,146 --> 01:35:40,382
SPEAKER_0:  They have certain tendencies and they can take them under their wing and persuade them to do things that might not even be in their interest.

01:35:41,154 --> 01:35:43,326
SPEAKER_0:  the really good diplomacy players are able to.

01:35:43,586 --> 01:35:45,310
SPEAKER_0:  to take advantage of the fact that.

01:35:45,570 --> 01:35:47,294
SPEAKER_0:  there is that there are some weak players in the game

01:35:47,394 --> 01:35:50,238
SPEAKER_1:  Okay, so if you have to incorporate human play data.

01:35:50,498 --> 01:35:51,518
SPEAKER_1:  How do you do that?

01:35:51,778 --> 01:35:55,230
SPEAKER_1:  How do you do that in order to train in the AI system to play diplomacy?

01:35:56,002 --> 01:35:56,766
SPEAKER_0:  Yeah, so that's...

01:35:57,186 --> 01:35:59,358
SPEAKER_0:  That's really the crux of the problem. How do we-

01:35:59,746 --> 01:36:00,126
SPEAKER_0:  Um.

01:36:00,578 --> 01:36:03,774
SPEAKER_0:  Leverage the benefits of self-play that have been so successful

01:36:04,066 --> 01:36:05,918
SPEAKER_0:  in all these other previous games.

01:36:06,242 --> 01:36:06,782
SPEAKER_0:  while

01:36:07,010 --> 01:36:10,174
SPEAKER_0:  keeping the strategy as human compatible as possible.

01:36:10,850 --> 01:36:14,430
SPEAKER_0:  And so what we did is we first trained a language model.

01:36:14,818 --> 01:36:17,662
SPEAKER_0:  And then we made that language model controllable.

01:36:18,018 --> 01:36:23,966
SPEAKER_0:  on a set of intents, what we call intents, which are basically like an action that we want to play.

01:36:24,226 --> 01:36:25,054
SPEAKER_0:  and an action that

01:36:25,314 --> 01:36:26,782
SPEAKER_0:  we would like the other player to play.

01:36:27,234 --> 01:36:31,710
SPEAKER_0:  And so this gives us a way to generate dialogue that's not just trying to imitate the human style.

01:36:32,194 --> 01:36:32,606
SPEAKER_0:  Um.

01:36:32,834 --> 01:36:37,406
SPEAKER_0:  whatever human was saying the situation but to actually give it up uh... an intent of purpose

01:36:37,634 --> 01:36:38,558
SPEAKER_0:  in its communication.

01:36:38,914 --> 01:36:42,270
SPEAKER_0:  we can talk about a specific move or we can make a specific request.

01:36:42,658 --> 01:36:45,406
SPEAKER_0:  and the determination of what that move is.

01:36:45,634 --> 01:36:46,558
SPEAKER_0:  that we're discussing.

01:36:46,978 --> 01:36:47,806
SPEAKER_0:  comes from.

01:36:48,130 --> 01:36:52,574
SPEAKER_0:  strategic reasoning model that uses reinforcement learning and planning.

01:36:52,898 --> 01:36:54,238
SPEAKER_1:  So, the...

01:36:54,594 --> 01:36:56,926
SPEAKER_1:  computing the intents for all the players.

01:36:58,018 --> 01:36:58,686
SPEAKER_1:  How's that done?

01:36:59,010 --> 01:36:59,550
SPEAKER_1:  Just sub-

01:36:59,842 --> 01:37:06,782
SPEAKER_1:  as a starting point. Is that with reinforcement learning or is that just optimal, determining what the optimal is for intents?

01:37:06,914 --> 01:37:09,758
SPEAKER_0:  It's a combination of reinforcement learning and.

01:37:10,114 --> 01:37:10,430
SPEAKER_0:  planning.

01:37:10,882 --> 01:37:14,654
SPEAKER_0:  Actually very similar to how we approached poker and

01:37:14,978 --> 01:37:15,390
SPEAKER_0:  Help.

01:37:15,874 --> 01:37:17,886
SPEAKER_0:  people have approached chess and Go as well.

01:37:18,274 --> 01:37:19,870
SPEAKER_0:  We're using Self-Play.

01:37:20,130 --> 01:37:21,566
SPEAKER_0:  and and search.

01:37:21,986 --> 01:37:25,214
SPEAKER_0:  to try to figure out what is an optimal move for us.

01:37:25,506 --> 01:37:28,510
SPEAKER_0:  And what is a desirable move that we would like this other player to play?

01:37:28,962 --> 01:37:29,406
SPEAKER_0:  Now.

01:37:29,826 --> 01:37:31,614
SPEAKER_0:  the difference between.

01:37:31,874 --> 01:37:35,134
SPEAKER_0:  the way that we approached reinforcement learning search in this game

01:37:35,490 --> 01:37:36,734
SPEAKER_0:  versus those previous games.

01:37:37,058 --> 01:37:39,934
SPEAKER_0:  is that we have to keep it human compatible. We have to understand.

01:37:40,354 --> 01:37:40,766
SPEAKER_0:  Hell.

01:37:41,122 --> 01:37:44,926
SPEAKER_0:  the other person is likely to play rather than just assuming that they're going to play like a machine.

01:37:45,538 --> 01:37:46,654
SPEAKER_1:  and how language.

01:37:47,042 --> 01:37:48,126
SPEAKER_1:  Get some to play.

01:37:49,122 --> 01:37:49,534
SPEAKER_1:  Um.

01:37:49,922 --> 01:37:53,694
SPEAKER_1:  in a way that maximizes the chance of following the intent you want them to follow.

01:37:54,114 --> 01:37:57,022
SPEAKER_1:  Okay, how do you do that? How do you connect language?

01:37:57,282 --> 01:37:57,918
SPEAKER_1:  to intent.

01:37:58,274 --> 01:37:58,686
SPEAKER_0:  So.

01:37:59,362 --> 01:38:03,902
SPEAKER_0:  The way that RL and planning is done is actually not using language. So we're...

01:38:04,322 --> 01:38:06,814
SPEAKER_0:  Coming up with this like plan for the action.

01:38:07,266 --> 01:38:11,582
SPEAKER_0:  that we're going to play and the other person is going to play and then we feed that action into the dialogue model.

01:38:11,874 --> 01:38:13,950
SPEAKER_0:  that will then send a message according to those plans.

01:38:14,178 --> 01:38:15,550
SPEAKER_1:  to the language model there.

01:38:15,778 --> 01:38:16,542
SPEAKER_1:  is mapping.

01:38:17,570 --> 01:38:18,750
SPEAKER_1:  Action 2.

01:38:19,522 --> 01:38:20,798
SPEAKER_1:  to message.

01:38:21,730 --> 01:38:22,526
SPEAKER_1:  One word at a time.

01:38:23,618 --> 01:38:31,038
SPEAKER_0:  Basically one message at a time. So we'll feed into the dialogue model. Like here are the actions that you should be discussing. Here's the message. Here's like the-

01:38:31,394 --> 01:38:32,030
SPEAKER_0:  The...

01:38:32,738 --> 01:38:34,782
SPEAKER_0:  content of the message that we would like you to send.

01:38:35,042 --> 01:38:36,766
SPEAKER_0:  and then it will actually generate a message.

01:38:37,058 --> 01:38:38,183
SPEAKER_0:  that corresponds to that.

01:38:38,183 --> 01:38:39,486
SPEAKER_1:  Does this actually work?

01:38:39,842 --> 01:38:40,766
SPEAKER_1:  It works surprisingly well.

01:38:41,250 --> 01:38:42,046
SPEAKER_1:  Okay, how?

01:38:43,938 --> 01:38:49,950
SPEAKER_1:  Oh man, the number of ways it probably goes horribly. I would have imagined it goes horribly wrong.

01:38:50,562 --> 01:38:53,342
SPEAKER_1:  uh... so how the heck is it effective at all

01:38:54,018 --> 01:38:57,502
SPEAKER_0:  I mean, there are a lot of ways that this could fail. So for example, I mean,

01:38:58,562 --> 01:39:01,310
SPEAKER_0:  you could have a situation where you're basically like...

01:39:02,658 --> 01:39:06,014
SPEAKER_0:  We don't tell the language model, like, here are the pieces of...

01:39:06,434 --> 01:39:11,198
SPEAKER_0:  our action or the other person's action that you should be communicating. And so like, let's say you're about to attack somebody.

01:39:11,554 --> 01:39:13,886
SPEAKER_0:  You probably don't want to tell them that you're going to attack them.

01:39:14,338 --> 01:39:17,630
SPEAKER_0:  but there's nothing in the language, like the language model's not very smart at the end of the day.

01:39:17,890 --> 01:39:19,966
SPEAKER_0:  So it doesn't really have a way of knowing.

01:39:20,258 --> 01:39:23,998
SPEAKER_0:  Well, what should I be talking about? Should I tell this person that I'm about to attack them or not?

01:39:24,514 --> 01:39:25,086
SPEAKER_0:  Um

01:39:25,346 --> 01:39:28,478
SPEAKER_0:  So we have to develop a lot of other techniques that deal with that.

01:39:28,962 --> 01:39:29,406
SPEAKER_0:  Um.

01:39:29,762 --> 01:39:32,254
SPEAKER_0:  Like one of the things we do, for example, is we try to calculate.

01:39:32,642 --> 01:39:34,302
SPEAKER_0:  If I'm going to send this message.

01:39:34,722 --> 01:39:37,086
SPEAKER_0:  what would I expect the other person to do in response?

01:39:37,474 --> 01:39:39,966
SPEAKER_0:  So if it's a message like, Hey, I'm going to attack you this turn.

01:39:40,322 --> 01:39:41,630
SPEAKER_0:  they're probably gonna, you know.

01:39:42,018 --> 01:39:44,254
SPEAKER_0:  attack us or defend against that attack.

01:39:44,674 --> 01:39:46,014
SPEAKER_0:  And so we have a way of.

01:39:46,274 --> 01:39:48,478
SPEAKER_0:  recognizing like, hey, sending this message.

01:39:48,930 --> 01:39:49,534
SPEAKER_0:  is

01:39:49,890 --> 01:39:51,934
SPEAKER_0:  a negative expected value action.

01:39:52,226 --> 01:39:53,822
SPEAKER_0:  and we should not send this message.

01:39:54,818 --> 01:39:58,494
SPEAKER_1:  So yes, for particular kinds of messages, you have like an extra.

01:39:59,074 --> 01:40:00,862
SPEAKER_1:  function that does the...

01:40:01,186 --> 01:40:02,974
SPEAKER_1:  estimates the value of that message.

01:40:03,714 --> 01:40:05,534
SPEAKER_0:  So we have these kinds of filters that like.

01:40:05,826 --> 01:40:07,838
SPEAKER_1:  So it's a filter. So there's a good.

01:40:08,162 --> 01:40:11,646
SPEAKER_1:  Is that a folder in your network or is it rule-based?

01:40:11,842 --> 01:40:17,214
SPEAKER_0:  That's a neural network. Well, it's a combination. It's a neural network, but it's also using planning.

01:40:17,634 --> 01:40:18,494
SPEAKER_0:  Um, it's

01:40:18,882 --> 01:40:20,958
SPEAKER_0:  trying to compute like what is.

01:40:21,314 --> 01:40:26,558
SPEAKER_0:  the policy that the other players are going to play, given that this message has been sent. And then KRP Dogs channel,

01:40:26,882 --> 01:40:28,757
SPEAKER_0:  Is that better than not sending the message or not?

01:40:28,757 --> 01:40:31,294
SPEAKER_1:  I feel like that's how my brain works too, like there's a-

01:40:31,714 --> 01:40:34,718
SPEAKER_1:  language model that generates random crap and then there's these

01:40:35,586 --> 01:40:36,126
SPEAKER_1:  Other

01:40:36,578 --> 01:40:38,494
SPEAKER_1:  neural nets that are essentially filters.

01:40:39,042 --> 01:40:40,062
SPEAKER_1:  At least that's what I tweet.

01:40:41,218 --> 01:40:42,238
SPEAKER_1:  I'll use you my-

01:40:42,466 --> 01:40:44,446
SPEAKER_1:  process of tweeting I'll think of something.

01:40:44,930 --> 01:40:50,270
SPEAKER_1:  And it's hilarious to me. And then about five seconds later, the filter network comes in and says no.

01:40:50,562 --> 01:40:55,262
SPEAKER_1:  No, that's not funny at all. I mean, there's something interesting to that kind of process.

01:40:55,586 --> 01:40:57,342
SPEAKER_1:  So you have a set of actions that you.

01:40:57,666 --> 01:41:00,190
SPEAKER_1:  You want, you have an intent.

01:41:00,578 --> 01:41:01,694
SPEAKER_1:  you want to achieve.

01:41:02,114 --> 01:41:05,534
SPEAKER_1:  an intent that you want your opponent to achieve then you generate messages.

01:41:05,794 --> 01:41:08,318
SPEAKER_1:  And then you evaluated those messages will.

01:41:08,578 --> 01:41:10,590
SPEAKER_1:  achieve the...

01:41:12,418 --> 01:41:12,958
SPEAKER_1:  Call you on.

01:41:13,538 --> 01:41:21,342
SPEAKER_0:  Yeah, we're filtering for several things. We're filtering like, is this a sensible message? So sometimes language models will generate messages that are just like.

01:41:21,858 --> 01:41:24,958
SPEAKER_0:  Totally nonsense. And we try to filter those out.

01:41:25,186 --> 01:41:25,950
SPEAKER_0:  We also.

01:41:26,306 --> 01:41:29,662
SPEAKER_0:  to filter out messages that are basically lies. So...

01:41:29,986 --> 01:41:30,366
SPEAKER_0:  You know.

01:41:30,658 --> 01:41:33,118
SPEAKER_0:  Diplomacy has its reputation as a game that's really about...

01:41:33,538 --> 01:41:35,582
SPEAKER_0:  deception and lying, but

01:41:36,130 --> 01:41:39,262
SPEAKER_0:  we try to actually minimize the amount that the bot would lie.

01:41:39,618 --> 01:41:40,990
SPEAKER_0:  Um, this was.

01:41:41,314 --> 01:41:42,439
SPEAKER_0:  Actually mostly a-

01:41:42,439 --> 01:41:43,390
SPEAKER_1:  Or are you?

01:41:43,714 --> 01:41:45,022
SPEAKER_1:  No, I'm just kidding.

01:41:45,858 --> 01:41:48,158
SPEAKER_0:  I mean, like, part of the reason for this is that...

01:41:48,418 --> 01:41:50,686
SPEAKER_0:  we actually found that lying

01:41:51,170 --> 01:41:54,782
SPEAKER_0:  would make the bot perform worse in the long run. It would end up with a lower score.

01:41:55,266 --> 01:41:56,702
SPEAKER_0:  Because once the bot lies...

01:41:57,090 --> 01:41:57,438
SPEAKER_0:  Um.

01:41:57,986 --> 01:41:58,974
SPEAKER_0:  People would never trust it again.

01:41:59,682 --> 01:42:02,307
SPEAKER_0:  and and trust is a huge aspect of the game of the plucking

01:42:02,307 --> 01:42:04,478
SPEAKER_1:  notes here because I think this applies to

01:42:05,538 --> 01:42:07,413
SPEAKER_1:  Life lessons too.

01:42:07,413 --> 01:42:08,913
SPEAKER_0:  It's a really, yeah, really strong. So like.

01:42:08,913 --> 01:42:10,526
SPEAKER_1:  lying is a dangerous thing to do.

01:42:11,042 --> 01:42:12,670
SPEAKER_1:  Like you want to avoid.

01:42:13,474 --> 01:42:14,398
SPEAKER_1:  obvious lying.

01:42:15,010 --> 01:42:17,854
SPEAKER_0:  Yeah, I mean, I think when people play diplomacy for the first time...

01:42:18,338 --> 01:42:21,406
SPEAKER_0:  they approach it as a game of deception and lying and

01:42:21,634 --> 01:42:22,174
SPEAKER_0:  and they...

01:42:23,010 --> 01:42:25,630
SPEAKER_0:  Ultimately, if you talk to top diplomacy players, they'll tell you.

01:42:26,018 --> 01:42:27,774
SPEAKER_0:  is that diplomacy is a game about trust.

01:42:28,226 --> 01:42:32,094
SPEAKER_0:  and being able to build trust in an environment that encourages people to not trust anyone.

01:42:33,442 --> 01:42:35,870
SPEAKER_0:  So that's the ultimate tension in diplomacy.

01:42:36,162 --> 01:42:37,566
SPEAKER_0:  How can this AI

01:42:37,858 --> 01:42:40,926
SPEAKER_0:  reason about whether you are being honest in your communication.

01:42:41,250 --> 01:42:42,494
SPEAKER_0:  And how can the AI...

01:42:42,850 --> 01:42:47,230
SPEAKER_0:  persuade you that it is being honest when it is telling you that hey I'm actually going to support you this turn.

01:42:48,194 --> 01:42:52,382
SPEAKER_1:  Is there some sense, I don't know if you step back and think, that this process

01:42:53,122 --> 01:42:53,662
SPEAKER_1:  Well...

01:42:54,594 --> 01:42:55,614
SPEAKER_1:  indirectly.

01:42:56,098 --> 01:42:57,822
SPEAKER_1:  help us study human psychology.

01:42:59,298 --> 01:43:01,406
SPEAKER_1:  like if trust is the ultimate goal.

01:43:02,562 --> 01:43:06,974
SPEAKER_1:  Wouldn't that help us understand what are the fundamental aspects of forming trust?

01:43:07,458 --> 01:43:09,566
SPEAKER_1:  between humans and between humans and AI.

01:43:10,050 --> 01:43:12,926
SPEAKER_1:  I mean, that's a really, really important question that's much bigger than-

01:43:13,506 --> 01:43:14,686
SPEAKER_1:  and strategy games.

01:43:14,978 --> 01:43:18,974
SPEAKER_1:  is how can this fundamental to the human-robot interaction problem.

01:43:19,586 --> 01:43:20,606
SPEAKER_1:  How do we form trust?

01:43:21,378 --> 01:43:22,942
SPEAKER_1:  between intelligent entities.

01:43:23,906 --> 01:43:26,686
SPEAKER_0:  So one of the things I'm really excited about with diplomacy.

01:43:27,266 --> 01:43:29,214
SPEAKER_0:  Um, there's never really been.

01:43:29,474 --> 01:43:32,478
SPEAKER_0:  a good domain to investigate these kinds of questions.

01:43:32,962 --> 01:43:36,158
SPEAKER_0:  Um, and diplomacy gives us a domain where

01:43:36,866 --> 01:43:42,366
SPEAKER_0:  trust is really at the center of it. And it's not just like you've hired a bunch of mechanical turkers that

01:43:42,594 --> 01:43:44,670
SPEAKER_0:  you know, are being paid and trying to...

01:43:45,186 --> 01:43:46,814
SPEAKER_0:  get through the task as quickly as possible.

01:43:47,074 --> 01:43:51,678
SPEAKER_0:  You have these people that are really invested in the outcome of the game and they're really trying to do the best that they can.

01:43:52,258 --> 01:43:52,734
SPEAKER_0:  Um.

01:43:53,026 --> 01:43:55,838
SPEAKER_0:  And so I'm really excited that we're able to.

01:43:56,162 --> 01:43:57,214
SPEAKER_0:  We actually have...

01:43:57,634 --> 01:43:58,430
SPEAKER_0:  put together this.

01:43:58,850 --> 01:44:01,118
SPEAKER_0:  We're open sourcing all of our models. We're open sourcing.

01:44:01,474 --> 01:44:03,294
SPEAKER_0:  all of the code.

01:44:03,554 --> 01:44:07,518
SPEAKER_0:  and we're making the data that we've used available to researchers.

01:44:08,002 --> 01:44:10,558
SPEAKER_0:  uh... so that they can investigate these kinds of questions

01:44:11,010 --> 01:44:17,182
SPEAKER_1:  So the data of the different, the human and the AI play of diplomacy and the models that you use for

01:44:17,570 --> 01:44:19,742
SPEAKER_1:  the generation of the messages and the filtering.

01:44:20,258 --> 01:44:24,062
SPEAKER_0:  Yeah, not just even the data of the AI playing with the humans, but...

01:44:24,674 --> 01:44:30,014
SPEAKER_0:  all the training data that we had that we used to train the AI to understand how humans play the game.

01:44:30,338 --> 01:44:33,790
SPEAKER_0:  We're setting up a system where researchers will be able to apply.

01:44:34,146 --> 01:44:38,325
SPEAKER_0:  to be able to gain access to that data and be able to use it in their own research.

01:44:38,325 --> 01:44:40,158
SPEAKER_1:  You should say, what is the name of the system?

01:44:41,090 --> 01:44:42,078
SPEAKER_0:  We're calling the bots this room.

01:44:42,402 --> 01:44:42,846
SPEAKER_1:  sister.

01:44:43,138 --> 01:44:44,190
SPEAKER_1:  And what's the name?

01:44:44,450 --> 01:44:47,102
SPEAKER_1:  Like you're open sourcing, what's the name of the repository?

01:44:47,522 --> 01:44:51,358
SPEAKER_1:  and the project, is it also just called Cicero the Big Project?

01:44:51,682 --> 01:44:53,118
SPEAKER_1:  Are you still coming up with a name?

01:44:53,282 --> 01:44:56,510
SPEAKER_0:  The dataset comes from this website, WebDiplomacy.net.

01:44:56,834 --> 01:45:02,302
SPEAKER_0:  is this site that's been online for like 20 years now. And it's one of the main sites that people use to play diplomacy on it.

01:45:02,850 --> 01:45:03,774
SPEAKER_0:  We've got like...

01:45:04,002 --> 01:45:08,478
SPEAKER_0:  50,000 games of diplomacy with natural language communication.

01:45:08,866 --> 01:45:10,846
SPEAKER_0:  um... over ten million messages

01:45:11,234 --> 01:45:21,169
SPEAKER_0:  So it's a pretty massive data set that people can use to, we're hoping that the academic community and the research community is able to use it for all sorts of interesting research questions.

01:45:21,169 --> 01:45:22,334
SPEAKER_1:  So do you.

01:45:22,562 --> 01:45:24,446
SPEAKER_1:  from having studied this game is this.

01:45:25,250 --> 01:45:29,374
SPEAKER_1:  a sufficiently rich problem space to explore this kind of

01:45:29,858 --> 01:45:31,262
SPEAKER_1:  human AI interaction.

01:45:31,714 --> 01:45:33,278
SPEAKER_0:  Yeah, absolutely. And I think it's

01:45:34,210 --> 01:45:37,214
SPEAKER_0:  I think it's maybe the best data set that I can think of out there.

01:45:37,506 --> 01:45:42,046
SPEAKER_0:  to investigate these kinds of questions of negotiation, trust.

01:45:42,434 --> 01:45:43,550
SPEAKER_0:  persuasion.

01:45:43,938 --> 01:45:46,366
SPEAKER_0:  I wouldn't say it's the best dataset in the world for

01:45:46,978 --> 01:45:47,390
SPEAKER_0:  Um.

01:45:47,714 --> 01:45:51,486
SPEAKER_0:  human AI interaction that's a very broad field, but I think that it's definitely up there is like.

01:45:51,906 --> 01:45:53,630
SPEAKER_0:  you know, if you're really interested in...

01:45:54,050 --> 01:45:56,126
SPEAKER_0:  language models interacting with humans in.

01:45:56,482 --> 01:45:57,758
SPEAKER_0:  you know, a setting where.

01:45:57,986 --> 01:46:02,110
SPEAKER_0:  their incentives are not fully aligned. This seems like an ideal data set for investigating that.

01:46:02,978 --> 01:46:03,614
SPEAKER_1:  So...

01:46:03,970 --> 01:46:05,150
SPEAKER_1:  You have, um...

01:46:05,730 --> 01:46:10,814
SPEAKER_1:  You have a paper with some impressive results and just an impressive paper that taking this problem on.

01:46:11,554 --> 01:46:16,606
SPEAKER_1:  What's the most exciting thing to you in terms of the results from the paper?

01:46:18,018 --> 01:46:19,902
SPEAKER_1:  Well, I think there's ideas or results.

01:46:20,706 --> 01:46:23,006
SPEAKER_0:  Yeah, I think there's a few aspects of the results.

01:46:23,234 --> 01:46:27,614
SPEAKER_0:  and that I think are really exciting. So first of all, the fact that we were able to achieve.

01:46:28,098 --> 01:46:29,182
SPEAKER_0:  such strong performance.

01:46:29,570 --> 01:46:30,462
SPEAKER_0:  I was

01:46:31,362 --> 01:46:33,502
SPEAKER_0:  surprised by and pleasantly surprised by.

01:46:33,922 --> 01:46:35,454
SPEAKER_0:  So we played 40 games of diplomacy.

01:46:35,714 --> 01:46:36,638
SPEAKER_0:  with real humans.

01:46:37,122 --> 01:46:39,326
SPEAKER_0:  and the bot placed second.

01:46:39,618 --> 01:46:43,902
SPEAKER_0:  out of all players that have played five or more games. So it's about 80 players total.

01:46:44,290 --> 01:46:48,542
SPEAKER_0:  19 of whom played five or more games and the bot was ranked second out of those players.

01:46:49,378 --> 01:46:49,854
SPEAKER_0:  Um...

01:46:50,306 --> 01:46:50,718
SPEAKER_0:  and

01:46:51,298 --> 01:46:59,134
SPEAKER_0:  The bot was really good in two dimensions. One, being able to establish strong connections with the other players on the board, being able to like...

01:46:59,682 --> 01:47:01,054
SPEAKER_0:  persuade them to work with it.

01:47:01,346 --> 01:47:04,574
SPEAKER_0:  Being able to coordinate with them about like how it's going to work with them.

01:47:05,026 --> 01:47:06,622
SPEAKER_0:  And then also the raw.

01:47:07,426 --> 01:47:09,182
SPEAKER_0:  tactical and strategic aspects of the game.

01:47:09,506 --> 01:47:10,718
SPEAKER_0:  you know, being able to...

01:47:11,074 --> 01:47:11,966
SPEAKER_0:  Understand.

01:47:12,322 --> 01:47:15,230
SPEAKER_0:  what the other players are likely to do, being able to model their behavior,

01:47:15,554 --> 01:47:17,502
SPEAKER_0:  and respond appropriately to that.

01:47:17,730 --> 01:47:19,070
SPEAKER_0:  The bot also really excelled at.

01:47:19,874 --> 01:47:21,918
SPEAKER_1:  What are some interesting things that the bot said?

01:47:23,458 --> 01:47:25,854
SPEAKER_1:  By the way, you're allowed to swear in the...

01:47:26,306 --> 01:47:29,278
SPEAKER_1:  Are there rules to what you're allowed to say and not in diplomacy?

01:47:29,570 --> 01:47:35,390
SPEAKER_0:  You can say whatever you want. I think the site will get very angry at you if you start threatening somebody.

01:47:35,778 --> 01:47:39,153
SPEAKER_1:  We actually... Like if you're threatened somewhere, you're supposed to do it, pull it.

01:47:39,153 --> 01:47:41,086
SPEAKER_0:  Yeah, politely. You know, keep it in character.

01:47:41,410 --> 01:47:41,918
SPEAKER_0:  Um...

01:47:42,562 --> 01:47:42,942
SPEAKER_0:

01:47:43,394 --> 01:47:49,598
SPEAKER_0:  The bot, we actually had a researcher watching the bot 24 seven for, well, whenever we play a game, we have a bot watching it to make sure that it wouldn't.

01:47:49,922 --> 01:47:52,547
SPEAKER_0:  go off the rails and start threatening somebody or something like that.

01:47:52,547 --> 01:47:54,782
SPEAKER_1:  I would just love it if the boss started mocking.

01:47:55,554 --> 01:47:59,422
SPEAKER_1:  Mocking everybody like some weird quirky strategies would emerge.

01:47:59,650 --> 01:48:02,718
SPEAKER_1:  Have you seen anything interesting that you huh that's a weird.

01:48:03,266 --> 01:48:03,614
SPEAKER_1:  That's a W-

01:48:04,834 --> 01:48:08,510
SPEAKER_1:  That's a weird behavior, either of the filter or the language model.

01:48:09,314 --> 01:48:10,302
SPEAKER_1:  That was weird to you.

01:48:10,914 --> 01:48:12,606
SPEAKER_0:  That was, yeah, there were definitely like.

01:48:12,994 --> 01:48:14,238
SPEAKER_0:  Things that the bot would.

01:48:14,850 --> 01:48:19,166
SPEAKER_0:  would do that were not in line with how humans would approach the game.

01:48:19,522 --> 01:48:20,286
SPEAKER_0:  and that

01:48:20,738 --> 01:48:30,814
SPEAKER_0:  In a good way, the humans actually, you know, we've talked to some expert diplomacy players about these results and their takeaway is that, well, maybe humans are approaching this the wrong way and this is actually like the right way to play the game.

01:48:31,682 --> 01:48:32,094
SPEAKER_0:  Um.

01:48:32,642 --> 01:48:35,006
SPEAKER_1:  So what's required to win?

01:48:35,298 --> 01:48:36,222
SPEAKER_1:  Like what um...

01:48:36,770 --> 01:48:40,958
SPEAKER_1:  What does it mean to mess up or to exploit the suboptimal behavior of a player?

01:48:41,410 --> 01:48:42,238
SPEAKER_1:  Like, uh...

01:48:42,626 --> 01:48:47,710
SPEAKER_1:  Is there optimally rational behavior and irrational behavior that you need to...

01:48:48,226 --> 01:48:53,342
SPEAKER_1:  Estimate that kind of stuff like what what stands out to you? Like is there a crack that you can exploit?

01:48:53,858 --> 01:48:54,590
SPEAKER_1:  Is there like...

01:48:55,010 --> 01:48:58,078
SPEAKER_1:  uh... a weakness the you can exploit in the game

01:48:58,338 --> 01:49:00,030
SPEAKER_1:  that everybody's looking for.

01:49:01,474 --> 01:49:01,790
SPEAKER_0:  Well-

01:49:02,050 --> 01:49:02,782
SPEAKER_0:  I think...

01:49:03,874 --> 01:49:07,198
SPEAKER_0:  You're asking kind of two questions there. So one, like modeling the.

01:49:07,490 --> 01:49:09,982
SPEAKER_0:  irrationality and the suboptimality of humans.

01:49:10,402 --> 01:49:10,910
SPEAKER_0:  Um

01:49:12,130 --> 01:49:16,702
SPEAKER_0:  you can't, in diplomacy, you can't treat all the other players like they're machines. And if you do that,

01:49:17,090 --> 01:49:18,270
SPEAKER_0:  You're going to end up...

01:49:18,562 --> 01:49:21,374
SPEAKER_0:  playing really poorly. And so we actually ran this experiment. So we.

01:49:21,666 --> 01:49:22,622
SPEAKER_0:  We trained a bot.

01:49:23,298 --> 01:49:25,502
SPEAKER_0:  in a two-player zero-sum version of diplomacy.

01:49:25,890 --> 01:49:28,190
SPEAKER_0:  uh... the same way that you might approach a game like

01:49:28,546 --> 01:49:29,246
SPEAKER_0:  Chester Poker.

01:49:29,730 --> 01:49:32,030
SPEAKER_0:  and the bot was superhuman. It would crush any competitor.

01:49:32,706 --> 01:49:37,598
SPEAKER_0:  And then we took that same training approach and we trained a bot for the full seven player version of the game.

01:49:37,890 --> 01:49:38,526
SPEAKER_0:  through self play.

01:49:39,010 --> 01:49:39,870
SPEAKER_0:  without any human data.

01:49:40,290 --> 01:49:42,014
SPEAKER_0:  and we stuck it in a game with six humans.

01:49:42,306 --> 01:49:43,326
SPEAKER_0:  and it got destroyed.

01:49:43,810 --> 01:49:47,326
SPEAKER_0:  Even in the version of the game where there's no explicit natural language communication.

01:49:47,778 --> 01:49:48,702
SPEAKER_0:  It still got destroyed.

01:49:49,122 --> 01:49:53,950
SPEAKER_0:  Because it just wouldn't be able to understand how the other players were approaching the game and be able to work with that.

01:49:54,722 --> 01:49:55,806
SPEAKER_1:  calm

01:49:56,258 --> 01:49:58,110
SPEAKER_1:  Lingering around that meeting like there's an individual.

01:49:58,562 --> 01:50:03,998
SPEAKER_1:  There's an individual personality to each player and then you're supposed to remember that. But what do you mean it's not able to understand?

01:50:04,706 --> 01:50:06,046
SPEAKER_1:  the players.

01:50:06,370 --> 01:50:10,718
SPEAKER_0:  Well, it would, for example, expect the human to support it in a certain way.

01:50:11,106 --> 01:50:13,214
SPEAKER_0:  when the human would simply like...

01:50:13,762 --> 01:50:15,550
SPEAKER_0:  Think it like, no, I'm not supposed to support you here.

01:50:16,002 --> 01:50:17,566
SPEAKER_0:  It's kind of like, you know, if you

01:50:17,858 --> 01:50:19,006
SPEAKER_0:  develop a self-driving car.

01:50:19,458 --> 01:50:22,366
SPEAKER_0:  and it's trained completely from scratch with other self-driving cars.

01:50:22,818 --> 01:50:24,446
SPEAKER_0:  it might learn to drive on the left side of the road.

01:50:25,026 --> 01:50:26,846
SPEAKER_0:  That's a totally reasonable thing to do if you're...

01:50:27,170 --> 01:50:29,662
SPEAKER_0:  with these other self-driving cars that are also driving on the left side of the road.

01:50:30,146 --> 01:50:31,774
SPEAKER_0:  But if you put it in American City...

01:50:32,098 --> 01:50:33,223
SPEAKER_0:  It's gonna crash.

01:50:33,223 --> 01:50:36,894
SPEAKER_1:  I guess the intuition I'm trying to build up is why does it then crush a human player?

01:50:37,122 --> 01:50:37,598
SPEAKER_1:  Heads up.

01:50:39,330 --> 01:50:40,455
SPEAKER_1:  This is multiple

01:50:40,455 --> 01:50:44,766
SPEAKER_0:  This is an aspect of two players zero song versus games that involve cooperation

01:50:45,282 --> 01:50:47,166
SPEAKER_0:  So in a two-player zero-sum game.

01:50:47,682 --> 01:50:48,286
SPEAKER_0:  Um...

01:50:48,674 --> 01:50:52,382
SPEAKER_0:  You can do self play from scratch and you will arrive at the Nash Equilibrium.

01:50:52,674 --> 01:50:54,622
SPEAKER_0:  where you don't have to worry about.

01:50:54,978 --> 01:50:55,934
SPEAKER_0:  the other player.

01:50:56,738 --> 01:51:00,190
SPEAKER_0:  playing in a very human suboptimal style. That's just going to be the only way that.

01:51:00,674 --> 01:51:02,398
SPEAKER_0:  deviating from Nash equilibrium.

01:51:02,914 --> 01:51:03,422
SPEAKER_0:  Um

01:51:04,290 --> 01:51:06,206
SPEAKER_0:  would change things as if it helped you.

01:51:06,754 --> 01:51:07,102
SPEAKER_1:  So I.

01:51:07,522 --> 01:51:10,558
SPEAKER_1:  What's the dynamic of cooperation that's effective in diplomacy?

01:51:11,330 --> 01:51:12,158
SPEAKER_1:  Do you always have to?

01:51:12,738 --> 01:51:14,302
SPEAKER_1:  to have one friend in the game.

01:51:14,946 --> 01:51:15,646
SPEAKER_0:  You always

01:51:15,906 --> 01:51:19,166
SPEAKER_0:  want to maximize your friends and minimize your enemies.

01:51:22,274 --> 01:51:22,718
SPEAKER_1:  Got it.

01:51:23,106 --> 01:51:23,582
SPEAKER_1:  and...

01:51:25,250 --> 01:51:25,758
SPEAKER_1:  Boy.

01:51:26,114 --> 01:51:28,606
SPEAKER_1:  And the, the line comes into play there.

01:51:29,698 --> 01:51:31,454
SPEAKER_1:  So the more friends you have, the better.

01:51:32,290 --> 01:51:35,665
SPEAKER_0:  Yeah, I mean, I guess you have to attack somebody or else you're not going to make progress.

01:51:35,665 --> 01:51:36,542
SPEAKER_1:  That's the tension.

01:51:37,250 --> 01:51:43,454
SPEAKER_1:  Man, this is too real. This is too real. This is too close to geopolitics of actual.

01:51:44,034 --> 01:51:45,758
SPEAKER_1:  military conflict in the world. Ok.

01:51:46,498 --> 01:51:50,878
SPEAKER_1:  uh... that's passing so that cooperation element is what makes the game really really

01:51:52,130 --> 01:51:56,734
SPEAKER_0:  And to give you an example of how this suboptimality and irrationality comes into play.

01:51:57,346 --> 01:51:59,358
SPEAKER_0:  There's a really common situation.

01:51:59,778 --> 01:52:00,894
SPEAKER_0:  in the game of diplomacy.

01:52:01,282 --> 01:52:06,718
SPEAKER_0:  where one player starts to win and they're like at the point where they're controlling about half the map.

01:52:07,330 --> 01:52:10,654
SPEAKER_0:  And the remaining players who have all been fighting each other the whole game.

01:52:11,042 --> 01:52:15,326
SPEAKER_0:  all have to like work together now to stop this other player from winning or else everybody's gonna lose.

01:52:15,810 --> 01:52:16,350
SPEAKER_0:  Um...

01:52:17,090 --> 01:52:26,430
SPEAKER_0:  And it's kind of like, you know, Game of Thrones, like I don't know if you've seen the show, where like, you know, you got the others coming from the North and like all the people have to start, work out their differences and stop them from taking over.

01:52:27,202 --> 01:52:27,710
SPEAKER_0:  Um.

01:52:28,706 --> 01:52:33,502
SPEAKER_0:  And the bot will do this. The bot will work with the other players to stop the superpower from winning.

01:52:33,986 --> 01:52:34,334
SPEAKER_0:  but...

01:52:34,658 --> 01:52:39,518
SPEAKER_0:  If it doesn't really, if it's trained from scratch, or it doesn't really have a good grounding in how humans approach it,

01:52:39,778 --> 01:52:41,374
SPEAKER_0:  It will also at the same time.

01:52:41,698 --> 01:52:44,094
SPEAKER_0:  attack the other players with its extra units.

01:52:44,386 --> 01:52:51,326
SPEAKER_0:  So all the units that are not necessary to stop the super power from winning, it will use those to grab as many centers as possible from the other players.

01:52:51,778 --> 01:52:52,414
SPEAKER_0:  and

01:52:53,186 --> 01:52:54,718
SPEAKER_0:  In totally rational play

01:52:55,042 --> 01:52:58,110
SPEAKER_0:  The other players should just live with that. You know, they have to understand like, hey-

01:52:58,530 --> 01:53:00,606
SPEAKER_0:  A score of 1 is better than a score of 0.

01:53:00,994 --> 01:53:02,302
SPEAKER_0:  So, so.

01:53:02,658 --> 01:53:03,966
SPEAKER_0:  Okay, he's grabbed my centers.

01:53:04,258 --> 01:53:05,374
SPEAKER_0:  but I'll just deal with it.

01:53:06,114 --> 01:53:08,798
SPEAKER_0:  but humans don't act that way, right? the human

01:53:09,058 --> 01:53:10,270
SPEAKER_0:  gets really angry at the bot.

01:53:10,562 --> 01:53:11,742
SPEAKER_0:  and ends up throwing the game.

01:53:12,322 --> 01:53:12,894
SPEAKER_0:  because...

01:53:13,218 --> 01:53:13,630
SPEAKER_0:  You know?

01:53:13,922 --> 01:53:17,214
SPEAKER_0:  I'm gonna screw you over because you did something that's not fair to me.

01:53:18,882 --> 01:53:22,462
SPEAKER_1:  Got it. And are you supposed to model that? Is the boss supposed to model that kind of...

01:53:22,850 --> 01:53:23,422
SPEAKER_1:  human

01:53:23,906 --> 01:53:24,702
SPEAKER_1:  Frustration.

01:53:24,834 --> 01:53:25,950
SPEAKER_0:  Yeah, exactly. And so.

01:53:26,274 --> 01:53:29,150
SPEAKER_0:  That is something that seems almost impossible to model.

01:53:29,570 --> 01:53:32,286
SPEAKER_0:  purely from scratch without any human data. It's a very cultural thing.

01:53:32,770 --> 01:53:34,526
SPEAKER_0:  And so you need

01:53:35,138 --> 01:53:38,462
SPEAKER_0:  human data to be able to understand that, hey, that's how humans behave.

01:53:38,818 --> 01:53:41,918
SPEAKER_0:  and you have to work around that. It might be suboptimal, it might be irrational.

01:53:42,338 --> 01:53:46,846
SPEAKER_0:  But that's an aspect of humanity that you have to deal with.

01:53:47,298 --> 01:53:52,734
SPEAKER_1:  So how difficult is it to train on human data, given that human data is very limited versus what the us.

01:53:52,962 --> 01:53:54,878
SPEAKER_1:  a purely self-playing mechanism can generate.

01:53:55,426 --> 01:53:58,462
SPEAKER_0:  That's actually one of the major challenges that we faced in the research that

01:53:58,818 --> 01:54:02,718
SPEAKER_0:  We had a good amount of human data. We had about 50,000 games. What we try to do is

01:54:03,362 --> 01:54:05,214
SPEAKER_0:  Leverage as much self-play as possible.

01:54:05,506 --> 01:54:07,902
SPEAKER_0:  while still leveraging the human data.

01:54:08,482 --> 01:54:10,814
SPEAKER_0:  So what we do is we do self play.

01:54:11,426 --> 01:54:13,822
SPEAKER_0:  Very similar to how it's been done in Poker and Go.

01:54:14,274 --> 01:54:14,718
SPEAKER_0:  But...

01:54:15,042 --> 01:54:21,182
SPEAKER_0:  we try to regularize the self-play towards the human data. basically the way to think about it is

01:54:22,274 --> 01:54:22,686
SPEAKER_0:  Um.

01:54:23,106 --> 01:54:23,678
SPEAKER_0:  We-

01:54:24,130 --> 01:54:24,894
SPEAKER_0:  penalize.

01:54:25,378 --> 01:54:25,886
SPEAKER_0:  the bot.

01:54:26,402 --> 01:54:26,974
SPEAKER_0:  for

01:54:27,266 --> 01:54:29,470
SPEAKER_0:  Choosing actions that are very unlikely.

01:54:30,050 --> 01:54:32,062
SPEAKER_0:  under the human data set.

01:54:32,930 --> 01:54:34,055
SPEAKER_0:  and

01:54:34,055 --> 01:54:36,286
SPEAKER_1:  there is some kind of function that says

01:54:36,514 --> 01:54:37,662
SPEAKER_1:  This is Human Like and Not.

01:54:38,242 --> 01:54:41,662
SPEAKER_0:  Yeah, so we train a bot through supervised learning.

01:54:42,018 --> 01:54:45,822
SPEAKER_0:  to model the human play as much as possible. So we basically like train a neural net.

01:54:46,274 --> 01:54:51,262
SPEAKER_0:  on those 50,000 games and that gives us an approximate, that gives us a policy that.

01:54:51,618 --> 01:54:53,822
SPEAKER_0:  resembles to some extent how humans actually play the game.

01:54:54,338 --> 01:54:55,390
SPEAKER_0:  Now this isn't a perfect...

01:54:55,714 --> 01:54:57,022
SPEAKER_0:  model of human play.

01:54:57,282 --> 01:55:00,734
SPEAKER_0:  because we don't have unlimited data. We don't have unlimited neural net capacity.

01:55:01,186 --> 01:55:02,942
SPEAKER_0:  but it gives us some approximation.

01:55:03,298 --> 01:55:07,198
SPEAKER_1:  Is there some data on the internet that's useful besides just diplomacy?

01:55:07,682 --> 01:55:09,566
SPEAKER_1:  So on the language side of things, is there some...

01:55:09,986 --> 01:55:11,198
SPEAKER_1:  Can you go to it like Reddit?

01:55:13,602 --> 01:55:14,078
SPEAKER_1:  Um...

01:55:14,434 --> 01:55:18,430
SPEAKER_1:  So sort of background model formulation that's useful for the game of diplomacy.

01:55:18,626 --> 01:55:20,542
SPEAKER_0:  Yeah, absolutely. And so for the language model, which

01:55:20,930 --> 01:55:25,246
SPEAKER_0:  It's kind of like a separate question. We didn't use the language model during self play training.

01:55:25,474 --> 01:55:26,430
SPEAKER_0:  but we...

01:55:27,330 --> 01:55:28,702
SPEAKER_0:  pre-trains the language model.

01:55:29,058 --> 01:55:30,878
SPEAKER_0:  on tons of internet data.

01:55:31,234 --> 01:55:32,478
SPEAKER_0:  as much as possible.

01:55:32,706 --> 01:55:35,454
SPEAKER_0:  and then we fine tuned it specifically on the diplomacy games.

01:55:35,778 --> 01:55:40,126
SPEAKER_0:  So we are able to leverage the wider data set in order to fill in.

01:55:40,994 --> 01:55:46,686
SPEAKER_0:  some of the gaps in how communication happens more broadly, besides just specifically in these diplomacy games.

01:55:47,106 --> 01:55:48,638
SPEAKER_1:  Okay, cool. So what's some?

01:55:48,994 --> 01:55:51,966
SPEAKER_1:  What are some interesting things that came to life from this work?

01:55:52,706 --> 01:55:54,718
SPEAKER_1:  to you, like what are some insights?

01:55:55,746 --> 01:55:57,662
SPEAKER_1:  about

01:55:58,530 --> 01:56:03,710
SPEAKER_1:  about games where natural language is involved and cooperation, deep cooperation is involved.

01:56:04,418 --> 01:56:07,230
SPEAKER_0:  Well, I think there's a few insights. So first of all.

01:56:07,938 --> 01:56:10,238
SPEAKER_0:  the fact that you can't rely.

01:56:10,882 --> 01:56:16,094
SPEAKER_0:  purely or even largely on self-play, that you really have to have an understanding of how humans approach the game.

01:56:16,546 --> 01:56:17,054
SPEAKER_0:  Um

01:56:17,506 --> 01:56:20,286
SPEAKER_0:  I think that that's one of the major conclusions that I'm drawing from this work.

01:56:20,674 --> 01:56:21,630
SPEAKER_0:  and that

01:56:21,954 --> 01:56:24,766
SPEAKER_0:  is I think applicable more broadly to a lot of different games.

01:56:25,058 --> 01:56:28,414
SPEAKER_0:  So we've actually already taken the approaches that we've used in diplomacy and tried them.

01:56:28,770 --> 01:56:31,486
SPEAKER_0:  on a cooperative card game called Hanabi.

01:56:31,874 --> 01:56:34,078
SPEAKER_0:  And we've had a lot of success in that game as well.

01:56:34,946 --> 01:56:36,638
SPEAKER_0:  Um, on the language side.

01:56:37,730 --> 01:56:38,270
SPEAKER_0:  I think

01:56:38,562 --> 01:56:41,086
SPEAKER_0:  The fact that we were able to control

01:56:41,474 --> 01:56:42,366
SPEAKER_0:  The language model.

01:56:42,786 --> 01:56:45,118
SPEAKER_0:  through this intense approach.

01:56:45,634 --> 01:56:48,414
SPEAKER_0:  was very effective and it allowed us.

01:56:48,738 --> 01:56:52,958
SPEAKER_0:  Instead of just imitating how humans would communicate, we're able to go beyond that and-

01:56:53,282 --> 01:56:54,110
SPEAKER_0:  able to

01:56:54,530 --> 01:56:54,942
SPEAKER_0:  Um.

01:56:55,234 --> 01:56:57,598
SPEAKER_0:  feed into its superhuman strategies.

01:56:57,922 --> 01:56:59,358
SPEAKER_0:  that it can then...

01:57:00,386 --> 01:57:01,822
SPEAKER_0:  generate messages corresponding to.

01:57:02,594 --> 01:57:07,646
SPEAKER_1:  Is there something you could say about detecting whether a person or AI is lying or not?

01:57:09,218 --> 01:57:09,758
SPEAKER_0:  The butt.

01:57:10,530 --> 01:57:12,382
SPEAKER_0:  doesn't explicitly try to

01:57:12,802 --> 01:57:14,558
SPEAKER_0:  calculate whether somebody is lying or not.

01:57:15,170 --> 01:57:16,734
SPEAKER_0:  But what it will do is...

01:57:17,154 --> 01:57:23,038
SPEAKER_0:  to predict what actions they're going to take given the communications, given the messages that they've sent to us.

01:57:23,554 --> 01:57:27,006
SPEAKER_0:  So given our conversation, what do I think you're going to do? And implicitly-

01:57:27,586 --> 01:57:30,398
SPEAKER_0:  There is a calculation about whether you're lying to me in that.

01:57:31,170 --> 01:57:32,574
SPEAKER_0:  You know, if, if you're

01:57:33,122 --> 01:57:35,614
SPEAKER_0:  based on your messages if I think you're going to attack me this turn.

01:57:36,098 --> 01:57:38,174
SPEAKER_0:  Even though your messages say that you're not

01:57:38,402 --> 01:57:41,598
SPEAKER_0:  then essentially the bot is predicting that you're lying.

01:57:42,242 --> 01:57:43,294
SPEAKER_0:  but it doesn't view it.

01:57:43,714 --> 01:57:46,398
SPEAKER_0:  as lying the same way that we would view it as lying.

01:57:47,330 --> 01:57:50,334
SPEAKER_1:  But you could probably reformulate with all the same data.

01:57:51,138 --> 01:57:51,550
SPEAKER_1:  and.

01:57:52,130 --> 01:57:54,014
SPEAKER_1:  make a classifier lying or not.

01:57:54,722 --> 01:58:00,094
SPEAKER_0:  Yeah, I think you could do that. That was not something that we were focused on, but I think that it is possible that-

01:58:00,738 --> 01:58:01,054
SPEAKER_0:  You know.

01:58:01,474 --> 01:58:06,302
SPEAKER_0:  if you came up with some measurements of like, what does it mean to tell a lie? Because there's a spectrum, right? Like...

01:58:06,594 --> 01:58:07,262
SPEAKER_0:  If you're

01:58:07,586 --> 01:58:09,982
SPEAKER_0:  Withholding some information is that a lie?

01:58:10,274 --> 01:58:14,782
SPEAKER_0:  If you mostly telling the truth but you forgot to mention this like one action out of like 10

01:58:15,138 --> 01:58:15,902
SPEAKER_0:  Is that a lie?

01:58:16,258 --> 01:58:19,102
SPEAKER_0:  It's hard to draw the line, but you know if you're willing to do that

01:58:19,394 --> 01:58:21,662
SPEAKER_0:  and then you could possibly use it to...

01:58:22,370 --> 01:58:25,438
SPEAKER_1:  This feels like an argument inside a relationship now.

01:58:25,762 --> 01:58:26,238
SPEAKER_1:  so

01:58:26,498 --> 01:58:27,742
SPEAKER_1:  What constitutes a lie?

01:58:28,386 --> 01:58:29,086
SPEAKER_1:  Um...

01:58:29,538 --> 01:58:33,598
SPEAKER_1:  Depends what you mean by the definition of the word is. OK.

01:58:34,754 --> 01:58:39,198
SPEAKER_1:  Still, it's fascinating because trust and lying is all intermixed into this.

01:58:39,426 --> 01:58:41,470
SPEAKER_1:  and its language models.

01:58:41,730 --> 01:58:46,046
SPEAKER_1:  that are becoming more and more sophisticated. It's just a fascinating space to explore.

01:58:46,338 --> 01:58:46,910
SPEAKER_1:  Um.

01:58:47,906 --> 01:58:51,582
SPEAKER_1:  What do you see as the future of this?

01:58:54,306 --> 01:58:58,014
SPEAKER_1:  is inspired by the breakthrough performance to you getting here with diplomacy.

01:59:00,418 --> 01:59:01,086
SPEAKER_0:  Um...

01:59:01,666 --> 01:59:04,574
SPEAKER_0:  I think there's a few different directions to take this work.

01:59:05,698 --> 01:59:08,542
SPEAKER_0:  I think really what it's showing us is...

01:59:09,186 --> 01:59:14,686
SPEAKER_0:  The potential that language models have, I mean, I think a lot of people didn't think that this kind of result was possible even today.

01:59:15,042 --> 01:59:17,182
SPEAKER_0:  despite all the progress that's been made in language models.

01:59:17,538 --> 01:59:19,422
SPEAKER_0:  And so it shows us how we can.

01:59:19,842 --> 01:59:20,670
SPEAKER_0:  leverage

01:59:20,994 --> 01:59:22,622
SPEAKER_0:  the power of things like self play.

01:59:23,010 --> 01:59:24,670
SPEAKER_0:  on top of language models to get.

01:59:24,898 --> 01:59:25,374
SPEAKER_0:  Um.

01:59:25,666 --> 01:59:27,070
SPEAKER_0:  Increasingly better performance.

01:59:27,522 --> 01:59:29,374
SPEAKER_0:  And the ceiling is really...

01:59:30,274 --> 01:59:31,806
SPEAKER_0:  much higher than what we have right now.

01:59:32,386 --> 01:59:34,846
SPEAKER_1:  Is this transferable somehow to?

01:59:35,522 --> 01:59:36,478
SPEAKER_1:  to chatbots.

01:59:37,154 --> 01:59:39,486
SPEAKER_1:  for the more general task of dialogue.

01:59:41,026 --> 01:59:44,222
SPEAKER_1:  because there is a kind of negotiation here, between

01:59:44,930 --> 01:59:49,406
SPEAKER_1:  entities that are trying to cooperate and at the same time a little bit adversarial.

01:59:49,922 --> 01:59:51,390
SPEAKER_1:  which I think maps...

01:59:51,906 --> 01:59:53,694
SPEAKER_1:  somewhat to the general.

01:59:54,434 --> 01:59:54,782
SPEAKER_1:  You know.

01:59:55,330 --> 01:59:56,510
SPEAKER_1:  the entire process of.

01:59:57,026 --> 01:59:58,078
SPEAKER_1:  Reddit.

01:59:58,338 --> 02:00:01,182
SPEAKER_1:  Or like internet communication, you're cooperating, you're

02:00:01,538 --> 02:00:04,350
SPEAKER_1:  adversarial, you're having debates, you're having uh...

02:00:04,610 --> 02:00:06,014
SPEAKER_1:  camaraderie all that kind of stuff.

02:00:06,786 --> 02:00:12,478
SPEAKER_0:  I think one of the things that's really useful about diplomacy is that we have a well-defined value function.

02:00:12,930 --> 02:00:17,086
SPEAKER_0:  There is a well-defined score that the bot is trying to optimize.

02:00:17,410 --> 02:00:19,774
SPEAKER_0:  in a setting like a general chat bot setting.

02:00:20,610 --> 02:00:22,622
SPEAKER_0:  It needs, it would need that kind of.

02:00:23,074 --> 02:00:27,102
SPEAKER_0:  objective in order to fully leverage the techniques that we've developed.

02:00:28,610 --> 02:00:32,702
SPEAKER_1:  What about like what we talked about earlier with NPCs inside video games?

02:00:33,410 --> 02:00:35,070
SPEAKER_1:  How can it be used to create?

02:00:36,098 --> 02:00:39,390
SPEAKER_1:  for Elder Scrolls 6 more compelling

02:00:40,706 --> 02:00:41,534
SPEAKER_1:  and PCs.

02:00:42,402 --> 02:00:46,430
SPEAKER_1:  that you could talk to instead of committing all kinds of violence with a sword.

02:00:46,690 --> 02:00:47,806
SPEAKER_1:  and fighting dragons.

02:00:48,066 --> 02:00:51,070
SPEAKER_1:  just sitting in a tavern and drink all day and talk to the chat bot.

02:00:51,618 --> 02:00:55,838
SPEAKER_0:  The way that we've approached AI and diplomacy is you condition the language on an intent.

02:00:56,418 --> 02:00:58,974
SPEAKER_0:  Now that intent in diplomacy is an action.

02:00:59,394 --> 02:01:01,502
SPEAKER_0:  but it doesn't have to be and you can imagine.

02:01:01,922 --> 02:01:02,430
SPEAKER_0:  You know.

02:01:02,690 --> 02:01:04,062
SPEAKER_0:  You could have NPCs.

02:01:04,354 --> 02:01:06,366
SPEAKER_0:  in video games or the metaverse or whatever.

02:01:06,722 --> 02:01:07,294
SPEAKER_0:  where

02:01:07,778 --> 02:01:11,934
SPEAKER_0:  there's some intent or there's some objective that they're trying to maximize and you can specify what that is.

02:01:12,354 --> 02:01:12,830
SPEAKER_0:  Um...

02:01:13,218 --> 02:01:15,102
SPEAKER_0:  and then the language.

02:01:15,618 --> 02:01:17,182
SPEAKER_0:  can correspond to that intent.

02:01:17,506 --> 02:01:23,422
SPEAKER_0:  Now, I'm not saying that this is happening imminently, but I'm saying that this is like a future application potentially of

02:01:23,714 --> 02:01:24,958
SPEAKER_0:  this direction of research.

02:01:25,090 --> 02:01:27,486
SPEAKER_1:  So what's the more general formulation of this?

02:01:27,874 --> 02:01:32,350
SPEAKER_1:  making self-play be able to scale the way self-play does and still maintain human-like.

02:01:32,578 --> 02:01:33,022
SPEAKER_1:  behavior.

02:01:33,826 --> 02:01:34,782
SPEAKER_0:  The way that we've...

02:01:35,362 --> 02:01:37,918
SPEAKER_0:  approach self-play in diplomacy is like.

02:01:38,850 --> 02:01:40,510
SPEAKER_0:  We're trying to...

02:01:41,378 --> 02:01:45,182
SPEAKER_0:  come up with good intents to condition the language model on. In the space of intents.

02:01:45,602 --> 02:01:47,678
SPEAKER_0:  is actions that can be played in the game.

02:01:48,002 --> 02:01:51,198
SPEAKER_0:  Now there is like the potential to have a broader set of intents.

02:01:51,490 --> 02:01:52,094
SPEAKER_0:  Things like.

02:01:52,770 --> 02:01:57,502
SPEAKER_0:  you know long-term cooperation or long-term objectives or

02:01:57,858 --> 02:02:00,446
SPEAKER_0:  you know gossip about what another player was saying

02:02:00,802 --> 02:02:05,022
SPEAKER_0:  These are things that we're currently not conditioning the language model on and so it's not able

02:02:05,346 --> 02:02:09,406
SPEAKER_0:  we're not able to control it to say like, Oh, you should be talking about this thing right now.

02:02:10,626 --> 02:02:13,662
SPEAKER_0:  it's quite possible that you could expand the scope of intents.

02:02:13,890 --> 02:02:15,902
SPEAKER_0:  to be able to allow it to talk about those things.

02:02:16,130 --> 02:02:17,630
SPEAKER_0:  Now in the process of doing that.

02:02:17,890 --> 02:02:20,062
SPEAKER_0:  the self-play would become much more complicated.

02:02:20,482 --> 02:02:20,894
SPEAKER_0:  Um.

02:02:21,634 --> 02:02:23,806
SPEAKER_0:  And so that is a potential for future work.

02:02:23,938 --> 02:02:27,614
SPEAKER_1:  Okay, they're increasing the number of intents. I still am not quite clear.

02:02:28,386 --> 02:02:29,214
SPEAKER_1:  how you keep.

02:02:29,794 --> 02:02:30,942
SPEAKER_1:  the self-play.

02:02:32,418 --> 02:02:34,430
SPEAKER_1:  integrated into the human world.

02:02:34,978 --> 02:02:37,790
SPEAKER_1:  I'm a little bit loose on the...

02:02:38,050 --> 02:02:39,294
SPEAKER_1:  on understanding how you do that.

02:02:39,426 --> 02:02:41,310
SPEAKER_0:  So we train a neural nets to.

02:02:41,794 --> 02:02:44,318
SPEAKER_0:  imitate the human data as closely as possible.

02:02:44,674 --> 02:02:46,398
SPEAKER_0:  And that's what we call the anchor policy.

02:02:47,010 --> 02:02:48,606
SPEAKER_0:  And now we're doing self play.

02:02:49,346 --> 02:02:51,454
SPEAKER_0:  The problem with the anchor policy is that...

02:02:51,778 --> 02:02:52,382
SPEAKER_0:  It's not.

02:02:52,610 --> 02:02:57,598
SPEAKER_0:  perfect approximation of how humans actually play. Because we don't have infinite data, because we don't have

02:02:57,922 --> 02:03:00,030
SPEAKER_0:  and limited neural network capacity.

02:03:00,258 --> 02:03:04,254
SPEAKER_0:  It's actually a relatively suboptimal approximation of how humans actually play.

02:03:04,706 --> 02:03:06,526
SPEAKER_0:  and we can improve that approximation.

02:03:06,914 --> 02:03:08,222
SPEAKER_0:  by adding planning.

02:03:08,450 --> 02:03:09,566
SPEAKER_0:  and RL.

02:03:10,242 --> 02:03:12,030
SPEAKER_0:  And so what we do is...

02:03:12,354 --> 02:03:15,326
SPEAKER_0:  We get a better approximation, a better model of human play.

02:03:15,778 --> 02:03:16,318
SPEAKER_0:  Bye.

02:03:17,122 --> 02:03:18,686
SPEAKER_0:  during the self play process.

02:03:19,106 --> 02:03:19,902
SPEAKER_0:  We say...

02:03:20,162 --> 02:03:22,174
SPEAKER_0:  you can deviate from this.

02:03:22,402 --> 02:03:23,646
SPEAKER_0:  human anchor policy.

02:03:24,098 --> 02:03:25,566
SPEAKER_0:  if there is an action that has

02:03:25,954 --> 02:03:28,126
SPEAKER_0:  you know, particularly high expected value.

02:03:28,546 --> 02:03:29,054
SPEAKER_0:  Um

02:03:29,282 --> 02:03:29,598
SPEAKER_0:  But.

02:03:30,306 --> 02:03:35,582
SPEAKER_0:  it would have to be a really high expected value in order to deviate from this human-like policy.

02:03:36,290 --> 02:03:40,670
SPEAKER_0:  So you basically say, try to maximize your expected value while at the same time,

02:03:40,962 --> 02:03:43,422
SPEAKER_0:  stay as close as possible to the human policy.

02:03:44,098 --> 02:03:45,182
SPEAKER_0:  and there is a parameter.

02:03:45,506 --> 02:03:46,398
SPEAKER_0:  that controls.

02:03:46,626 --> 02:03:46,942
SPEAKER_0:  Those

02:03:47,266 --> 02:03:49,918
SPEAKER_0:  the relative weighting of those competing objectives.

02:03:50,722 --> 02:03:51,998
SPEAKER_1:  So the question I have.

02:03:52,642 --> 02:03:55,198
SPEAKER_1:  is how sophisticated can the anchor policy get?

02:03:56,578 --> 02:03:59,102
SPEAKER_1:  So have a policy that approximates human behavior.

02:03:59,362 --> 02:03:59,678
SPEAKER_1:  Right?

02:04:00,706 --> 02:04:01,278
SPEAKER_1:  So...

02:04:01,602 --> 02:04:04,862
SPEAKER_1:  as you increase the number of intents, as you generalize.

02:04:05,634 --> 02:04:07,742
SPEAKER_1:  the space in which this is applicable.

02:04:08,354 --> 02:04:12,606
SPEAKER_1:  And given that the human data is limited, try to anticipate.

02:04:13,154 --> 02:04:17,342
SPEAKER_1:  A policy that works for in much larger number of cases.

02:04:17,826 --> 02:04:21,758
SPEAKER_1:  Like how difficult is the process of forming a damn good anchor policy?

02:04:22,626 --> 02:04:24,894
SPEAKER_0:  Well, it really comes down to how much human data you have.

02:04:25,346 --> 02:04:27,070
SPEAKER_1:  So it's all bar scaling the human data.

02:04:27,586 --> 02:04:34,238
SPEAKER_0:  I think the more human data you have, the better. And I think that that's going to be the major bottleneck in scaling to

02:04:34,754 --> 02:04:37,726
SPEAKER_0:  more complicated domains. With that said, I hope you enjoyed this video.

02:04:38,338 --> 02:04:41,854
SPEAKER_0:  You know, there might be the potential, just like in the language model where we leveraged.

02:04:42,210 --> 02:04:45,822
SPEAKER_0:  you know, tons of data on the internet and then specialized it for diplomacy.

02:04:46,306 --> 02:04:50,398
SPEAKER_0:  There is the future potential that you can leverage huge amounts of data across the board.

02:04:50,850 --> 02:04:51,390
SPEAKER_0:  And then...

02:04:51,714 --> 02:04:52,542
SPEAKER_0:  Specialize it.

02:04:52,866 --> 02:04:57,438
SPEAKER_0:  in the data set that you have for diplomacy. And that way you're essentially augmenting the amount of data that you have.

02:04:58,626 --> 02:05:00,382
SPEAKER_1:  To what degree does this apply?

02:05:01,506 --> 02:05:02,142
SPEAKER_1:  Shoo!

02:05:02,530 --> 02:05:03,518
SPEAKER_1:  The General.

02:05:04,034 --> 02:05:05,182
SPEAKER_1:  the real world.

02:05:05,410 --> 02:05:06,206
SPEAKER_1:  diplomacy.

02:05:06,722 --> 02:05:07,838
SPEAKER_1:  the geopolitics.

02:05:08,514 --> 02:05:12,830
SPEAKER_1:  You know, there's a game theory has a history of being applied to understand.

02:05:13,154 --> 02:05:17,598
SPEAKER_1:  and to give us hope about nuclear weapons, for example, the mutually assured destruction.

02:05:17,858 --> 02:05:20,638
SPEAKER_1:  is a game theoretic concept that you can formulate.

02:05:21,090 --> 02:05:24,542
SPEAKER_1:  Some people say it's oversimplified, but nevertheless, here we are.

02:05:24,834 --> 02:05:26,814
SPEAKER_1:  and we somehow haven't blown ourselves up.

02:05:27,362 --> 02:05:28,830
SPEAKER_1:  Do you see a future where...

02:05:29,666 --> 02:05:30,558
SPEAKER_1:  this kind of

02:05:31,362 --> 02:05:32,510
SPEAKER_1:  this kind of system.

02:05:32,994 --> 02:05:34,654
SPEAKER_1:  can be used to help us make.

02:05:35,202 --> 02:05:37,534
SPEAKER_1:  geopolitical decisions in the world.

02:05:38,978 --> 02:05:45,758
SPEAKER_0:  Well, like I said, the original motivation for the game of diplomacy was the failures of World War I. The diplomatic failures that led to war.

02:05:46,338 --> 02:05:50,110
SPEAKER_0:  And the real take-home message of diplomacy is that

02:05:50,562 --> 02:05:51,262
SPEAKER_0:  You know, if...

02:05:51,906 --> 02:05:53,630
SPEAKER_0:  people approach diplomacy the right way.

02:05:54,018 --> 02:05:54,622
SPEAKER_0:  then

02:05:55,138 --> 02:05:57,182
SPEAKER_0:  war is ultimately unsuccessful.

02:05:57,794 --> 02:05:58,206
SPEAKER_0:  Um...

02:05:58,434 --> 02:05:59,710
SPEAKER_0:  The way that I see it war is...

02:06:00,418 --> 02:06:03,390
SPEAKER_0:  an inherently negative sum game, right? There's always a better outcome.

02:06:03,778 --> 02:06:05,406
SPEAKER_0:  than war for all the parties involved.

02:06:06,018 --> 02:06:06,622
SPEAKER_0:  and

02:06:07,202 --> 02:06:08,350
SPEAKER_0:  My hope is that...

02:06:08,674 --> 02:06:12,254
SPEAKER_0:  you know as a i progresses then maybe this technology could be used

02:06:12,514 --> 02:06:13,182
SPEAKER_0:  to help.

02:06:13,666 --> 02:06:15,134
SPEAKER_0:  People make better decisions.

02:06:15,522 --> 02:06:16,638
SPEAKER_0:  across the board.

02:06:16,994 --> 02:06:18,974
SPEAKER_0:  and hopefully avoid...

02:06:19,234 --> 02:06:20,606
SPEAKER_0:  negative some outcomes like war.

02:06:21,346 --> 02:06:25,278
SPEAKER_1:  Yeah, I would look, I mean, I just came back from Ukraine. I'm going back there.

02:06:25,794 --> 02:06:26,398
SPEAKER_1:  and

02:06:26,690 --> 02:06:28,286
SPEAKER_1:  deep personal levels.

02:06:28,994 --> 02:06:30,174
SPEAKER_1:  think a lot about.

02:06:32,546 --> 02:06:36,382
SPEAKER_1:  how peace can be achieved. And I'm a big believer in conversation.

02:06:37,058 --> 02:06:41,406
SPEAKER_1:  leaders getting together and having conversations and trying to understand each other.

02:06:42,594 --> 02:06:44,350
SPEAKER_1:  Yeah, it's fascinating to think, um...

02:06:44,866 --> 02:06:47,742
SPEAKER_1:  whether each one of those leaders can run a simulation ahead of time.

02:06:48,322 --> 02:06:49,822
SPEAKER_1:  Like if I'm an asshole.

02:06:51,586 --> 02:06:55,550
SPEAKER_1:  What are the possible consequences? If I'm nice, what are the possible consequences?

02:06:56,066 --> 02:06:56,542
SPEAKER_1:  Um.

02:06:56,770 --> 02:06:57,598
SPEAKER_1:  My guess.

02:06:58,946 --> 02:07:02,782
SPEAKER_1:  is that if the president of the United States got together with

02:07:03,586 --> 02:07:06,206
SPEAKER_1:  Vladimir Zelensky and Vladimir Putin.

02:07:06,434 --> 02:07:09,502
SPEAKER_1:  that there will be significant benefits.

02:07:10,306 --> 02:07:10,782
SPEAKER_1:  to.

02:07:11,042 --> 02:07:11,518
SPEAKER_1:  Um

02:07:11,970 --> 02:07:14,142
SPEAKER_1:  the president of the United States not having the ego.

02:07:14,978 --> 02:07:15,678
SPEAKER_1:  of kind of.

02:07:16,386 --> 02:07:18,974
SPEAKER_1:  playing down of giving away a lot of chips.

02:07:19,458 --> 02:07:21,886
SPEAKER_1:  for the future success of the world.

02:07:22,274 --> 02:07:25,726
SPEAKER_1:  So giving a lot of power to the two presidents of the competing nations.

02:07:26,114 --> 02:07:27,134
SPEAKER_1:  to achieve peace.

02:07:27,682 --> 02:07:31,166
SPEAKER_1:  Um, that's my guess, but it'd be nice to run a bunch of simulations.

02:07:31,682 --> 02:07:38,718
SPEAKER_1:  but then you have to have human data, right? You really, because it's like the game with diplomacy is fundamentally different than geopolitics. You need data.

02:07:39,042 --> 02:07:39,902
SPEAKER_1:  Any like.

02:07:40,578 --> 02:07:44,286
SPEAKER_1:  I guess that's the question I have, like how transferable is this to uh...

02:07:45,026 --> 02:07:46,718
SPEAKER_1:  Like, I don't know, any kind of negotiation.

02:07:47,618 --> 02:07:49,374
SPEAKER_1:  like to any kind of local, some local.

02:07:49,954 --> 02:07:51,102
SPEAKER_1:  I don't know, a bunch of lawyers.

02:07:51,426 --> 02:07:53,918
SPEAKER_1:  like arguing, like at a divorce.

02:07:54,242 --> 02:07:58,494
SPEAKER_1:  like divorce lawyers, like how transferable this all kinds of human negotiation.

02:07:58,850 --> 02:08:02,462
SPEAKER_0:  Well, I feel like this isn't a question that's unique to diplomacy. I mean, I think you look at-

02:08:02,914 --> 02:08:08,862
SPEAKER_0:  RL breakthroughs, reinforcement learning breakthroughs in previous games as well, like AI for StarCraft, AI for Atari.

02:08:09,218 --> 02:08:11,486
SPEAKER_0:  You haven't really seen it deployed in the real world.

02:08:11,842 --> 02:08:15,614
SPEAKER_0:  because you have these problems of it's really hard to collect a lot of data.

02:08:16,066 --> 02:08:18,302
SPEAKER_0:  Um, and you don't have.

02:08:18,562 --> 02:08:19,006
SPEAKER_0:  And...

02:08:19,714 --> 02:08:23,166
SPEAKER_0:  You don't have a well-defined action space. You don't have a well-defined reward function.

02:08:23,426 --> 02:08:25,278
SPEAKER_0:  These are all things that you really need.

02:08:25,634 --> 02:08:29,054
SPEAKER_0:  for reinforcement learning and planning to be really successful today.

02:08:29,474 --> 02:08:31,870
SPEAKER_0:  Now there are some domains where you do have that.

02:08:32,290 --> 02:08:34,590
SPEAKER_0:  Um, code generation is one example.

02:08:34,946 --> 02:08:40,510
SPEAKER_0:  Theorem proving mathematics, that's another example where you have a well defined action space, you have a well defined reward function.

02:08:40,962 --> 02:08:41,502
SPEAKER_0:  And.

02:08:41,730 --> 02:08:43,678
SPEAKER_0:  Those are the kinds of domains where I can see.

02:08:43,970 --> 02:08:45,214
SPEAKER_0:  RL in the short term.

02:08:45,602 --> 02:08:46,910
SPEAKER_0:  being incredibly powerful.

02:08:47,394 --> 02:08:47,806
SPEAKER_0:  Um.

02:08:49,250 --> 02:08:53,534
SPEAKER_0:  Yeah, I think that those are the barriers to deploying this at scale in the real world, but

02:08:54,114 --> 02:08:56,158
SPEAKER_0:  The hope is that in the long run we'll be able to get there.

02:08:57,122 --> 02:08:58,910
SPEAKER_1:  Yeah, but see diplomacy feels like.

02:08:59,170 --> 02:09:01,886
SPEAKER_1:  closer to the real world than does StarCraft.

02:09:02,626 --> 02:09:12,126
SPEAKER_1:  Like, because it's natural language, right? You're operating in the space of intents and in the space of natural language. That feels very close to the real world. It also feels like you could get data on that.

02:09:12,962 --> 02:09:13,758
SPEAKER_1:  from the internet.

02:09:14,786 --> 02:09:16,926
SPEAKER_0:  Yeah, and that's why I do think that diplomacy...

02:09:17,410 --> 02:09:19,038
SPEAKER_0:  is taking a big step.

02:09:19,362 --> 02:09:22,974
SPEAKER_0:  closer to the real world than anything that's came before in terms of game AI breakthroughs.

02:09:23,234 --> 02:09:23,998
SPEAKER_0:  The fact that

02:09:24,930 --> 02:09:25,854
SPEAKER_0:  You know, we're...

02:09:26,082 --> 02:09:28,798
SPEAKER_0:  communicating in natural language we've

02:09:29,218 --> 02:09:34,942
SPEAKER_0:  we're leveraging the fact that we have this like general dataset of dialogue and communication.

02:09:35,394 --> 02:09:36,958
SPEAKER_0:  from a breadth of the internet.

02:09:37,378 --> 02:09:41,150
SPEAKER_0:  That is a big step in that direction. We're not 100% there.

02:09:41,506 --> 02:09:43,166
SPEAKER_0:  but we're getting closer at least.

02:09:44,066 --> 02:09:46,846
SPEAKER_1:  So if we actually return back to poker and chess...

02:09:47,362 --> 02:09:49,822
SPEAKER_1:  are some of the ideas that you're learning here with diplomacy.

02:09:50,402 --> 02:09:54,270
SPEAKER_1:  Could you construct AI systems that play like humans?

02:09:55,106 --> 02:09:55,838
SPEAKER_1:  Like, um...

02:09:56,578 --> 02:09:57,950
SPEAKER_1:  Make for a fun opponent.

02:09:58,690 --> 02:09:59,774
SPEAKER_1:  in a game of chess.

02:10:00,418 --> 02:10:06,494
SPEAKER_0:  Yeah, absolutely. We've already started looking into this direction a bit, so we tried to use the techniques that we've developed for diplomacy.

02:10:06,882 --> 02:10:11,134
SPEAKER_0:  to make Chess and Go AIs. And what we found is that it led to

02:10:11,490 --> 02:10:12,830
SPEAKER_0:  much more human-like.

02:10:13,122 --> 02:10:13,598
SPEAKER_0:  strong

02:10:13,986 --> 02:10:14,974
SPEAKER_0:  Chess and Go players.

02:10:15,682 --> 02:10:16,542
SPEAKER_0:  The way that...

02:10:17,474 --> 02:10:19,038
SPEAKER_0:  AIs like stockfish today.

02:10:19,330 --> 02:10:24,414
SPEAKER_0:  is in a very inhuman style. It's very strong, but it's very different from how humans play.

02:10:25,122 --> 02:10:29,374
SPEAKER_0:  And so we can take the techniques that we've developed for diplomacy, we do something similar.

02:10:29,762 --> 02:10:30,590
SPEAKER_0:  And, um.

02:10:30,882 --> 02:10:31,902
SPEAKER_0:  and chess and go.

02:10:32,418 --> 02:10:36,286
SPEAKER_0:  and we end up with a bot that's both strong and human-like.

02:10:37,410 --> 02:10:37,822
SPEAKER_0:  Um.

02:10:38,306 --> 02:10:39,742
SPEAKER_0:  To elaborate on this a bit, like...

02:10:40,194 --> 02:10:41,598
SPEAKER_0:  One way to approach

02:10:41,986 --> 02:10:44,414
SPEAKER_0:  Making a human-like AI for chess?

02:10:44,802 --> 02:10:45,246
SPEAKER_0:  is

02:10:45,474 --> 02:10:49,246
SPEAKER_0:  to collect a bunch of human games, like a bunch of human grand master games.

02:10:49,602 --> 02:10:51,710
SPEAKER_0:  and just to supervise learning on those games.

02:10:52,258 --> 02:10:56,894
SPEAKER_0:  But the problem is that if you do that, what you end up with is an AI that's substantially weaker.

02:10:57,250 --> 02:10:59,198
SPEAKER_0:  than the human grandmasters that you trained on.

02:10:59,810 --> 02:11:00,766
SPEAKER_0:  because the neural net.

02:11:01,186 --> 02:11:03,326
SPEAKER_0:  is not able to approximate.

02:11:03,586 --> 02:11:10,206
SPEAKER_0:  the nuance of the strategy. This goes back to the planning thing that I mentioned, the search thing that I talked about before.

02:11:11,074 --> 02:11:14,494
SPEAKER_0:  These human grandmasters when they're playing, they're using search and they're using.

02:11:14,818 --> 02:11:15,198
SPEAKER_0:  planning.

02:11:15,586 --> 02:11:21,630
SPEAKER_0:  And the neural net alone, unless you have a massive neural net that's like a thousand times bigger than what we have right now.

02:11:21,858 --> 02:11:23,262
SPEAKER_0:  It's not able to...

02:11:23,586 --> 02:11:25,598
SPEAKER_0:  approximate those details very effectively.

02:11:26,594 --> 02:11:27,198
SPEAKER_0:  and

02:11:27,682 --> 02:11:30,110
SPEAKER_0:  On the other hand, you can leverage.

02:11:30,434 --> 02:11:32,126
SPEAKER_0:  search and planning very heavily.

02:11:32,514 --> 02:11:36,862
SPEAKER_0:  But then what you end up with is an AI that plays in a very different style from how humans play the game.

02:11:37,762 --> 02:11:40,734
SPEAKER_0:  Now if you strike this intermediate balance by setting...

02:11:41,122 --> 02:11:46,494
SPEAKER_0:  the regularization parameters correctly and say you can do planning but try to keep it close to the human policy.

02:11:46,946 --> 02:11:47,870
SPEAKER_0:  Then you end up...

02:11:48,130 --> 02:11:48,542
SPEAKER_0:  with.

02:11:48,770 --> 02:11:51,294
SPEAKER_0:  an AI that plays in both a very human-like style

02:11:51,586 --> 02:11:52,734
SPEAKER_0:  and a very strong.

02:11:53,090 --> 02:11:53,630
SPEAKER_0:  style.

02:11:54,114 --> 02:11:55,742
SPEAKER_0:  and you can actually even tune it.

02:11:56,610 --> 02:11:57,886
SPEAKER_0:  have a certain ELO rating.

02:11:58,402 --> 02:12:01,310
SPEAKER_0:  So you can say, playing the style of like a 2800 ELO human.

02:12:01,794 --> 02:12:02,270
SPEAKER_0:  Um...

02:12:02,498 --> 02:12:06,174
SPEAKER_1:  I wonder if you could do specific type of humans or categories of humans.

02:12:06,626 --> 02:12:08,702
SPEAKER_1:  Not just skill, but style.

02:12:09,538 --> 02:12:11,934
SPEAKER_0:  Yeah, I think so. And so this is this is where the

02:12:12,418 --> 02:12:13,982
SPEAKER_0:  the research gets interesting.

02:12:14,402 --> 02:12:16,318
SPEAKER_0:  You know, one of the things that I was thinking about is...

02:12:16,802 --> 02:12:20,766
SPEAKER_0:  And this is actually already being done. There's a researcher at the University of Toronto that's working on this.

02:12:21,154 --> 02:12:24,478
SPEAKER_0:  is to make an AI that plays in the style of a particular

02:12:24,802 --> 02:12:25,118
SPEAKER_0:  player.

02:12:25,506 --> 02:12:29,278
SPEAKER_0:  like Magnus Carlsen for example, you can make an AI that plays like Magnus Carlsen.

02:12:29,698 --> 02:12:34,750
SPEAKER_0:  And then where I think this gets interesting is like, maybe you're up against Magnus Carlsen in the World Championship or something.

02:12:35,202 --> 02:12:35,774
SPEAKER_0:  You can.

02:12:36,226 --> 02:12:38,462
SPEAKER_0:  against this Magnus Carlsen bot to prepare.

02:12:38,722 --> 02:12:42,398
SPEAKER_0:  against the real Magnus Carlsen and you can try to explore strategies.

02:12:43,138 --> 02:12:44,222
SPEAKER_0:  he might struggle with.

02:12:44,642 --> 02:12:47,742
SPEAKER_0:  and try to figure out how do you beat this player in particular.

02:12:48,482 --> 02:12:53,406
SPEAKER_0:  Um, on the other hand, you can also have Mac is Carlson working with this spot to try to figure out where he's weak.

02:12:53,730 --> 02:12:56,126
SPEAKER_0:  and where he needs to improve his strategy.

02:12:56,642 --> 02:12:57,150
SPEAKER_0:  Um...

02:12:57,602 --> 02:12:59,230
SPEAKER_0:  And so I can envision this future.

02:12:59,586 --> 02:13:00,126
SPEAKER_0:  where

02:13:00,834 --> 02:13:05,982
SPEAKER_0:  Data on specific chess and Go players becomes extremely valuable because you can use that data.

02:13:06,370 --> 02:13:06,974
SPEAKER_0:  to create.

02:13:07,202 --> 02:13:09,726
SPEAKER_0:  specific models of how these particular players play.

02:13:10,082 --> 02:13:14,078
SPEAKER_1:  So increasingly human-like behavior in bots, however,

02:13:14,306 --> 02:13:16,222
SPEAKER_1:  as you've mentioned, makes cheating.

02:13:17,154 --> 02:13:18,558
SPEAKER_1:  cheat detection much harder.

02:13:19,042 --> 02:13:21,694
SPEAKER_0:  It does, yeah. The way that sheet detection works.

02:13:22,210 --> 02:13:24,414
SPEAKER_0:  in a game like poker and a game like...

02:13:24,642 --> 02:13:26,526
SPEAKER_0:  chess and go from what I understand is.

02:13:26,914 --> 02:13:29,374
SPEAKER_0:  trying to see like is this person making...

02:13:29,730 --> 02:13:30,398
SPEAKER_0:  moves that

02:13:30,722 --> 02:13:35,454
SPEAKER_0:  are very common among chess AIs, or AIs in general.

02:13:35,810 --> 02:13:36,286
SPEAKER_0:  Um...

02:13:36,546 --> 02:13:37,118
SPEAKER_0:  but very.

02:13:37,570 --> 02:13:40,062
SPEAKER_0:  uncommon among top human players.

02:13:40,706 --> 02:13:41,150
SPEAKER_0:  and

02:13:41,378 --> 02:13:42,398
SPEAKER_0:  If you have...

02:13:42,626 --> 02:13:44,766
SPEAKER_0:  development of these AIs that play

02:13:45,090 --> 02:13:47,902
SPEAKER_0:  in a very strong style, but also a very human-like style.

02:13:48,322 --> 02:13:51,038
SPEAKER_0:  then that poses serious challenges for cheat detection.

02:13:51,298 --> 02:13:52,446
SPEAKER_1:  and it makes you...

02:13:52,674 --> 02:13:56,414
SPEAKER_1:  Now ask yourself a hard question about what is the role of AI systems.

02:13:56,770 --> 02:13:59,230
SPEAKER_1:  as they become more and more integrated in our society.

02:13:59,842 --> 02:14:01,854
SPEAKER_1:  and this kind of human AI.

02:14:02,562 --> 02:14:03,166
SPEAKER_1:  Um...

02:14:03,906 --> 02:14:04,958
SPEAKER_1:  integration.

02:14:05,346 --> 02:14:06,366
SPEAKER_1:  has some.

02:14:06,978 --> 02:14:08,318
SPEAKER_1:  deep ethical issues.

02:14:08,578 --> 02:14:09,566
SPEAKER_1:  that we should be aware of.

02:14:09,794 --> 02:14:12,798
SPEAKER_1:  And also it's a kind of cybersecurity challenge, right?

02:14:13,282 --> 02:14:16,862
SPEAKER_1:  to make one of the assumptions we have when we play games.

02:14:17,122 --> 02:14:19,358
SPEAKER_1:  is that there's a trust that it's only humans involved.

02:14:20,162 --> 02:14:21,310
SPEAKER_1:  and there.

02:14:22,594 --> 02:14:31,966
SPEAKER_1:  The better AI systems to create, which makes it super exciting. Human like AI systems with different styles of humans is really exciting, but then we have to have the defenses.

02:14:32,194 --> 02:14:33,246
SPEAKER_1:  better and better and better.

02:14:33,634 --> 02:14:35,454
SPEAKER_1:  if we're to trust that we are.

02:14:35,938 --> 02:14:38,238
SPEAKER_1:  can enjoy human versus human game.

02:14:39,458 --> 02:14:42,110
SPEAKER_1:  deeply fair way. It's fascinating. It's just.

02:14:42,914 --> 02:14:43,678
SPEAKER_1:  It's humbling.

02:14:44,386 --> 02:14:48,702
SPEAKER_0:  Yeah, I think there is a lot of negative potential for this kind of technology, but...

02:14:48,994 --> 02:14:51,358
SPEAKER_0:  You know, at the same time, there's a lot of upside for it as well.

02:14:51,746 --> 02:14:52,190
SPEAKER_0:  So...

02:14:52,546 --> 02:14:58,430
SPEAKER_0:  You know, for example, right now it's really hard to learn how to get better in games like chess and poker and go because

02:14:58,850 --> 02:15:02,046
SPEAKER_0:  The way that the AI plays is so foreign and incomprehensible.

02:15:02,338 --> 02:15:04,254
SPEAKER_0:  But if you have these AIs that are playing...

02:15:04,610 --> 02:15:07,998
SPEAKER_0:  You know, you can say like, I'm a 2000 Elo human, how do I get to 2200?

02:15:08,290 --> 02:15:12,222
SPEAKER_0:  Now you can have an AI that plays in the style of a 2200 ELO human.

02:15:12,514 --> 02:15:13,822
SPEAKER_0:  and that will help you get better.

02:15:14,210 --> 02:15:14,750
SPEAKER_0:  or

02:15:15,138 --> 02:15:21,758
SPEAKER_0:  You know, you mentioned this problem of like, how do you know that you're actually playing with humans when you're playing like online and in video games?

02:15:22,082 --> 02:15:24,574
SPEAKER_0:  Well, now we have the potential of populating.

02:15:24,834 --> 02:15:26,398
SPEAKER_0:  these like virtual worlds.

02:15:26,626 --> 02:15:27,166
SPEAKER_0:  with

02:15:27,522 --> 02:15:31,838
SPEAKER_0:  uh... agents the agents that are actually fun to play with you don't have to

02:15:32,226 --> 02:15:35,486
SPEAKER_0:  always be playing with other humans to have a fun time.

02:15:36,418 --> 02:15:41,054
SPEAKER_0:  So yeah, a lot of upside potential too. And I think, you know, with any sort of tool, there's the-

02:15:41,570 --> 02:15:44,318
SPEAKER_0:  potential for a lot of greatness and a lot of downsides as well.

02:15:44,642 --> 02:15:48,350
SPEAKER_1:  So in the paper that I got a chance to look at, there's a section on

02:15:48,994 --> 02:15:53,406
SPEAKER_1:  ethical considerations. What's in that section? What are some ethical considerations here?

02:15:53,634 --> 02:15:55,198
SPEAKER_1:  Is it some of the stuff we already talked about?

02:15:55,842 --> 02:15:58,110
SPEAKER_0:  There's some things that we've already talked about, I think.

02:15:58,882 --> 02:15:59,998
SPEAKER_0:  specific to...

02:16:00,226 --> 02:16:02,622
SPEAKER_0:  Diplomacy, you know, there's also the...

02:16:02,914 --> 02:16:04,222
SPEAKER_0:  the challenge that the game is.

02:16:05,026 --> 02:16:06,686
SPEAKER_0:  You know, there is a deception aspect to the game.

02:16:07,010 --> 02:16:08,158
SPEAKER_0:  And so.

02:16:09,122 --> 02:16:09,566
SPEAKER_0:  You know.

02:16:09,890 --> 02:16:11,678
SPEAKER_0:  have developing language models that

02:16:11,970 --> 02:16:15,262
SPEAKER_0:  are capable of deception is I think a dicey issue and something that

02:16:15,938 --> 02:16:18,654
SPEAKER_0:  you know, makes research on diplomacy particularly challenging.

02:16:19,106 --> 02:16:19,614
SPEAKER_0:  Um...

02:16:20,354 --> 02:16:21,022
SPEAKER_0:  and

02:16:22,082 --> 02:16:24,158
SPEAKER_0:  You know, so those kinds of issues of like...

02:16:24,610 --> 02:16:29,822
SPEAKER_0:  Should we even be developing AIs that are capable of lying to people? That's something that we have to think carefully about.

02:16:30,114 --> 02:16:37,950
SPEAKER_1:  Uh, that's so cool. I mean, you have to do that kind of stuff in order to figure out where the ethical lines are, but I can see in the future it being illegal to have a

02:16:38,530 --> 02:16:40,158
SPEAKER_1:  consumer product that.

02:16:41,058 --> 02:16:41,726
SPEAKER_1:  Lies.

02:16:43,010 --> 02:16:52,958
SPEAKER_1:  Yeah. Yeah. Like your personal assistant AI system is not a lot is always have to tell the truth. But if, if I ask it, do I, do I look, did I get fatter over the past month?

02:16:53,442 --> 02:16:55,742
SPEAKER_1:  I sure as hell want that AI system to lie to me.

02:16:56,642 --> 02:17:00,894
SPEAKER_1:  So there's a trade-off between lying and being nice.

02:17:01,314 --> 02:17:02,430
SPEAKER_1:  I have to somehow find.

02:17:03,938 --> 02:17:08,382
SPEAKER_1:  What is the ethics in that? And we're back to discussions inside relationships. Anyway, what were you saying?

02:17:08,610 --> 02:17:16,478
SPEAKER_0:  I was thinking like, yeah, that's kind of going to the question of like, what is a lie? You know, is a white lie a bad lie? Is it an ethical lie? You know, those kinds of questions.

02:17:16,930 --> 02:17:17,342
SPEAKER_1:  Uh...

02:17:17,762 --> 02:17:26,046
SPEAKER_1:  Boy, we return time and time again to deep human questions as we design AI systems. That's exactly what they do. They put a mirror to humanity.

02:17:26,722 --> 02:17:28,414
SPEAKER_1:  to help us understand ourselves.

02:17:29,058 --> 02:17:33,374
SPEAKER_0:  There's also the issue of like, you know, in these diplomacy experiments in order to do...

02:17:34,498 --> 02:17:36,766
SPEAKER_0:  a fair comparison. What we found is that

02:17:37,026 --> 02:17:42,110
SPEAKER_0:  there's an inherent anti-AI bias in these kinds of games. So we actually played.

02:17:42,434 --> 02:17:44,606
SPEAKER_0:  tournaments in a non-language version of the game.

02:17:45,026 --> 02:17:48,926
SPEAKER_0:  where you know we we told the participants like hey every single game is going to be an AI.

02:17:49,442 --> 02:17:51,550
SPEAKER_0:  And what we found is that the humans...

02:17:52,002 --> 02:17:58,974
SPEAKER_0:  would spend basically the entire game like trying to figure out who the bot was. And then as soon as they thought they figured it out, they would all team up and try to kill it.

02:17:59,394 --> 02:18:00,030
SPEAKER_0:  and

02:18:00,354 --> 02:18:05,521
SPEAKER_0:  You know, overcoming that inherent anti-AI bias is a challenge.

02:18:05,521 --> 02:18:06,718
SPEAKER_1:  On the flip side.

02:18:07,586 --> 02:18:08,286
SPEAKER_1:  I think.

02:18:09,154 --> 02:18:11,070
SPEAKER_1:  when robots become the enemy.

02:18:11,362 --> 02:18:14,302
SPEAKER_1:  That's when we get to heal our human divisions.

02:18:14,626 --> 02:18:15,902
SPEAKER_1:  and we can become one.

02:18:16,834 --> 02:18:23,870
SPEAKER_1:  As long as we have one enemy, it's that Reagan thing when the aliens show up. That's when we put our side, our divisions will become one.

02:18:24,130 --> 02:18:25,310
SPEAKER_1:  one human species.

02:18:25,410 --> 02:18:27,614
SPEAKER_0:  We might have our differences, but we're at least all human.

02:18:27,746 --> 02:18:30,622
SPEAKER_1:  At least we all hate the robots. No.

02:18:31,202 --> 02:18:39,422
SPEAKER_1:  No, no, no. I think there will be actually in the future something like a civil rights movement for robots. I think that's the fascinating thing about AI systems is they ask.

02:18:39,714 --> 02:18:41,246
SPEAKER_1:  they force us to ask about.

02:18:42,530 --> 02:18:49,758
SPEAKER_1:  ethical questions about what is sentience, what is, how do we feel about systems that are capable of suffering, are capable of displaying suffering.

02:18:50,466 --> 02:18:53,438
SPEAKER_1:  and how do we design products that show emotion and not?

02:18:53,858 --> 02:18:56,606
SPEAKER_1:  How do we feel about that? Lying is another topic.

02:18:56,962 --> 02:18:57,342
SPEAKER_1:  Awee.

02:18:57,634 --> 02:18:59,678
SPEAKER_1:  going to allow bots to lie and not...

02:19:00,098 --> 02:19:06,814
SPEAKER_1:  and where's the balance between being nice and telling the truth? I mean these are all fascinating human questions.

02:19:07,042 --> 02:19:09,374
SPEAKER_1:  It's like so exciting to be in the century with

02:19:09,794 --> 02:19:11,774
SPEAKER_1:  We create systems that...

02:19:12,962 --> 02:19:13,534
SPEAKER_1:  Take this.

02:19:13,954 --> 02:19:16,254
SPEAKER_1:  philosophical questions that have been asked for.

02:19:16,482 --> 02:19:23,102
SPEAKER_1:  centuries and now we can engineer them inside systems where like you really have to answer them because you'll have

02:19:24,194 --> 02:19:30,814
SPEAKER_1:  transformational impact on human society, depending on what you design inside those systems. It's fascinating.

02:19:31,426 --> 02:19:35,838
SPEAKER_1:  And like you said, I feel like diplomacy is a step towards the direction of the real world.

02:19:36,066 --> 02:19:38,046
SPEAKER_1:  applying these RL methods towards the real world.

02:19:38,978 --> 02:19:39,326
SPEAKER_1:  from.

02:19:39,970 --> 02:19:41,502
SPEAKER_1:  from all the breakthrough performances.

02:19:41,954 --> 02:19:44,158
SPEAKER_1:  in Go and Chess and StarCraft and Dota.

02:19:44,418 --> 02:19:46,398
SPEAKER_1:  This is, this feels like the real world.

02:19:46,786 --> 02:19:48,798
SPEAKER_1:  Especially now my mind's been on war.

02:19:49,314 --> 02:19:56,446
SPEAKER_1:  military conflict. This feels like it can give us some deep insights about human behavior at the large geopolitical scale.

02:19:57,218 --> 02:19:57,694
SPEAKER_1:  Um...

02:19:58,658 --> 02:19:59,710
SPEAKER_1:  What do you think?

02:20:01,026 --> 02:20:02,174
SPEAKER_1:  is the...

02:20:02,530 --> 02:20:03,422
SPEAKER_1:  breakthrough.

02:20:04,546 --> 02:20:04,862
SPEAKER_1:  or

02:20:06,178 --> 02:20:10,686
SPEAKER_1:  the directions of work that will take us towards solving intelligence.

02:20:11,010 --> 02:20:12,990
SPEAKER_1:  towards creating AGI systems.

02:20:13,442 --> 02:20:14,398
SPEAKER_1:  You've been a part of.

02:20:14,786 --> 02:20:15,454
SPEAKER_1:  creating

02:20:16,066 --> 02:20:17,918
SPEAKER_1:  By the way, we should say part of.

02:20:18,338 --> 02:20:20,126
SPEAKER_1:  great teams that do this.

02:20:20,418 --> 02:20:20,958
SPEAKER_1:  Um...

02:20:21,218 --> 02:20:21,886
SPEAKER_1:  of creating.

02:20:22,338 --> 02:20:25,950
SPEAKER_1:  systems that achieve breakthrough performances on before.

02:20:26,498 --> 02:20:29,374
SPEAKER_1:  thought unsolvable problems like poker.

02:20:29,698 --> 02:20:31,902
SPEAKER_1:  multiplayer poker, diplomacy.

02:20:33,282 --> 02:20:35,390
SPEAKER_1:  We're taking steps towards that direction.

02:20:35,714 --> 02:20:39,934
SPEAKER_1:  What do you think it takes to go all the way to create superhuman level intelligence?

02:20:40,642 --> 02:20:43,710
SPEAKER_0:  There's a lot of people trying to figure that out right now.

02:20:44,002 --> 02:20:48,958
SPEAKER_0:  I should say the amount of progress that's been made, especially in the past few years is truly phenomenal.

02:20:49,410 --> 02:20:51,038
SPEAKER_0:  I mean, you look at where AI was.

02:20:51,522 --> 02:20:59,838
SPEAKER_0:  10 years ago and the idea that you can have AIs that can generate language and generate images the way they're doing today and able to play a game like diplomacy was just

02:21:00,162 --> 02:21:00,990
SPEAKER_0:  Like, unthinkable.

02:21:01,250 --> 02:21:04,318
SPEAKER_0:  Even five years ago, let alone 10 years ago.

02:21:04,994 --> 02:21:05,598
SPEAKER_0:  Um...

02:21:07,042 --> 02:21:08,574
SPEAKER_0:  Now, there are aspects.

02:21:08,834 --> 02:21:10,430
SPEAKER_0:  of AI that I think are still.

02:21:10,658 --> 02:21:11,486
SPEAKER_0:  Lacking, um...

02:21:12,290 --> 02:21:13,278
SPEAKER_0:  I think...

02:21:13,602 --> 02:21:14,750
SPEAKER_0:  There's general agreements.

02:21:15,266 --> 02:21:20,574
SPEAKER_0:  that one of the major issues with AI today is that it's very data inefficient. It requires

02:21:20,930 --> 02:21:23,678
SPEAKER_0:  a huge number of samples of training examples.

02:21:24,066 --> 02:21:24,958
SPEAKER_0:  to be able to train.

02:21:25,346 --> 02:21:26,942
SPEAKER_0:  and you look at it and the other place go

02:21:27,330 --> 02:21:29,566
SPEAKER_0:  and it needs millions of games of go.

02:21:29,986 --> 02:21:31,646
SPEAKER_0:  to learn how to play the game well.

02:21:32,034 --> 02:21:38,302
SPEAKER_0:  whereas a human can pick it up and like, you know, I don't know how many games as a human go player, go, go grand master playing their lifetime.

02:21:38,818 --> 02:21:39,326
SPEAKER_0:  Probably.

02:21:39,682 --> 02:21:40,030
SPEAKER_0:  You know.

02:21:40,738 --> 02:21:42,430
SPEAKER_0:  in the thousands or tens of thousands, I guess.

02:21:43,138 --> 02:21:43,710
SPEAKER_0:  Um...

02:21:45,122 --> 02:21:51,102
SPEAKER_0:  So that's one issue, overcoming this challenge of data efficiency. This is particularly important if we want to.

02:21:51,458 --> 02:21:52,958
SPEAKER_0:  deploy AI systems in.

02:21:53,378 --> 02:21:58,238
SPEAKER_0:  real world settings where they're interacting with humans because you know for example with robotics

02:21:58,498 --> 02:22:01,150
SPEAKER_0:  it's really hard to generate a huge number of samples.

02:22:01,410 --> 02:22:04,510
SPEAKER_0:  It's a different story when you're working in these, you know.

02:22:04,994 --> 02:22:08,030
SPEAKER_0:  Totally virtual games where you can play a million games and it's no big deal.

02:22:08,514 --> 02:22:11,710
SPEAKER_1:  I was planning on just launching like a thousand of these robots in Austin.

02:22:12,386 --> 02:22:16,670
SPEAKER_1:  I don't think it's illegal for legged robots to roam the streets and just collect data.

02:22:16,930 --> 02:22:18,055
SPEAKER_1:  That's not a crazy idea.

02:22:18,055 --> 02:22:18,814
SPEAKER_0:  happened. Yeah.

02:22:19,298 --> 02:22:22,750
SPEAKER_0:  I mean, that's one way to overcome the data efficiency problem, is scale it.

02:22:23,810 --> 02:22:28,926
SPEAKER_1:  Like I actually tried to see if there's a law against robots, like legged robots, which is.

02:22:29,250 --> 02:22:29,982
SPEAKER_1:  operating.

02:22:30,978 --> 02:22:33,950
SPEAKER_1:  in the streets of a major city and there isn't.

02:22:34,210 --> 02:22:35,006
SPEAKER_1:  couldn't find any.

02:22:36,450 --> 02:22:38,878
SPEAKER_1:  I'll take it all the way to the Supreme Court.

02:22:39,970 --> 02:22:43,326
SPEAKER_1:  Robot rights. Okay, anyway, sorry, you were saying. So this is the.

02:22:43,970 --> 02:22:47,134
SPEAKER_1:  So what are the ideas for becoming more data efficient?

02:22:47,682 --> 02:22:54,814
SPEAKER_0:  I mean, that's the trillion dollar question in AI today. I mean, if you can figure out how to make AI systems more data efficient, then...

02:22:55,298 --> 02:22:55,870
SPEAKER_0:  That's.

02:22:56,514 --> 02:22:57,310
SPEAKER_0:  A huge breakthrough.

02:22:57,602 --> 02:22:59,477
SPEAKER_0:  So nobody really knows right now. It could be just.

02:22:59,477 --> 02:23:02,270
SPEAKER_1:  just a gigantic background language model.

02:23:02,626 --> 02:23:03,582
SPEAKER_1:  And then you do.

02:23:04,226 --> 02:23:04,702
SPEAKER_1:  uh...

02:23:04,930 --> 02:23:07,198
SPEAKER_1:  the training becomes like prompting that model.

02:23:08,258 --> 02:23:09,022
SPEAKER_1:  to, uh...

02:23:10,050 --> 02:23:14,366
SPEAKER_1:  to essentially do a kind of querying, a search into the space of the things that's learned.

02:23:14,626 --> 02:23:21,854
SPEAKER_1:  to customize that to whatever problem you're trying to solve. So maybe if you form a large enough language model, you can go quite a long way.

02:23:22,178 --> 02:23:22,526
SPEAKER_0:  That.

02:23:22,946 --> 02:23:32,542
SPEAKER_0:  You know, I think there's some truth to that. I mean, you look at the way humans approach a game like poker, they're not coming at it from scratch. They're coming at it with a huge amount of background knowledge about.

02:23:32,930 --> 02:23:35,934
SPEAKER_0:  You know, how humans work, how the world works.

02:23:36,322 --> 02:23:37,726
SPEAKER_0:  um, the idea of money.

02:23:38,114 --> 02:23:38,590
SPEAKER_0:  So.

02:23:39,234 --> 02:23:43,646
SPEAKER_0:  they're able to leverage that kind of information to pick up the game faster.

02:23:44,194 --> 02:23:49,950
SPEAKER_0:  So it's not really a fair comparison to then compare it to an AI that's like learning from scratch. And maybe one of the ways that we address this.

02:23:50,306 --> 02:23:51,838
SPEAKER_0:  sample complexity problem.

02:23:52,194 --> 02:23:56,350
SPEAKER_0:  is by allowing AIs to leverage that general knowledge across a ton of different domains.

02:23:57,986 --> 02:23:59,934
SPEAKER_1:  So like I said you did uh

02:24:00,162 --> 02:24:03,006
SPEAKER_1:  A lot of incredible work in the space of research.

02:24:03,266 --> 02:24:07,518
SPEAKER_1:  and actually building systems, what advice would you give to, let's start with beginners.

02:24:07,778 --> 02:24:11,262
SPEAKER_1:  What advice would you give to beginners interested in machine learning?

02:24:11,874 --> 02:24:14,910
SPEAKER_1:  Just, they're at the very start of their journey there in high school and college.

02:24:15,330 --> 02:24:17,822
SPEAKER_1:  thinking like this seems like a fascinating world.

02:24:18,370 --> 02:24:19,326
SPEAKER_1:  What advice would you give them?

02:24:20,354 --> 02:24:21,950
SPEAKER_0:  Um, I would say...

02:24:23,138 --> 02:24:24,286
SPEAKER_0:  that there are a lot of

02:24:24,514 --> 02:24:26,270
SPEAKER_0:  people working on similar.

02:24:26,498 --> 02:24:29,630
SPEAKER_0:  aspects of machine learning and to not be afraid to try something.

02:24:29,986 --> 02:24:32,190
SPEAKER_0:  a bit different. My own path.

02:24:33,026 --> 02:24:34,046
SPEAKER_0:  in AI.

02:24:34,466 --> 02:24:37,214
SPEAKER_0:  is pretty atypical for a machine learning researcher today.

02:24:37,570 --> 02:24:40,702
SPEAKER_0:  I mean, I started out working on game theory and...

02:24:41,410 --> 02:24:44,510
SPEAKER_0:  and then shifting more towards reinforcement learning as time went on.

02:24:44,930 --> 02:24:47,870
SPEAKER_0:  And that actually had a lot of benefits, I think, because it allowed me to...

02:24:48,130 --> 02:24:53,566
SPEAKER_0:  look at these problems in a very different way from the way a lot of machine learning researchers view it.

02:24:54,146 --> 02:24:54,814
SPEAKER_0:  Um, and.

02:24:55,266 --> 02:24:55,614
SPEAKER_0:  that

02:24:55,970 --> 02:25:00,286
SPEAKER_0:  comes with drawbacks in some respects. I think there's definitely aspects of machine learning.

02:25:00,610 --> 02:25:00,990
SPEAKER_0:  where

02:25:01,314 --> 02:25:02,654
SPEAKER_0:  No, I'm weaker than-

02:25:02,882 --> 02:25:04,254
SPEAKER_0:  most of the researchers out there.

02:25:04,674 --> 02:25:06,686
SPEAKER_0:  But I think that diversity of perspective.

02:25:07,074 --> 02:25:08,574
SPEAKER_0:  you know, when I'm working with.

02:25:08,898 --> 02:25:09,758
SPEAKER_0:  My teammates.

02:25:10,018 --> 02:25:12,638
SPEAKER_0:  They're saying that I'm bringing to the table and something that they're bringing to the table.

02:25:12,898 --> 02:25:15,230
SPEAKER_0:  And that kind of collaboration becomes very fruitful for that reason.

02:25:15,810 --> 02:25:17,438
SPEAKER_1:  So there could be problems like...

02:25:17,986 --> 02:25:19,134
SPEAKER_1:  like poker lucky.

02:25:19,426 --> 02:25:24,158
SPEAKER_1:  you've chosen diplomacy, there could be problems like that still out there that you can just tackle.

02:25:24,642 --> 02:25:26,142
SPEAKER_1:  even if it seems extremely difficult.

02:25:27,874 --> 02:25:32,030
SPEAKER_0:  Um, I think that there's a lot of challenges, challenges left. And I think having a diversity.

02:25:32,514 --> 02:25:34,110
SPEAKER_0:  of viewpoints.

02:25:34,594 --> 02:25:36,958
SPEAKER_0:  and backgrounds is really helpful for.

02:25:37,666 --> 02:25:39,102
SPEAKER_0:  working together to figure out how to.

02:25:39,490 --> 02:25:40,615
SPEAKER_0:  tackle those kinds of challenges.

02:25:40,615 --> 02:25:43,230
SPEAKER_1:  as a beginner. So that I would say that's

02:25:43,810 --> 02:25:45,438
SPEAKER_1:  That's more for like a grad student.

02:25:45,762 --> 02:25:48,030
SPEAKER_1:  They already built up a bass like a complete beginner.

02:25:48,418 --> 02:25:53,694
SPEAKER_1:  What's a good journey? So for you that was doing some more on the math side of things to game theory.

02:25:53,986 --> 02:25:59,294
SPEAKER_1:  So it's basically build up a foundation in something. So programming, mathematics.

02:25:59,554 --> 02:26:00,606
SPEAKER_1:  even be physics.

02:26:00,866 --> 02:26:02,558
SPEAKER_1:  but build that foundation.

02:26:03,426 --> 02:26:10,398
SPEAKER_0:  Yeah, I would say build a strong foundation in math and computer science and statistics and these kinds of areas. But don't be afraid to

02:26:10,818 --> 02:26:14,174
SPEAKER_0:  try something that's different and learn something that's different from, you know, the

02:26:14,658 --> 02:26:17,246
SPEAKER_0:  the thing that everybody else is doing to get into machine learning.

02:26:17,474 --> 02:26:18,846
SPEAKER_0:  You know, there's there's value.

02:26:19,170 --> 02:26:21,918
SPEAKER_0:  in having a different background than everybody else.

02:26:22,434 --> 02:26:22,974
SPEAKER_0:  Um...

02:26:23,426 --> 02:26:24,126
SPEAKER_0:  Yeah, so.

02:26:24,546 --> 02:26:29,918
SPEAKER_0:  But certainly having a strong math background, especially in things like linear algebra and statistics and probability,

02:26:30,210 --> 02:26:33,278
SPEAKER_0:  are incredibly helpful today for learning about.

02:26:33,762 --> 02:26:34,846
SPEAKER_0:  understanding machine learning.

02:26:35,522 --> 02:26:41,438
SPEAKER_1:  You think one day we'll be able to, since you're taking steps from poker to diplomacy, one day we'll be able to, uh,

02:26:43,298 --> 02:26:45,182
SPEAKER_1:  figure out how to live life optimally.

02:26:46,562 --> 02:26:52,638
SPEAKER_0:  Well, what is it like in poker and diplomacy, you need a value function, you need to have a reward system. And so

02:26:53,122 --> 02:26:54,654
SPEAKER_0:  What does it mean to live a life that's optimal?

02:26:55,202 --> 02:26:57,278
SPEAKER_1:  So, okay, so then you can.

02:26:57,634 --> 02:27:01,726
SPEAKER_1:  Exactly, like lay down a reward function being like, I want to be rich.

02:27:02,306 --> 02:27:04,062
SPEAKER_1:  or I want to be...

02:27:05,218 --> 02:27:07,262
SPEAKER_1:  I want to be in a happy relationship.

02:27:07,554 --> 02:27:08,606
SPEAKER_1:  and then you'll say well

02:27:10,018 --> 02:27:10,718
SPEAKER_1:  Do X.

02:27:11,746 --> 02:27:17,662
SPEAKER_0:  You know, there's a lot of talk today about, in AI safety circles about like misspecification of.

02:27:18,050 --> 02:27:19,806
SPEAKER_0:  you know, reward functions. So you...

02:27:20,066 --> 02:27:27,166
SPEAKER_0:  You say like, okay, my objective is to be rich. And maybe the AI tells you like, okay, well, if you want to maximize the probability that you're rich, go rob a bank.

02:27:27,522 --> 02:27:29,086
SPEAKER_0:  Sure. So you want to.

02:27:29,378 --> 02:27:35,003
SPEAKER_0:  Is that really what you want? Is your objective really to be rich at all costs, or is it more nuanced than that?

02:27:35,003 --> 02:27:37,182
SPEAKER_1:  So the unintended consequences.

02:27:37,442 --> 02:27:37,886
SPEAKER_1:  Yeah.

02:27:39,490 --> 02:27:41,470
SPEAKER_1:  Yeah, so, yeah.

02:27:41,762 --> 02:27:42,654
SPEAKER_1:  That's...

02:27:43,202 --> 02:27:48,926
SPEAKER_1:  So maybe life is more about defining the reward function that minimizes the unintended consequences.

02:27:49,794 --> 02:27:53,694
SPEAKER_1:  then it is about the actual policy that gets you to the reward function.

02:27:53,954 --> 02:27:55,518
SPEAKER_1:  Maybe life is just about.

02:27:56,258 --> 02:27:58,334
SPEAKER_1:  constantly updating the reward function.

02:27:59,618 --> 02:28:01,694
SPEAKER_0:  I think one of the challenges in life is...

02:28:02,242 --> 02:28:04,926
SPEAKER_0:  figuring out exactly what that reward function is. Sometimes

02:28:05,314 --> 02:28:07,102
SPEAKER_0:  it's pretty hard to specify. The same way that.

02:28:07,650 --> 02:28:11,742
SPEAKER_0:  You know, trying to handcraft the optimal policy in a game like chess is really difficult.

02:28:12,002 --> 02:28:14,366
SPEAKER_0:  It's not so clear-cut what the reward function is for.

02:28:14,722 --> 02:28:15,134
SPEAKER_0:  life.

02:28:16,002 --> 02:28:17,886
SPEAKER_1:  I think one day AI will figure it out.

02:28:20,354 --> 02:28:21,534
SPEAKER_1:  And I wonder what that would be.

02:28:21,986 --> 02:28:22,814
SPEAKER_1:  Until then.

02:28:23,746 --> 02:28:26,430
SPEAKER_1:  I just really appreciate the kind of work you're doing.

02:28:26,882 --> 02:28:31,774
SPEAKER_1:  It's really fascinating taking a leap into a more and more real world like.

02:28:32,738 --> 02:28:34,462
SPEAKER_1:  problem space.

02:28:34,882 --> 02:28:37,118
SPEAKER_1:  and just achieving incredible.

02:28:37,538 --> 02:28:38,238
SPEAKER_1:  Results.

02:28:38,722 --> 02:28:44,094
SPEAKER_1:  by applying reinforcement learning. Since I saw your work on poker, you've been a constant inspiration.

02:28:44,386 --> 02:28:47,518
SPEAKER_1:  It's an honor to get to finally talk to you and this is really fun.

02:28:47,842 --> 02:28:48,414
SPEAKER_1:  Thanks for having me.

02:28:49,442 --> 02:28:51,774
SPEAKER_1:  Thanks for listening to this conversation with No Brown.

02:28:52,482 --> 02:28:55,582
SPEAKER_1:  To support this podcast, please check out our sponsors in the description.

02:28:56,162 --> 02:28:56,606
SPEAKER_1:  And now.

02:28:57,122 --> 02:28:59,166
SPEAKER_1:  Let me leave you with some words from Sun Tzu.

02:28:59,586 --> 02:29:00,862
SPEAKER_1:  and the art of war.

02:29:01,858 --> 02:29:04,446
SPEAKER_1:  The whole secret lies in confusing the enemy.

02:29:05,026 --> 02:29:06,558
SPEAKER_1:  so that he cannot fathom.

02:29:06,786 --> 02:29:07,870
SPEAKER_1:  our real intent.

02:29:09,378 --> 02:29:10,174
SPEAKER_1:  Thank you for listening.

02:29:10,658 --> 02:29:11,326
SPEAKER_1:  Hope to see you.

02:29:11,714 --> 02:29:12,254
SPEAKER_1:  next time.
