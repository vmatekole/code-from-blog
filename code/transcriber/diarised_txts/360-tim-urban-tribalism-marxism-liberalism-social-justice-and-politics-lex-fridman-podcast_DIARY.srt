00:00:00,098 --> 00:00:00,990
SPEAKER_0:  of Radical.

00:00:01,314 --> 00:00:01,822
SPEAKER_0:  Um...

00:00:02,306 --> 00:00:04,894
SPEAKER_0:  political movement, of which there will always be a lot in the country.

00:00:05,858 --> 00:00:10,270
SPEAKER_0:  has managed to do something that a radical movement's not supposed to be able to do in the US.

00:00:10,850 --> 00:00:12,542
SPEAKER_0:  which is they've managed to.

00:00:12,866 --> 00:00:18,942
SPEAKER_0:  hijack institutions all across the country and hijack medical journals and universities and

00:00:19,650 --> 00:00:25,470
SPEAKER_0:  you know, the ACLU, you know, all the activist organizations and nonprofits and many tech companies.

00:00:25,890 --> 00:00:28,510
SPEAKER_0:  And the way I view a liberal democracy is it is it.

00:00:28,834 --> 00:00:32,798
SPEAKER_0:  It is a bunch of these institutions that were trial and error crafted over, you know.

00:00:33,314 --> 00:00:34,174
SPEAKER_0:  hundreds of years.

00:00:35,106 --> 00:00:37,086
SPEAKER_0:  and they all rely on trust.

00:00:37,314 --> 00:00:41,982
SPEAKER_0:  public trust and a certain kind of feeling of unity that actually is critical.

00:00:42,370 --> 00:00:43,838
SPEAKER_0:  to a liberal democracy is functioning.

00:00:44,546 --> 00:00:47,358
SPEAKER_0:  And what I see this thing is, is as a parasite.

00:00:47,874 --> 00:00:49,662
SPEAKER_0:  on that, that whose goal

00:00:49,986 --> 00:00:54,078
SPEAKER_0:  is, and I'm not saying each, by the way, each individual in this is, I don't think they're bad people.

00:00:54,498 --> 00:00:57,662
SPEAKER_0:  I think that it's the ideology itself has the property of.

00:00:58,018 --> 00:00:59,966
SPEAKER_0:  Its goal is to tear apart.

00:01:00,802 --> 00:01:03,646
SPEAKER_0:  pretty delicate workings of the liberal democracy shred

00:01:03,874 --> 00:01:05,022
SPEAKER_0:  critical lines of trust.

00:01:07,618 --> 00:01:09,630
SPEAKER_1:  The following is a conversation with Tim Urban.

00:01:09,954 --> 00:01:11,486
SPEAKER_1:  his second time in the podcast.

00:01:11,810 --> 00:01:16,222
SPEAKER_1:  He's the author and illustrator of the amazing blog called Wait, But Why.

00:01:16,642 --> 00:01:17,086
SPEAKER_1:  and

00:01:17,314 --> 00:01:18,462
SPEAKER_1:  is the author of a new book.

00:01:18,722 --> 00:01:19,646
SPEAKER_1:  coming out tomorrow.

00:01:20,066 --> 00:01:21,406
SPEAKER_1:  called What's Our Problem?

00:01:21,762 --> 00:01:23,422
SPEAKER_1:  self-help book for societies.

00:01:23,906 --> 00:01:26,334
SPEAKER_1:  We talk a lot about this book in this podcast.

00:01:26,690 --> 00:01:27,198
SPEAKER_1:  for you.

00:01:27,426 --> 00:01:29,950
SPEAKER_1:  really do need to get it and experience it for yourself.

00:01:30,274 --> 00:01:31,998
SPEAKER_1:  It is a fearless, insightful

00:01:32,354 --> 00:01:34,878
SPEAKER_1:  hilarious and I think important book.

00:01:35,202 --> 00:01:36,894
SPEAKER_1:  in this divisive time that we live in.

00:01:37,346 --> 00:01:39,358
SPEAKER_1:  the Kindle version, the audiobook and

00:01:39,650 --> 00:01:42,718
SPEAKER_1:  The web version should be all available on date of publication.

00:01:43,842 --> 00:01:44,798
SPEAKER_1:  I should also mention.

00:01:45,250 --> 00:01:47,390
SPEAKER_1:  that my face might be a bit more beat up then.

00:01:47,874 --> 00:01:48,414
SPEAKER_1:  Usual.

00:01:48,898 --> 00:01:50,334
SPEAKER_1:  I got hit in the...

00:01:50,594 --> 00:01:50,974
SPEAKER_1:  Shin.

00:01:51,618 --> 00:01:52,510
SPEAKER_1:  pretty good.

00:01:52,802 --> 00:01:56,702
SPEAKER_1:  since I've been getting back into training jujitsu, a sport I love very much.

00:01:57,058 --> 00:01:58,526
SPEAKER_1:  after recovering from an injury.

00:01:59,074 --> 00:01:59,422
SPEAKER_1:  So.

00:01:59,682 --> 00:02:03,038
SPEAKER_1:  if you see marks on my face during these intros or conversations.

00:02:03,394 --> 00:02:04,510
SPEAKER_1:  You know that, uh...

00:02:04,994 --> 00:02:06,654
SPEAKER_1:  My life is in a pretty good place.

00:02:07,426 --> 00:02:09,406
SPEAKER_1:  This is the Lex Friedman podcast.

00:02:09,666 --> 00:02:12,222
SPEAKER_1:  To support it, please check out our sponsors in the description.

00:02:12,546 --> 00:02:13,726
SPEAKER_1:  And now, dear friends.

00:02:14,146 --> 00:02:14,686
SPEAKER_1:  Here's Tim.

00:02:15,202 --> 00:02:15,614
SPEAKER_1:  Urban.

00:02:16,834 --> 00:02:22,270
SPEAKER_1:  You wrote an incredible book called What's Our Problem? A self-help book for societies.

00:02:23,330 --> 00:02:24,190
SPEAKER_1:  in the beginning.

00:02:24,610 --> 00:02:25,278
SPEAKER_1:  You are-

00:02:25,570 --> 00:02:26,302
SPEAKER_1:  Present this.

00:02:27,170 --> 00:02:31,198
SPEAKER_1:  view of human history as a thousand page book.

00:02:32,066 --> 00:02:34,174
SPEAKER_1:  where each page is 250 years.

00:02:35,394 --> 00:02:35,806
SPEAKER_1:  and

00:02:36,098 --> 00:02:37,726
SPEAKER_1:  It's a brilliant visualization because

00:02:37,954 --> 00:02:39,038
SPEAKER_1:  Almost nothing happens.

00:02:39,266 --> 00:02:39,966
SPEAKER_1:  for most of it.

00:02:40,258 --> 00:02:46,014
SPEAKER_1:  So what blows your mind most about that visualization is just sit back and think about it.

00:02:46,210 --> 00:02:47,134
SPEAKER_0:  It's a boring book.

00:02:47,714 --> 00:02:49,918
SPEAKER_0:  So 950 pages, 95% of the book.

00:02:50,498 --> 00:02:56,702
SPEAKER_0:  hunter gatherers kind of doing their thing. I'm sure there's, you know, there's some, there's obviously some major cognitive advancements along the way in language.

00:02:57,474 --> 00:03:00,510
SPEAKER_0:  And I'm sure, you know, the bow and arrow comes around at some point, you know, so,

00:03:00,738 --> 00:03:03,550
SPEAKER_0:  tiny things, but it's like, oh, now we have 400 pages until the next thing.

00:03:04,002 --> 00:03:05,246
SPEAKER_0:  Then you get to page 950.

00:03:05,858 --> 00:03:06,983
SPEAKER_0:  and things start moving. Record.

00:03:06,983 --> 00:03:09,694
SPEAKER_1:  The order of history starts at 976. Right.

00:03:10,050 --> 00:03:11,454
SPEAKER_1:  It's basically the bottom row.

00:03:11,746 --> 00:03:15,230
SPEAKER_1:  is when anything interesting happens. There's a bunch of agriculture for a while.

00:03:15,938 --> 00:03:17,534
SPEAKER_1:  before we know anything about it.

00:03:18,082 --> 00:03:19,207
SPEAKER_1:  and then record a history.

00:03:19,207 --> 00:03:21,054
SPEAKER_0:  Yeah, 25 pages.

00:03:21,666 --> 00:03:22,366
SPEAKER_0:  of.

00:03:22,786 --> 00:03:24,734
SPEAKER_0:  actual like recorded history. So when we think of

00:03:25,026 --> 00:03:25,726
SPEAKER_0:  Prehistoric?

00:03:26,178 --> 00:03:29,150
SPEAKER_0:  We're talking about pages 1 through 975.

00:03:29,538 --> 00:03:30,238
SPEAKER_0:  of the book.

00:03:30,818 --> 00:03:31,614
SPEAKER_0:  And then.

00:03:32,066 --> 00:03:35,582
SPEAKER_0:  History is page 976 to 1000.

00:03:35,874 --> 00:03:37,758
SPEAKER_0:  If you were reading the book, it would be like epilogue.

00:03:37,986 --> 00:03:40,830
SPEAKER_0:  you know, the last little 10 pages of the book.

00:03:41,282 --> 00:03:47,006
SPEAKER_0:  And we think of AD as super long, right? 2000 years, the Roman Empire, 2000 years ago. Like, that's so long.

00:03:48,226 --> 00:03:49,118
SPEAKER_0:  Human history.

00:03:49,570 --> 00:03:52,446
SPEAKER_0:  has been going on for over 2000 centuries.

00:03:53,410 --> 00:03:55,902
SPEAKER_0:  Like that is, it's just, it's hard to wrap your head around.

00:03:56,578 --> 00:04:00,318
SPEAKER_0:  And this is, I mean, even that's just the end of a very long road. Like, you know.

00:04:00,610 --> 00:04:03,390
SPEAKER_0:  The 100,000 years before that, it's not like.

00:04:03,714 --> 00:04:04,158
SPEAKER_0:  You know.

00:04:04,642 --> 00:04:09,022
SPEAKER_0:  It's not like that was that different. So it's just there's been people like us.

00:04:09,506 --> 00:04:11,262
SPEAKER_0:  that have emotions like us.

00:04:11,522 --> 00:04:14,334
SPEAKER_0:  that have physical sensations like us.

00:04:15,426 --> 00:04:21,662
SPEAKER_0:  for so, so long and who are they all and what was their life like and it's

00:04:22,146 --> 00:04:24,894
SPEAKER_0:  You know, I think we have no idea what it was like to be them.

00:04:25,154 --> 00:04:26,686
SPEAKER_0:  The thing that's craziest about...

00:04:27,042 --> 00:04:28,606
SPEAKER_0:  people of the far past is.

00:04:29,186 --> 00:04:34,814
SPEAKER_0:  Not just that they had different lives, they had different fears, they had different dangers and different responsibilities and they lived in tribes and everything.

00:04:35,170 --> 00:04:36,702
SPEAKER_0:  but they didn't know anything.

00:04:37,410 --> 00:04:42,558
SPEAKER_0:  We just take it for granted that we're born on top of this tower of knowledge. And from the very beginning, we

00:04:43,746 --> 00:04:49,694
SPEAKER_0:  We know that the earth is a ball floating in space, and we know that we're going to die one day, and we know that we're going to die one day.

00:04:50,082 --> 00:04:50,654
SPEAKER_0:  Um.

00:04:50,882 --> 00:04:52,958
SPEAKER_0:  You know, we evolved from animals and all the...

00:04:53,730 --> 00:04:54,878
SPEAKER_0:  those were all like in

00:04:55,106 --> 00:04:55,614
SPEAKER_0:

00:04:55,842 --> 00:04:57,790
SPEAKER_0:  you know, epiphanies quite recently.

00:04:58,018 --> 00:05:01,790
SPEAKER_0:  And the people a long time ago, they just had no idea what was going on and like-

00:05:02,114 --> 00:05:12,574
SPEAKER_0:  I'm kind of jealous because I feel like it, I mean, it might've been scary to not know what's going on, but it also, I feel like would be, you'd have a sense of awe and wonder all the time and you don't know what's gonna happen next. And once you learn, you're kind of like, oh, that's like.

00:05:12,930 --> 00:05:13,662
SPEAKER_0:  It's a little grim.

00:05:14,082 --> 00:05:17,310
SPEAKER_1:  But they probably had the same capacity for consciousness.

00:05:17,826 --> 00:05:19,262
SPEAKER_1:  to experience the world.

00:05:20,002 --> 00:05:21,406
SPEAKER_1:  to wander about the world.

00:05:21,826 --> 00:05:24,510
SPEAKER_1:  Maybe to construct narratives about the world and myths and so on.

00:05:25,634 --> 00:05:26,878
SPEAKER_1:  They just had less.

00:05:27,298 --> 00:05:30,078
SPEAKER_1:  grounded systematic effects to play with.

00:05:30,594 --> 00:05:32,382
SPEAKER_1:  they still probably felt.

00:05:33,154 --> 00:05:36,158
SPEAKER_1:  the narratives, the myths that construct it as intensely as we do.

00:05:36,450 --> 00:05:36,862
SPEAKER_1:  Oh yeah.

00:05:36,962 --> 00:05:38,142
SPEAKER_0:  They also fell in love.

00:05:38,754 --> 00:05:39,582
SPEAKER_0:  They also.

00:05:39,906 --> 00:05:40,222
SPEAKER_0:  had.

00:05:40,610 --> 00:05:44,222
SPEAKER_0:  friends and they had falling outs with friends. Didn't shower much though.

00:05:44,482 --> 00:05:46,686
SPEAKER_0:  No, they did not smell nice.

00:05:47,330 --> 00:05:48,382
SPEAKER_1:  Maybe they did

00:05:49,346 --> 00:05:49,758
SPEAKER_1:  Maybe.

00:05:50,114 --> 00:05:52,739
SPEAKER_1:  Beauty's in the high of the beholden. Maybe it's all like relative.

00:05:52,739 --> 00:05:56,222
SPEAKER_0:  So how many people in history have experienced a hot shower?

00:05:56,770 --> 00:05:57,822
SPEAKER_0:  Like almost none.

00:05:58,338 --> 00:06:00,862
SPEAKER_0:  That's like what one more hot shower was invented 100 years ago?

00:06:01,698 --> 00:06:03,038
SPEAKER_0:  like less.

00:06:03,618 --> 00:06:03,998
SPEAKER_0:  So.

00:06:04,610 --> 00:06:06,462
SPEAKER_0:  Like George Washington never had a hot shower.

00:06:07,138 --> 00:06:11,454
SPEAKER_0:  It's like it's just kind of weird like he took cold showers all the time or like yeah

00:06:11,682 --> 00:06:12,222
SPEAKER_0:  Um...

00:06:12,546 --> 00:06:14,750
SPEAKER_0:  And again, we just take this for granted, but that's like an.

00:06:15,042 --> 00:06:17,438
SPEAKER_0:  unbelievable life experience to have.

00:06:18,338 --> 00:06:22,814
SPEAKER_0:  rain, a controlled little booth where it rains hot water on your head.

00:06:23,362 --> 00:06:25,694
SPEAKER_0:  and then you get out and it's not everywhere, it's like contained.

00:06:26,146 --> 00:06:26,558
SPEAKER_0:  Um.

00:06:27,266 --> 00:06:33,470
SPEAKER_0:  That was like, you know, a lot of people probably lived and died with never experiencing hot water. Maybe they had a way to heat water over a fire.

00:06:33,858 --> 00:06:35,550
SPEAKER_0:  But like then it's, I don't know. It's just like.

00:06:36,066 --> 00:06:37,918
SPEAKER_0:  There's so many things about our lives.

00:06:38,146 --> 00:06:38,686
SPEAKER_0:  now.

00:06:39,010 --> 00:06:39,518
SPEAKER_0:  that.

00:06:39,746 --> 00:06:42,371
SPEAKER_0:  are complete and just total anomaly.

00:06:42,371 --> 00:06:46,014
SPEAKER_1:  It makes me wonder like what is the thing that I would notice the most? I mean the sewer system.

00:06:46,562 --> 00:06:48,030
SPEAKER_1:  Like it doesn't smell.

00:06:48,578 --> 00:06:49,342
SPEAKER_1:  in series.

00:06:50,050 --> 00:06:55,550
SPEAKER_1:  Incredible. What does the sewer system do? I mean, it gets rid of waste efficiently, et cetera, we don't have to confront it.

00:06:55,874 --> 00:06:57,822
SPEAKER_1:  both with our, with any of our senses.

00:06:58,178 --> 00:06:59,390
SPEAKER_1:  And that probably wasn't there.

00:07:00,034 --> 00:07:02,773
SPEAKER_1:  I mean, what else? Plus all the medical stuff associated with- Je-

00:07:02,773 --> 00:07:09,502
SPEAKER_0:  I mean, how about the disease? How about the cockroaches and the rats and the disease and the plagues and you know?

00:07:09,858 --> 00:07:13,598
SPEAKER_0:  And then when they got, so they caught more diseases, but then when they caught the disease.

00:07:13,922 --> 00:07:14,718
SPEAKER_0:  They also didn't.

00:07:14,946 --> 00:07:33,726
SPEAKER_0:  have treatment for. So they often would die or they would just be in a huge amount of pain. They also didn't know what the disease was. They don't know about microbes. That was this new thing, the idea that these tiny little animals that are causing these diseases. So what did they think? You know, in the bubonic plague, you know, in the Black Death, the 1300s, people...

00:07:34,114 --> 00:07:36,606
SPEAKER_0:  thought that it was an act of God because

00:07:36,898 --> 00:07:38,334
SPEAKER_0:  You know, God's angry at us.

00:07:38,626 --> 00:07:41,918
SPEAKER_0:  Why would you not think that if you didn't know what it was?

00:07:42,562 --> 00:07:51,294
SPEAKER_0:  And so the crazy thing is that these were the same primates. So I do know something about them. I know in some sense what it's like to be them because I am a human as well.

00:07:51,650 --> 00:07:55,582
SPEAKER_0:  and to know that this particular primate that I know what it's like to be

00:07:56,098 --> 00:08:02,622
SPEAKER_0:  experienced such different things. It's, and like this isn't, our life is not the life that this primate.

00:08:02,882 --> 00:08:05,022
SPEAKER_0:  has experienced almost ever. So it's just a-

00:08:05,570 --> 00:08:06,462
SPEAKER_0:  It's just a bit strange.

00:08:06,978 --> 00:08:09,406
SPEAKER_1:  I don't know, I have a sense that we would get.

00:08:09,634 --> 00:08:10,462
SPEAKER_1:  Acclimated.

00:08:10,946 --> 00:08:13,374
SPEAKER_1:  very quickly, like if we threw ourselves back.

00:08:13,922 --> 00:08:15,038
SPEAKER_1:  a few thousand years ago.

00:08:15,490 --> 00:08:20,414
SPEAKER_1:  it would be very uncomfortable at first, but the whole hot shower thing, you'll get used to it. after a year.

00:08:20,738 --> 00:08:22,494
SPEAKER_1:  You would not even like miss it.

00:08:22,754 --> 00:08:23,166
SPEAKER_1:  Is that?

00:08:23,650 --> 00:08:28,190
SPEAKER_1:  was there's a few trying to remember which book that talks about hiking that Appalachian trail.

00:08:29,410 --> 00:08:31,070
SPEAKER_1:  but you kinda miss those hot showers.

00:08:31,298 --> 00:08:34,110
SPEAKER_1:  But I have a sense like after a few months, after a few years.

00:08:34,466 --> 00:08:38,174
SPEAKER_0:  Well, your skill recalibrates. Yeah. I was saying the other day.

00:08:38,658 --> 00:08:44,094
SPEAKER_0:  to a friend that whatever you're used to, you start to think that, oh, that the people that have more than me are more fortunate.

00:08:44,578 --> 00:08:49,118
SPEAKER_0:  It just sounds incredible. I would be so happy, but you know that's not true. Experience.

00:08:49,378 --> 00:08:50,430
SPEAKER_0:  What would happen is you would.

00:08:50,818 --> 00:08:53,150
SPEAKER_0:  you would get these new things or you would...

00:08:53,442 --> 00:08:58,558
SPEAKER_0:  get these new opportunities and then you would get used to it and then you would this the hedonic treadmill you'd come back to where you are.

00:08:58,818 --> 00:09:01,822
SPEAKER_0:  And likewise though, because you think, Oh my God, what if I had to?

00:09:02,210 --> 00:09:07,582
SPEAKER_0:  you know, have this kind of job that I never would want, or I had this kind of marriage that I never would want.

00:09:07,842 --> 00:09:18,590
SPEAKER_0:  You know what, if you did, you would adjust and you get used to it and you might not be that much less happy than you are now. So on the other side of the you being okay going back, we would survive if we had to go back.

00:09:18,850 --> 00:09:27,774
SPEAKER_0:  You know, we'd have to learn some skills, but we would buck up. And people have gone to war before that. We're in the shopkeepers a year before that. They were in the trenches the next year.

00:09:28,642 --> 00:09:30,558
SPEAKER_0:  But on the other hand, if you brought them here.

00:09:31,298 --> 00:09:33,278
SPEAKER_0:  You know, I always think it'd be so fun to just bring forget them.

00:09:33,506 --> 00:09:39,902
SPEAKER_0:  hunter gatherers, bring a 1700s person here and tour them around, take them on an airplane show them your phone and all the things it can do.

00:09:40,130 --> 00:09:43,710
SPEAKER_0:  Show them the internet, show them the grocery store. Imagine taking them to a Whole Foods.

00:09:44,994 --> 00:09:48,606
SPEAKER_0:  Likewise, I think they would be completely awestruck and on their knees.

00:09:49,122 --> 00:09:50,174
SPEAKER_0:  crying tears of joy.

00:09:50,786 --> 00:09:55,390
SPEAKER_0:  And then they'd get used to it and they'd be a complainer about like, you don't have this oranges in stock. Cause like, you know.

00:09:55,746 --> 00:09:56,871
SPEAKER_0:  And that's it.

00:09:56,871 --> 00:10:00,606
SPEAKER_1:  Actually, a store is a tough one to get used to. Like when I first came to this country.

00:10:01,186 --> 00:10:05,342
SPEAKER_1:  the abundance of bananas was the thing that struck me the most. Like,

00:10:06,530 --> 00:10:11,582
SPEAKER_1:  Fruits in general, but food in general, but bananas somehow struck me the most.

00:10:12,002 --> 00:10:14,334
SPEAKER_1:  that you could just eat them as much as you want. That took-

00:10:14,722 --> 00:10:15,806
SPEAKER_1:  A long time.

00:10:16,034 --> 00:10:16,478
SPEAKER_1:  for me.

00:10:16,802 --> 00:10:18,622
SPEAKER_1:  Probably took several years to really.

00:10:19,554 --> 00:10:21,630
SPEAKER_1:  get acclimated to that.

00:10:21,986 --> 00:10:24,062
SPEAKER_0:  Is that? Why didn't you have bananas?

00:10:24,578 --> 00:10:27,454
SPEAKER_1:  Uh, the number of banana, fresh bananas.

00:10:28,226 --> 00:10:29,470
SPEAKER_1:  that that wasn't available.

00:10:30,210 --> 00:10:31,294
SPEAKER_1:  Bread, yes.

00:10:31,650 --> 00:10:32,670
SPEAKER_1:  Bananas, no.

00:10:33,826 --> 00:10:35,422
SPEAKER_0:  Yeah, it's like we don't even know what.

00:10:36,258 --> 00:10:38,622
SPEAKER_0:  to have, like we don't even know the proper levels of gratitude.

00:10:38,946 --> 00:10:40,926
SPEAKER_0:  You know, walking around the grocery store, I don't know what to be like.

00:10:41,442 --> 00:10:46,462
SPEAKER_0:  The bread's nice, but the bananas are like, we're so lucky. I don't know. I'm like, oh, I could have been the other way. I have no idea.

00:10:46,850 --> 00:10:47,966
SPEAKER_1:  What's interesting then?

00:10:48,450 --> 00:10:51,006
SPEAKER_1:  where we point our gratitude.

00:10:51,714 --> 00:10:52,638
SPEAKER_1:  in the West.

00:10:52,866 --> 00:10:53,726
SPEAKER_1:  in the United States.

00:10:54,754 --> 00:10:55,582
SPEAKER_1:  Probably

00:10:58,914 --> 00:11:01,534
SPEAKER_1:  Do we point it away from materialist possessions?

00:11:01,954 --> 00:11:02,494
SPEAKER_1:  towards.

00:11:03,010 --> 00:11:06,814
SPEAKER_1:  Or do we just aspire to do that towards other human beings that we love?

00:11:07,394 --> 00:11:08,606
SPEAKER_1:  because in the...

00:11:08,834 --> 00:11:10,302
SPEAKER_1:  East in the Soviet Union.

00:11:10,946 --> 00:11:12,350
SPEAKER_1:  While growing up poor.

00:11:12,930 --> 00:11:14,846
SPEAKER_1:  It's having food is the gratitude.

00:11:15,490 --> 00:11:16,062
SPEAKER_1:  having

00:11:16,386 --> 00:11:17,886
SPEAKER_1:  Transportation is gratitude.

00:11:18,434 --> 00:11:20,606
SPEAKER_1:  having warmth and shelter's gratitude.

00:11:21,762 --> 00:11:22,430
SPEAKER_1:  And now...

00:11:22,850 --> 00:11:23,966
SPEAKER_1:  But see, within that...

00:11:24,482 --> 00:11:28,254
SPEAKER_1:  The deep gratitude is for other human beings. It's the penguins huddling together.

00:11:28,706 --> 00:11:29,950
SPEAKER_1:  for warmth in the cold.

00:11:30,146 --> 00:11:41,182
SPEAKER_0:  I think it's a person by person basis. I mean, I'm sure yes, of course in the West, we will on average feel gratitude towards different things or maybe it's a different level of gratitude. Maybe we feel less gratitude than countries that.

00:11:42,594 --> 00:11:51,390
SPEAKER_0:  You know, obviously I think the easiest, that the person that's most likely to feel gratitude is going to be someone whose life happens to be one where they just move up, up, up throughout their life.

00:11:51,938 --> 00:11:55,614
SPEAKER_0:  A lot of people in the greatest generation, you know, people who were born in the twenties or whatever.

00:11:55,842 --> 00:11:57,182
SPEAKER_0:  and a lot of the boomers too.

00:11:57,474 --> 00:12:01,246
SPEAKER_0:  This story is the greatest generation grew up dirt poor and they often ended up middle class.

00:12:01,858 --> 00:12:06,270
SPEAKER_0:  And the boomers, some of them started off middle class and many of them ended up quite wealthy.

00:12:06,850 --> 00:12:07,358
SPEAKER_0:  And I feel like.

00:12:07,682 --> 00:12:09,086
SPEAKER_0:  that life trajectory.

00:12:09,730 --> 00:12:11,358
SPEAKER_0:  is naturally going to.

00:12:12,226 --> 00:12:17,566
SPEAKER_0:  foster gratitude, right? Because you're not gonna take for granted these things because you didn't have them.

00:12:18,114 --> 00:12:20,766
SPEAKER_0:  I didn't go out of the country really in my childhood.

00:12:21,090 --> 00:12:22,494
SPEAKER_0:  You know, like, you know.

00:12:22,754 --> 00:12:24,318
SPEAKER_0:  We traveled but it was

00:12:24,962 --> 00:12:30,462
SPEAKER_0:  Virginia to see my grandparents or Wisconsin to see other relatives or you know maybe Florida after going to the beach.

00:12:30,818 --> 00:12:34,750
SPEAKER_0:  And then I started going out of the country crazy in my 20s because I really, you know.

00:12:35,554 --> 00:12:36,542
SPEAKER_0:  my favorite thing.

00:12:36,898 --> 00:12:44,446
SPEAKER_0:  And I feel like because I, if I had grown up always doing that, it would have been another thing. I'm like, yeah, it's just something I do. But I still, every time I go to a new country.

00:12:44,770 --> 00:12:49,886
SPEAKER_0:  I'm like, oh my God, this is so cool. I'm in another country, this thing I've only seen on the map. I'm like, I'm there now.

00:12:50,242 --> 00:12:51,102
SPEAKER_0:  And so I feel like.

00:12:51,682 --> 00:12:56,094
SPEAKER_0:  It's a lot of times it's a product of what you didn't have and then you suddenly had.

00:12:56,866 --> 00:12:58,974
SPEAKER_0:  but I still think it's case by case in that.

00:12:59,586 --> 00:13:01,150
SPEAKER_0:  There's like a meter.

00:13:01,730 --> 00:13:03,230
SPEAKER_0:  in everyone's head, you know.

00:13:03,746 --> 00:13:05,918
SPEAKER_0:  that I think on, on, on, on, on.

00:13:06,210 --> 00:13:07,486
SPEAKER_0:  at a ten.

00:13:07,874 --> 00:13:13,534
SPEAKER_0:  you're experiencing just immense gratitude, right? which is a euphoric feeling, it's a great feeling.

00:13:14,146 --> 00:13:15,262
SPEAKER_0:  Um, and it's.

00:13:16,098 --> 00:13:19,358
SPEAKER_0:  It makes you happy. It's to savor what you have.

00:13:20,194 --> 00:13:23,262
SPEAKER_0:  down at the mountain of stuff you have that you're standing on.

00:13:23,714 --> 00:13:29,758
SPEAKER_0:  Right, to look down at it and say, oh my God, I'm so lucky. And I'm so grateful for this and this and this.

00:13:30,050 --> 00:13:35,934
SPEAKER_0:  obviously that's a happy exercise. Now when you move the meter down to six or seven, maybe you think that sometimes but you're

00:13:36,194 --> 00:13:37,278
SPEAKER_0:  You're not always thinking that.

00:13:37,538 --> 00:13:38,078
SPEAKER_0:  Um,

00:13:38,498 --> 00:13:39,710
SPEAKER_0:  because you're sometimes looking up.

00:13:40,194 --> 00:13:40,798
SPEAKER_0:  at this.

00:13:41,218 --> 00:13:58,750
SPEAKER_0:  of things that you don't have and the things that they have, but you don't or the things you wished you had or you thought you were gonna have or whatever. And that's the opposite direction to look, right? And that's the, either that's envy, that's yearning, or often it's if you think about your past, it's grievance.

00:13:59,010 --> 00:14:03,486
SPEAKER_0:  Right? And so then you go into a one and you have someone who feels like a complete victim.

00:14:03,842 --> 00:14:09,342
SPEAKER_0:  They are just a victim of the society, of their siblings and their parents and their loved one.

00:14:09,762 --> 00:14:10,142
SPEAKER_0:  I'm

00:14:10,754 --> 00:14:11,614
SPEAKER_0:  And they are.

00:14:11,906 --> 00:14:13,566
SPEAKER_0:  They're wallowing in.

00:14:14,402 --> 00:14:29,886
SPEAKER_0:  everything that's happened wrong to me, everything I should have that I don't, everything that has gone wrong for me. And so that's a very unhealthy, mentally unhealthy place to be. Anyone can go there. You know, there's an endless list of stuff you can be, it can be aggrieved about and an endless list of stuff you can have gratitude for.

00:14:30,242 --> 00:14:32,670
SPEAKER_0:  And so in some ways it's a choice and it's a habit.

00:14:33,090 --> 00:14:37,790
SPEAKER_0:  And maybe it's part of how we were raised or our natural demeanor, but it's such a good ex. You were really good at this, by the way.

00:14:38,178 --> 00:14:41,246
SPEAKER_0:  Your Twitter is like, go on.

00:14:41,826 --> 00:14:54,270
SPEAKER_0:  Like you're, you are constantly just saying, man, I'm lucky, or like, I'm so grateful for this. And that's, it's a good thing to do because you're reminding yourself, but you're also reminding other people to think that way. And it's like.

00:14:54,786 --> 00:14:57,374
SPEAKER_0:  We are lucky, you know, and.

00:14:58,018 --> 00:15:04,446
SPEAKER_0:  And so anyway, I think that scale can go from one to 10. And I think it's hard to be a 10. I think you'd be very happy if you could be. But I think trying to be above a five.

00:15:04,674 --> 00:15:06,846
SPEAKER_0:  and looking down at the things you have.

00:15:07,682 --> 00:15:14,238
SPEAKER_0:  more often than you are looking up at the things you don't or being resentful about the things that people have wronged you and

00:15:14,434 --> 00:15:15,550
SPEAKER_1:  Well the interesting thing...

00:15:15,778 --> 00:15:17,694
SPEAKER_1:  I think was an open question.

00:15:18,178 --> 00:15:21,822
SPEAKER_1:  but I suspect that you can control that knob for the individual.

00:15:22,146 --> 00:15:25,470
SPEAKER_1:  Like you yourself can choose. Like the Stoic philosophy you could choose.

00:15:25,890 --> 00:15:26,654
SPEAKER_1:  where you are.

00:15:26,882 --> 00:15:30,494
SPEAKER_1:  as a matter of habit, like you said. But you can also probably control that on a scale.

00:15:30,850 --> 00:15:32,446
SPEAKER_1:  of a family, of a tribe.

00:15:32,834 --> 00:15:34,814
SPEAKER_1:  of a nation, of a society.

00:15:35,138 --> 00:15:35,582
SPEAKER_1:  Hey, dude!

00:15:35,906 --> 00:15:40,382
SPEAKER_1:  You can describe a lot of the things that happened in Nazi Germany and different other parts.

00:15:40,610 --> 00:15:41,790
SPEAKER_1:  a history through a sort of...

00:15:42,050 --> 00:15:42,910
SPEAKER_1:  societal.

00:15:43,458 --> 00:15:45,214
SPEAKER_1:  envy and resentment that builds up.

00:15:45,858 --> 00:15:47,262
SPEAKER_1:  Maybe certain narratives.

00:15:47,906 --> 00:15:52,190
SPEAKER_1:  Pick up and then they infiltrate your mind and then now your knob goes to.

00:15:52,482 --> 00:15:55,678
SPEAKER_1:  from the gratitude for everything, it goes to resentment and envy.

00:15:55,970 --> 00:15:59,134
SPEAKER_0:  Germany between the two world wars, you know, like

00:15:59,362 --> 00:16:00,382
SPEAKER_0:  Like you said, the Soviet.

00:16:01,218 --> 00:16:01,662
SPEAKER_0:  Um...

00:16:02,498 --> 00:16:03,454
SPEAKER_0:  kind of mentality.

00:16:04,098 --> 00:16:06,590
SPEAKER_0:  Um, so yeah, and then when you're soaking in a culture.

00:16:06,818 --> 00:16:08,478
SPEAKER_0:  So there's this kind of two factors, right? It's

00:16:09,122 --> 00:16:09,566
SPEAKER_0:  Um...

00:16:10,434 --> 00:16:16,766
SPEAKER_0:  It's what's going on in your own head and then what's surrounding you and what's surrounding you kind of has concentric circles. There's your immediate

00:16:17,602 --> 00:16:18,238
SPEAKER_0:  group of people.

00:16:18,562 --> 00:16:20,542
SPEAKER_0:  because that group of people, if they're a certain way...

00:16:20,834 --> 00:16:28,958
SPEAKER_0:  they feel a lot of gratitude and they talk about it a lot, that kind of insulates you from the broader culture because people are gonna have the most impact on you or the ones closest.

00:16:29,602 --> 00:16:37,854
SPEAKER_0:  But often, all the concentric circles are saying the same thing, the people around you are feeling the same way that the broader community, which is feeling the same way as the broader country.

00:16:38,210 --> 00:16:38,686
SPEAKER_0:  Um.

00:16:39,266 --> 00:16:40,542
SPEAKER_0:  And you know, I think...

00:16:40,802 --> 00:16:42,718
SPEAKER_0:  This is why I think American patriotism.

00:16:43,618 --> 00:16:44,702
SPEAKER_0:  You know, nationalism.

00:16:45,186 --> 00:16:47,326
SPEAKER_0:  can be tribal, can be very, not a good thing.

00:16:48,162 --> 00:16:49,150
SPEAKER_0:  Patriotism.

00:16:49,538 --> 00:16:50,046
SPEAKER_0:  Um...

00:16:50,658 --> 00:16:54,750
SPEAKER_0:  I think is a great thing because really, what is patriotism? I mean, it's.

00:16:55,202 --> 00:17:00,702
SPEAKER_0:  If you love your country, you should love your fellow countrymen. You know, to patriot, you know, that's a Reagan quote. It's like patriotism is like.

00:17:01,218 --> 00:17:02,686
SPEAKER_0:  I think a feeling of like.

00:17:03,074 --> 00:17:03,646
SPEAKER_0:  Um...

00:17:03,970 --> 00:17:04,766
SPEAKER_0:  Unity.

00:17:05,442 --> 00:17:15,038
SPEAKER_0:  And, but it also comes along with an implicit kind of concept of gratitude, because it's like we are so lucky to live in, you know, people, you know, think it's chauvinist to say we live in the best country in the world.

00:17:15,522 --> 00:17:18,174
SPEAKER_0:  And yes, when Americans say that, no one likes it.

00:17:19,074 --> 00:17:27,230
SPEAKER_0:  Actually, it's not a bad thing to think. It's a nice thing to think. It's a way of saying, I'm so grateful for all the great things this country gives to me and this country has done.

00:17:27,522 --> 00:17:36,190
SPEAKER_0:  And I think if you heard a Filipino person say, you know what, the Philippines is the best country in the world. No one in America would say that's chauvinist. They'd say awesome, right? Because when they're coming from the.

00:17:36,418 --> 00:17:39,454
SPEAKER_0:  someone who's not American, it sounds totally fine.

00:17:39,682 --> 00:17:47,358
SPEAKER_0:  But I think, I think, you know, national pride is actually good. Now again, that can quickly translate into xenophobia and nationalism. So, you know, you have to make sure it doesn't go off that cliff.

00:17:47,906 --> 00:17:48,574
SPEAKER_1:  Yeah, there's.

00:17:48,866 --> 00:17:53,502
SPEAKER_1:  good ways to formulate that like you talk about, we'll talk about like high wrong progressivism.

00:17:53,858 --> 00:17:55,198
SPEAKER_1:  Hyron conservatism.

00:17:55,586 --> 00:17:55,966
SPEAKER_1:  desert.

00:17:56,514 --> 00:17:57,662
SPEAKER_1:  two different ways.

00:17:57,890 --> 00:17:58,270
SPEAKER_1:  of

00:17:59,138 --> 00:17:59,870
SPEAKER_1:  of uh...

00:18:00,098 --> 00:18:01,406
SPEAKER_1:  embodying patriotism.

00:18:02,050 --> 00:18:07,934
SPEAKER_1:  So you could talk about maybe loving the tradition that this country stands for, or you could talk about loving the people.

00:18:08,514 --> 00:18:09,438
SPEAKER_1:  the uh... that

00:18:09,666 --> 00:18:11,614
SPEAKER_1:  ultimately push progress and those are

00:18:12,578 --> 00:18:22,270
SPEAKER_1:  from an intellectual perspective, a good way to represent patriotism. We gotta zoom out, because this graphic is epic. A lot of images in your book are just epic on their own.

00:18:22,818 --> 00:18:23,902
SPEAKER_1:  is brilliantly done.

00:18:24,162 --> 00:18:26,206
SPEAKER_1:  but this one has famous people.

00:18:26,914 --> 00:18:27,710
SPEAKER_1:  Reach of the cards?

00:18:29,410 --> 00:18:30,462
SPEAKER_1:  Like the best of?

00:18:30,850 --> 00:18:32,958
SPEAKER_0:  Yeah. And by the way, good for them.

00:18:33,218 --> 00:18:34,846
SPEAKER_0:  To be the person, yeah.

00:18:35,362 --> 00:18:41,406
SPEAKER_0:  It's not that I could have chosen lots of people for each card, but I think most people would agree. Pretty fair choice for each.

00:18:41,794 --> 00:18:42,558
SPEAKER_0:  each page.

00:18:42,882 --> 00:18:48,126
SPEAKER_0:  And good for them to be, you know, you crushed it if you can be the person for your whole 250 year page.

00:18:48,674 --> 00:18:51,262
SPEAKER_1:  Well, I noticed he put Gandhi, didn't put Hitler. I mean...

00:18:51,906 --> 00:18:55,281
SPEAKER_1:  There's a lot of people gonna argue with you about that particular last page.

00:18:55,281 --> 00:19:12,062
SPEAKER_0:  True. Yes, you're right. I could have put it, I actually, I was thinking about Darwin there too. Darwin, Einstein. Yeah, exactly. You really could have put anyone. Did you think about putting yourself for a second? Yeah, I should have. I should have. That would have been awesome. I'm sure that would have endeared the readers to me from right from the beginning of the first page of the book.

00:19:12,418 --> 00:19:17,438
SPEAKER_1:  A little bit of a messianic complex going on, but yeah. So the list of people just so you know, so these are.

00:19:17,954 --> 00:19:22,654
SPEAKER_1:  250 year chunks, the last one being from 1770 to 2020.

00:19:22,946 --> 00:19:25,822
SPEAKER_1:  And so it goes Gandhi, Shakespeare, Joan of Arc.

00:19:26,274 --> 00:19:28,062
SPEAKER_1:  Jankos Khan, Charlemagne.

00:19:28,514 --> 00:19:29,182
SPEAKER_1:  Muhammad.

00:19:29,474 --> 00:19:32,126
SPEAKER_1:  Constantine Jesus Cleopatra Aristotle Buddha

00:19:33,442 --> 00:19:37,630
SPEAKER_1:  That's so interesting to think about this very recent human history.

00:19:37,730 --> 00:19:42,142
SPEAKER_0:  that's 11 pages, so it would be 2750, almost 3000 years.

00:19:43,202 --> 00:19:45,278
SPEAKER_1:  that there's these figures that stand out.

00:19:45,666 --> 00:19:46,686
SPEAKER_1:  and then define.

00:19:46,946 --> 00:19:48,350
SPEAKER_1:  the course of human history.

00:19:48,514 --> 00:19:52,638
SPEAKER_0:  It's like the cra- the craziest thing to me is that like Buddha was a dude.

00:19:53,346 --> 00:19:55,646
SPEAKER_0:  He was a guy with like...

00:19:56,418 --> 00:20:02,462
SPEAKER_0:  arms and legs and fingernails that he may be bit and like he liked certain foods and maybe he got like you

00:20:02,786 --> 00:20:05,150
SPEAKER_0:  Uh, you know, he had like

00:20:05,378 --> 00:20:10,334
SPEAKER_0:  digestive issues sometimes and like he got cuts and they stung and like he was a guy.

00:20:10,978 --> 00:20:15,998
SPEAKER_0:  And he had hopes and dreams and he probably had a big ego for a while before he I guess Buddha totally

00:20:16,354 --> 00:20:17,630
SPEAKER_0:  overcame that one, but like.

00:20:18,530 --> 00:20:27,166
SPEAKER_0:  And it's like, who knows, you know, the mythical figure of Buddha, who knows how similar he was, but the fact, same with Jesus, this was a guy, like to me, he's a primate.

00:20:28,034 --> 00:20:29,909
SPEAKER_0:  What a impact.

00:20:29,909 --> 00:20:31,934
SPEAKER_1:  a cell first and then a baby. Yeah!

00:20:32,354 --> 00:20:34,229
SPEAKER_0:  You should feed us at some point. That's a dumb bamb-

00:20:34,229 --> 00:20:34,979
SPEAKER_1:  be trying to learn.

00:20:34,979 --> 00:20:37,086
SPEAKER_0:  Yeah, like having a tantrum.

00:20:37,378 --> 00:20:41,503
SPEAKER_0:  Yeah. Because he's frustrated, because he's in the terrible twos. Jesus was in the terrible twos.

00:20:41,503 --> 00:20:43,751
SPEAKER_1:  I should have never had a tantrum, let's be honest.

00:20:43,751 --> 00:20:46,974
SPEAKER_0:  The mother is like, this baby's great.

00:20:47,266 --> 00:20:48,903
SPEAKER_1:  Wow. Let's figure it out.

00:20:48,903 --> 00:20:52,862
SPEAKER_0:  something up. It just let me this I mean listen hearing learning about Genghis Khan.

00:20:53,122 --> 00:20:55,454
SPEAKER_0:  It's incredible to me because it's just like.

00:20:55,714 --> 00:20:56,766
SPEAKER_0:  This was some.

00:20:57,826 --> 00:20:59,422
SPEAKER_0:  um Mongolian

00:21:00,354 --> 00:21:00,830
SPEAKER_0:  You know.

00:21:01,186 --> 00:21:05,022
SPEAKER_0:  Herder guy who was taken as a slave and he was like dirt poor

00:21:05,410 --> 00:21:11,102
SPEAKER_0:  you know, catching rats is a young teen with, you know, to feed him and his mom and his...

00:21:11,426 --> 00:21:12,318
SPEAKER_0:  I think his brother.

00:21:12,674 --> 00:21:13,118
SPEAKER_0:  Um...

00:21:13,570 --> 00:21:14,238
SPEAKER_0:  And it's just like.

00:21:15,458 --> 00:21:17,022
SPEAKER_0:  The odds?

00:21:18,082 --> 00:21:18,462
SPEAKER_0:  UN-

00:21:18,818 --> 00:21:20,190
SPEAKER_0:  When he was born, he was just one of-

00:21:21,826 --> 00:21:26,142
SPEAKER_0:  probably tens of thousands of random teen boys living in Mongolia.

00:21:26,722 --> 00:21:28,446
SPEAKER_0:  in the 1200s, the odds of that.

00:21:28,770 --> 00:21:29,150
SPEAKER_0:  person.

00:21:29,442 --> 00:21:32,542
SPEAKER_0:  any one of them being a household name today that we're talking about.

00:21:33,346 --> 00:21:37,630
SPEAKER_0:  It's just crazy, like what had to happen. And for that guy to, for that poor.

00:21:38,594 --> 00:21:41,278
SPEAKER_0:  Dirt poor herder to take over the world?

00:21:41,922 --> 00:21:45,054
SPEAKER_0:  I don't know, so history just continually blows my mind.

00:21:45,378 --> 00:21:45,758
SPEAKER_0:  You know.

00:21:45,922 --> 00:21:47,710
SPEAKER_1:  And here's the reason you and I are related.

00:21:48,194 --> 00:21:52,894
SPEAKER_0:  Probably. Yeah, no, I mean, it's also, that's the other thing, is that some of these dudes.

00:21:53,282 --> 00:21:53,726
SPEAKER_0:  Bye.

00:21:54,370 --> 00:22:01,662
SPEAKER_0:  becoming king by having a better army at the right time. William the Conqueror, whatever, is in the right place at the right time with the right army.

00:22:01,890 --> 00:22:05,630
SPEAKER_0:  and there's a weakness at the right moment and he comes over and he exploits it and ends up

00:22:06,434 --> 00:22:07,582
SPEAKER_0:  probably having, you know.

00:22:07,906 --> 00:22:08,350
SPEAKER_0:  I don't know.

00:22:08,770 --> 00:22:10,654
SPEAKER_0:  thousand children and those children are.

00:22:11,042 --> 00:22:12,766
SPEAKER_0:  high up people who might be having a ton of.

00:22:13,730 --> 00:22:19,038
SPEAKER_0:  the species is different now because of him. Like, forget England's different or, you know.

00:22:19,298 --> 00:22:21,566
SPEAKER_0:  European borders look different.

00:22:22,178 --> 00:22:30,558
SPEAKER_0:  Like we are, like we look different because of a small handful of people. You know, when I sometimes I think I'm like, oh, you know, this part of the world.

00:22:31,106 --> 00:22:33,118
SPEAKER_0:  I can recognize someone's Greek, you know, someone's.

00:22:33,474 --> 00:22:36,350
SPEAKER_0:  someone's wherever because they kind of have certain facial features.

00:22:36,642 --> 00:22:36,990
SPEAKER_0:  And I'm like.

00:22:37,218 --> 00:22:41,470
SPEAKER_0:  It may have happened. I mean, obviously that's a population, but it may be that like someone-

00:22:41,986 --> 00:22:43,966
SPEAKER_0:  600 years ago that looked like that.

00:22:44,546 --> 00:22:48,030
SPEAKER_0:  really spread their seed and that's why the ethnicity looks

00:22:48,354 --> 00:22:49,982
SPEAKER_0:  kind of like that now. Sorry, anyway.

00:22:51,138 --> 00:22:51,902
SPEAKER_1:  Yeah.

00:22:52,194 --> 00:22:54,654
SPEAKER_1:  Yeah. Do you think individuals like that can turn?

00:22:54,946 --> 00:22:56,062
SPEAKER_1:  the direction of history.

00:22:57,058 --> 00:23:00,318
SPEAKER_1:  Or is that an illusion that narrative would tell ourselves?

00:23:01,026 --> 00:23:04,446
SPEAKER_0:  Well, it's both. I mean, so I said William the Conqueror, right? Or Hitler, right?

00:23:05,666 --> 00:23:08,222
SPEAKER_0:  It's not that Hitler was born and destined to be great at all.

00:23:09,314 --> 00:23:11,326
SPEAKER_0:  I think in a lot of cases he's some.

00:23:11,650 --> 00:23:16,990
SPEAKER_0:  frustrated artist with a temper who's turning over the table in his studio and hitting his wife and being kind of a dick.

00:23:17,570 --> 00:23:19,262
SPEAKER_0:  and a total nobody, right?

00:23:19,714 --> 00:23:20,254
SPEAKER_0:  Um...

00:23:20,770 --> 00:23:24,062
SPEAKER_0:  I think almost all the times you could have put Hitler baby on Earth.

00:23:24,386 --> 00:23:34,590
SPEAKER_0:  He's a rando, right? And maybe sometimes he becomes some kind of, he uses the speaking ability because that ability was gonna be there either way, or maybe he uses it for something else.

00:23:36,610 --> 00:23:42,398
SPEAKER_0:  But that said, I also think, but it's not that World War II was going to happen either way.

00:23:42,626 --> 00:23:44,094
SPEAKER_0:  Right? So it's both. It's that like.

00:23:44,706 --> 00:23:47,710
SPEAKER_0:  these circumstances were one way and this person.

00:23:48,418 --> 00:23:52,094
SPEAKER_0:  came along at the right time and those two made a match made in this case hell.

00:23:52,386 --> 00:23:53,758
SPEAKER_1:  but it makes you wonder.

00:23:54,402 --> 00:23:56,478
SPEAKER_1:  Yes, it's a match in hell.

00:23:56,898 --> 00:23:58,942
SPEAKER_1:  Are there other people that could have taken this place?

00:23:59,298 --> 00:24:00,862
SPEAKER_1:  Or do these people that stand out?

00:24:01,282 --> 00:24:03,038
SPEAKER_1:  They're the rare spark of...

00:24:04,450 --> 00:24:07,454
SPEAKER_1:  genius whether it takes us towards evil or towards good.

00:24:07,842 --> 00:24:08,446
SPEAKER_1:  whether...

00:24:09,026 --> 00:24:13,214
SPEAKER_1:  those figures singularly define the trajectory of humanity.

00:24:13,538 --> 00:24:13,854
SPEAKER_1:  You know.

00:24:14,146 --> 00:24:20,862
SPEAKER_1:  what defines the trajectory of humanity in the 21st century, for example, might be the influence of AI, might be the influence of nuclear war.

00:24:21,154 --> 00:24:22,750
SPEAKER_1:  negative or positive.

00:24:23,586 --> 00:24:25,886
SPEAKER_1:  not in the case of nuclear war but

00:24:26,178 --> 00:24:27,230
SPEAKER_1:  The...

00:24:27,810 --> 00:24:28,830
SPEAKER_1:  bioengineering.

00:24:30,242 --> 00:24:31,166
SPEAKER_1:  Nanotech.

00:24:33,122 --> 00:24:34,270
SPEAKER_1:  virology.

00:24:35,266 --> 00:24:38,526
SPEAKER_1:  What else is there? Maybe the structure of governments and so on.

00:24:39,266 --> 00:24:43,134
SPEAKER_1:  maybe the structure of universities. I don't know. There could be singular figures that stand up.

00:24:43,746 --> 00:24:46,206
SPEAKER_1:  and lead the way for human. There will be.

00:24:46,498 --> 00:24:49,438
SPEAKER_1:  but I wonder if the society is the thing that manifests that.

00:24:49,954 --> 00:24:50,686
SPEAKER_1:  that person.

00:24:51,074 --> 00:24:52,190
SPEAKER_1:  or that person.

00:24:52,866 --> 00:24:54,974
SPEAKER_1:  really does have a huge

00:24:55,362 --> 00:24:58,334
SPEAKER_0:  I think it's probably a spectrum where there are some cases

00:24:58,786 --> 00:25:00,798
SPEAKER_0:  when a circumstance was such.

00:25:01,314 --> 00:25:02,174
SPEAKER_0:  something.

00:25:02,594 --> 00:25:04,382
SPEAKER_0:  like what happened was gonna happen.

00:25:04,706 --> 00:25:09,758
SPEAKER_0:  If you pluck that person from the earth, I don't know whether the Mongols is a good example or not, but maybe.

00:25:10,146 --> 00:25:12,510
SPEAKER_0:  It could be that if you plucked Genghis Khan as a baby.

00:25:13,474 --> 00:25:15,998
SPEAKER_0:  there was because of the specific way.

00:25:16,226 --> 00:25:18,910
SPEAKER_0:  Chinese civilization was at that time and the specific.

00:25:19,746 --> 00:25:20,702
SPEAKER_0:  you know, climate.

00:25:20,994 --> 00:25:24,094
SPEAKER_0:  you know, that was causing a certain kind of pressure on the Mongols.

00:25:24,354 --> 00:25:29,790
SPEAKER_0:  and the way they still had their great archers and they had their horses and they had a lot of the same advantages. So maybe it was like.

00:25:30,306 --> 00:25:33,022
SPEAKER_0:  It was waiting to happen, right? It was going to happen either way and-

00:25:33,442 --> 00:25:40,158
SPEAKER_0:  It might not have happened to the sit to the extent or whatever. So maybe, or you could go the full other direction and say, actually, this was probably not gonna happen.

00:25:40,450 --> 00:25:41,182
SPEAKER_0:  Um

00:25:41,442 --> 00:25:47,806
SPEAKER_0:  And I think World War II is an example. I kind of think World War II really was kind of the work.

00:25:48,034 --> 00:25:52,958
SPEAKER_0:  Of course it relied on all these other circumstances. You had to have the resentment in Germany. You have to have the Great Depression.

00:25:53,346 --> 00:25:53,854
SPEAKER_0:  But like.

00:25:54,306 --> 00:25:56,798
SPEAKER_0:  uh... i think you take it or out of pretty sure world

00:25:57,090 --> 00:25:57,982
SPEAKER_0:  World War II doesn't happen.

00:25:59,490 --> 00:26:00,615
SPEAKER_0:  Well then, it's...

00:26:00,615 --> 00:26:07,294
SPEAKER_1:  It's easier to answer these questions when you look at history, even recent history, but let's look at now. Let's look at, I'm sure we'll talk about social media.

00:26:07,714 --> 00:26:09,598
SPEAKER_1:  So who are the key players in social media?

00:26:10,274 --> 00:26:11,198
SPEAKER_1:  Mark Zuckerberg.

00:26:12,386 --> 00:26:13,854
SPEAKER_1:  What's the name of the MySpace guy, Tom?

00:26:14,274 --> 00:26:17,214
SPEAKER_1:  It's just Tom, yeah.

00:26:17,538 --> 00:26:23,166
SPEAKER_1:  There's a meme going around where like MySpace is like the perfect social media because no algorithmic involvement.

00:26:23,522 --> 00:26:24,647
SPEAKER_1:  Everybody's happy and positive.

00:26:24,647 --> 00:26:25,918
SPEAKER_0:  Plus, also, Tom did it right.

00:26:26,242 --> 00:26:30,494
SPEAKER_0:  At the time, you were like, oh man, Tom only made like a few million dollars.

00:26:30,914 --> 00:26:31,934
SPEAKER_0:  sucks to not be zuck.

00:26:32,834 --> 00:26:38,334
SPEAKER_0:  Tom might be living a nice life right now where he doesn't have this nightmare that these other people have.

00:26:38,914 --> 00:26:41,374
SPEAKER_1:  And he's always smiling in his profile picture.

00:26:41,634 --> 00:26:49,694
SPEAKER_1:  And so there's like Larry Pesce with Google that's kind of intermingled into that whole thing, into the development of the internet. Jack Dorsey now Elon.

00:26:50,242 --> 00:26:52,382
SPEAKER_1:  uh... who else i mean this people

00:26:52,962 --> 00:26:54,974
SPEAKER_1:  playing with the evolution of social media and

00:26:55,266 --> 00:26:57,118
SPEAKER_1:  To me, that seems to be.

00:26:58,850 --> 00:27:00,222
SPEAKER_1:  connected to the development.

00:27:00,450 --> 00:27:03,486
SPEAKER_1:  of AI and it seems like those singular figures.

00:27:03,842 --> 00:27:10,174
SPEAKER_1:  will define the direction of AI development and social media development, with social media seeming to have such a huge impact.

00:27:10,658 --> 00:27:12,254
SPEAKER_1:  on our collective.

00:27:12,514 --> 00:27:13,639
SPEAKER_1:  intelligence.

00:27:13,639 --> 00:27:15,134
SPEAKER_0:  It does feel in one way like.

00:27:16,258 --> 00:27:21,886
SPEAKER_0:  Individuals have an especially big impact right now in that a small number of people are pulling some big levers.

00:27:22,914 --> 00:27:23,486
SPEAKER_0:  Um...

00:27:24,002 --> 00:27:24,702
SPEAKER_0:  and you know.

00:27:24,930 --> 00:27:31,678
SPEAKER_0:  There can be a little meeting of three people at Facebook and they come out of that meeting and make a decision that totally changes the world, right?

00:27:32,322 --> 00:27:33,278
SPEAKER_0:  On the other hand...

00:27:33,890 --> 00:27:34,910
SPEAKER_0:  You see a lot of.

00:27:35,170 --> 00:27:36,766
SPEAKER_0:  a lot of conformity.

00:27:36,994 --> 00:27:39,422
SPEAKER_0:  You see a lot of green, they all pulled the plug on Trump the same day.

00:27:41,634 --> 00:27:45,758
SPEAKER_0:  So that suggests that there's some bigger force that is also kind of driving them.

00:27:46,306 --> 00:27:48,670
SPEAKER_0:  in which case it's less about the individuals.

00:27:49,154 --> 00:27:52,894
SPEAKER_0:  I think this is what is leadership, right? I mean, to me, leadership.

00:27:53,730 --> 00:27:59,806
SPEAKER_0:  is the ability to move things in a direction that the cultural forces are not already taking things.

00:28:00,450 --> 00:28:02,238
SPEAKER_0:  Right, a lot of times people seem like a leader.

00:28:02,690 --> 00:28:11,102
SPEAKER_0:  because they're just kind of hopping on the cultural wave and they happen to be the person who gets to the top of it and now it seems like they're, but actually the wave was already going.

00:28:12,162 --> 00:28:14,046
SPEAKER_0:  Real leadership is when...

00:28:14,338 --> 00:28:14,878
SPEAKER_0:  Um.

00:28:15,170 --> 00:28:17,854
SPEAKER_0:  is when someone actually changes the wave.

00:28:18,082 --> 00:28:20,766
SPEAKER_0:  changes the shape of the wave. Like I think Elon with.

00:28:21,058 --> 00:28:23,806
SPEAKER_0:  you know, SpaceX and with Tesla, like.

00:28:24,322 --> 00:28:25,374
SPEAKER_0:  genuinely like.

00:28:26,082 --> 00:28:30,846
SPEAKER_0:  shaped a wave. You know, maybe you could say that EVs were actually, they were gonna happen anyway, but it's.

00:28:31,362 --> 00:28:32,894
SPEAKER_0:  There's not much evidence.

00:28:33,282 --> 00:28:35,390
SPEAKER_0:  about at least happening when it did.

00:28:35,618 --> 00:28:40,062
SPEAKER_0:  If we end up on Mars, you can say that Elon was a genuine leader there.

00:28:40,354 --> 00:28:41,758
SPEAKER_0:  And so there are examples now like.

00:28:42,018 --> 00:28:44,062
SPEAKER_0:  Zuckerberg definitely has done a lot of leadership.

00:28:44,418 --> 00:28:45,214
SPEAKER_0:  along the way.

00:28:46,114 --> 00:28:47,806
SPEAKER_0:  He's also...

00:28:48,162 --> 00:28:50,526
SPEAKER_0:  potentially kind of like caught in a...

00:28:51,234 --> 00:28:54,686
SPEAKER_0:  a storm that is happening and he's one of the figures in it. So I don't know.

00:28:55,042 --> 00:28:56,126
SPEAKER_1:  And it's possible that...

00:28:56,546 --> 00:29:02,814
SPEAKER_1:  He is a big shaper if the metaverse becomes a reality. If in 30 years we're all living in a virtual world.

00:29:03,202 --> 00:29:04,958
SPEAKER_1:  To many people, it seems ridiculous now.

00:29:05,442 --> 00:29:06,846
SPEAKER_1:  That was a poor investment.

00:29:07,042 --> 00:29:08,286
SPEAKER_0:  We talked about getting...

00:29:08,642 --> 00:29:16,862
SPEAKER_0:  you know, I think it was something like a billion people with a VR headset in their pocket in, I think it was 10 years from now back in 2015. So-

00:29:17,122 --> 00:29:18,462
SPEAKER_0:  We're behind that.

00:29:19,138 --> 00:29:21,726
SPEAKER_0:  But when he was talking about that, and honestly, I...

00:29:22,754 --> 00:29:24,510
SPEAKER_0:  This is something I've been wrong about, because I-

00:29:24,738 --> 00:29:25,566
SPEAKER_0:  went to.

00:29:26,146 --> 00:29:28,222
SPEAKER_0:  one of the Facebook conferences and

00:29:28,610 --> 00:29:29,726
SPEAKER_0:  Try it out, all the new.

00:29:30,018 --> 00:29:32,350
SPEAKER_0:  Oculus stuff and I was like, you know, pretty early talking to

00:29:32,610 --> 00:29:36,702
SPEAKER_0:  some of the major players there, because I was gonna write a big post about it that then got swallowed by this book.

00:29:37,154 --> 00:29:37,470
SPEAKER_0:  But.

00:29:37,890 --> 00:29:44,734
SPEAKER_0:  But I would have been wrong in the post because in what I would have said was that this thing is when I tried it I was like

00:29:45,570 --> 00:29:51,518
SPEAKER_0:  This is, you know, some of them are sucks. Some of them make you nauseous and they're just not that, you know, the headsets were big and, you know.

00:29:51,906 --> 00:29:54,302
SPEAKER_0:  But I was like, the times when this is good, it is.

00:29:54,594 --> 00:30:00,318
SPEAKER_0:  I have this feeling I haven't had, it reminds me of the feeling I had when I first was five and I went to a friend's house and he had Nintendo.

00:30:00,834 --> 00:30:05,278
SPEAKER_0:  and he gave me the controller and I was looking at the screen and I pressed the button and Mario jumped.

00:30:05,634 --> 00:30:06,046
SPEAKER_0:  And I said.

00:30:06,658 --> 00:30:07,294
SPEAKER_0:  I said.

00:30:08,354 --> 00:30:15,678
SPEAKER_0:  I can make something on the screen move. And the same feeling I had the first time someone showed me how to send an email. It was like really early and he's like, you can send this. And I was like, he goes.

00:30:15,906 --> 00:30:18,590
SPEAKER_0:  I can press enter on my computer and something happens on your computer.

00:30:18,978 --> 00:30:23,806
SPEAKER_0:  Those were obviously, you know, when you have that feeling, it often means you're witnessing a paradigm shift.

00:30:24,322 --> 00:30:25,918
SPEAKER_0:  And I thought this is one of those things.

00:30:27,010 --> 00:30:29,854
SPEAKER_0:  And I still kind of think it is, but it's kind of weird that it hasn't.

00:30:30,498 --> 00:30:31,358
SPEAKER_0:  You know, like, we're...

00:30:31,714 --> 00:30:33,310
SPEAKER_0:  Where's the VR revolution, Mike?

00:30:33,954 --> 00:30:39,070
SPEAKER_1:  Yeah, I'm surprised because I'm with you. My first and still instinct is this feels like it changes everything.

00:30:39,682 --> 00:30:41,342
SPEAKER_1:  VR feels like it changes everything.

00:30:41,666 --> 00:30:42,910
SPEAKER_1:  but it's not changing anything.

00:30:43,106 --> 00:30:50,526
SPEAKER_0:  Like a dumb part of my brain is genuinely convinced that it's real. And then the smart part knows it's not. But that's why the dumb part was like, we're not walking off that cliff.

00:30:50,914 --> 00:30:55,582
SPEAKER_0:  The smart part is like you're on your rug. It's fine. The dumb part of my brain is like, I'm not walking off the cliff.

00:30:56,162 --> 00:30:57,342
SPEAKER_0:  So it's like, it's crazy. The

00:30:57,474 --> 00:30:58,846
SPEAKER_1:  Feel like it's waiting.

00:31:00,034 --> 00:31:03,550
SPEAKER_1:  for like that revolutionary person who comes in and says, I'm going to create a headset.

00:31:03,906 --> 00:31:06,531
SPEAKER_0:  Like honestly, like a Steve Jobs iPhone of honestly.

00:31:06,531 --> 00:31:10,526
SPEAKER_1:  a little bit of a Carmack type guy which is why it was really interesting for him to be involved with

00:31:10,978 --> 00:31:13,886
SPEAKER_1:  Facebook is basically how do we create a simple dumb thing?

00:31:14,242 --> 00:31:19,838
SPEAKER_1:  That's a hundred bucks, but actually creates that experience. And then there's going to be some viral killer app on it.

00:31:20,226 --> 00:31:22,430
SPEAKER_1:  and that's going to be the gateway into a thing.

00:31:22,882 --> 00:31:23,934
SPEAKER_1:  that's gonna change everything.

00:31:24,194 --> 00:31:27,934
SPEAKER_1:  I mean, I don't know what exactly was the thing that changed everything with a personal computer.

00:31:28,482 --> 00:31:28,862
SPEAKER_1:  at

00:31:29,634 --> 00:31:30,942
SPEAKER_1:  Is that understood? Why tha?

00:31:31,330 --> 00:31:34,773
SPEAKER_1:  Maybe graphics? What was the use case?

00:31:34,773 --> 00:31:35,582
SPEAKER_0:  I mean...

00:31:35,874 --> 00:31:45,249
SPEAKER_0:  Exactly wasn't wasn't the the the 84 Macintosh like a moment when it was like this is actually something that normal people can and want to.

00:31:45,249 --> 00:31:48,249
SPEAKER_1:  because it was less than $5,000 I think.

00:31:48,249 --> 00:31:56,862
SPEAKER_0:  I just think it had some like Steve Jobs user friendliness already to it that other ones hadn't had. I think Windows 95 was a really big deal. I remember like

00:31:57,250 --> 00:32:01,150
SPEAKER_0:  because I'm old enough to remember the MS DOS when I was like, I kind of remembered the command.

00:32:01,442 --> 00:32:05,982
SPEAKER_0:  And then suddenly this concept of like a window you drag something into or you double click an icon.

00:32:06,274 --> 00:32:08,510
SPEAKER_0:  which now seems like so obvious to us was like.

00:32:08,834 --> 00:32:12,094
SPEAKER_0:  Revolutionary, because it made it intuitive.

00:32:13,218 --> 00:32:14,174
SPEAKER_0:  You know, I don't, yeah.

00:32:14,274 --> 00:32:22,558
SPEAKER_1:  Windows 95 was good. It was crazy, yeah. I forget what the big leaps was, because Windows 2000 sucked, and then Windows XP was good.

00:32:23,042 --> 00:32:26,417
SPEAKER_0:  I moved to Mac around 2004, so I started.

00:32:26,417 --> 00:32:28,638
SPEAKER_1:  in losing control

00:32:28,994 --> 00:32:31,614
SPEAKER_1:  Well, us, the people, still use.

00:32:32,194 --> 00:32:33,502
SPEAKER_1:  Windows and Android.

00:32:34,114 --> 00:32:38,238
SPEAKER_1:  the device and the operating system of the people. not you elitist folk.

00:32:38,562 --> 00:32:39,422
SPEAKER_1:  with your books.

00:32:40,322 --> 00:32:43,198
SPEAKER_1:  and your what else and success.

00:32:43,426 --> 00:32:43,774
SPEAKER_1:  Okay.

00:32:44,194 --> 00:32:45,246
SPEAKER_1:  Uh...

00:32:46,690 --> 00:32:51,966
SPEAKER_1:  You write, more technology means better good times, but it also means better bad times.

00:32:52,418 --> 00:32:58,558
SPEAKER_1:  And the scary thing is, if the good and bad keep exponentially growing, it doesn't matter how great the good times become.

00:32:58,818 --> 00:33:02,558
SPEAKER_1:  If the bad gets to a certain level of bad, it's all over for us.

00:33:02,850 --> 00:33:05,118
SPEAKER_1:  Can you elaborate on this? Why is there...

00:33:05,506 --> 00:33:06,974
SPEAKER_1:  Why does the bad have a-

00:33:07,650 --> 00:33:08,510
SPEAKER_1:  that property.

00:33:09,794 --> 00:33:13,694
SPEAKER_1:  that if it's all exponentially getting more powerful, then the bat is gonna win in the end.

00:33:14,050 --> 00:33:15,925
SPEAKER_1:  was is my misinterpreting that you know

00:33:15,925 --> 00:33:18,302
SPEAKER_0:  Oh, so the first thing is I noticed a trend.

00:33:18,978 --> 00:33:19,774
SPEAKER_0:  Which was like...

00:33:21,122 --> 00:33:25,502
SPEAKER_0:  The centuries, the good is getting better every century, like the 20th century.

00:33:25,858 --> 00:33:35,646
SPEAKER_0:  was the best century yet in terms of prosperity, in terms of GDP per capita, in terms of life expectancy, in terms of poverty and disease, every metric that matters.

00:33:35,970 --> 00:33:37,278
SPEAKER_0:  The 20th century was incredible.

00:33:37,762 --> 00:33:43,902
SPEAKER_0:  It also had the biggest wars in history, the biggest genocide in history, the biggest existential threat yet with nuclear weapons.

00:33:44,258 --> 00:33:51,262
SPEAKER_0:  Right? You know, the depression was, you know, probably as big an economic. So it's this interesting thing where the stakes are getting high.

00:33:51,586 --> 00:33:52,702
SPEAKER_0:  in both directions.

00:33:53,218 --> 00:33:55,710
SPEAKER_0:  And so the question is like, if you get enough good.

00:33:56,194 --> 00:33:56,798
SPEAKER_0:  does that.

00:33:57,154 --> 00:34:03,294
SPEAKER_0:  protect you against the bad, right? The dream, and I do think this is possible too, because the good gets so good.

00:34:03,682 --> 00:34:06,046
SPEAKER_0:  You know, have you ever read the culture series the Ian Banks books?

00:34:06,370 --> 00:34:11,774
SPEAKER_1:  Not yet, but I get criticized on a daily basis by some of the mutual folks we know for not having done so.

00:34:12,130 --> 00:34:14,005
SPEAKER_1:  I feel like a lesser man for it, yes.

00:34:14,005 --> 00:34:17,534
SPEAKER_0:  I need to change. So that's how I got onto it. And I read six of the ten.

00:34:18,114 --> 00:34:18,494
SPEAKER_0:  Um.

00:34:18,946 --> 00:34:22,494
SPEAKER_0:  and they're great. But the thing I love about them is like, it just paints one of these.

00:34:23,362 --> 00:34:24,670
SPEAKER_0:  Nutristic societies.

00:34:25,218 --> 00:34:26,430
SPEAKER_0:  where the good has.

00:34:27,042 --> 00:34:34,142
SPEAKER_0:  has gotten so good that the bad is no longer even an issue. Like basically in the way that this works is the AI.

00:34:34,562 --> 00:34:35,678
SPEAKER_0:  You know, the AIs.

00:34:35,970 --> 00:34:36,478
SPEAKER_0:  Um...

00:34:37,474 --> 00:34:40,254
SPEAKER_0:  are benevolent and they control everything. And so like this one.

00:34:40,930 --> 00:34:44,030
SPEAKER_0:  random anecdote where they're like, you know, what happens if you murder someone?

00:34:44,962 --> 00:34:49,118
SPEAKER_0:  in because you still know there's still people with rage and jealousy or whatever so so someone murder someone

00:34:49,442 --> 00:34:51,198
SPEAKER_0:  First of all, that person's backed up.

00:34:51,458 --> 00:34:53,022
SPEAKER_0:  So it's like they have to get a new body and it's.

00:34:53,282 --> 00:34:55,678
SPEAKER_0:  It's annoying, but it's like it's not death.

00:34:56,098 --> 00:35:12,414
SPEAKER_0:  And secondly, that person, what are they gonna do? Put them in jail? No, no, no, they're just gonna send a slap drone around, which is this little like tiny, you know, random drone that just will float around next to them forever. And by the way, kind of be their servant. Like it's kind of fun to have a slap drone, but it's just making sure that they never do anything. And it's like, I was like, oh man, it could just be, everyone could be so safe.

00:35:12,770 --> 00:35:22,878
SPEAKER_0:  And everything can be so like, you know, you want a house, you know, the AIs will build your house, there's endless space, there's endless resources. So I do think that that could be part of our future. That's part of what excites me is like there is.

00:35:23,682 --> 00:35:25,662
SPEAKER_0:  Today would seem like a utopia to Thomas Jefferson.

00:35:25,890 --> 00:35:28,606
SPEAKER_0:  Thomas Jefferson's world would seem like a utopia to a caveman.

00:35:30,178 --> 00:35:34,238
SPEAKER_0:  There is a future, and by the way, these are happening faster, these jumps, right?

00:35:34,562 --> 00:35:38,814
SPEAKER_0:  So the thing that would seem like a utopia to us, we could experience in our own lifetimes.

00:35:39,074 --> 00:35:39,646
SPEAKER_0:  Right?

00:35:39,874 --> 00:35:43,838
SPEAKER_0:  It's especially if you know life extension combines with exponential progress.

00:35:44,514 --> 00:35:53,662
SPEAKER_0:  I want to get there and I think in that part of what makes it utopia is you don't have to be as scared of the Worst bad guy in the world trying to do the worst damage because we have protection.

00:35:54,818 --> 00:35:55,646
SPEAKER_0:  But that said...

00:35:56,034 --> 00:35:58,878
SPEAKER_0:  I'm not sure how that happens.

00:36:00,514 --> 00:36:07,390
SPEAKER_0:  It's easier said than done. Nick Bostrom uses the example of, if nuclear weapons could be manufactured by microwaving sand.

00:36:07,970 --> 00:36:08,926
SPEAKER_0:  for example.

00:36:09,570 --> 00:36:12,094
SPEAKER_0:  we'd probably be in the Stone Age right now because-

00:36:12,642 --> 00:36:17,918
SPEAKER_0:  0.001% of people would love to destroy all of humanity, right? Some 16 year old.

00:36:18,274 --> 00:36:22,494
SPEAKER_0:  with huge mental health problems, who right now goes and shoots up a school, would say, oh, even better.

00:36:22,818 --> 00:36:24,094
SPEAKER_0:  I'm going to blow up a city.

00:36:24,514 --> 00:36:27,198
SPEAKER_0:  and now suddenly there's copycats, right? ASOUND

00:36:27,458 --> 00:36:27,934
SPEAKER_0:  So.

00:36:28,866 --> 00:36:29,534
SPEAKER_0:  That's like.

00:36:30,082 --> 00:36:31,646
SPEAKER_0:  as our technology grows.

00:36:31,906 --> 00:36:32,926
SPEAKER_0:  It's going to be.

00:36:33,218 --> 00:36:34,078
SPEAKER_0:  Easier for

00:36:34,754 --> 00:36:36,094
SPEAKER_0:  the worst bad guys.

00:36:36,898 --> 00:36:44,030
SPEAKER_0:  to do tremendous damage and it's easier to destroy than to build, so it takes a tiny, tiny number of these people.

00:36:44,930 --> 00:36:46,814
SPEAKER_0:  with enough power to do that. So.

00:36:47,874 --> 00:36:52,414
SPEAKER_0:  To me, I'm like the stakes are going up because what we have to lose is this incredible utopia, but also-

00:36:53,090 --> 00:36:58,814
SPEAKER_0:  Dystopia is real, it happens. The Romans ended up in a dystopia they probably earlier thought that was never possible.

00:36:59,714 --> 00:37:00,990
SPEAKER_0:  We should not get cocky.

00:37:01,730 --> 00:37:03,518
SPEAKER_0:  And so to me that trend.

00:37:04,418 --> 00:37:13,054
SPEAKER_0:  is the exponential tech is a double edged sword. It's so exciting. I'm happy to be alive now overall because I'm an optimist and I find it exciting, but.

00:37:13,282 --> 00:37:22,654
SPEAKER_0:  It's really scary and the dumbest thing we can do is not be scared. Dumbest thing we can do is get cocky and think, well, my life is always, in the last couple generations everything's been fine.

00:37:23,554 --> 00:37:23,966
SPEAKER_0:  Stop that.

00:37:24,962 --> 00:37:25,854
SPEAKER_1:  What's your gut?

00:37:26,306 --> 00:37:29,086
SPEAKER_1:  what percentage of trajectories take us towards the.

00:37:29,442 --> 00:37:32,062
SPEAKER_1:  as you put unimaginably good future versus

00:37:32,450 --> 00:37:33,790
SPEAKER_1:  unimaginably bad future.

00:37:34,402 --> 00:37:36,510
SPEAKER_1:  is an optimist.

00:37:36,834 --> 00:37:41,182
SPEAKER_0:  It's really hard to know. I mean, all I can, you know, one of the things we can do is look at history.

00:37:41,730 --> 00:37:42,462
SPEAKER_0:  and

00:37:43,618 --> 00:37:44,382
SPEAKER_0:  On one hand.

00:37:44,770 --> 00:37:49,310
SPEAKER_0:  There's a lot of stories. I actually listening to a great podcast right now called The Fall of Civilizations.

00:37:50,050 --> 00:37:58,686
SPEAKER_0:  And it's literally every episode is like, you know, a little like two hour deep dive into some civilization Some are really famous like the Roman Empire some are more obscure.

00:37:58,978 --> 00:38:00,830
SPEAKER_0:  like the Norris in Greenland.

00:38:02,370 --> 00:38:05,470
SPEAKER_0:  Um, but it's each one is so interesting, but

00:38:05,922 --> 00:38:06,462
SPEAKER_0:  What's?

00:38:07,234 --> 00:38:14,686
SPEAKER_0:  It's, I mean, there's a lot of civilizations that had their peak. There's always the peak, right? And they're thriving and they're at their max size and-

00:38:15,074 --> 00:38:18,334
SPEAKER_0:  and they have their waterways and they have their.

00:38:18,594 --> 00:38:25,950
SPEAKER_0:  It's civilized and it's representative and it's fair and whatever. Not always, but the peak is great. If I could go back in time...

00:38:26,210 --> 00:38:32,990
SPEAKER_0:  You know, it's not that you don't, the farther you go back, the worse it gets. No, no, no, you wanna go back to a civilization, I would go to the Roman Empire in the year 100.

00:38:33,282 --> 00:38:37,649
SPEAKER_0:  sounds great, right? You don't wanna go to the Roman Empire in the year 400. We might be in the peak of-

00:38:37,649 --> 00:38:39,149
SPEAKER_1:  right now here, wherever.

00:38:39,149 --> 00:38:42,238
SPEAKER_0:  Honestly, I think about like the 80s.

00:38:42,466 --> 00:38:48,030
SPEAKER_0:  You know, the 70s, the 80s. Oh, here we go. The music. No, no, I hate these. It's so much better. They're so annoying.

00:38:48,418 --> 00:38:49,854
SPEAKER_0:  It's just like, I'm...

00:38:50,082 --> 00:38:53,310
SPEAKER_0:  When I listen to these things, I'm thinking, you know, the 80s and 90s.

00:38:53,826 --> 00:38:55,998
SPEAKER_0:  America in the 90s was popular.

00:38:56,354 --> 00:39:05,214
SPEAKER_0:  People forget that now. Clinton was a superstar around the world. Michael Jordan was exported internationally, then basketball was everywhere suddenly. You had like.

00:39:05,442 --> 00:39:12,862
SPEAKER_0:  music, the sports, whatever. It was a little probably like the 50s, you know, coming out of the World War and the Depression before it, it was like this kind of like.

00:39:13,122 --> 00:39:15,518
SPEAKER_0:  Everyone was in a good mood kind of time, you know.

00:39:15,778 --> 00:39:20,094
SPEAKER_0:  It's like a finish a big project and it's Saturday. It was like, I feel like the 50s was kind of like, everyone was

00:39:20,578 --> 00:39:21,726
SPEAKER_0:  Haven't it, you know, the...

00:39:22,146 --> 00:39:25,022
SPEAKER_0:  20s, I feel like everyone was in good mood randomly.

00:39:25,474 --> 00:39:27,422
SPEAKER_0:  then the 30s everyone was in a bad mood.

00:39:27,650 --> 00:39:28,030
SPEAKER_0:  Um.

00:39:28,578 --> 00:39:35,614
SPEAKER_0:  The 90s, I think, will look back on it as a time when everyone was in a good mood. And it was like, you know, again, of course, at the time it doesn't feel that way necessarily, but...

00:39:36,066 --> 00:39:38,654
SPEAKER_0:  I look at that, I'm like, maybe that was kind of America's peak.

00:39:38,978 --> 00:39:40,798
SPEAKER_0:  and like, maybe not, but like.

00:39:41,218 --> 00:39:46,142
SPEAKER_0:  It hasn't been popular since really worldwide. It's gone in and out depending on the country, but like.

00:39:46,402 --> 00:39:49,694
SPEAKER_0:  hasn't reached that level of like America's awesome around the world.

00:39:50,338 --> 00:39:55,646
SPEAKER_0:  and the political, you know, situations gotten, you know, really ugly and

00:39:56,258 --> 00:40:01,214
SPEAKER_0:  you know, maybe it's social media, maybe who knows, but I wonder if it'll ever be as simple.

00:40:01,890 --> 00:40:05,726
SPEAKER_0:  and positive as it was then. Like maybe we are in the in the

00:40:06,210 --> 00:40:12,414
SPEAKER_0:  you know, it feels a little like maybe we're in the beginning of the downfall or not, because these things don't just go, it's not a perfect smooth.

00:40:12,738 --> 00:40:16,113
SPEAKER_0:  It goes up and down and up and down. So maybe there's another big upcoming.

00:40:16,113 --> 00:40:19,710
SPEAKER_1:  unclear whether public opinion, which is kind of what you're talking to.

00:40:20,194 --> 00:40:23,134
SPEAKER_1:  is correlated strongly with influence.

00:40:23,618 --> 00:40:27,230
SPEAKER_1:  As you could say that even though America has been in decline in terms of public opinion,

00:40:27,938 --> 00:40:29,630
SPEAKER_1:  the exporting of technology.

00:40:30,242 --> 00:40:31,742
SPEAKER_1:  that America has still.

00:40:32,290 --> 00:40:36,574
SPEAKER_1:  with all the talk of China has still been leading the way in terms of AI, in terms of

00:40:36,930 --> 00:40:43,518
SPEAKER_1:  social media in terms of just basically any software related product. Like chips. Yeah, chips. So hardware and software.

00:40:44,162 --> 00:40:46,718
SPEAKER_1:  I mean, America leads the way. You could argue that Google and-

00:40:47,266 --> 00:40:52,926
SPEAKER_1:  Microsoft and Facebook are no longer American companies, they're international companies, but they really are still.

00:40:53,570 --> 00:40:56,926
SPEAKER_1:  at the headquartered in Silicon Valley, broadly speaking.

00:40:57,890 --> 00:40:59,358
SPEAKER_1:  uh... and that's of course and is

00:40:59,586 --> 00:41:05,118
SPEAKER_1:  All the technological innovation still seems to be happening in the United States.

00:41:05,570 --> 00:41:06,942
SPEAKER_1:  Although culturally.

00:41:07,618 --> 00:41:08,766
SPEAKER_1:  and politically.

00:41:10,050 --> 00:41:12,670
SPEAKER_1:  This is not good.

00:41:13,250 --> 00:41:14,558
SPEAKER_1:  Well, maybe that could shift.

00:41:15,426 --> 00:41:19,006
SPEAKER_1:  at any moment when all the technological development can actually be.

00:41:19,746 --> 00:41:20,350
SPEAKER_1:  Create some

00:41:20,962 --> 00:41:23,038
SPEAKER_1:  positive impact in the world that could shift it.

00:41:23,458 --> 00:41:25,566
SPEAKER_1:  with the right leadership and so on, with the right messaging.

00:41:26,786 --> 00:41:28,222
SPEAKER_0:  Yeah, I think...

00:41:29,186 --> 00:41:35,518
SPEAKER_0:  I don't feel confident at all about whether, no, no, I don't mean that. I don't feel confident in my opinion that.

00:41:36,034 --> 00:41:40,286
SPEAKER_0:  we may be on the downswing or that we may be, I truly don't know. It's like, I think the people.

00:41:41,634 --> 00:41:46,174
SPEAKER_0:  These are really big macro stories that are really hard to see when you're inside of them. It's like.

00:41:46,402 --> 00:41:52,574
SPEAKER_0:  It's like being on a beach and running around, you know, a few miles this way and trying to suss out the shape of the coastline. Like,

00:41:52,898 --> 00:42:01,918
SPEAKER_0:  It's just really hard to see the big picture. You know, you get caught up in the micro stories, the little tiny ups and downs that are part of some bigger trend.

00:42:02,498 --> 00:42:08,190
SPEAKER_0:  And also giant paradigm shifts happen quickly nowadays. The internet came out of nowhere and suddenly was like...

00:42:09,090 --> 00:42:14,750
SPEAKER_0:  you know, change everything. So there could be a change everything thing on the way. It seems like there's a few candidates for it. And like, but, but I mean.

00:42:15,106 --> 00:42:20,318
SPEAKER_0:  It feels like the stakes are just high, higher than it even was for the Romans, higher than it was for because.

00:42:20,770 --> 00:42:21,214
SPEAKER_0:  Um.

00:42:21,922 --> 00:42:26,622
SPEAKER_0:  that we're more powerful as a species. We have god-like powers with technology.

00:42:27,138 --> 00:42:29,950
SPEAKER_0:  that other civilizations at their peak didn't have.

00:42:30,530 --> 00:42:39,774
SPEAKER_1:  I wonder if those high stakes and powers will feel laughable to people that live humans, aliens, cyborgs, whatever lives 100 years from now.

00:42:40,610 --> 00:42:42,718
SPEAKER_1:  that maybe are a little.

00:42:42,946 --> 00:42:43,262
SPEAKER_1:  like.

00:42:44,002 --> 00:42:47,230
SPEAKER_1:  feeling of political and technological turmoil is nothing.

00:42:47,362 --> 00:42:48,670
SPEAKER_0:  Well, that's the big question.

00:42:49,090 --> 00:42:55,614
SPEAKER_0:  You could eat, so right now, you know, you know the 1890s was like a super politically contentious decade in the US. It was like.

00:42:55,906 --> 00:43:04,542
SPEAKER_0:  immense tribalism and the newspapers were all like lying and telling you know, you know, there was a lot of like what we would associate with today's media, the worst of it.

00:43:04,770 --> 00:43:09,246
SPEAKER_0:  And it was over gold or silver being this, I don't know, it's something that I don't understand. But

00:43:09,922 --> 00:43:17,342
SPEAKER_0:  The point is, it was a little bit of a blip, right? It happened, it must've felt like the end of days at the time, and then now we, most people don't even know about it.

00:43:18,274 --> 00:43:25,950
SPEAKER_0:  versus, you know, again, the Roman Empire actually collapsed. And so the question is just like, is yeah, you know, will in 50 years, will this be?

00:43:26,530 --> 00:43:31,678
SPEAKER_0:  Like, or like McCarthyism. Oh, they had like, oh, that was like a crazy few years in America and then it was fine.

00:43:32,290 --> 00:43:34,654
SPEAKER_0:  Or is this the beginning of something really big?

00:43:37,122 --> 00:43:38,846
SPEAKER_1:  Well, I wonder if we can predict the...

00:43:39,106 --> 00:43:43,742
SPEAKER_1:  what the big thing is at the beginning. it feels like we're not we're just here along for the ride.

00:43:44,162 --> 00:43:45,374
SPEAKER_1:  and at the local level.

00:43:45,858 --> 00:43:48,158
SPEAKER_1:  and at every level are trying to do our best.

00:43:48,514 --> 00:43:50,430
SPEAKER_0:  But we're all just. Your best what's the.

00:43:50,658 --> 00:43:51,966
SPEAKER_0:  That's the one thing I know for sure.

00:43:52,514 --> 00:43:54,718
SPEAKER_0:  is that we need to have our wits about us and.

00:43:54,946 --> 00:43:57,406
SPEAKER_0:  do our best and the way that we can do that.

00:43:57,634 --> 00:43:59,390
SPEAKER_0:  We have to be as wise as possible.

00:43:59,714 --> 00:44:01,406
SPEAKER_0:  Right. To proceed forward.

00:44:01,698 --> 00:44:02,654
SPEAKER_0:  and wisdom.

00:44:03,394 --> 00:44:06,014
SPEAKER_0:  is an emergent property of discourse.

00:44:06,306 --> 00:44:08,382
SPEAKER_1:  So you're a proponent of wisdom versus stupidity?

00:44:08,642 --> 00:44:09,822
SPEAKER_1:  because you can make a...

00:44:10,210 --> 00:44:11,966
SPEAKER_1:  I can still man the case for stupidity.

00:44:13,442 --> 00:44:16,510
SPEAKER_1:  I probably can't but there's some

00:44:17,090 --> 00:44:21,598
SPEAKER_1:  I think wisdom, and you talk about this, can come with a false confidence.

00:44:21,890 --> 00:44:23,966
SPEAKER_1:  arrogance, I mean you talk about this in the book.

00:44:24,098 --> 00:44:25,534
SPEAKER_0:  That's too easy. What's that wisdom?

00:44:26,082 --> 00:44:27,454
SPEAKER_0:  If you're being arrogant, you're being unwary.

00:44:28,002 --> 00:44:33,982
SPEAKER_0:  Unwise. Yeah, I think wisdom is doing what people 100 years from now with the hindsight that we don't have.

00:44:34,466 --> 00:44:40,841
SPEAKER_0:  would do if they could come back in time and they knew everything. It's like, how do we figure out how to have hindsight when we actually are not? What if stupidity-

00:44:40,841 --> 00:44:44,734
SPEAKER_1:  is the thing that people from 100 years from now will see as wise.

00:44:45,538 --> 00:44:49,767
SPEAKER_1:  i mean the idiot by the c f keeping they even uh... trusting everybody

00:44:49,767 --> 00:44:55,550
SPEAKER_0:  Maybe that's what. Well then you get lucky. Then maybe you get to a good future by stumbling upon it.

00:44:56,066 --> 00:44:56,606
SPEAKER_0:  Um.

00:44:57,026 --> 00:45:00,478
SPEAKER_0:  But ideally you can get there. Like I think a lot of we.

00:45:01,026 --> 00:45:04,126
SPEAKER_0:  America, the great things about it are a product of.

00:45:05,090 --> 00:45:06,782
SPEAKER_0:  the wisdom of previous Americans.

00:45:07,042 --> 00:45:08,350
SPEAKER_0:  You know, the Constitution was...

00:45:08,674 --> 00:45:09,054
SPEAKER_0:  Pretty.

00:45:10,210 --> 00:45:12,254
SPEAKER_0:  pretty wise system to set up.

00:45:12,770 --> 00:45:20,862
SPEAKER_1:  There's not much stupid stumbling around. Well, there is, I mean, with Dostoevsky's The Idiot, Prince Michigan and brothers Karamazov.

00:45:21,314 --> 00:45:23,998
SPEAKER_1:  There's Alosha Karmasov.

00:45:24,418 --> 00:45:25,694
SPEAKER_1:  You err on the side.

00:45:26,530 --> 00:45:27,486
SPEAKER_1:  of love.

00:45:28,962 --> 00:45:32,350
SPEAKER_1:  and almost like a naive trust in other human beings.

00:45:32,738 --> 00:45:35,678
SPEAKER_1:  And that turns out to be, at least in my perspective, more than long term.

00:45:36,642 --> 00:45:39,326
SPEAKER_1:  for the success of the species is actually wisdom.

00:45:39,618 --> 00:45:40,743
SPEAKER_1:  It's a compass.

00:45:40,743 --> 00:45:44,574
SPEAKER_0:  No, it's a good compass. It's a compass when you're in the fog. In the fog, yeah. Yeah.

00:45:45,410 --> 00:45:51,518
SPEAKER_0:  Love is a compass. Okay, but here's the thing. So I think we should have, a compass is nice, but you know what else is nice? A flashlight in the fog.

00:45:51,906 --> 00:45:57,022
SPEAKER_0:  that can help, you can't see that far, but you can see, oh, you can see four feet ahead instead of one foot, and that to me is discourse.

00:45:57,250 --> 00:45:57,822
SPEAKER_0:  That is...

00:45:58,210 --> 00:45:59,806
SPEAKER_0:  open, vigorous, like

00:46:00,226 --> 00:46:01,150
SPEAKER_0:  discussion.

00:46:01,410 --> 00:46:08,542
SPEAKER_0:  in a culture that fosters that is how the species, how the American citizens as a unit can be.

00:46:09,986 --> 00:46:12,734
SPEAKER_0:  as wise as possible, can maybe see four feet ahead instead of one foot ahead.

00:46:12,898 --> 00:46:18,782
SPEAKER_1:  That said, Charles Bukowski said that love is a fog that fades with the first light of reality.

00:46:19,202 --> 00:46:23,358
SPEAKER_1:  I don't know how that works out, but I feel like there's intermixing of metaphors that works.

00:46:23,746 --> 00:46:29,246
SPEAKER_1:  Okay, you also write that quote as the authors of The Story of Us, which is this thousand-page book.

00:46:29,986 --> 00:46:36,990
SPEAKER_1:  We have no mentors, no editors, no one to make sure it all turns out okay. It's all in our hands.

00:46:37,442 --> 00:46:38,270
SPEAKER_1:  This scares me.

00:46:38,690 --> 00:46:39,966
SPEAKER_1:  but it's also what gives me hope.

00:46:40,482 --> 00:46:43,070
SPEAKER_1:  if we can all get just a little wiser together.

00:46:43,298 --> 00:46:47,486
SPEAKER_1:  It may be enough to nudge the story onto a trajectory that points towards

00:46:48,258 --> 00:46:50,046
SPEAKER_1:  unimaginably good future.

00:46:51,234 --> 00:46:51,742
SPEAKER_1:  Do you think?

00:46:52,226 --> 00:46:55,262
SPEAKER_1:  we can possibly define what a good future looks like.

00:46:56,674 --> 00:46:59,134
SPEAKER_1:  I mean, this is the problem with.

00:46:59,490 --> 00:47:00,702
SPEAKER_1:  that we ran into.

00:47:01,186 --> 00:47:01,950
SPEAKER_1:  with communism.

00:47:02,882 --> 00:47:04,542
SPEAKER_1:  of thinking of utopia.

00:47:05,986 --> 00:47:06,302
SPEAKER_1:  of.

00:47:06,562 --> 00:47:07,198
SPEAKER_1:  having a...

00:47:08,418 --> 00:47:11,006
SPEAKER_1:  deep confidence about what a utopian world looks like.

00:47:11,906 --> 00:47:15,102
SPEAKER_0:  Well, it's a deep confidence. That was a deep confidence about the instrumental.

00:47:15,810 --> 00:47:16,734
SPEAKER_0:  way to get there.

00:47:16,962 --> 00:47:19,614
SPEAKER_0:  It was that, you know, I think a lot of us can agree that.

00:47:20,226 --> 00:47:27,358
SPEAKER_0:  if everyone had everything they needed and we didn't have disease or poverty and people could live as long as they wanted to and choose when to die.

00:47:27,842 --> 00:47:28,702
SPEAKER_0:  and...

00:47:29,058 --> 00:47:34,270
SPEAKER_0:  there was no major existential threat because we had control. Almost everyone can agree that would be great.

00:47:34,850 --> 00:47:35,806
SPEAKER_0:  that Communism

00:47:36,066 --> 00:47:36,542
SPEAKER_0:  is.

00:47:36,802 --> 00:47:37,374
SPEAKER_0:  A

00:47:37,826 --> 00:47:41,502
SPEAKER_0:  They said, this is the way to get there. And that is.

00:47:42,370 --> 00:47:43,166
SPEAKER_0:  That's a different.

00:47:43,586 --> 00:47:44,638
SPEAKER_0:  question, you know, so.

00:47:44,898 --> 00:47:50,046
SPEAKER_0:  The unimaginably good future I'm picturing, I think a lot of people would picture, and I think most people would agree.

00:47:50,306 --> 00:47:57,534
SPEAKER_0:  No, not everyone. There's a lot of people out there who would say humans are the scourge on the earth and we should de-growth or something. But I think a lot of people would agree that.

00:47:57,762 --> 00:48:06,110
SPEAKER_0:  You know, just again, take Thomas Jefferson, bring him here. He would see it as a utopia for obvious reasons. For the medicine, the food, the transportation.

00:48:06,466 --> 00:48:06,974
SPEAKER_0:  Um.

00:48:07,298 --> 00:48:08,158
SPEAKER_0:  just how.

00:48:08,610 --> 00:48:11,326
SPEAKER_0:  uh... the quality of life and the safety and all of that

00:48:12,770 --> 00:48:16,702
SPEAKER_0:  Extrapolate that forward for us. Now we're Thomas Jefferson, you know, what's the equivalent?

00:48:17,794 --> 00:48:23,262
SPEAKER_0:  That's what I'm talking about. And the big question is, I don't try to say, here's the way to get there.

00:48:23,842 --> 00:48:25,694
SPEAKER_0:  Here's the actual specific way to get there.

00:48:26,178 --> 00:48:26,878
SPEAKER_0:  I try to say.

00:48:27,682 --> 00:48:35,006
SPEAKER_0:  how do we have a flashlight so that we can together figure it out? Like how do we give ourselves the best chance of figuring out the way to get there? And I think part of the problem with

00:48:35,426 --> 00:48:36,478
SPEAKER_0:  with communists.

00:48:36,706 --> 00:48:47,038
SPEAKER_0:  and people, ideologues, is that they're way too overconfident that they know the way to get there and it becomes a religion to them, this solution.

00:48:47,362 --> 00:48:50,814
SPEAKER_0:  and then you can't update once you have a solution as a religion.

00:48:51,330 --> 00:48:55,038
SPEAKER_1:  I felt a little violated when you said communist and stared deeply into myself.

00:48:55,746 --> 00:48:56,478
SPEAKER_1:  uh...

00:48:57,794 --> 00:49:01,918
SPEAKER_1:  In this book, you've developed a framework for how to fix everything.

00:49:02,146 --> 00:49:04,638
SPEAKER_1:  It's called the ladder. Can you explain it?

00:49:04,738 --> 00:49:06,526
SPEAKER_0:  Okay, it's not a framework for how to fix everything.

00:49:06,754 --> 00:49:08,629
SPEAKER_0:  I would never say that. I'll explain.

00:49:08,629 --> 00:49:12,382
SPEAKER_1:  to Tim Urban at some point. Okay. How this humor thing works. Yeah, no.

00:49:12,738 --> 00:49:14,814
SPEAKER_1:  of how to.

00:49:15,330 --> 00:49:17,630
SPEAKER_1:  think about collaboration between humans.

00:49:17,922 --> 00:49:19,838
SPEAKER_1:  such that we could fix things.

00:49:20,130 --> 00:49:23,326
SPEAKER_0:  I think it's a compass. It's like a, it's like a.

00:49:24,130 --> 00:49:29,854
SPEAKER_0:  It's a ruler that we can, once we look at it together and see what it is, we can all say, oh, we want to go to that side of the room.

00:49:30,626 --> 00:49:31,294
SPEAKER_0:  Not this side.

00:49:31,682 --> 00:49:32,094
SPEAKER_0:  Um.

00:49:32,514 --> 00:49:36,030
SPEAKER_0:  And so it gives us a direction to go. And so what are the parts of the ladder?

00:49:36,930 --> 00:49:40,510
SPEAKER_0:  So I have these two characters, this orange guy, and this primitive mind is-

00:49:41,090 --> 00:49:42,046
SPEAKER_0:  This is our software.

00:49:42,306 --> 00:49:44,222
SPEAKER_0:  That is the software that was in.

00:49:44,706 --> 00:49:45,406
SPEAKER_0:  A

00:49:45,666 --> 00:49:55,774
SPEAKER_0:  50,000 BC person's head that was specifically optimized to help that person survive in that world. And not even, not just, not really survive, but help them pass their genes on in that world.

00:49:56,898 --> 00:49:57,406
SPEAKER_0:  Um...

00:49:58,178 --> 00:49:58,974
SPEAKER_0:  And

00:49:59,874 --> 00:50:01,406
SPEAKER_0:  Civilization happened quickly.

00:50:01,634 --> 00:50:03,678
SPEAKER_0:  and brains change slowly.

00:50:03,938 --> 00:50:04,606
SPEAKER_0:  And so.

00:50:05,442 --> 00:50:08,926
SPEAKER_0:  unchanged dude is still running the show in our head.

00:50:09,346 --> 00:50:09,950
SPEAKER_0:  Um,

00:50:10,178 --> 00:50:12,030
SPEAKER_0:  And I use the example of like Skittles.

00:50:13,122 --> 00:50:13,502
SPEAKER_0:  like.

00:50:13,986 --> 00:50:15,614
SPEAKER_0:  Why do we eat Skittles?

00:50:16,290 --> 00:50:18,014
SPEAKER_0:  It's obviously bad for you.

00:50:18,658 --> 00:50:19,454
SPEAKER_0:  And it's because.

00:50:20,098 --> 00:50:21,150
SPEAKER_0:  The primitive mind.

00:50:21,922 --> 00:50:30,526
SPEAKER_0:  in the world that it was programmed for. There was no Skittles, it was just fruit, and if there was a dense, chewy, sweet fruit like that, it meant it wasn't really that bad.

00:50:30,978 --> 00:50:32,830
SPEAKER_0:  You just found like a calorie gold mine.

00:50:33,378 --> 00:50:34,782
SPEAKER_0:  Energy, energy, take it.

00:50:35,202 --> 00:50:36,254
SPEAKER_0:  Take it, eat as much as you can.

00:50:36,738 --> 00:50:37,342
SPEAKER_0:  Gorge on it.

00:50:37,666 --> 00:50:43,166
SPEAKER_0:  Hopefully you get a little fat. That would be the dream. And now we're so good with energy for a while. We don't have to stress about it.

00:50:44,194 --> 00:50:44,542
SPEAKER_0:  So.

00:50:44,802 --> 00:50:46,238
SPEAKER_0:  Today, Mars, Inc.

00:50:47,362 --> 00:50:49,406
SPEAKER_0:  is clever and says, let's not

00:50:49,826 --> 00:51:01,635
SPEAKER_0:  sell things to people's higher minds, who's the other character, let's sell to people's primitive minds. Primitive minds are dumb, and let's trick them into thinking this thing you should eat, and then they'll eat it. Now, Mars Inc. is a huge company.

00:51:01,635 --> 00:51:06,238
SPEAKER_1:  She's just still linger real quick. So you said primitive mind and higher mind. So those are the two things that make up.

00:51:07,138 --> 00:51:08,126
SPEAKER_1:  this bigger mind that it.

00:51:08,578 --> 00:51:09,703
SPEAKER_1:  that it is the modern human being.

00:51:09,703 --> 00:51:18,462
SPEAKER_0:  Yeah, it's like, you know, it's not perfect. Obviously, there's a lot of crossover. There's people who will yell at me for saying there's two minds and you know that but to me it's still a useful.

00:51:19,042 --> 00:51:20,702
SPEAKER_0:  framework where you have this software.

00:51:20,962 --> 00:51:22,366
SPEAKER_0:  that has making decisions.

00:51:22,722 --> 00:51:24,446
SPEAKER_0:  based on a world that you're not in anymore.

00:51:24,802 --> 00:51:35,326
SPEAKER_0:  and then you've got this other character, I call it the higher mind, and it's the part of you that knows that skills are not good and can override the instinct. And the reason you don't always eat skills is because the higher mind says, no, no, no, we're not doing that.

00:51:35,586 --> 00:51:37,342
SPEAKER_0:  because that's bad and I know that, right?

00:51:37,730 --> 00:51:44,894
SPEAKER_0:  Now, you can apply that to a lot of things. The higher mind is the one that knows I shouldn't procrastinate. The primitive mind is the one that wants to conserve energy and not do anything icky and-

00:51:45,122 --> 00:51:50,206
SPEAKER_0:  You know, can't see the future. So he procrastinates that, you know, you can apply this. No, I in this book apply it too.

00:51:50,594 --> 00:51:50,942
SPEAKER_1:  Uh...

00:51:52,098 --> 00:51:52,542
SPEAKER_0:  to.

00:51:53,090 --> 00:51:57,374
SPEAKER_0:  how we form our beliefs is one of the ways. And then eventually to politics and political movements. one way or another to form our beliefs.

00:51:58,018 --> 00:52:01,054
SPEAKER_0:  If you think about, well, what's the equivalent of the Skittles?

00:52:01,538 --> 00:52:04,606
SPEAKER_0:  tug of war in your head for how do you form your beliefs?

00:52:05,058 --> 00:52:05,566
SPEAKER_0:  Um.

00:52:06,402 --> 00:52:07,038
SPEAKER_0:  and

00:52:07,490 --> 00:52:11,198
SPEAKER_0:  It's that the primitive mind in the world that it was.

00:52:11,842 --> 00:52:12,830
SPEAKER_0:  optimized for.

00:52:13,282 --> 00:52:13,854
SPEAKER_0:  Um...

00:52:14,306 --> 00:52:16,478
SPEAKER_0:  It wanted to.

00:52:17,378 --> 00:52:18,910
SPEAKER_0:  feel conviction.

00:52:19,298 --> 00:52:21,662
SPEAKER_0:  about its beliefs. It wanted to.

00:52:21,890 --> 00:52:23,102
SPEAKER_0:  Be sure.

00:52:23,394 --> 00:52:24,510
SPEAKER_0:  that it was.

00:52:25,026 --> 00:52:25,406
SPEAKER_0:  Um.

00:52:25,794 --> 00:52:40,286
SPEAKER_0:  It wanted to feel conviction and it wanted to agree with the people around there. It didn't want to stand out. It wanted to perfectly agree with the tribe about the tribe's sacred beliefs, right? And so there's a big part of us that wants to do that. That doesn't like changing our mind. It feels like it's part of our, the primitive mind identifies with beliefs.

00:52:40,642 --> 00:52:43,198
SPEAKER_0:  feels like it's a threat, a physical threat to you.

00:52:43,778 --> 00:52:46,270
SPEAKER_0:  to your primitive mind when you change your mind or when someone

00:52:46,754 --> 00:52:48,446
SPEAKER_0:  disagrees with you in a smart way.

00:52:48,738 --> 00:52:58,014
SPEAKER_0:  So there's that huge force in us, which is confirmation bias. That's where that comes from. It's this desire to keep believing what we believe and this desire to also.

00:52:58,402 --> 00:53:01,150
SPEAKER_0:  fit in with our beliefs, to believe what the people around us believe.

00:53:01,506 --> 00:53:10,206
SPEAKER_0:  And that can be fun in some ways. We all like the same sports team and we're all super into it and we're all gonna be biased about that call together. I mean, it's not always bad.

00:53:10,658 --> 00:53:11,006
SPEAKER_0:  But.

00:53:11,298 --> 00:53:13,726
SPEAKER_0:  It's not a very smart way to be and you're actually...

00:53:14,018 --> 00:53:18,462
SPEAKER_0:  You're working kind of for those ideas. Those ideas are like your boss, and you're working so hard.

00:53:18,914 --> 00:53:28,126
SPEAKER_0:  to keep believing those. Those ideas are, you know, a really good paper comes in that you read that conflicts with those ideas. And you will do all this work to say that paper is bullshit because.

00:53:29,282 --> 00:53:40,190
SPEAKER_0:  You're a faithful employee of those ideas. Now the higher mind, to me, the same party that can override the Skittles, can override this and can search for something that makes a lot more sense, which is truth.

00:53:40,706 --> 00:53:44,862
SPEAKER_0:  Because what rational being wouldn't want to know the truth who wants to be delusional?

00:53:45,506 --> 00:53:47,582
SPEAKER_0:  And so there's this tug of war because.

00:53:48,290 --> 00:53:56,798
SPEAKER_0:  The higher mind doesn't identify with ideas. Why would you? It's an experiment you're doing and it's a mental model. And if someone can come over and say, you're wrong, you'd say, where, show me, show me.

00:53:57,250 --> 00:53:57,758
SPEAKER_0:  and

00:53:58,050 --> 00:54:00,190
SPEAKER_0:  If they point out something that is wrong, you say, oh, thanks.

00:54:00,578 --> 00:54:06,558
SPEAKER_0:  Oh good, I just got a little smarter, right? You're not gonna identify with the thing. Oh go ahead, kick it, see if you can break it. If you can break it, it's not that good, right?

00:54:06,946 --> 00:54:25,470
SPEAKER_0:  So there's both of these in our heads and there's this tug of war between them. And sometimes, you know, if you're telling me about something with AI, I'm probably gonna think with my higher mind, because I'm not identified with it. But if you go and you criticize the ideas in this book or you criticize my religious beliefs, or you criticize, I might have a harder time because the primitive mind says, no, no, no, those are our special ideas.

00:54:25,954 --> 00:54:26,718
SPEAKER_0:  So yeah, so that's.

00:54:26,946 --> 00:54:29,758
SPEAKER_0:  That's one way to use this ladder is like, it's a spectrum.

00:54:30,018 --> 00:54:32,062
SPEAKER_0:  You know, at the top, the higher mind's doing all the thinking.

00:54:32,450 --> 00:54:36,446
SPEAKER_0:  And then as you go down, it becomes more of a tug of war. And at the bottom, the primitive mind is in total control.

00:54:36,994 --> 00:54:40,510
SPEAKER_1:  And this is distinct as you show from the spectrum of ideas.

00:54:40,898 --> 00:54:45,726
SPEAKER_1:  So this is how you think versus what you think. And those are distinct, those are different dimensions.

00:54:46,210 --> 00:54:48,510
SPEAKER_0:  We need a vertical axis.

00:54:48,802 --> 00:54:53,790
SPEAKER_0:  We have all these horizontal axes, left, right, center, or this opinion all the way to this opinion, but it's like...

00:54:54,562 --> 00:54:55,902
SPEAKER_0:  What's much more important than-

00:54:56,482 --> 00:54:57,886
SPEAKER_0:  where you stand is how you got that.

00:54:58,434 --> 00:54:59,550
SPEAKER_0:  right, and how you think.

00:55:00,002 --> 00:55:00,478
SPEAKER_0:  So.

00:55:00,706 --> 00:55:05,630
SPEAKER_0:  This helps if I can say this person's kind of on the left or on the right, but they're up high.

00:55:05,922 --> 00:55:07,774
SPEAKER_0:  I think, in other words, I think they got there.

00:55:08,098 --> 00:55:10,654
SPEAKER_0:  using evidence and reason, and they were willing to change their mind.

00:55:10,882 --> 00:55:22,590
SPEAKER_0:  Now that means a lot to me what they have to say. If I think they're just a tribal person and I can predict all their beliefs from hearing one because it's so obvious what political beliefs, that person's views are irrelevant to me because they're not real. They didn't come from.

00:55:23,202 --> 00:55:24,990
SPEAKER_0:  information they came from.

00:55:25,378 --> 00:55:26,558
SPEAKER_0:  a tribe's kind of.

00:55:27,266 --> 00:55:29,054
SPEAKER_0:  you know, sacred Ten Commandments.

00:55:29,410 --> 00:55:31,870
SPEAKER_1:  I really like the comic you have in here with the boxer.

00:55:32,578 --> 00:55:35,262
SPEAKER_1:  This is the best boxer in the world. Wow, cool.

00:55:35,586 --> 00:55:36,990
SPEAKER_1:  Who has he beaten?

00:55:37,570 --> 00:55:39,294
SPEAKER_1:  No one, he's never fought anyone.

00:55:39,810 --> 00:55:43,070
SPEAKER_1:  then how do you know he's the best boxer in the world? I can just tell.

00:55:43,362 --> 00:55:51,550
SPEAKER_1:  I mean, this connects with me and I think with a lot of people just because in martial arts, it's especially kind of true that this is this whole legend about different martial artists.

00:55:52,066 --> 00:55:52,926
SPEAKER_1:  that kind of.

00:55:53,250 --> 00:55:54,654
SPEAKER_1:  we construct like.

00:55:55,170 --> 00:55:56,158
SPEAKER_1:  action figures.

00:55:56,482 --> 00:55:56,830
SPEAKER_1:  Like.

00:55:57,186 --> 00:56:01,630
SPEAKER_1:  you know, thinking that Stephen Seagal is the best fighter in the world or Chuck Norris.

00:56:01,954 --> 00:56:08,254
SPEAKER_1:  Chuck Norris is actually backed up. He's done really well in competition, but still the ultimate test for particular for martial arts is

00:56:08,610 --> 00:56:11,838
SPEAKER_1:  what we now know is mixed martial arts, UFC and so on.

00:56:12,066 --> 00:56:13,941
SPEAKER_1:  That's the actual scientific testing ground.

00:56:13,941 --> 00:56:15,358
SPEAKER_0:  It's a meritocracy. Get out Belorussian.

00:56:15,682 --> 00:56:27,774
SPEAKER_1:  Exactly. I mean there's within certain rules and you can criticize those rules like this doesn't actually represent the broader combat That you would think of when you're thinking about martial arts But reality is you're actually testing things and that's when you realize that

00:56:28,034 --> 00:56:30,942
SPEAKER_1:  Aikido and some of these kind of woo-woo martial arts.

00:56:31,330 --> 00:56:33,982
SPEAKER_1:  uh... in their certain limitations don't work

00:56:34,402 --> 00:56:36,062
SPEAKER_1:  in the way you think they would.

00:56:36,354 --> 00:56:42,462
SPEAKER_1:  in the context of fighting. I think this is one of the places where everyone can agree, which is why it's a really nice comic.

00:56:42,754 --> 00:56:44,350
SPEAKER_1:  because then you start to talk about.

00:56:46,274 --> 00:56:47,422
SPEAKER_1:  Map this unto.

00:56:47,778 --> 00:56:49,790
SPEAKER_1:  ideas that people take personally.

00:56:50,210 --> 00:56:52,894
SPEAKER_1:  it starts becoming a lot more difficult to, um...

00:56:54,370 --> 00:56:56,254
SPEAKER_1:  Basically highlight that we're thinking with.

00:56:56,674 --> 00:56:59,646
SPEAKER_1:  not with our higher mind but with our primitive mind.

00:57:00,418 --> 00:57:01,502
SPEAKER_0:  Yeah, I mean if...

00:57:01,922 --> 00:57:08,350
SPEAKER_0:  If I'm thinking of my higher mind and now here is I use different things for an idea as a metaphor. So here the metaphor is a boxer. Near.

00:57:09,442 --> 00:57:11,838
SPEAKER_0:  for one of your conclusions, one of your beliefs.

00:57:12,418 --> 00:57:12,990
SPEAKER_0:  And...

00:57:14,434 --> 00:57:14,974
SPEAKER_0:  If I'm

00:57:15,490 --> 00:57:17,182
SPEAKER_0:  If all I care about is truth.

00:57:17,826 --> 00:57:21,054
SPEAKER_0:  In other words, that means all I care about is having a good boxer.

00:57:21,826 --> 00:57:22,238
SPEAKER_0:  I would.

00:57:23,074 --> 00:57:29,502
SPEAKER_0:  Say, go, go, yeah, try, see if this person's good. Go, go, go, in other words, I would get into arguments, which is throwing my boxer out there.

00:57:29,794 --> 00:57:30,462
SPEAKER_0:  Fight against other one.

00:57:31,106 --> 00:57:44,478
SPEAKER_0:  And if I think my argument is good, by the way, I love boxing, right? If I think my guy is amazing, you know, Mike Tyson, I'm thinking, oh yeah, bring it on. Who wants to come see? I bet no one can beat my boxer. I love a good debate, right? In that case.

00:57:45,506 --> 00:57:46,142
SPEAKER_0:  now.

00:57:46,402 --> 00:57:55,166
SPEAKER_0:  What would you think about my boxer if not only was I telling you he was great, but he's never boxed anyone, but then you said, okay, well, your idea came over to try to punch him.

00:57:55,682 --> 00:57:59,774
SPEAKER_0:  And I screamed and I said, what are you doing? That's violence. And you're in your.

00:58:00,130 --> 00:58:03,198
SPEAKER_0:  and you're an awful person. And I don't want to be friends with you anymore because you're-

00:58:03,618 --> 00:58:07,902
SPEAKER_0:  you would think this boxer obviously sucks, or at least I think it sucks, deep down.

00:58:08,450 --> 00:58:09,214
SPEAKER_0:  because...

00:58:09,634 --> 00:58:12,574
SPEAKER_0:  Why would I be so empty anyone no boxing allowed?

00:58:13,186 --> 00:58:15,102
SPEAKER_0:  You know, people, so I think.

00:58:15,906 --> 00:58:16,542
SPEAKER_0:  If you're in.

00:58:17,186 --> 00:58:18,590
SPEAKER_0:  So this, I call this a ladder, right?

00:58:19,074 --> 00:58:20,350
SPEAKER_0:  If you're in low wrong land.

00:58:20,802 --> 00:58:22,430
SPEAKER_0:  you know whether it's a culture or whatever.

00:58:22,658 --> 00:58:25,182
SPEAKER_0:  a debate, an argument when someone says, no, that's totally wrong.

00:58:25,794 --> 00:58:26,174
SPEAKER_0:  Ah.

00:58:26,498 --> 00:58:29,406
SPEAKER_0:  what you're saying about that and here's why. You're actually being totally biased.

00:58:30,978 --> 00:58:33,022
SPEAKER_0:  It sounds like a fight people are gonna say.

00:58:33,538 --> 00:58:36,894
SPEAKER_0:  Oh wow, we got in like a fight. It was really awkward. Are we still friends with that person?

00:58:37,122 --> 00:58:37,726
SPEAKER_0:  Because...

00:58:38,114 --> 00:58:42,814
SPEAKER_0:  that's not a culture of boxing. It's a culture where you don't touch each other's ideas. That's insensitive.

00:58:43,266 --> 00:58:47,038
SPEAKER_0:  versus in a high rung culture.

00:58:47,522 --> 00:58:51,678
SPEAKER_0:  It's sport. I mean, like every one of your podcasts, you're-

00:58:52,066 --> 00:59:00,999
SPEAKER_0:  Whether you're agreeing or disagreeing, the tone is the same. It's not like, oh, this got awkward. It's like, the tone is identical because you're just playing intellectually either way because it's a good high-wrong space.

00:59:00,999 --> 00:59:04,574
SPEAKER_1:  best at his best but people do take stuff personally.

00:59:05,058 --> 00:59:08,958
SPEAKER_1:  And then that's actually one of the skills of conversation just as a fan of podcasts is.

00:59:09,186 --> 00:59:11,326
SPEAKER_1:  when you sense that people take a thing personally.

00:59:11,778 --> 00:59:12,574
SPEAKER_1:  You have to like.

00:59:13,090 --> 00:59:14,206
SPEAKER_1:  There's sort of...

00:59:14,466 --> 00:59:17,854
SPEAKER_1:  Methodologies and little paths you can take to like calm things down.

00:59:18,306 --> 00:59:20,931
SPEAKER_1:  I go around the- don't take it as a violation-

00:59:20,931 --> 00:59:23,870
SPEAKER_0:  of like that. You're trying to suss out which of their ideas are sacred to them.

00:59:24,098 --> 00:59:26,599
SPEAKER_0:  and which ones are, bring it on.

00:59:26,599 --> 00:59:31,230
SPEAKER_1:  And sometimes it's actually, I mean, that's the skill of it, I suppose, that sometimes it's the certain wordings.

00:59:32,450 --> 00:59:36,638
SPEAKER_1:  in the way you challenge those ideas are important. You can challenge them indirectly.

00:59:37,058 --> 00:59:39,422
SPEAKER_1:  and then together walk together in that way.

00:59:39,906 --> 00:59:41,662
SPEAKER_1:  because they're what I've learned.

00:59:42,402 --> 00:59:42,910
SPEAKER_1:  is.

00:59:43,362 --> 00:59:44,286
SPEAKER_1:  People are used to

00:59:45,186 --> 00:59:47,646
SPEAKER_1:  their ideas being attacked in a certain way.

00:59:48,098 --> 00:59:51,134
SPEAKER_1:  in a certain tribal way, and if you just avoid those.

00:59:51,522 --> 00:59:55,358
SPEAKER_1:  For example, if you have political discussions and just never mention left or right.

00:59:55,682 --> 00:59:58,302
SPEAKER_1:  or a republican and democrat, none of that.

00:59:58,626 --> 00:59:59,326
SPEAKER_1:  Just talk about.

00:59:59,586 --> 01:00:00,574
SPEAKER_1:  different ideas.

01:00:00,834 --> 01:00:05,694
SPEAKER_1:  and avoid certain kind of triggering words, you can actually talk about ideas versus.

01:00:06,018 --> 01:00:07,102
SPEAKER_1:  falling into this.

01:00:07,970 --> 01:00:08,414
SPEAKER_1:  path.

01:00:08,642 --> 01:00:09,982
SPEAKER_1:  that's well established too.

01:00:10,530 --> 01:00:12,414
SPEAKER_1:  battles that people have previously fought.

01:00:12,674 --> 01:00:14,558
SPEAKER_0:  When you say triggering, I mean, who's getting triggered?

01:00:14,850 --> 01:00:18,270
SPEAKER_0:  primitive mind. I'm trying to do what you're saying in this language is

01:00:18,690 --> 01:00:22,014
SPEAKER_0:  How do you have conversations with other people's higher minds? Almost like whispering?

01:00:22,594 --> 01:00:26,686
SPEAKER_0:  without waking up the primitive mind. The primitive mind is there sleeping, right? And as soon as you say...

01:00:26,914 --> 01:00:31,006
SPEAKER_0:  As soon as you say something, the left primitive mind gets up and says, what are you saying about the left? And now...

01:00:31,650 --> 01:00:33,182
SPEAKER_0:  Now everything goes off the rails.

01:00:33,602 --> 01:00:36,190
SPEAKER_1:  What do you make of conspiracy theories under this framework?

01:00:36,770 --> 01:00:37,374
SPEAKER_1:  of the ladder.

01:00:37,730 --> 01:00:38,142
SPEAKER_0:  So.

01:00:38,914 --> 01:00:41,118
SPEAKER_0:  Here's the thing about conspiracy theories is that.

01:00:41,826 --> 01:00:42,878
SPEAKER_0:  Once in a while, they're true.

01:00:43,298 --> 01:00:49,054
SPEAKER_0:  Right? Because sometimes there's a natural conspiracy. Actually, humans are pretty good at real conspiracies, secret things.

01:00:49,378 --> 01:00:50,430
SPEAKER_0:  uh... and

01:00:50,754 --> 01:00:52,894
SPEAKER_0:  then I just watched the made-off doc.

01:00:53,154 --> 01:00:54,942
SPEAKER_0:  Great new Netflix doc.

01:00:55,490 --> 01:00:55,838
SPEAKER_0:  um sid

01:00:56,066 --> 01:00:57,822
SPEAKER_0:  And so.

01:00:58,274 --> 01:00:59,038
SPEAKER_0:  The question is...

01:01:00,130 --> 01:01:01,822
SPEAKER_0:  How do you create a...

01:01:02,050 --> 01:01:03,806
SPEAKER_0:  system that is good.

01:01:04,386 --> 01:01:07,326
SPEAKER_0:  at you put the conspiracy theory in.

01:01:07,778 --> 01:01:16,254
SPEAKER_0:  and it either goes eh or it says, this is interesting, let's keep exploring it. Like how do you do something that it can, how do you assess it? And I think the high rung culture.

01:01:17,602 --> 01:01:19,358
SPEAKER_0:  is really good at it because...

01:01:19,746 --> 01:01:29,150
SPEAKER_0:  a real conspiracy, what's gonna happen is, you put it, it's like a little machine you put in the middle of the table, and everyone starts firing darts at it, or bow and arrow, or whatever, and everyone starts kicking it and trying to...

01:01:29,826 --> 01:01:32,766
SPEAKER_0:  and almost all conspiracy theories, they quickly crumble.

01:01:33,282 --> 01:01:34,558
SPEAKER_0:  right, because they actually, you know.

01:01:35,362 --> 01:01:36,254
SPEAKER_0:  Trump's election.

01:01:36,546 --> 01:01:41,406
SPEAKER_0:  I actually dug in and I looked at every claim that he or his team made and it was like...

01:01:42,242 --> 01:01:46,558
SPEAKER_0:  All of these, none of these hold up to scrutiny. None of them. I was open-minded, but none of them did.

01:01:46,882 --> 01:01:48,862
SPEAKER_0:  So that was one that as soon as it's open to scrutiny.

01:01:49,538 --> 01:01:50,302
SPEAKER_0:  It crumbles.

01:01:51,266 --> 01:01:52,990
SPEAKER_0:  The only way that conspiracy.

01:01:53,250 --> 01:01:55,358
SPEAKER_0:  can stick around in a community.

01:01:55,714 --> 01:01:56,446
SPEAKER_0:  is if.

01:01:57,218 --> 01:02:05,918
SPEAKER_0:  It is a culture where that's being treated as a sacred idea that no one should kick or throw a dart at, because if you throw a dart, it's gonna break. So it's being, and so...

01:02:06,242 --> 01:02:09,310
SPEAKER_0:  What you want is a culture where no idea is sacred.

01:02:09,698 --> 01:02:14,078
SPEAKER_0:  anything can get thrown at. And so I think that then what you'll find is that 990.

01:02:14,338 --> 01:02:25,438
SPEAKER_0:  four out of 100 conspiracy theories come in and they fall down. The other, maybe four of the others come in and there's something there, but it's not as extreme as people say. And then maybe one is a huge deal and it's actually a real conspiracy.

01:02:25,858 --> 01:02:29,854
SPEAKER_1:  Well, isn't there a lot of gray area and there's a lot of mystery? Isn't that where the-

01:02:30,466 --> 01:02:32,126
SPEAKER_1:  conspiracy theories seep in.

01:02:32,546 --> 01:02:32,862
SPEAKER_1:  So.

01:02:33,122 --> 01:02:33,598
SPEAKER_1:  Uh...

01:02:33,954 --> 01:02:38,558
SPEAKER_1:  It's great to hear that you've really looked into the Trump election fraud claims.

01:02:39,458 --> 01:02:39,774
SPEAKER_1:  but.

01:02:40,066 --> 01:02:43,550
SPEAKER_1:  Aren't they resting in a lot of kind of gray area? like

01:02:44,258 --> 01:02:52,894
SPEAKER_1:  basically saying that there is dark forces in the shadows that are actually controlling everything. I mean, the same thing with maybe you can, like,

01:02:53,122 --> 01:02:53,918
SPEAKER_1:  safer.

01:02:54,402 --> 01:02:58,174
SPEAKER_1:  conspiracy theories, less controversial, like have we landed on the moon?

01:02:58,882 --> 01:03:01,502
SPEAKER_1:  Did the United States ever land on the moon?

01:03:02,594 --> 01:03:02,974
SPEAKER_1:  There's.

01:03:03,426 --> 01:03:04,190
SPEAKER_1:  You know, you can...

01:03:04,514 --> 01:03:07,966
SPEAKER_1:  Like the reason why conspiracy theories work is you could construct.

01:03:08,994 --> 01:03:11,422
SPEAKER_1:  Incentives and motivation for faking the moon landing.

01:03:11,746 --> 01:03:12,894
SPEAKER_1:  There's a lot of...

01:03:14,050 --> 01:03:14,718
SPEAKER_1:  Um...

01:03:14,946 --> 01:03:15,294
SPEAKER_1:  .

01:03:15,618 --> 01:03:17,310
SPEAKER_1:  There's very little data.

01:03:17,762 --> 01:03:19,102
SPEAKER_1:  supporting the moon landing.

01:03:19,874 --> 01:03:21,749
SPEAKER_1:  like that's very public and kind of.

01:03:21,749 --> 01:03:25,534
SPEAKER_0:  looks fake, space kind of looks fake. That would be a big story if it turned out to be fake.

01:03:25,826 --> 01:03:28,254
SPEAKER_1:  That's the argument, that would be the argument against it.

01:03:28,514 --> 01:03:33,086
SPEAKER_1:  Are people really as a collective going to hold onto a story that big?

01:03:34,082 --> 01:03:35,326
SPEAKER_1:  Um, yeah, so that.

01:03:35,778 --> 01:03:38,933
SPEAKER_1:  But the reason they work is there's mystery.

01:03:38,933 --> 01:03:41,406
SPEAKER_0:  Yeah, it was a great documentary called Behind the Curve About...

01:03:41,794 --> 01:03:46,174
SPEAKER_0:  flat earthers and one of the things that you learn about flat earthers is they believe

01:03:46,946 --> 01:03:49,214
SPEAKER_0:  all the conspiracies, not just the flat earth.

01:03:49,634 --> 01:03:53,374
SPEAKER_0:  They are convinced the moon landing is fake. They're convinced 9-11 was an American.

01:03:53,602 --> 01:03:54,110
SPEAKER_0:  Con job.

01:03:55,202 --> 01:03:56,286
SPEAKER_0:  they're convinced, you know.

01:03:56,898 --> 01:04:00,510
SPEAKER_0:  that name a conspiracy and they believe it. And so it's so interesting.

01:04:00,962 --> 01:04:01,342
SPEAKER_0:  is that.

01:04:02,242 --> 01:04:03,262
SPEAKER_0:  I think of it as a.

01:04:04,578 --> 01:04:06,654
SPEAKER_0:  as a skepticism spectrum.

01:04:07,106 --> 01:04:08,446
SPEAKER_0:  So on one side...

01:04:08,898 --> 01:04:13,790
SPEAKER_0:  It's like a filter in your head, a filter in the beliefs section of your brain.

01:04:14,050 --> 01:04:15,294
SPEAKER_0:  on one end of the spectrum.

01:04:15,650 --> 01:04:16,030
SPEAKER_0:  You b-

01:04:16,258 --> 01:04:16,990
SPEAKER_0:  are gullible.

01:04:17,314 --> 01:04:21,502
SPEAKER_0:  perfectly gullible, you believe anything someone says, right? On the other side, you're paranoid, everyone's lying to you.

01:04:22,082 --> 01:04:24,510
SPEAKER_0:  Everything is false. Nothing anyone says is true.

01:04:25,154 --> 01:04:26,910
SPEAKER_0:  Right, so obviously those aren't good places to be.

01:04:27,138 --> 01:04:29,886
SPEAKER_0:  Now, the healthy place, I think that the...

01:04:30,562 --> 01:04:32,830
SPEAKER_0:  So I think the healthy place is to be somewhere in the middle.

01:04:33,218 --> 01:04:40,446
SPEAKER_0:  And, but also you can learn to trust certain sources and then, you know, you don't have to do as much, apply as much skepticism to them. And so here's what.

01:04:41,794 --> 01:04:42,110
SPEAKER_0:  Like

01:04:42,818 --> 01:04:45,502
SPEAKER_0:  And when you start having a bias, just so you have a political bias.

01:04:46,434 --> 01:05:02,558
SPEAKER_0:  When your side says something, you will find yourself moving towards the gullible side of the spectrum. You read an article written that supports your views, you move to the gullible side of the spectrum, and you just believe it, and you don't have any, where's that skepticism that you normally have, right? And then you move, and then you, as soon as it's the other person talking, the other team talking, you move to the...

01:05:02,946 --> 01:05:06,590
SPEAKER_0:  skeptical, closer to the denial, paranoid side.

01:05:07,874 --> 01:05:09,278
SPEAKER_0:  Flat earthers are the extreme.

01:05:09,570 --> 01:05:10,174
SPEAKER_0:  They are.

01:05:10,562 --> 01:05:12,158
SPEAKER_0:  either at 10 or 1.

01:05:13,026 --> 01:05:19,678
SPEAKER_0:  So it's like, it's so interesting because they're the people who are saying, ah, nah, I won't believe you. I'm not gullible. No, everyone else is gullible about the moon landing. I won't.

01:05:20,162 --> 01:05:21,246
SPEAKER_0:  And then yet, when there's this.

01:05:21,602 --> 01:05:23,870
SPEAKER_0:  Evidence like, oh, because you can't see Seattle.

01:05:24,098 --> 01:05:25,438
SPEAKER_0:  You can't see the buildings over that.

01:05:25,762 --> 01:05:31,262
SPEAKER_0:  at horizon and you should, which isn't true. You should be, if the earth around you, wouldn't be able to see them.

01:05:31,778 --> 01:05:39,518
SPEAKER_0:  Therefore, so suddenly they become the most gullible person. They hear any theory about the earth flat, they believe it. It goes right into their beliefs. So they're actually jumping back and forth between.

01:05:39,970 --> 01:05:41,310
SPEAKER_0:  fuses to believe anything and

01:05:41,570 --> 01:05:42,174
SPEAKER_0:  believe anything.

01:05:42,498 --> 01:05:47,486
SPEAKER_0:  And so they're the extreme example, but I think when it comes to conspiracy theories, the people that get themselves into trouble.

01:05:48,130 --> 01:05:53,406
SPEAKER_0:  are the ones who, they become really gullible when they hear a conspiracy theory that kind of fits with their worldview.

01:05:53,666 --> 01:06:00,158
SPEAKER_0:  And they likewise, when there's something that's kind of obviously true and it's not a big lie, they will actually...

01:06:00,386 --> 01:06:05,086
SPEAKER_0:  they'll think it is, they just tighten up their kind of skepticism filter. and so

01:06:05,602 --> 01:06:08,670
SPEAKER_0:  Yeah, so I think the healthy places to be is where you are not.

01:06:08,898 --> 01:06:15,742
SPEAKER_0:  Cause you also don't want to be the person who says every conspiracy, you hear the word conspiracy theory and it sounds like a synonym for like quack.

01:06:16,002 --> 01:06:18,526
SPEAKER_0:  job crazy theory, right? So

01:06:18,754 --> 01:06:24,743
SPEAKER_0:  Yeah, so I think it's to be somewhere in the middle of that spectrum and to learn to fine tune it. Which is a tricky place to opt.

01:06:24,743 --> 01:06:25,278
SPEAKER_1:  operate.

01:06:25,666 --> 01:06:28,414
SPEAKER_1:  because you kind of have to every time you hear a new conspiracy theory

01:06:28,770 --> 01:06:30,558
SPEAKER_1:  You should approach it with an open mind.

01:06:31,458 --> 01:06:32,286
SPEAKER_1:  and you know

01:06:32,866 --> 01:06:36,478
SPEAKER_1:  And also if you don't have enough time to investigate, which most people don't.

01:06:36,802 --> 01:06:38,206
SPEAKER_1:  kind of still have a humility.

01:06:38,530 --> 01:06:41,118
SPEAKER_1:  not to make a conclusive statement that that's nonsense.

01:06:41,410 --> 01:06:42,366
SPEAKER_0:  There's a lot of um...

01:06:42,882 --> 01:06:43,870
SPEAKER_0:  social pressure.

01:06:44,130 --> 01:06:45,118
SPEAKER_0:  Actually, yeah.

01:06:45,698 --> 01:06:46,206
SPEAKER_0:  to.

01:06:46,658 --> 01:06:57,886
SPEAKER_0:  immediately laugh off any conspiracy theory, if it's done by the bad guys, right? You will quickly get mocked and laughed at and not taken seriously. If you give any credence, you know, like the lab leak was a good one where it's like...

01:06:58,402 --> 01:07:02,526
SPEAKER_0:  turned out that that was at least very credible, if not true.

01:07:03,170 --> 01:07:03,934
SPEAKER_0:  And

01:07:05,378 --> 01:07:07,838
SPEAKER_0:  That was a perfect example of one where when it first came out...

01:07:08,482 --> 01:07:11,038
SPEAKER_0:  And not only so, so, so Brett Weinstein talked about.

01:07:11,746 --> 01:07:12,158
SPEAKER_0:  And then.

01:07:12,450 --> 01:07:17,534
SPEAKER_0:  I, in a totally different conversation, said something complimentary about him on a totally different subject.

01:07:18,946 --> 01:07:19,870
SPEAKER_0:  And people were saying.

01:07:20,546 --> 01:07:27,166
SPEAKER_0:  Tim, you might have gone a little off the deep end. You're like quoting someone who is like a lab leak person. So I was getting my reputation dinged.

01:07:28,194 --> 01:07:30,526
SPEAKER_0:  for complimenting on a different topic.

01:07:30,754 --> 01:07:33,278
SPEAKER_0:  someone whose reputation was totally sullied.

01:07:33,666 --> 01:07:36,670
SPEAKER_0:  because they questioned an orthodoxy.

01:07:37,282 --> 01:07:40,350
SPEAKER_0:  Right? So you see, so what does that make me want to do?

01:07:41,186 --> 01:07:57,086
SPEAKER_0:  distance myself from Bret Weinstein. That's the, at least they see incentive that's a, and what does that make other people wanna do? Don't become the next Bret Weinstein. Don't say it out loud because you don't wanna become someone that no one wants to compliment anymore, right? You can see the social pressure and that's, and of course, when there is a conspiracy, that social pressure is its best friend.

01:07:58,818 --> 01:07:59,550
SPEAKER_1:  because then...

01:08:00,098 --> 01:08:02,654
SPEAKER_1:  that they see the people from outside.

01:08:03,298 --> 01:08:05,502
SPEAKER_1:  are seeing that social pressure enact.

01:08:05,954 --> 01:08:11,134
SPEAKER_1:  like a tear membrane becoming more and more and more extreme to the other side so they're going to take the more and more and more extreme

01:08:11,490 --> 01:08:12,350
SPEAKER_1:  I mean this...

01:08:12,642 --> 01:08:14,526
SPEAKER_1:  What do you see?

01:08:14,850 --> 01:08:16,478
SPEAKER_1:  that the pandemic did.

01:08:17,218 --> 01:08:18,590
SPEAKER_1:  like COVID did to our...

01:08:18,850 --> 01:08:22,910
SPEAKER_1:  civilization in that regard, in the forces. Why was it so-

01:08:24,386 --> 01:08:24,990
SPEAKER_1:  divisive.

01:08:25,410 --> 01:08:26,174
SPEAKER_1:  Do you understand that?

01:08:26,978 --> 01:08:28,638
SPEAKER_0:  Yeah, so COVID, you know.

01:08:29,122 --> 01:08:38,622
SPEAKER_0:  I thought might be, you know, the ultimate example of a topic that will unite us all is the alien attack. Although honestly, I don't even have that much faith then. I think there'd be like, some people are super like.

01:08:39,074 --> 01:08:39,614
SPEAKER_0:  You know.

01:08:39,842 --> 01:08:41,717
SPEAKER_0:  pro alien and some people are anti alien. but anyways-

01:08:41,717 --> 01:08:45,150
SPEAKER_1:  I was actually trying to interrupt because I was talking a few

01:08:45,730 --> 01:08:47,774
SPEAKER_1:  astronomers and they do the

01:08:49,186 --> 01:08:49,662
SPEAKER_1:  Focus it.

01:08:50,690 --> 01:08:51,902
SPEAKER_1:  made me kinda sad.

01:08:52,418 --> 01:08:55,550
SPEAKER_1:  in that if we discover life on Mars for example

01:08:56,258 --> 01:08:59,358
SPEAKER_1:  that there's going to be potentially a division over that too.

01:08:59,586 --> 01:09:01,278
SPEAKER_1:  half the people will not believe that's real.

01:09:01,954 --> 01:09:03,006
SPEAKER_0:  Well, because...

01:09:04,738 --> 01:09:07,518
SPEAKER_0:  We live in a current society where...

01:09:08,194 --> 01:09:09,470
SPEAKER_0:  The political divide.

01:09:10,530 --> 01:09:11,934
SPEAKER_0:  has subsumed everything.

01:09:12,258 --> 01:09:12,894
SPEAKER_0:  And that's not.

01:09:13,602 --> 01:09:14,366
SPEAKER_0:  I always liked that.

01:09:14,914 --> 01:09:19,422
SPEAKER_0:  It goes into stages like that. We're in a really bad one where it's actually.

01:09:19,778 --> 01:09:24,574
SPEAKER_0:  In the book, I call it like a vortex, like a, like a, like a, almost like a whirlpool that pulls.

01:09:25,282 --> 01:09:30,110
SPEAKER_0:  everything into it, it pulls, it pulls. And so normally you'd say, okay, you know, immigration.

01:09:30,402 --> 01:09:33,150
SPEAKER_0:  now truly gonna be contentious. That's always political, right? I guess like separate parts or a whole lot of people are going for atan and

01:09:33,698 --> 01:09:34,398
SPEAKER_0:  but like.

01:09:35,554 --> 01:09:39,774
SPEAKER_0:  COVID seemed like, oh, that's one of those that will unite us all. Let's fight this.

01:09:40,322 --> 01:09:52,510
SPEAKER_0:  not human virus thing, like obvious, no one's sensitive, no one's getting hurt when we insult the virus, like let's all be, we have this threat, this common threat that's a threat to everyone of every nationality in every country.

01:09:52,738 --> 01:09:53,630
SPEAKER_0:  every ethnicity.

01:09:53,890 --> 01:09:54,398
SPEAKER_0:  and

01:09:55,298 --> 01:09:56,254
SPEAKER_0:  And what it didn't do that.

01:09:56,738 --> 01:09:59,742
SPEAKER_0:  The Whirlpool was too powerful, so it pulled...

01:10:00,002 --> 01:10:01,630
SPEAKER_0:  COVID in and suddenly masks?

01:10:02,370 --> 01:10:04,414
SPEAKER_0:  If you're on the left, you like them. If you're on the right, you hate them.

01:10:05,250 --> 01:10:08,926
SPEAKER_0:  and suddenly lockdowns, if you're on the left you like them and on the right you hate them.

01:10:09,346 --> 01:10:14,398
SPEAKER_0:  And vaccines, this is people forget this, when Trump first started talking about the vaccine.

01:10:15,138 --> 01:10:15,678
SPEAKER_0:  Biden.

01:10:16,322 --> 01:10:16,958
SPEAKER_0:  Harris

01:10:17,922 --> 01:10:21,543
SPEAKER_0:  Cuomo. They're all saying I'm not taking that vaccine. Not from this CDC.

01:10:21,543 --> 01:10:23,043
SPEAKER_1:  Because I was too rushed or something.

01:10:23,043 --> 01:10:29,534
SPEAKER_0:  No, but because I'm not trusting anything that Trump says. Trump wants me to take it, I'm not taking it. I'm not taking it from this CDC.

01:10:29,922 --> 01:10:30,238
SPEAKER_0:  So.

01:10:31,074 --> 01:10:36,894
SPEAKER_0:  This was Trump was almost out of office, but at the time, if Trump had been, it would have been, I'm pretty sure it would have stayed.

01:10:37,154 --> 01:10:38,398
SPEAKER_0:  Write likes, vaccines.

01:10:38,626 --> 01:10:41,758
SPEAKER_0:  The left doesn't like vaccines. Instead, the president switched.

01:10:42,082 --> 01:10:43,966
SPEAKER_0:  and all those people are suddenly saying...

01:10:44,898 --> 01:10:53,022
SPEAKER_0:  They were actually specifically saying that if you, you know, that like, if you're saying the CDC is not trustworthy, that's misinformation, which is exactly what they were saying.

01:10:53,282 --> 01:10:57,310
SPEAKER_0:  about the other CDC and they were saying it because they genuinely didn't trust Trump, which is fair.

01:10:57,986 --> 01:10:59,358
SPEAKER_0:  Now when other people don't trust.

01:10:59,714 --> 01:11:00,862
SPEAKER_0:  The Biden CDC.

01:11:01,122 --> 01:11:03,774
SPEAKER_0:  Suddenly it's this kind of misinformation that needs to be censored.

01:11:04,258 --> 01:11:09,534
SPEAKER_0:  It was a sad moment because it was a couple of months, even a week or so, I mean a month or so at the very beginning when...

01:11:10,626 --> 01:11:15,230
SPEAKER_0:  it felt like a lot of our other squabbles were kind of like, oh, I feel like they're kind of irrelevant right now. if

01:11:15,618 --> 01:11:18,462
SPEAKER_0:  And then very quickly the whirlpool sucked it in and.

01:11:19,138 --> 01:11:24,958
SPEAKER_0:  And in a way where I think it damaged the reputation of these, a lot of the trust in a lot of these institutions for the long run.

01:11:25,282 --> 01:11:30,558
SPEAKER_1:  but there's also an individual psychological impact. It's like a vicious negative feedback cycle.

01:11:30,786 --> 01:11:33,150
SPEAKER_1:  where they were deeply affected on an emotional level.

01:11:33,506 --> 01:11:35,422
SPEAKER_1:  and people just were not their best selves.

01:11:36,258 --> 01:11:38,046
SPEAKER_0:  That's definitely true. Yeah, I mean...

01:11:38,754 --> 01:11:40,606
SPEAKER_0:  Talk about the primitive mind. I mean, what, what?

01:11:40,930 --> 01:11:43,678
SPEAKER_0:  one thing that we've been dealing with for our whole human history

01:11:43,938 --> 01:11:48,382
SPEAKER_0:  is pathogens and it's emotional, right? It brings out.

01:11:48,898 --> 01:11:51,038
SPEAKER_0:  You know, there's really interesting studies where like...

01:11:52,034 --> 01:11:52,414
SPEAKER_0:  F

01:11:53,186 --> 01:11:55,550
SPEAKER_0:  Wait, wait, they studied this, the, they,

01:11:56,034 --> 01:11:58,430
SPEAKER_0:  the phenomenon of disgust, which is one of these like.

01:11:59,298 --> 01:12:01,406
SPEAKER_0:  you know, smiling is universal.

01:12:01,634 --> 01:12:03,550
SPEAKER_0:  You don't have to ever translate a smile, right?

01:12:03,778 --> 01:12:04,542
SPEAKER_0:  certain, you know.

01:12:04,802 --> 01:12:05,118
SPEAKER_0:  you know.

01:12:05,506 --> 01:12:09,310
SPEAKER_0:  throwing your hands up when your sports team wins. This is universal because it's part of our-

01:12:09,634 --> 01:12:10,206
SPEAKER_0:  We're coding.

01:12:10,658 --> 01:12:16,574
SPEAKER_0:  And so is disgust to kind of make this like, you know, face where you wrinkle up your nose and you kind of put out your tongue and maybe even gag.

01:12:16,802 --> 01:12:21,790
SPEAKER_0:  That's to expel, expel whatever, because it's the reaction when something is potentially.

01:12:22,434 --> 01:12:25,886
SPEAKER_0:  a pathogen that might harm us, right? Feces, vomit, whatever.

01:12:26,626 --> 01:12:28,094
SPEAKER_0:  but they did this interesting study where.

01:12:28,610 --> 01:12:29,118
SPEAKER_0:  people.

01:12:30,242 --> 01:12:30,654
SPEAKER_0:  Cool.

01:12:31,170 --> 01:12:35,518
SPEAKER_0:  in two groups, the control group was shown images of

01:12:36,002 --> 01:12:41,278
SPEAKER_0:  And I might be getting two studies mixed up, but they were showing images of car crashes and like disturbing but not disgusting.

01:12:41,506 --> 01:12:48,254
SPEAKER_0:  And the other one was showing like, you know, like, you know, rotting things and just things that were disgusting. And then they were asked about immigration. These are Canadians.

01:12:48,642 --> 01:12:52,862
SPEAKER_0:  and the group that had the disgust feeling going, pulsing through their body.

01:12:53,122 --> 01:12:54,974
SPEAKER_0:  was way more likely to prefer.

01:12:55,426 --> 01:12:57,534
SPEAKER_0:  like immigrants from white countries.

01:12:58,498 --> 01:13:04,542
SPEAKER_0:  and the group that had to show in car accidents, they still prefer the groups from white countries, but much less so.

01:13:05,218 --> 01:13:08,702
SPEAKER_0:  And so what does that mean? It's because with the disgust impulse...

01:13:09,122 --> 01:13:10,430
SPEAKER_0:  makes us scared of.

01:13:10,658 --> 01:13:19,774
SPEAKER_0:  you know, sexual practices that are foreign, of ethnicities that are not, that don't look like us, of, it's still xenophobia, so it's ugly. It's really ugly stuff. This is of course also how.

01:13:20,002 --> 01:13:20,350
SPEAKER_0:  You know.

01:13:20,610 --> 01:13:21,054
SPEAKER_0:  them.

01:13:21,570 --> 01:13:28,222
SPEAKER_0:  Nazi propaganda with cockroaches and it was Rwandan was cockroaches, you know, the Nazis with rats and you know.

01:13:28,994 --> 01:13:31,934
SPEAKER_0:  It's specifically, it's a dehumanizing emotion. So.

01:13:32,802 --> 01:13:34,558
SPEAKER_0:  anyway we were we were we were

01:13:34,850 --> 01:13:36,190
SPEAKER_0:  We were talking about.

01:13:36,610 --> 01:13:44,158
SPEAKER_0:  COVID, but I think it taps deep into the human psyche. And I don't think it brings out our, I think, like you said, I think it brings out an ugly side in us.

01:13:44,994 --> 01:13:49,534
SPEAKER_1:  you describe an idea lab as being opposite of echo chambers.

01:13:50,178 --> 01:13:54,142
SPEAKER_1:  so we know what echo chambers are. and you said like just basically no good term.

01:13:54,594 --> 01:13:56,094
SPEAKER_1:  for the opposite of an echo chamber.

01:13:56,482 --> 01:13:57,822
SPEAKER_1:  So what's an idea lab?

01:13:58,370 --> 01:14:05,886
SPEAKER_0:  Yeah, well, first of all, both of these, we think of an echo chamber as like a group maybe or even a place, but it's a culture. It's an intellectual culture.

01:14:06,786 --> 01:14:07,838
SPEAKER_0:  And this goes along with.

01:14:08,098 --> 01:14:13,982
SPEAKER_0:  the high wrong, low wrong. So high wrong and low wrong thinking is individual. So I was talking about what's going on in your head, but this is very connected to.

01:14:14,594 --> 01:14:15,614
SPEAKER_0:  the social scene.

01:14:15,842 --> 01:14:16,574
SPEAKER_0:  around us.

01:14:16,802 --> 01:14:18,654
SPEAKER_0:  And so groups will do.

01:14:19,234 --> 01:14:21,310
SPEAKER_0:  high wrong and low wrong thinking.

01:14:21,698 --> 01:14:22,270
SPEAKER_0:  together.

01:14:22,754 --> 01:14:23,294
SPEAKER_0:  Um...

01:14:23,778 --> 01:14:28,190
SPEAKER_0:  Basically, it's colla... So an echo chamber to me is a collaborative low rung thinking. It is...

01:14:28,610 --> 01:14:29,982
SPEAKER_0:  It's a culture where.

01:14:30,338 --> 01:14:33,886
SPEAKER_0:  The cool, it's based around a sacred set of ideas.

01:14:34,402 --> 01:14:37,822
SPEAKER_0:  and it's the coolest thing you can do in an echo chamber culture.

01:14:38,274 --> 01:14:42,750
SPEAKER_0:  is talk about how great the sacred ideas are and how bad and evil and stupid and wrong

01:14:43,394 --> 01:14:43,870
SPEAKER_0:  The.

01:14:44,514 --> 01:14:46,718
SPEAKER_0:  people are who have the other views.

01:14:47,170 --> 01:14:49,374
SPEAKER_0:  and this and and and and

01:14:49,698 --> 01:14:53,886
SPEAKER_0:  It's quite boring. You know, it's quite boring. You know, it's very hard to learn.

01:14:54,306 --> 01:14:54,654
SPEAKER_0:  and

01:14:54,946 --> 01:14:57,502
SPEAKER_0:  changing your mind is not cool in an echo chamber culture.

01:14:57,858 --> 01:14:59,198
SPEAKER_0:  makes you seem wishy washy.

01:14:59,778 --> 01:15:00,734
SPEAKER_0:  It makes you seem.

01:15:01,026 --> 01:15:02,142
SPEAKER_0:  Um, like, you know.

01:15:02,370 --> 01:15:02,974
SPEAKER_0:  Uh...

01:15:03,202 --> 01:15:13,278
SPEAKER_0:  like you're waffling and you're flip-flopping or whatever, showing conviction about the sacred ideas in echo chamber culture is awesome. If you're just like, you know, obviously this makes you seem smart.

01:15:13,570 --> 01:15:16,478
SPEAKER_0:  while being humble makes you seem dumb. So now flip all of those things.

01:15:16,770 --> 01:15:21,406
SPEAKER_0:  on their heads and you have the opposite, which is idea lab culture, which is collaborative high-rank thinking.

01:15:21,858 --> 01:15:26,654
SPEAKER_0:  It's collaborative truth finding, but it's also just, it's just a totally different vibe. It's.

01:15:27,234 --> 01:15:28,382
SPEAKER_0:  It's a place where...

01:15:28,962 --> 01:15:30,238
SPEAKER_0:  arguing is...

01:15:30,658 --> 01:15:40,510
SPEAKER_0:  a fun thing, no one's getting offended, and criticizing the thing everyone believes is actually, it makes you seem interesting. Like, oh, really? Why do you think we're all wrong?

01:15:40,866 --> 01:15:41,246
SPEAKER_0:  uh...

01:15:41,634 --> 01:15:48,606
SPEAKER_0:  Expressing too much conviction makes people lose trust in you. Doesn't make you seem smart, it makes you seem stupid if you don't really know what you're talking about or you're acting like you do.

01:15:48,930 --> 01:15:50,974
SPEAKER_1:  I really like this diagram of where.

01:15:51,298 --> 01:15:55,646
SPEAKER_1:  on the x-axis agreement on the y-axis is decency. That's in an idea lab.

01:15:55,874 --> 01:15:58,398
SPEAKER_1:  And Echo Chamber, there's only one axis. It's...

01:15:58,914 --> 01:16:00,542
SPEAKER_1:  uh... asshole to non-asshole

01:16:01,186 --> 01:16:02,334
SPEAKER_1:  This is really important.

01:16:02,786 --> 01:16:04,030
SPEAKER_1:  think to understand?

01:16:04,418 --> 01:16:07,262
SPEAKER_1:  about the difference between, you call it decency here about.

01:16:07,746 --> 01:16:10,014
SPEAKER_1:  assholishness and disagreement.

01:16:10,466 --> 01:16:15,230
SPEAKER_0:  So my college friends, we love to argue, right? And no one thought anyone was an asshole for.

01:16:15,522 --> 01:16:23,486
SPEAKER_0:  It was just for sport. Sometimes we'd realize we're not even disagreeing on something, and that would be disappointing. We'd be like, ugh, I think we agree. And it was kind of like sad. It was like, oh, well, there goes the fun.

01:16:24,514 --> 01:16:25,630
SPEAKER_0:  And one of the.

01:16:26,114 --> 01:16:27,134
SPEAKER_0:  Members of this group.

01:16:27,554 --> 01:16:32,094
SPEAKER_0:  has this, she brought her new boyfriend to one of our like hangouts.

01:16:32,610 --> 01:16:34,110
SPEAKER_0:  And there was like a heated.

01:16:34,498 --> 01:16:38,430
SPEAKER_0:  heated debate, you know, just one of our typical things. Great, thanks for watching and afterwards...

01:16:38,946 --> 01:16:41,886
SPEAKER_0:  You know, the next day he said, like, is everything okay? He was like, what do you mean?

01:16:42,178 --> 01:16:44,478
SPEAKER_0:  And he said, like after the fight, And she was like, what fight?

01:16:44,802 --> 01:16:47,358
SPEAKER_0:  And he was like, you know, the fight last night and she was like, and she had to.

01:16:47,618 --> 01:16:48,830
SPEAKER_0:  Then she was like, you mean like that.

01:16:49,058 --> 01:16:54,654
SPEAKER_0:  and he was like, yeah, and so that's someone who is not used to idea lab culture coming into it.

01:16:55,138 --> 01:16:58,078
SPEAKER_0:  And seeing it is like, that was like, this is like, are they still friends?

01:16:58,658 --> 01:17:03,230
SPEAKER_0:  And Idea Lab is nice for the people in them because individuals thrive.

01:17:03,842 --> 01:17:17,694
SPEAKER_0:  You don't wanna just conform, it makes you seem boring in an idea. But you wanna be yourself, you wanna challenge things, you wanna have a unique brain. So that's great. And you also have people criticizing your ideas, which makes you smarter. It doesn't always feel good, but you become more correct and smarter.

01:17:18,530 --> 01:17:23,774
SPEAKER_0:  An echo chamber is the opposite, where it's not good for the people in it. It does, your learning skills, atrophy.

01:17:24,066 --> 01:17:25,150
SPEAKER_0:  Um, and.

01:17:25,442 --> 01:17:28,958
SPEAKER_0:  And I think it's boring. But the thing is they also have emergent properties.

01:17:29,346 --> 01:17:29,662
SPEAKER_0:  So.

01:17:30,370 --> 01:17:34,174
SPEAKER_0:  The emergent property of an idealab is like super intelligence.

01:17:34,466 --> 01:17:37,022
SPEAKER_0:  Just you and me alone, just the two of us.

01:17:37,538 --> 01:17:45,374
SPEAKER_0:  if we're working together on something, but we're being really grown up about it, we're disagreeing, we're not, you know, no one's sensitive about anything.

01:17:45,762 --> 01:17:49,598
SPEAKER_0:  We're gonna each find flaws in the other one's arguments that you wouldn't have found on your own.

01:17:50,018 --> 01:18:00,510
SPEAKER_0:  and we're going to have double the epiphanies, right? So it's almost like the two of us together is like as smart as 1.5, it's like 50% smarter than either of us alone, right? So you have this 1.5 intelligent.

01:18:00,834 --> 01:18:01,246
SPEAKER_0:  kind of.

01:18:01,794 --> 01:18:03,038
SPEAKER_0:  joint being that we've made.

01:18:03,330 --> 01:18:07,614
SPEAKER_0:  Now bringing a third person in, fourth person in, right? You see it starts to scale up. This is why science.

01:18:08,002 --> 01:18:08,926
SPEAKER_0:  institutions.

01:18:09,218 --> 01:18:09,598
SPEAKER_0:  Ken.

01:18:10,018 --> 01:18:10,654
SPEAKER_0:  Discover.

01:18:11,234 --> 01:18:16,158
SPEAKER_0:  relativity and quantum mechanics and these things that no individual human, you know, is going to come up with without a

01:18:16,418 --> 01:18:18,718
SPEAKER_0:  of collaboration because it's this giant.

01:18:19,106 --> 01:18:21,662
SPEAKER_0:  Idea Lab, so it has an emergent property of super intelligence.

01:18:22,146 --> 01:18:23,134
SPEAKER_0:  an echo chamber,

01:18:23,490 --> 01:18:24,542
SPEAKER_0:  is the opposite where

01:18:24,898 --> 01:18:26,782
SPEAKER_0:  It has the emergent property of.

01:18:28,642 --> 01:18:33,662
SPEAKER_0:  I mean, has the emergent property of a bunch of people all, you know, paying field, you know,

01:18:33,922 --> 01:18:35,038
SPEAKER_0:  filthy to this.

01:18:35,394 --> 01:18:41,566
SPEAKER_0:  set of sacred ideas and so you lose this magical thing about language and humans which is

01:18:42,050 --> 01:18:44,414
SPEAKER_0:  Collaborative intelligence, you lose it. It disappears.

01:18:45,122 --> 01:18:47,646
SPEAKER_1:  but there is that access of decency.

01:18:48,098 --> 01:18:53,342
SPEAKER_1:  which is really interesting because you kind of painted this picture of you and your friends arguing really harshly.

01:18:53,826 --> 01:18:56,030
SPEAKER_1:  but underlying that.

01:18:56,674 --> 01:18:57,086
SPEAKER_1:  is a

01:18:57,410 --> 01:18:59,710
SPEAKER_1:  basic camaraderie, respect.

01:19:00,546 --> 01:19:01,790
SPEAKER_1:  There's a...

01:19:02,818 --> 01:19:06,078
SPEAKER_1:  all kinds of mechanisms we humans have constructed to communicate.

01:19:07,074 --> 01:19:10,430
SPEAKER_1:  like mutual respect or maybe communicate that you're here for the idea lab.

01:19:10,594 --> 01:19:14,110
SPEAKER_0:  version of this. Totally. You don't get personal.

01:19:14,882 --> 01:19:18,686
SPEAKER_0:  Right? You're not getting personal. You're not taking things personally.

01:19:20,386 --> 01:19:24,222
SPEAKER_0:  People are respected in an idea lab and ideas are disrespected.

01:19:24,322 --> 01:19:25,918
SPEAKER_1:  And there's ways to signal that.

01:19:26,978 --> 01:19:32,894
SPEAKER_1:  Like with friends, you've already done the signaling, you've already established a relationship. The interesting thing is online.

01:19:33,218 --> 01:19:34,750
SPEAKER_1:  I think you have to do some of that work.

01:19:35,170 --> 01:19:36,574
SPEAKER_1:  To me...

01:19:37,122 --> 01:19:39,934
SPEAKER_1:  sort of steel manning the other side or no.

01:19:40,226 --> 01:19:46,686
SPEAKER_1:  uh... having empathy and hearing out being able to basically repeat the argument the other person is making before you

01:19:47,010 --> 01:19:53,310
SPEAKER_1:  and showing respect to that argument. I could see how you could think that before you make a counter argument. This is just a bunch of-

01:19:54,210 --> 01:19:56,158
SPEAKER_1:  ways to communicate that you're here.

01:19:56,514 --> 01:19:58,206
SPEAKER_1:  not to, uh...

01:19:58,786 --> 01:19:59,550
SPEAKER_1:  do kind of.

01:19:59,938 --> 01:20:01,566
SPEAKER_1:  What is it? Low rung.

01:20:02,210 --> 01:20:04,606
SPEAKER_1:  you know, shit talking, mockery, derision.

01:20:04,866 --> 01:20:11,518
SPEAKER_1:  but are actually here ultimately to discover the truth in the space of ideas and the tension of those ideas. And I think.

01:20:12,578 --> 01:20:13,630
SPEAKER_1:  It's a...

01:20:14,786 --> 01:20:19,806
SPEAKER_1:  I think that's a skill that we're all learning as a civilization of how to do that kind of communication effectively.

01:20:20,418 --> 01:20:25,502
SPEAKER_1:  I think disagreement as I'm learning on the internet is actually a really tricky skill like high

01:20:25,922 --> 01:20:28,030
SPEAKER_1:  effort high decency disagreement.

01:20:28,482 --> 01:20:32,254
SPEAKER_1:  I got to listen to, there's a really good debate podcast.

01:20:32,834 --> 01:20:33,854
SPEAKER_1:  Intelligence Squared.

01:20:34,690 --> 01:20:36,565
SPEAKER_1:  And like they can go pretty hard.

01:20:36,565 --> 01:20:37,315
SPEAKER_0:  in the paint.

01:20:37,315 --> 01:20:38,270
SPEAKER_1:  Classic Idea Lab.

01:20:38,498 --> 01:20:41,118
SPEAKER_1:  Exactly, but like how do we map that?

01:20:41,634 --> 01:20:44,862
SPEAKER_1:  to social media when people like will say.

01:20:45,730 --> 01:20:51,806
SPEAKER_1:  We'll say, well, like Lex or anybody, you're not, you hate disagreement. You want to sense their disagreement, no.

01:20:52,226 --> 01:20:54,910
SPEAKER_1:  uh... i love intelligence square type of disagreement

01:20:55,106 --> 01:20:58,997
SPEAKER_0:  That's fun. You want to reduce asshole. And for me personally I think that's a good settings for the video.

01:20:58,997 --> 01:21:00,094
SPEAKER_1:  Personally, I don't want to-

01:21:00,322 --> 01:21:04,542
SPEAKER_1:  reduce asshole if you know, I kind of like asshole it's like fun in many ways

01:21:04,802 --> 01:21:07,806
SPEAKER_1:  But the problem is when the asshole shows up to the party...

01:21:08,098 --> 01:21:08,702
SPEAKER_1:  They.

01:21:09,026 --> 01:21:10,462
SPEAKER_1:  make it less fun for the-

01:21:11,042 --> 01:21:12,862
SPEAKER_1:  for the party that's there for the idea lab.

01:21:13,154 --> 01:21:14,430
SPEAKER_1:  and the other people, especially the-

01:21:14,754 --> 01:21:16,926
SPEAKER_1:  quiet voices at the back of the room they leave.

01:21:17,218 --> 01:21:19,038
SPEAKER_1:  And so all you're left is was.

01:21:19,330 --> 01:21:20,455
SPEAKER_1:  is with assholes.

01:21:20,455 --> 01:21:26,686
SPEAKER_0:  But political Twitter to me is one of those parties. It's a big party where a few assholes...

01:21:27,106 --> 01:21:29,214
SPEAKER_0:  have really sent a lot of the.

01:21:29,506 --> 01:21:29,950
SPEAKER_0:  Quiet.

01:21:30,818 --> 01:21:31,550
SPEAKER_0:  Thinkers?

01:21:32,002 --> 01:21:32,382
SPEAKER_0:  away.

01:21:32,674 --> 01:21:34,302
SPEAKER_0:  Yeah.

01:21:34,562 --> 01:21:36,894
SPEAKER_0:  And so if you think about this graph again.

01:21:39,170 --> 01:21:41,182
SPEAKER_0:  A place like Twitter.

01:21:41,602 --> 01:21:42,078
SPEAKER_0:  Um...

01:21:42,786 --> 01:21:44,286
SPEAKER_0:  A great way to get followers.

01:21:44,546 --> 01:21:45,438
SPEAKER_0:  is to be.

01:21:45,666 --> 01:21:46,910
SPEAKER_0:  an asshole with a certain.

01:21:47,714 --> 01:21:48,990
SPEAKER_0:  pumping a certain ideology.

01:21:49,282 --> 01:21:55,550
SPEAKER_0:  you'll get a huge amount of followers. And for those followers, and the followers you're gonna get, the people who like you.

01:21:56,706 --> 01:22:05,534
SPEAKER_0:  are probably going to be people who are really thinking with their primitive mind because they're seeing you're being an asshole, but because you agree with them, they love you.

01:22:05,890 --> 01:22:13,214
SPEAKER_0:  And they think they don't see any problem with how you're being. Yeah, they don't see the asshole. This is a fascinating thing. Because look at the thing on the right. Agreement and decency.

01:22:13,442 --> 01:22:14,014
SPEAKER_0:  are the same.

01:22:14,242 --> 01:22:15,902
SPEAKER_0:  So if you're in that mindset.

01:22:16,290 --> 01:22:18,622
SPEAKER_0:  The bigger the asshole, the better. If you're agreeing with me, you're my man.

01:22:18,850 --> 01:22:20,414
SPEAKER_0:  I love what you're saying. Yes, show them.

01:22:20,706 --> 01:22:25,150
SPEAKER_0:  Right? And the algorithm helps those people. Those people do great on the algorithm.

01:22:25,698 --> 01:22:27,742
SPEAKER_1:  There's a fascinating dynamic that happens.

01:22:28,162 --> 01:22:29,278
SPEAKER_1:  uh... does a have

01:22:29,890 --> 01:22:31,102
SPEAKER_1:  currently hired somebody that.

01:22:31,458 --> 01:22:32,542
SPEAKER_1:  looks at my social media.

01:22:32,802 --> 01:22:33,790
SPEAKER_1:  and they block people?

01:22:34,082 --> 01:22:39,710
SPEAKER_1:  because the assholes will roll in. They're not actually there to have an interesting disagreement, which I love.

01:22:40,226 --> 01:22:42,046
SPEAKER_1:  They're there to do kind of mockery.

01:22:42,594 --> 01:22:44,094
SPEAKER_1:  And then when they get blocked.

01:22:45,314 --> 01:22:48,926
SPEAKER_1:  they then celebrate that to their echo chamber. Like, look at this.

01:22:49,474 --> 01:22:53,470
SPEAKER_0:  I got them or whatever. Or they'll say some annoying thing like, oh, so it's.

01:22:53,762 --> 01:23:01,342
SPEAKER_0:  So he talks about, he likes, you know, if I'd done this, they'll say, oh, he says he likes idea labs, but he actually wants to create an echo chamber. I'm like, nope.

01:23:01,794 --> 01:23:02,974
SPEAKER_0:  You're an asshole, I'm not-

01:23:03,202 --> 01:23:13,886
SPEAKER_0:  I'm not, look at the other 50 people on this thread that disagreed with me respectfully, they're not blocked. Yep, exactly. You know, and so they see it as some kind of hypocrisy because again, they only see the thing on the right.

01:23:14,626 --> 01:23:24,926
SPEAKER_0:  and they're not understanding that there's two axes, or that I see it as two axes. And so you seem petty in that moment, but it's like, no, no, no, this is very specific what I'm doing. You're actually killing the conversation.

01:23:25,346 --> 01:23:33,278
SPEAKER_1:  I, I, in generally, I give all those folks a pass and just send them love telepathically. But yes, like this.

01:23:33,666 --> 01:23:36,126
SPEAKER_1:  getting rid of assholes in the conversation is.

01:23:36,386 --> 01:23:38,261
SPEAKER_1:  the way you allow for the disagreement.

01:23:38,261 --> 01:23:40,990
SPEAKER_0:  You do a lot of like when I think when like.

01:23:41,282 --> 01:23:43,262
SPEAKER_0:  primitive mindedness comes at you?

01:23:43,746 --> 01:23:45,214
SPEAKER_0:  At least on Twitter, I don't know what you're feeling.

01:23:45,698 --> 01:23:47,806
SPEAKER_0:  internally in that moment, but you do a lot of like.

01:23:48,866 --> 01:23:50,366
SPEAKER_0:  I'm gonna meet that with my higher.

01:23:51,202 --> 01:23:53,246
SPEAKER_0:  and you come out and you'll be like...

01:23:53,538 --> 01:23:54,910
SPEAKER_0:  Thanks for all the criticism.

01:23:55,202 --> 01:23:57,918
SPEAKER_0:  I love you. And that's the that that.

01:23:58,178 --> 01:24:02,110
SPEAKER_0:  That's actually an amazing response because it just.

01:24:02,594 --> 01:24:03,582
SPEAKER_0:  It- what it does-

01:24:04,194 --> 01:24:04,542
SPEAKER_0:  Is it?

01:24:05,026 --> 01:24:12,510
SPEAKER_0:  it unrials up that person's primitive mind and actually wakes up their higher mind who says, oh, okay, you know, this guy's not so bad. And suddenly, like,

01:24:12,994 --> 01:24:15,619
SPEAKER_0:  civility comes back so it's a very powerful

01:24:15,619 --> 01:24:20,126
SPEAKER_1:  Hopefully long term, but the thing is they do seem to drive away.

01:24:21,154 --> 01:24:22,462
SPEAKER_1:  high quality disagreement.

01:24:22,978 --> 01:24:24,958
SPEAKER_1:  Cause like, cause it takes so much effort.

01:24:25,474 --> 01:24:28,099
SPEAKER_1:  to disagree in a high quality way.

01:24:28,099 --> 01:24:29,950
SPEAKER_0:  I've noticed this in my blog, like...

01:24:30,178 --> 01:24:35,326
SPEAKER_0:  One of the things I pride myself on is like, my comment section is awesome. Like there's, there's, there's.

01:24:36,034 --> 01:24:37,854
SPEAKER_0:  Everyone's being respectful.

01:24:38,786 --> 01:24:47,134
SPEAKER_0:  No one's afraid to disagree with me and tell them and say, you know, tear my post apart, but in a totally respectful way where the underlying thing is like, I'm here because I like this guy and his writing.

01:24:48,418 --> 01:24:52,286
SPEAKER_0:  and people disagree with each other and they get in these long and it's interesting and I read it and I'm learning

01:24:52,930 --> 01:24:56,094
SPEAKER_0:  And then I have a couple posts, especially the ones I've written about politics.

01:24:56,514 --> 01:24:57,054
SPEAKER_0:  It's not like.

01:24:57,666 --> 01:25:01,470
SPEAKER_0:  It seems like any other comment section, people are being nasty to me, they're being nasty to each other.

01:25:02,082 --> 01:25:02,654
SPEAKER_0:  And then I...

01:25:02,914 --> 01:25:04,606
SPEAKER_0:  looked down one of them and I realized like.

01:25:05,026 --> 01:25:07,710
SPEAKER_0:  Almost all of this is the work of like three people. Yeah.

01:25:08,226 --> 01:25:14,238
SPEAKER_0:  That's who you need to block. Those people need to be blocked. You're not being thin skinned. You're not being petty doing it. You're actually-

01:25:14,466 --> 01:25:15,166
SPEAKER_0:  protecting

01:25:15,394 --> 01:25:16,414
SPEAKER_0:  uh... an ideal

01:25:16,994 --> 01:25:21,054
SPEAKER_0:  Because what really aggressive people like that do is they'll turn it into their own echo chamber.

01:25:21,570 --> 01:25:30,663
SPEAKER_0:  because now everyone is scared to kind of disagree with them and it's unpleasant. And so people who will chime in are the people who agree with them and suddenly like they've taken over the space. And I kind of believe that the-

01:25:30,663 --> 01:25:31,390
SPEAKER_1:  those people.

01:25:31,714 --> 01:25:41,310
SPEAKER_1:  on a different day could actually do high effort disagreement, it's just that they're in a certain kind of mood, and a lot of us, just like you said, with a primitive mind, could get into that mood.

01:25:41,634 --> 01:25:41,982
SPEAKER_1:  and

01:25:42,754 --> 01:25:46,462
SPEAKER_1:  I believe it's actually the job of the technology, the platform.

01:25:46,882 --> 01:25:48,574
SPEAKER_1:  to incentivize those folks to be like.

01:25:49,378 --> 01:25:51,582
SPEAKER_1:  Are you sure this is the best you can do?

01:25:52,034 --> 01:25:54,686
SPEAKER_1:  Like if you really want to talk shit about this idea.

01:25:55,010 --> 01:25:55,774
SPEAKER_1:  They do better.

01:25:56,674 --> 01:26:00,158
SPEAKER_1:  Yeah. And then we need to create incentives where you get likes.

01:26:00,514 --> 01:26:02,110
SPEAKER_1:  for high effort disagreement.

01:26:02,658 --> 01:26:04,894
SPEAKER_1:  because currently you get likes for like a

01:26:05,538 --> 01:26:09,662
SPEAKER_1:  something that's slightly funny and is a little bit like mockery.

01:26:10,338 --> 01:26:10,974
SPEAKER_1:  I got him.

01:26:12,034 --> 01:26:14,846
SPEAKER_1:  Yeah, basically signals to some kind of echo chamber.

01:26:15,682 --> 01:26:16,670
SPEAKER_1:  this person is.

01:26:17,154 --> 01:26:19,742
SPEAKER_1:  A horrible person is a hypocrite is evil whatever

01:26:20,130 --> 01:26:25,630
SPEAKER_1:  that feels like it's solvable with technology. Cause I think in our private lives, none of us want that.

01:26:26,274 --> 01:26:28,350
SPEAKER_0:  I wonder if it's making me think that I want to like.

01:26:28,706 --> 01:26:31,966
SPEAKER_0:  because a much easier way for me to do it just for my...

01:26:32,738 --> 01:26:35,198
SPEAKER_0:  my world would be to say something like, you know.

01:26:35,650 --> 01:26:38,526
SPEAKER_0:  Here's this axis, this is part of what I...

01:26:38,786 --> 01:26:43,710
SPEAKER_0:  part of what I like about the latter is it's a language that we can use, specifically what we're talking about is

01:26:45,122 --> 01:26:46,526
SPEAKER_0:  High-Rung Disagreement, good.

01:26:46,786 --> 01:26:47,870
SPEAKER_0:  Lowering disagreement.

01:26:48,098 --> 01:26:57,918
SPEAKER_0:  bad, right? And so it gives us like a language for that. And so what I would say is I would, you know, my, you know, I would have my readers, you know, understand this axis. And then I would specifically say something like.

01:26:58,242 --> 01:26:58,718
SPEAKER_0:  Please.

01:26:59,362 --> 01:27:01,470
SPEAKER_0:  Do the...

01:27:01,986 --> 01:27:03,070
SPEAKER_0:  Do it, but why a favor?

01:27:03,330 --> 01:27:09,502
SPEAKER_0:  and up vote regardless of what they're saying horizontally, right? Regardless of what their actual view is. Up vote, high wrongness.

01:27:10,210 --> 01:27:11,390
SPEAKER_0:  It could be tearing me apart.

01:27:11,650 --> 01:27:12,382
SPEAKER_0:  they can be.

01:27:12,706 --> 01:27:14,590
SPEAKER_0:  saying great, they can be praising me, whatever.

01:27:15,842 --> 01:27:18,430
SPEAKER_0:  Uproad high rungness and downvote low rungness.

01:27:18,786 --> 01:27:24,478
SPEAKER_0:  And if enough people are doing that, suddenly there's all this incentive to try to say, I need to calm my emotion down here and not.

01:27:24,770 --> 01:27:27,678
SPEAKER_0:  be it personal because I'm gonna get voted into oblivion by these people.

01:27:28,482 --> 01:27:29,607
SPEAKER_0:  I think a lot of people...

01:27:29,607 --> 01:27:30,622
SPEAKER_1:  be very good at that.

01:27:31,426 --> 01:27:32,062
SPEAKER_1:  They...

01:27:32,514 --> 01:27:33,406
SPEAKER_1:  and they not.

01:27:33,634 --> 01:27:36,222
SPEAKER_1:  are only would they be good at that they would want that.

01:27:36,770 --> 01:27:41,982
SPEAKER_1:  that task of saying, I know I completely disagree with this person, but this was a high effort.

01:27:42,242 --> 01:27:43,006
SPEAKER_1:  I wrong

01:27:44,002 --> 01:27:50,494
SPEAKER_0:  Disagreement gets everyone thinking about that other axis to you're not just looking at where do you stand horizontally? You're saying well how did you get there and how are you you know?

01:27:51,458 --> 01:27:56,391
SPEAKER_0:  Are you treating ideas like machines or are you treating them like little babies?

01:27:56,391 --> 01:28:03,262
SPEAKER_1:  should be some kind of labeling on personal attacks versus idea disagreement. Sometimes people like throw in both a little bit. Alright?

01:28:03,554 --> 01:28:07,679
SPEAKER_1:  That's like, all right, no, there should be a disincentive at personal attacks versus idea attacks.

01:28:07,679 --> 01:28:09,694
SPEAKER_0:  Well, you can also what one metric is.

01:28:11,138 --> 01:28:12,350
SPEAKER_0:  A respectful disagreement.

01:28:12,578 --> 01:28:18,878
SPEAKER_0:  If I see just say someone else's Twitter and I see, you know, you put out a thought and I see someone say, you know,

01:28:19,362 --> 01:28:20,094
SPEAKER_0:  someone say.

01:28:21,090 --> 01:28:24,830
SPEAKER_0:  I don't see it that way. Here's where I think you went wrong, and they're just explaining.

01:28:26,050 --> 01:28:28,094
SPEAKER_0:  I'm thinking that if Lex Reed is that, he's gonna...

01:28:28,578 --> 01:28:33,054
SPEAKER_0:  He's gonna wanna post more stuff, right? Because he's gonna like that. If I see someone being like...

01:28:33,442 --> 01:28:34,046
SPEAKER_0:  Um...

01:28:34,946 --> 01:28:45,118
SPEAKER_0:  Wow, this really shows the kind of person that you become or shows everyone. I'm thinking that person is making Lex want to be on Twitter less. It's making him, and so what's that doing? What that person's actually doing is they're putting...

01:28:45,346 --> 01:28:57,411
SPEAKER_0:  is they're actually, they're chilling discussion because they're making it unpleasant to, they're making it scary to say what you think. And the first person isn't at all. The first person is making you wanna say more stuff. So, and those are both disagree, those are people who both disagree with you. Exactly.

01:28:57,411 --> 01:28:58,238
SPEAKER_1:  Exactly, exactly.

01:28:59,266 --> 01:29:02,462
SPEAKER_1:  I want to, great disagreements with friends in.

01:29:03,138 --> 01:29:04,062
SPEAKER_1:  MeetSpace.

01:29:04,578 --> 01:29:05,118
SPEAKER_1:  It's like...

01:29:05,378 --> 01:29:05,982
SPEAKER_1:  your

01:29:06,274 --> 01:29:07,742
SPEAKER_1:  They disagree with you.

01:29:08,194 --> 01:29:09,950
SPEAKER_1:  that could be even yelling at you.

01:29:10,850 --> 01:29:15,463
SPEAKER_1:  Honestly, they could even have some shit talk where it's like personal attacks. It is still feels good

01:29:15,463 --> 01:29:21,918
SPEAKER_0:  because you know them well and you know that that shit talk, because yeah, friends shit talk all the time, playing a sport or a game.

01:29:22,210 --> 01:29:26,046
SPEAKER_0:  And again, it's because they know each other well enough to know that this is

01:29:27,074 --> 01:29:32,670
SPEAKER_0:  We're having fun and obviously I love you. Like, you know, and that's important online. It's a lot harder.

01:29:32,994 --> 01:29:34,119
SPEAKER_0:  Yeah, that...

01:29:34,119 --> 01:29:37,246
SPEAKER_1:  Obviously I love you that underlies a lot of human interaction.

01:29:37,858 --> 01:29:43,486
SPEAKER_1:  seems to be easily lost online. I've seen some people on Twitter and elsewhere just behave their worst.

01:29:44,514 --> 01:29:46,398
SPEAKER_1:  And it's like, I know that's not who you are.

01:29:46,498 --> 01:29:51,070
SPEAKER_0:  Like why are you? Actually, you know, I know is this human. I know someone personally who is.

01:29:51,490 --> 01:29:51,902
SPEAKER_0:  One of the-

01:29:52,226 --> 01:29:52,894
SPEAKER_0:  best people.

01:29:53,762 --> 01:29:55,742
SPEAKER_0:  I love this guy.

01:29:56,098 --> 01:29:59,550
SPEAKER_0:  One of the best, like fun, funny, and like nicest dudes.

01:30:00,706 --> 01:30:06,430
SPEAKER_0:  And he, if you would, if you looked at his Twitter only, you'd think he's a culture worry or an awful culture worry.

01:30:07,298 --> 01:30:08,574
SPEAKER_0:  and you know.

01:30:09,186 --> 01:30:12,734
SPEAKER_0:  biased and just stoking anger.

01:30:13,474 --> 01:30:22,366
SPEAKER_0:  And it comes out of a good place. And I'm not gonna give any other info about specific, but like- I think you're describing a lot of people. It comes out of a good place because he really cares about what he, you know, it comes out-

01:30:22,722 --> 01:30:30,654
SPEAKER_0:  but it's just, I can't square the two. It's so, and that's it. You have to, once you know someone like that, you can realize, okay, apply that to everyone because a lot of these people are lovely people.

01:30:31,362 --> 01:30:35,710
SPEAKER_0:  And it just bring, even just, you know, back in the before social media, you ever had a friend who like...

01:30:37,090 --> 01:30:38,622
SPEAKER_0:  was just like, they had this like.

01:30:38,882 --> 01:30:44,062
SPEAKER_0:  dickishness on text or email that they didn't have in person. And you're like, wow, email you is kind of a dick.

01:30:44,546 --> 01:30:47,806
SPEAKER_0:  And it's like, it just, certain people have a different persona behind the screen.

01:30:48,322 --> 01:30:51,326
SPEAKER_1:  It has for me personally become a bit of a meme that I-

01:30:51,586 --> 01:30:53,278
SPEAKER_1:  Lex Blocks with Love.

01:30:53,634 --> 01:30:56,254
SPEAKER_1:  But there is a degree to that where this is

01:30:56,482 --> 01:31:05,438
SPEAKER_1:  I don't see people on social media as representing who they really are. I really do have love for them. I really do think positive thoughts of them throughout the entire experience. I see this as some-

01:31:05,794 --> 01:31:09,246
SPEAKER_1:  weird side effect of online communication.

01:31:10,434 --> 01:31:11,838
SPEAKER_1:  So it's like to me blocking.

01:31:12,322 --> 01:31:13,406
SPEAKER_1:  is not

01:31:13,698 --> 01:31:17,073
SPEAKER_1:  some kind of a derisive act towards that individual. It's just like saying-

01:31:17,073 --> 01:31:18,654
SPEAKER_0:  A lot of times what's happened.

01:31:19,042 --> 01:31:22,526
SPEAKER_0:  is they have slipped into a very common delusion.

01:31:23,522 --> 01:31:23,870
SPEAKER_0:  that.

01:31:24,482 --> 01:31:25,726
SPEAKER_0:  dehumanizes others.

01:31:25,954 --> 01:31:36,414
SPEAKER_0:  So that doesn't mean they're a bad person. We all can do it, but they're dehumanizing you or whoever they're being nasty to because in a way they would never do in person because in a person they're reminded that's a person.

01:31:37,474 --> 01:31:44,894
SPEAKER_0:  Remember I said the dumb part of my brain when I'm doing VR, like, won't step off the cliff, but the smart part of my brain knows I'm just on the rug? That dumb part of our brain...

01:31:45,762 --> 01:31:49,470
SPEAKER_0:  is really dumb in a lot of ways. It's the part of your brain where you can.

01:31:50,146 --> 01:31:56,094
SPEAKER_0:  to set the clock five minutes fast to help you not be late. The smart part of your brain knows that you did that, but the dumb part will fall for it, right?

01:31:56,418 --> 01:32:02,206
SPEAKER_0:  That same dumb part of your brain can forget that the person behind that screen, that behind that handle.

01:32:02,562 --> 01:32:03,038
SPEAKER_0:  is a human.

01:32:03,426 --> 01:32:07,582
SPEAKER_0:  that has feelings. And that doesn't mean they're a bad person for forgetting that, because it's-

01:32:07,874 --> 01:32:08,414
SPEAKER_0:  possible.

01:32:08,578 --> 01:32:11,710
SPEAKER_1:  Well, this really interesting idea, I wonder if it's true that you write

01:32:12,738 --> 01:32:17,758
SPEAKER_1:  is that both primitive mindedness and high-mindedness tend to be contagious.

01:32:18,818 --> 01:32:19,870
SPEAKER_1:  I hope you're right.

01:32:20,258 --> 01:32:22,782
SPEAKER_1:  that it's possible to make both contagious.

01:32:23,362 --> 01:32:23,806
SPEAKER_1:  is our

01:32:24,386 --> 01:32:25,758
SPEAKER_1:  are sort of...

01:32:26,114 --> 01:32:27,390
SPEAKER_1:  popular intuition is.

01:32:27,938 --> 01:32:32,446
SPEAKER_1:  Only one of them, the primitive mindedness is contagious, as exhibited by social media.

01:32:32,834 --> 01:32:36,510
SPEAKER_0:  compliment you again. Don't you think that your Twitter to me is like.

01:32:37,218 --> 01:32:39,422
SPEAKER_0:  I was just looking down and I mean it is a

01:32:40,354 --> 01:32:51,390
SPEAKER_0:  It's just high-mindedness. It's just high-mindedness, down, down, down, down, down. It's gratitude, it's optimism, it's love, it's forgiveness. It's all these things that are the opposite of grievance and victimhood and...

01:32:51,714 --> 01:32:53,406
SPEAKER_0:  resentment and pessimism, right?

01:32:53,890 --> 01:32:54,494
SPEAKER_0:  and

01:32:56,066 --> 01:32:59,198
SPEAKER_0:  There's I think a reason that a lot of people follow you because it positions you in a consecutive trolling movement

01:32:59,746 --> 01:33:02,371
SPEAKER_0:  is contagious. It makes other people feel those feelings.

01:33:02,371 --> 01:33:05,630
SPEAKER_1:  I don't know, I've been recently.

01:33:06,146 --> 01:33:07,390
SPEAKER_1:  over the past few months.

01:33:07,650 --> 01:33:11,934
SPEAKER_1:  Attack quite a lot and it's fascinating to watch because it's over things that

01:33:12,482 --> 01:33:16,094
SPEAKER_1:  I think I probably have done stupid things, but I'm being attacked for things that are-

01:33:16,738 --> 01:33:19,390
SPEAKER_1:  Totally not worthy of attack. I got attacked.

01:33:19,810 --> 01:33:20,190
SPEAKER_1:  for it.

01:33:20,418 --> 01:33:21,150
SPEAKER_1:  A booklist?

01:33:22,178 --> 01:33:26,206
SPEAKER_0:  I saw that, by the way. I thought it was great. But like, you can always kind of f-

01:33:26,306 --> 01:33:28,254
SPEAKER_1:  find ways to

01:33:28,834 --> 01:33:30,846
SPEAKER_1:  You know, I guess the assumption is...

01:33:31,426 --> 01:33:33,214
SPEAKER_1:  This person surely is a fraud.

01:33:33,826 --> 01:33:38,526
SPEAKER_1:  or some other explanation. He sure has dead bodies in the basement when he's hiding or something like this.

01:33:38,978 --> 01:33:44,862
SPEAKER_1:  and then I'm going to construct a narrative around that and mock and attack that. I don't know how that works, but there is a-

01:33:45,474 --> 01:33:48,830
SPEAKER_1:  There does, and I think you write this in the book, there seems to be a gravity.

01:33:49,890 --> 01:33:51,230
SPEAKER_1:  pulling people towards the

01:33:51,490 --> 01:33:53,470
SPEAKER_0:  primitive mind. And it's like. To anything.

01:33:54,018 --> 01:33:54,590
SPEAKER_0:  political.

01:33:54,882 --> 01:33:56,958
SPEAKER_0:  Right, religious, certain things.

01:33:57,698 --> 01:34:07,166
SPEAKER_0:  are bottom heavy, you know, for our psyche. They have a magnet that pulls our psyches downwards on the ladder. And why? Why does politics pull our psyches?

01:34:07,938 --> 01:34:09,470
SPEAKER_0:  down on the ladder because...

01:34:11,266 --> 01:34:14,750
SPEAKER_0:  for the tens of thousands of years that we were evolving.

01:34:16,002 --> 01:34:16,542
SPEAKER_0:  lens.

01:34:16,834 --> 01:34:18,462
SPEAKER_0:  you know, during human history.

01:34:18,850 --> 01:34:19,838
SPEAKER_0:  It was life or death.

01:34:20,546 --> 01:34:22,014
SPEAKER_0:  politics was life or death and.

01:34:22,306 --> 01:34:23,134
SPEAKER_0:  And so.

01:34:23,394 --> 01:34:23,934
SPEAKER_0:  Um...

01:34:24,226 --> 01:34:26,494
SPEAKER_0:  There's actually an amazing study where it's like.

01:34:26,850 --> 01:34:27,358
SPEAKER_0:  Um...

01:34:28,002 --> 01:34:28,414
SPEAKER_0:  day.

01:34:29,154 --> 01:34:31,998
SPEAKER_0:  challenged like 20 different beliefs of a person.

01:34:32,610 --> 01:34:38,014
SPEAKER_0:  and different parts of the person's brain, and then you had an MRI going, different parts of the person's brain lit up.

01:34:38,338 --> 01:34:41,406
SPEAKER_0:  when non-political beliefs were challenged versus political beliefs were challenged.

01:34:41,986 --> 01:34:45,662
SPEAKER_0:  When political beliefs were challenged, when non-political beliefs were challenged,

01:34:47,874 --> 01:34:59,422
SPEAKER_0:  like the rational, like the prefrontal cortex type areas were lit up, when the political beliefs were challenged, and then I'm getting over my head here, but it's like the parts of your brain, the default mode network, the parts of your brain associated with like...

01:34:59,746 --> 01:35:02,206
SPEAKER_0:  introspection and like your own identity.

01:35:02,722 --> 01:35:03,070
SPEAKER_0:  We'll see you later.

01:35:03,554 --> 01:35:03,998
SPEAKER_0:  and

01:35:04,770 --> 01:35:11,966
SPEAKER_0:  they were much more likely to change their mind on all the beliefs, the non-political beliefs. When that default mode network part of your brain...

01:35:12,386 --> 01:35:17,694
SPEAKER_0:  uh... lit up you were you were gonna if anything get more firm in those beliefs when you when you had them challenged

01:35:18,178 --> 01:35:18,526
SPEAKER_0:  So.

01:35:19,458 --> 01:35:21,310
SPEAKER_0:  Politics is one of those topics.

01:35:21,666 --> 01:35:28,606
SPEAKER_0:  that just literally, literally lights up different part of our brain. Again, I think we come back to primitive mind, higher mind here, it's like.

01:35:29,058 --> 01:35:33,118
SPEAKER_0:  It gets our higher, this is one of the things our primitive mind comes programmed.

01:35:33,634 --> 01:35:38,846
SPEAKER_0:  to care a ton about and so it's gonna be very hard for us to stay rational and calm and.

01:35:39,106 --> 01:35:42,325
SPEAKER_0:  and looking for truth because we have all this gravity.

01:35:42,325 --> 01:35:48,286
SPEAKER_1:  It's weird because politics, like what is politics? Like you talk about it's a bunch of different issues and each individual issue.

01:35:48,802 --> 01:35:49,927
SPEAKER_1:  If we really talk about it.

01:35:49,927 --> 01:35:52,927
SPEAKER_0:  Tax policy, like why are we being emotional about this?

01:35:52,927 --> 01:35:55,966
SPEAKER_1:  I think we're actually that, I mean, yeah, we're emotional about-

01:35:56,738 --> 01:35:57,598
SPEAKER_1:  something else.

01:35:57,890 --> 01:35:59,550
SPEAKER_0:  Yeah, I think what we're emotional about is.

01:36:00,418 --> 01:36:04,030
SPEAKER_0:  This my side, the side I've identified with is in power.

01:36:04,290 --> 01:36:05,662
SPEAKER_0:  and making the decisions?

01:36:05,986 --> 01:36:07,614
SPEAKER_0:  and your side is out of power.

01:36:07,938 --> 01:36:13,022
SPEAKER_0:  And if your side's in power, that's really scary for me because that goes back to the idea of...

01:36:13,410 --> 01:36:18,846
SPEAKER_0:  Who is making, who's pulling the strings in this tribe? Right, who's the chief? Is it your families?

01:36:19,202 --> 01:36:24,158
SPEAKER_0:  Patriarch or is it mine? You know, you might not have food if we don't win this, you know.

01:36:24,578 --> 01:36:27,006
SPEAKER_0:  kind of whatever, you know, chief election. So.

01:36:27,682 --> 01:36:35,070
SPEAKER_0:  I think that it's not about the tax policy or anything like that. And then it gets tied to this broader, I think a lot of our tribalism has.

01:36:35,842 --> 01:36:43,326
SPEAKER_0:  really coalesced around this. We don't have that much religious tribalism in the US, right? The Protestants and the Catholics hate each other. We don't have that, really.

01:36:43,586 --> 01:36:48,190
SPEAKER_0:  And honestly, you say people like to say we have racial tribalism and everything, but

01:36:49,410 --> 01:36:53,630
SPEAKER_0:  a white, even kind of a racist white conservative guy.

01:36:54,402 --> 01:36:58,590
SPEAKER_0:  I think takes the black conservative over the woke white person.

01:36:58,818 --> 01:37:04,766
SPEAKER_0:  any day of the week right now. So that's the strongest source of division. It tells me that I think politics is way stronger tribalism right now.

01:37:05,314 --> 01:37:07,678
SPEAKER_0:  I think that that white racist guy, you know.

01:37:08,226 --> 01:37:09,054
SPEAKER_0:  loves the black.

01:37:09,762 --> 01:37:10,846
SPEAKER_0:  conservative guy.

01:37:11,138 --> 01:37:17,726
SPEAKER_0:  compared to the white woke guy, right? There's no, so to me, again, not that racial tribalism isn't a thing, of course it's always a thing, but like.

01:37:18,210 --> 01:37:19,518
SPEAKER_0:  Political tribalism is.

01:37:19,970 --> 01:37:21,342
SPEAKER_0:  the number one right now.

01:37:21,506 --> 01:37:26,381
SPEAKER_1:  So race is almost a topic for the political division versus the actual element.

01:37:26,381 --> 01:37:28,350
SPEAKER_0:  It's a political football.

01:37:28,770 --> 01:37:29,726
SPEAKER_0:  It's, yeah.

01:37:29,954 --> 01:37:34,046
SPEAKER_1:  So there's a, I mean, this is dark because, so this is.

01:37:34,466 --> 01:37:35,262
SPEAKER_1:  a book about.

01:37:35,490 --> 01:37:37,470
SPEAKER_1:  Human civilization. This is a book about

01:37:38,018 --> 01:37:41,374
SPEAKER_1:  human nature but it's also a book of politics, about politics.

01:37:43,042 --> 01:37:43,646
SPEAKER_1:  Um...

01:37:44,066 --> 01:37:44,638
SPEAKER_1:  It is.

01:37:45,154 --> 01:37:47,486
SPEAKER_1:  Just the way you listed out in the book.

01:37:48,674 --> 01:37:51,294
SPEAKER_1:  It's kind of dark how we just fall into these.

01:37:52,002 --> 01:37:53,598
SPEAKER_1:  left and right checklist.

01:37:54,626 --> 01:37:55,614
SPEAKER_1:  So if you're on the left.

01:37:56,066 --> 01:37:58,430
SPEAKER_1:  It's maintained, roly-wayed.

01:37:59,682 --> 01:38:05,214
SPEAKER_1:  Universal health care good, mainstream media fine, guns kill people, US is a racist country.

01:38:05,506 --> 01:38:10,782
SPEAKER_1:  Protect immigrants, tax cuts bad, climate change awful, raise minimum wage. And on the right is the flip of that.

01:38:11,010 --> 01:38:12,414
SPEAKER_1:  Reverse-row-we-wade.

01:38:12,674 --> 01:38:15,486
SPEAKER_1:  You notice the healthcare bad, mainstream media bad.

01:38:15,938 --> 01:38:17,822
SPEAKER_1:  People kill people, not guns kill people.

01:38:18,114 --> 01:38:22,814
SPEAKER_1:  US was a racist country, protect borders, tax cuts good, climate change overblown.

01:38:23,042 --> 01:38:24,702
SPEAKER_1:  Don't raise minimum wage.

01:38:24,930 --> 01:38:25,886
SPEAKER_1:  I mean, it has...

01:38:26,242 --> 01:38:28,867
SPEAKER_1:  You almost don't have to think about any of this. But, but

01:38:28,867 --> 01:38:32,094
SPEAKER_0:  So when you say it's a book about politics, it's interesting because it

01:38:32,386 --> 01:38:39,934
SPEAKER_0:  It's a book about the vertical axis. It's specifically not a book about the horizontal axis in that I don't actually talk about any of these issues.

01:38:40,386 --> 01:38:42,206
SPEAKER_0:  I don't put out an opinion on them.

01:38:42,466 --> 01:38:44,606
SPEAKER_0:  Those are all horizontal, right?

01:38:45,346 --> 01:38:51,070
SPEAKER_0:  But when you, so rather than, you know, having another book about those issues, about right versus left,

01:38:52,194 --> 01:38:55,870
SPEAKER_0:  I wanted to do a book about this other axis, and so on this axis.

01:38:57,506 --> 01:39:02,622
SPEAKER_0:  The reason I have this checklist is that this is a low, part of the low rung politics world.

01:39:03,362 --> 01:39:06,014
SPEAKER_0:  Right? Low-Rong Politics is a checklist.

01:39:06,530 --> 01:39:21,982
SPEAKER_0:  And that checklist evolves, right? Like Russia suddenly is like popular with the right as opposed to, you know, it used to be in the 60s, the left was the one defending Stalin. Like, so they'll switch. It doesn't even matter. The substance doesn't matter. It's that this is the approved checklist of the capital P party. And this is what everyone believes.

01:39:22,210 --> 01:39:23,326
SPEAKER_0:  That's a low rung thing.

01:39:23,650 --> 01:39:24,574
SPEAKER_0:  The high rungs?

01:39:25,058 --> 01:39:25,726
SPEAKER_0:  This is not-

01:39:26,146 --> 01:39:26,782
SPEAKER_0:  what it's like.

01:39:27,170 --> 01:39:34,142
SPEAKER_0:  Tyrone politics, you tell me your one view on this, I have no idea what you think about anything else, right? And you're going to say, I don't know.

01:39:34,562 --> 01:39:39,998
SPEAKER_0:  about a lot of stuff because inherently you're not gonna have that strong an opinion because you don't have that much info. These are complex things.

01:39:40,834 --> 01:39:43,198
SPEAKER_0:  So there's a lot of I don't know, and people are all over the place.

01:39:43,874 --> 01:39:49,598
SPEAKER_0:  It's the, when it's, you know you're in, you know you're talking to someone who has been subsumed with low-rung politics.

01:39:49,922 --> 01:39:50,430
SPEAKER_0:  when

01:39:51,170 --> 01:39:56,318
SPEAKER_0:  If they tell you their opinion on any one of these issues, you know you could just rattle off their opinion on every single other.

01:39:56,866 --> 01:40:01,182
SPEAKER_0:  And if in three years it becomes fashionable to have this new view, they're gonna have.

01:40:01,602 --> 01:40:02,174
SPEAKER_0:  That's.

01:40:02,530 --> 01:40:04,798
SPEAKER_0:  You're not thinking, that's echo chamber culture.

01:40:05,186 --> 01:40:08,510
SPEAKER_1:  And I've been using kind of a shorthand of centrist.

01:40:09,122 --> 01:40:10,494
SPEAKER_1:  describe this kind of...

01:40:11,234 --> 01:40:12,382
SPEAKER_1:  a high wrong thinking.

01:40:13,282 --> 01:40:18,814
SPEAKER_1:  People tend to, I mean, it seems to be difficult to be a centrist or whatever, a high-ranked thinker. It's like...

01:40:19,202 --> 01:40:24,286
SPEAKER_1:  people want to label you as a person who's too cowardly to take stands. yeah

01:40:24,610 --> 01:40:24,926
SPEAKER_1:  Um.

01:40:25,346 --> 01:40:28,721
SPEAKER_1:  Somehow I supposed to ask saying I don't know is a first

01:40:28,721 --> 01:40:43,838
SPEAKER_0:  Well, the problem with centrist is that would mean that in each of these, tax cuts bag, tax cuts good. It means that you are saying, I am in that I think we should have some tax cuts, but not that many. You might not think that you might actually come do some research and say, actually, I think tax cuts are really important.

01:40:45,538 --> 01:40:53,022
SPEAKER_0:  That doesn't mean, oh, I'm not a centrist anymore. I guess I'm a far, you know, no, no, no. That's why we need the second axis. So what you're trying to be when you say centrist is higher.

01:40:53,314 --> 01:40:55,294
SPEAKER_0:  Which means you might be all over the place horizontally.

01:40:55,618 --> 01:41:00,286
SPEAKER_0:  You might agree with the far left on this thing, the far right on this thing, you might agree with the centrists on this thing.

01:41:00,994 --> 01:41:02,974
SPEAKER_0:  But calling yourself a centrist actually like...

01:41:03,842 --> 01:41:04,478
SPEAKER_0:  Putting yourself in a.

01:41:05,090 --> 01:41:06,878
SPEAKER_0:  prison on the horizontal axis.

01:41:07,202 --> 01:41:13,598
SPEAKER_0:  And it's saying that, you know, whatever, on the different topics, I'm right in between the two policy-wise. That's not where you are.

01:41:14,050 --> 01:41:17,118
SPEAKER_0:  So yeah, that's what we, we're badly missing this other axis.

01:41:17,346 --> 01:41:20,222
SPEAKER_1:  Yeah, I mean, I still do think it's a s-

01:41:20,994 --> 01:41:21,342
SPEAKER_1:  The

01:41:22,018 --> 01:41:25,502
SPEAKER_1:  For me, I am a centrist when you project it down to the horizontal.

01:41:26,114 --> 01:41:28,414
SPEAKER_1:  But the point is you're missing so much data.

01:41:28,706 --> 01:41:30,398
SPEAKER_1:  by not considering the vertical.

01:41:30,626 --> 01:41:33,854
SPEAKER_1:  because on average, maybe it falls somewhere in the middle.

01:41:34,338 --> 01:41:41,502
SPEAKER_1:  but in reality there's just a lot of nuance issue to issue that involves just thinking and uncertainty and changing the ______-

01:41:42,050 --> 01:41:43,486
SPEAKER_1:  given the context of the current.

01:41:43,810 --> 01:41:48,446
SPEAKER_1:  geopolitics and economics is just always considering, always questioning.

01:41:48,866 --> 01:41:49,991
SPEAKER_1:  always evolving your views.

01:41:49,991 --> 01:41:51,838
SPEAKER_0:  Not just about like...

01:41:52,194 --> 01:41:55,294
SPEAKER_0:  Oh, I think we should be in the center on this. But another way to be in the center is.

01:41:55,650 --> 01:41:59,550
SPEAKER_0:  If there's some phenomenon happening, there's always a terrorist attack.

01:42:00,130 --> 01:42:03,358
SPEAKER_0:  You know, and one side wants to say this has nothing to do with Islam.

01:42:03,650 --> 01:42:06,078
SPEAKER_0:  And the other one, the other side wants to say this is radical Islam.

01:42:08,322 --> 01:42:15,358
SPEAKER_0:  What's in between those is saying this is complicated and nuanced and we have to learn more and it probably has something to do with Islam and something to do with...

01:42:15,586 --> 01:42:18,974
SPEAKER_0:  the economic circumstances and something to do with geopolitics.

01:42:19,586 --> 01:42:20,030
SPEAKER_0:  So.

01:42:20,546 --> 01:42:28,702
SPEAKER_0:  In a case like that, you actually do get really unnuanced when you go to the extremes and all of that nuance, which is where all the truth usually is, is going to be in the middle. So,

01:42:29,314 --> 01:42:29,886
SPEAKER_1:  But there is a-

01:42:30,434 --> 01:42:32,990
SPEAKER_1:  to the fact that if you take that nuance on those issues.

01:42:33,378 --> 01:42:34,270
SPEAKER_1:  Like, why in Ukraine?

01:42:34,978 --> 01:42:35,518
SPEAKER_1:  COVID.

01:42:36,194 --> 01:42:37,319
SPEAKER_1:  You're going to be attacked by bull-

01:42:37,319 --> 01:42:37,950
SPEAKER_0:  Yes.

01:42:38,242 --> 01:42:41,566
SPEAKER_0:  people who are really strongly on one side or the other.

01:42:41,826 --> 01:42:42,366
SPEAKER_0:  Heat.

01:42:43,010 --> 01:42:54,846
SPEAKER_0:  centrist people. I've gotten this myself and you know, the slur that I've had thrown at me is I'm an enlightened centrist in a very mocking way. So what are they actually saying? What does enlightened centrist mean? It means someone who is.

01:42:55,330 --> 01:42:58,910
SPEAKER_0:  Steven Pinker or Jonathan Haidt gets accused of is, you know, that they're...

01:42:59,778 --> 01:43:04,094
SPEAKER_0:  highfalutin intellectual world, and they don't actually have any.

01:43:04,450 --> 01:43:08,382
SPEAKER_0:  They don't actually take a side. They don't actually get their hands dirty and they can be

01:43:08,770 --> 01:43:14,526
SPEAKER_0:  superior to both sides without actually taking a stand, right? So I see the argument and I disagree with it because...

01:43:15,202 --> 01:43:16,958
SPEAKER_0:  I firmly believe that the

01:43:17,218 --> 01:43:18,686
SPEAKER_0:  Hardcore tribes?

01:43:19,266 --> 01:43:27,646
SPEAKER_0:  they think they're taking a stand and they're out in the streets and they're pushing for something, I think what they're doing is they're just driving the whole country downwards. And I think they're hurting all the causes they care about.

01:43:28,162 --> 01:43:33,630
SPEAKER_0:  And so it's not that we need everyone to be sitting there, refusing to take a side, it's that.

01:43:34,018 --> 01:43:37,767
SPEAKER_0:  You can be far left and far right, but be upper left and upper right. If we talk.

01:43:37,767 --> 01:43:40,894
SPEAKER_1:  about the use the word liberal a lot in the book.

01:43:41,154 --> 01:43:42,014
SPEAKER_1:  means something.

01:43:42,658 --> 01:43:43,006
SPEAKER_1:  that we'd.

01:43:43,458 --> 01:43:45,406
SPEAKER_1:  don't in modern political discourse mean.

01:43:45,922 --> 01:43:50,494
SPEAKER_1:  uh... since this high philosophical view and then you use the words progressive to mean the left

01:43:51,170 --> 01:43:56,094
SPEAKER_1:  and conservative to mean the right. Can you describe the concept of liberal games?

01:43:56,546 --> 01:43:57,502
SPEAKER_1:  and power games.

01:43:58,242 --> 01:44:00,318
SPEAKER_0:  So the power games is.

01:44:00,866 --> 01:44:04,446
SPEAKER_0:  is what I call the like, basically just the laws of nature.

01:44:05,218 --> 01:44:05,982
SPEAKER_0:  as the

01:44:06,850 --> 01:44:08,862
SPEAKER_0:  When laws of nature are the laws of the land.

01:44:09,410 --> 01:44:11,006
SPEAKER_0:  That's the power game, so animals.

01:44:11,298 --> 01:44:12,862
SPEAKER_0:  watch any David Attenborough special.

01:44:13,474 --> 01:44:16,894
SPEAKER_0:  and when the little lizard is running away from the...

01:44:17,410 --> 01:44:18,846
SPEAKER_0:  you know, the bigger animal or whatever.

01:44:19,522 --> 01:44:26,654
SPEAKER_0:  And I use an example of a bunny and a bear. I don't even know if bears eat bunnies. They probably don't, but pretend bears eat bunnies. So it's like in the power games, the bears...

01:44:27,074 --> 01:44:27,678
SPEAKER_0:  Chasing the Bunny.

01:44:28,674 --> 01:44:34,622
SPEAKER_0:  There's no fairness. There's no, okay, well, what's right? But you know, what's legal? No, no, no, if the bear is fast enough.

01:44:35,362 --> 01:44:36,382
SPEAKER_0:  it can eat the bunny.

01:44:36,674 --> 01:44:41,822
SPEAKER_0:  If the bunny can get away, it can stay living in mid. So that's it, that's the only rule. Now, humans-

01:44:43,042 --> 01:44:47,550
SPEAKER_0:  have spent a lot of time in essentially that environment. So when you have a totalitarian dictatorship.

01:44:48,546 --> 01:44:54,174
SPEAKER_0:  It's, and so what's the rule of the power games? It's everyone can do whatever they want if they have the power to do so. It's just a game of power.

01:44:54,658 --> 01:44:58,206
SPEAKER_0:  So if the bunny gets away, the bunny actually has more power than the bear in that situation.

01:44:58,626 --> 01:45:01,278
SPEAKER_0:  Right? That's a totalitarian dictatorship.

01:45:01,602 --> 01:45:06,462
SPEAKER_0:  There's no rules a dictator can do whatever they want. They can torture, they can-

01:45:07,074 --> 01:45:09,182
SPEAKER_0:  You know, flatten a rebellion with a lot of murder.

01:45:09,602 --> 01:45:12,798
SPEAKER_0:  because they have the power to do so. What are you gonna do, right? And that's-

01:45:13,090 --> 01:45:18,046
SPEAKER_0:  That's kind of the state of nature. That's our natural way. You know, when you look at a mafia, watch a mafia movie.

01:45:18,626 --> 01:45:19,038
SPEAKER_0:  You know.

01:45:19,458 --> 01:45:22,270
SPEAKER_0:  There's what we do a lot of, we have it in us. We all have-

01:45:22,786 --> 01:45:24,958
SPEAKER_0:  we all can snap into Power Games mode.

01:45:25,346 --> 01:45:27,998
SPEAKER_0:  when it becomes all about.

01:45:28,418 --> 01:45:28,830
SPEAKER_0:  You know?

01:45:29,282 --> 01:45:32,990
SPEAKER_0:  just actual raw power. Now, the liberal games is-

01:45:33,218 --> 01:45:39,870
SPEAKER_0:  is something that civilizations for thousands of years have been working on. It's not invented by America or modern times, but.

01:45:40,162 --> 01:45:44,254
SPEAKER_0:  America's kind of was like the latest crack at it yet, which is this idea.

01:45:44,642 --> 01:45:46,334
SPEAKER_0:  instead of everyone can do what they want.

01:45:46,626 --> 01:45:50,782
SPEAKER_0:  if they have the power to do so. Everyone can do what they want, as long as it doesn't harm anyone else.

01:45:51,106 --> 01:45:55,006
SPEAKER_0:  Now that's really complicated, how do you define harm? And the idea is that everyone has.

01:45:55,298 --> 01:45:58,110
SPEAKER_0:  There are a list of rights which are protected by the government.

01:45:58,882 --> 01:46:04,158
SPEAKER_0:  and then they have, they're inalienable rights and they're protected, you know, those are protected.

01:46:04,642 --> 01:46:06,302
SPEAKER_0:  again by, you know.

01:46:06,626 --> 01:46:08,286
SPEAKER_0:  from an invasion by other people.

01:46:08,546 --> 01:46:10,526
SPEAKER_0:  And so you have this kind of fragile balance.

01:46:10,850 --> 01:46:16,126
SPEAKER_0:  And so the idea with the liberal games is that there are laws, but it's not totalitarian.

01:46:16,482 --> 01:46:17,726
SPEAKER_0:  They will build very-

01:46:18,274 --> 01:46:22,846
SPEAKER_0:  strict laws kind of around the edges of what you can and can't do and then everything else

01:46:23,234 --> 01:46:23,550
SPEAKER_0:  Freedom.

01:46:24,290 --> 01:46:27,806
SPEAKER_0:  So unlike a totalitarian dictatorship, actually it's very loose.

01:46:28,066 --> 01:46:30,782
SPEAKER_0:  There's a lot of things can happen and it's kind of up to the people.

01:46:31,266 --> 01:46:38,014
SPEAKER_0:  but there are still laws that protect the very basic and alienable rights and stuff like that. So it's this much looser thing. Now, the vulnerability there.

01:46:38,306 --> 01:46:38,814
SPEAKER_0:  is that.

01:46:39,554 --> 01:46:40,830
SPEAKER_0:  So, so, so the...

01:46:42,210 --> 01:46:48,926
SPEAKER_0:  the benefits of it are obvious, right? Freedom is great. It seems like it's the most fair, that equality of opportunity.

01:46:49,186 --> 01:46:50,654
SPEAKER_0:  seem like the most fair.

01:46:51,042 --> 01:46:52,030
SPEAKER_0:  thing and.

01:46:52,418 --> 01:46:52,990
SPEAKER_0:  Um...

01:46:53,506 --> 01:46:55,486
SPEAKER_0:  and equality before the law.

01:46:55,810 --> 01:46:58,238
SPEAKER_0:  you know, due process and all of this stuff. So it seems fair.

01:46:58,754 --> 01:47:04,254
SPEAKER_0:  to the founders of the US and other enlightenment thinkers, and it also is a great way to manifest productivity.

01:47:04,738 --> 01:47:05,950
SPEAKER_0:  Right, you know, you have...

01:47:06,370 --> 01:47:06,942
SPEAKER_0:  you have.

01:47:07,170 --> 01:47:14,654
SPEAKER_0:  Adam Smith saying, it's not from the benevolence of the butcher or the baker that we get our dinner, but from their own self-interest. So you can harness kind of selfishness.

01:47:14,882 --> 01:47:15,774
SPEAKER_0:  for progress.

01:47:16,098 --> 01:47:16,542
SPEAKER_0:  But.

01:47:16,930 --> 01:47:22,206
SPEAKER_0:  It has a vulnerability which is that because of the laws, it's like the totalitarian laws, they don't.

01:47:22,658 --> 01:47:23,070
SPEAKER_0:  Have.

01:47:23,554 --> 01:47:25,086
SPEAKER_0:  and excessive laws for no reason.

01:47:25,474 --> 01:47:29,566
SPEAKER_0:  they want to control everything. And the US, you know, in the US we say, they're not going to do that. So.

01:47:30,370 --> 01:47:32,830
SPEAKER_0:  The second, it's almost two puzzle pieces. Do you have the laws?

01:47:33,058 --> 01:47:33,822
SPEAKER_0:  and then you've got.

01:47:34,370 --> 01:47:37,726
SPEAKER_0:  liberal culture. Liberal laws have to be married to liberal culture.

01:47:38,114 --> 01:47:40,414
SPEAKER_0:  kind of a defense of liberal spirit.

01:47:40,802 --> 01:47:43,262
SPEAKER_0:  in order to truly have the liberal games going.

01:47:44,066 --> 01:47:45,566
SPEAKER_0:  And so that's vulnerable because.

01:47:45,922 --> 01:47:47,326
SPEAKER_0:  free speech you can have.

01:47:47,842 --> 01:47:49,854
SPEAKER_0:  The First Amendment, that's the laws part.

01:47:50,114 --> 01:47:52,798
SPEAKER_0:  But if you're in a culture where anyone who...

01:47:53,570 --> 01:47:54,046
SPEAKER_0:  you know.

01:47:54,466 --> 01:47:58,590
SPEAKER_0:  speaks out against orthodoxy is going to be shunned from the community.

01:47:59,042 --> 01:48:03,390
SPEAKER_0:  Well, you're lacking the second piece of the puzzle there. You're lacking liberal culture. And so therefore,

01:48:03,810 --> 01:48:07,262
SPEAKER_0:  you might as well not even have the first amendment.

01:48:08,066 --> 01:48:13,278
SPEAKER_0:  And there's a lot of examples like that where the culture has to do its part for the true liberal games to be enjoyed.

01:48:14,402 --> 01:48:16,446
SPEAKER_0:  So it's just much more complicated, much more nuanced.

01:48:16,706 --> 01:48:18,302
SPEAKER_0:  than the power games. It's kind of-

01:48:18,690 --> 01:48:19,934
SPEAKER_0:  It's kind of a set of...

01:48:20,418 --> 01:48:21,374
SPEAKER_0:  Basic laws.

01:48:21,666 --> 01:48:22,526
SPEAKER_0:  that then.

01:48:23,330 --> 01:48:26,334
SPEAKER_0:  are coupled with a basic spirit to create this.

01:48:27,842 --> 01:48:29,086
SPEAKER_0:  very awesome.

01:48:29,378 --> 01:48:31,582
SPEAKER_0:  in a human environment that's also very vulnerable.

01:48:32,418 --> 01:48:36,862
SPEAKER_1:  So what do you mean the culture has to play along? So for something like a freedom of speech to

01:48:37,250 --> 01:48:37,662
SPEAKER_1:  work.

01:48:38,658 --> 01:48:40,862
SPEAKER_1:  There has to be a basic, what, decency.

01:48:41,218 --> 01:48:43,326
SPEAKER_1:  that if all people are perfectly good...

01:48:43,874 --> 01:48:47,006
SPEAKER_1:  then perfect freedom without any restrictions is great.

01:48:47,586 --> 01:48:50,398
SPEAKER_1:  It's where the human nature starts getting a little iffy.

01:48:50,722 --> 01:48:52,702
SPEAKER_1:  We start being cruel to each other. We start being-

01:48:53,186 --> 01:48:54,878
SPEAKER_1:  greedy and

01:48:55,330 --> 01:48:57,310
SPEAKER_1:  desiring of harm and also them.

01:48:57,666 --> 01:49:04,638
SPEAKER_1:  narcissists and sociopaths and psychopaths in society. All of that, that's when you start to have to inject some limitations on that freedom.

01:49:05,442 --> 01:49:06,718
SPEAKER_0:  Yeah, I mean, if, if, um...

01:49:08,162 --> 01:49:12,638
SPEAKER_0:  So what the government basically says is, we're going to let everyone be mostly free.

01:49:13,794 --> 01:49:16,094
SPEAKER_0:  But no one is going to be free to

01:49:17,026 --> 01:49:19,646
SPEAKER_0:  physically harm other people or to steal their property.

01:49:19,874 --> 01:49:20,222
SPEAKER_0:  right?

01:49:20,770 --> 01:49:21,278
SPEAKER_0:  Um...

01:49:21,794 --> 01:49:26,622
SPEAKER_0:  And so we're all agreeing to sacrifice that 20% of our freedom.

01:49:26,850 --> 01:49:29,566
SPEAKER_0:  and then in return, all of us in theory.

01:49:30,146 --> 01:49:33,182
SPEAKER_0:  can be 80% free. And that's kind of the bargain.

01:49:34,114 --> 01:49:35,934
SPEAKER_0:  But now that's a lot of freedom.

01:49:36,194 --> 01:49:37,566
SPEAKER_0:  to leave people with and.

01:49:38,466 --> 01:49:44,638
SPEAKER_0:  A lot of people choose, it's like you're so free in the US, you're actually free to be unfree if you choose. That's kind of what an echo chamber is to me.

01:49:44,866 --> 01:49:45,310
SPEAKER_0:  It's.

01:49:45,666 --> 01:49:46,942
SPEAKER_0:  You know, um.

01:49:47,170 --> 01:49:48,798
SPEAKER_0:  You can choose to kind of.

01:49:49,058 --> 01:49:50,494
SPEAKER_0:  Be friends with people who...

01:49:52,482 --> 01:49:53,534
SPEAKER_0:  uh... essentially

01:49:53,762 --> 01:49:56,158
SPEAKER_0:  make it so uncomfortable.

01:49:56,418 --> 01:49:56,862
SPEAKER_0:  too.

01:49:57,698 --> 01:49:58,558
SPEAKER_0:  Speak your mind.

01:49:59,298 --> 01:50:00,510
SPEAKER_0:  that it's no.

01:50:00,738 --> 01:50:02,654
SPEAKER_0:  actual effective difference for you.

01:50:02,914 --> 01:50:06,238
SPEAKER_0:  than if you lived in a country. If you can't, you know, criticize.

01:50:07,330 --> 01:50:08,926
SPEAKER_0:  Christianity in a certain community.

01:50:10,402 --> 01:50:14,526
SPEAKER_0:  that you have a First Amendment, so you're not gonna get arrested by the government for criticizing Christianity.

01:50:16,290 --> 01:50:20,094
SPEAKER_0:  But if you have this, if the social penalties are so extreme.

01:50:20,674 --> 01:50:24,414
SPEAKER_0:  that it's just never worth it. You might as well be in a country.

01:50:24,802 --> 01:50:25,534
SPEAKER_0:  That's it.

01:50:26,050 --> 01:50:28,478
SPEAKER_0:  imprisons people for criticizing Christianity.

01:50:29,186 --> 01:50:37,470
SPEAKER_0:  And so that same thing goes for wokeness, right? This is what people get, you know, cancel culture and stuff. So when the reason these things are bad is because they're actually-

01:50:38,434 --> 01:50:39,550
SPEAKER_0:  They're depriving.

01:50:40,258 --> 01:50:41,118
SPEAKER_0:  Americans.

01:50:41,442 --> 01:50:51,550
SPEAKER_0:  of the beauty of the freedom of the liberal games by, you know, imposing a social culture that is very power games. It's basically a power games culture comes in.

01:50:51,778 --> 01:50:53,566
SPEAKER_0:  and you might as well be in the power games now.

01:50:54,082 --> 01:50:56,670
SPEAKER_0:  And so liberal, if you live in a liberal democracy.

01:50:57,314 --> 01:51:00,638
SPEAKER_0:  it's it's you there will be always be challenges.

01:51:00,930 --> 01:51:02,174
SPEAKER_0:  to a liberal culture.

01:51:03,714 --> 01:51:05,374
SPEAKER_0:  Lowercase L.

01:51:05,666 --> 01:51:08,574
SPEAKER_0:  liberal. There'll always be challenges to a liberal culture.

01:51:08,994 --> 01:51:11,198
SPEAKER_0:  from people who are much more interested in playing the power games.

01:51:11,618 --> 01:51:18,622
SPEAKER_0:  And there has to be kind of an immune system that stands up to that culture and says, that's not how we do things here in America actually.

01:51:19,138 --> 01:51:27,550
SPEAKER_0:  don't excommunicate people for not having the right religious beliefs or not, you know, we don't disinvite a speaker from campus for having the wrong political beliefs.

01:51:27,810 --> 01:51:33,054
SPEAKER_0:  And if it doesn't stand up for itself, it's like the immune system of the country failing.

01:51:33,378 --> 01:51:34,110
SPEAKER_0:  And.

01:51:34,850 --> 01:51:35,902
SPEAKER_0:  Power Games rushes in.

01:51:37,090 --> 01:51:37,470
SPEAKER_1:  Huh.

01:51:37,890 --> 01:51:40,030
SPEAKER_1:  So before chapter four in your book.

01:51:41,186 --> 01:51:41,918
SPEAKER_1:  Uh...

01:51:42,690 --> 01:51:46,494
SPEAKER_1:  and the chapters that will surely result in you being burned at the stake.

01:51:46,786 --> 01:51:48,286
SPEAKER_1:  You write quote, we'll start.

01:51:48,546 --> 01:51:55,966
SPEAKER_1:  our Pitchfork Tour in this chapter by taking a brief trip through the history of the Republican Party. In the following chapters we'll take a...

01:51:56,386 --> 01:52:03,134
SPEAKER_1:  Tim's career tanking deep dive into America's social justice movement as you started to talk about.

01:52:03,618 --> 01:52:07,518
SPEAKER_1:  Okay, so let's go. What's the history of the Republican Party?

01:52:08,578 --> 01:52:09,886
SPEAKER_0:  I'm looking at this through my vertical left.

01:52:10,146 --> 01:52:10,750
SPEAKER_0:  saying what is.

01:52:11,266 --> 01:52:12,766
SPEAKER_0:  this familiar story.

01:52:13,314 --> 01:52:13,790
SPEAKER_0:  of.

01:52:14,082 --> 01:52:16,286
SPEAKER_0:  the Republicans from the 60s to today.

01:52:16,642 --> 01:52:17,758
SPEAKER_0:  What does it look like?

01:52:17,986 --> 01:52:19,006
SPEAKER_0:  through the vertical lens.

01:52:19,266 --> 01:52:21,246
SPEAKER_0:  Right? Does it look different? And show it off on the fly for a few minutes. Right?

01:52:21,666 --> 01:52:25,662
SPEAKER_0:  And is there an interesting story here that's been kind of hidden because we're always looking at the horizontal.

01:52:25,954 --> 01:52:32,062
SPEAKER_0:  Now the horizontal story, you'll hear people talk about it and they'll say something like, the Republicans have moved farther and farther to the right.

01:52:35,330 --> 01:52:38,494
SPEAKER_0:  And to me, that's not really true.

01:52:38,914 --> 01:52:39,998
SPEAKER_0:  Like, it was Trump.

01:52:40,226 --> 01:52:41,854
SPEAKER_0:  More right-wing than Reagan?

01:52:42,114 --> 01:52:44,926
SPEAKER_0:  I don't think so. I think he's left less. In terms of actual policy, yeah.

01:52:45,250 --> 01:52:50,590
SPEAKER_0:  So we're using this, again, it's just like you're calling yourself centrist when it's not exactly what you mean, even though it also is.

01:52:52,322 --> 01:52:58,270
SPEAKER_0:  So again, I was like, okay, look, this vertical lens helps with other things. Let's apply it to the Republicans. Here's what I saw is.

01:52:59,362 --> 01:53:00,318
SPEAKER_0:  I looked at the 60s.

01:53:01,122 --> 01:53:01,822
SPEAKER_0:  and

01:53:02,658 --> 01:53:05,310
SPEAKER_0:  I saw an interesting story which I don't think that, you know...

01:53:05,826 --> 01:53:07,006
SPEAKER_0:  Not everyone's familiar with like.

01:53:07,426 --> 01:53:08,990
SPEAKER_0:  what happened in the early 60s, but.

01:53:09,282 --> 01:53:18,686
SPEAKER_0:  In 1960, the Republican Party was very, it was a plurality. You had progressives, like genuine Rockefeller, pretty progressive people.

01:53:19,458 --> 01:53:24,254
SPEAKER_0:  all the way to, you know, then you had the moderates like Eisenhower and Dewey.

01:53:24,546 --> 01:53:28,574
SPEAKER_0:  And then you go all the way to the farther right, you had.

01:53:28,930 --> 01:53:33,086
SPEAKER_0:  Goldwater and what you might call, I call them the fundamentalists.

01:53:34,530 --> 01:53:35,134
SPEAKER_0:  and

01:53:35,746 --> 01:53:36,286
SPEAKER_0:  So.

01:53:37,122 --> 01:53:39,838
SPEAKER_0:  It's this interesting plurality, right? Something we don't have today.

01:53:40,290 --> 01:53:43,422
SPEAKER_0:  And what happened was the Goldwater contingent.

01:53:44,002 --> 01:53:44,862
SPEAKER_0:  which was the.

01:53:45,602 --> 01:53:46,046
SPEAKER_0:  underdog.

01:53:46,274 --> 01:53:48,894
SPEAKER_0:  They were small, right? Eisenhower was the president.

01:53:49,570 --> 01:53:58,270
SPEAKER_0:  had just been the president and it seemed like the moderates were, you know, he said you have to be close to the center of the chessboard. That's how you maintain power.

01:53:58,914 --> 01:54:01,758
SPEAKER_0:  These people were very far from the center of the chessboard, but they ended up-

01:54:01,986 --> 01:54:03,582
SPEAKER_0:  basically like a hostile takeover.

01:54:03,938 --> 01:54:05,758
SPEAKER_0:  They conquered their own party.

01:54:06,178 --> 01:54:07,038
SPEAKER_0:  And they did it.

01:54:07,362 --> 01:54:11,134
SPEAKER_0:  by breaking all of the kind of unwritten rules and norms.

01:54:11,970 --> 01:54:19,198
SPEAKER_0:  So they did things like they first started with like the college Republicans, which was like this feeder group that turned in, you know, a lot of the politicians started there.

01:54:19,426 --> 01:54:22,974
SPEAKER_0:  and they went to the election and they wouldn't let the.

01:54:23,330 --> 01:54:26,014
SPEAKER_0:  the current president, the incumbent, speak.

01:54:26,306 --> 01:54:35,838
SPEAKER_0:  and they were throwing chairs and they were fistfights. And eventually people gave up and they just sat there and they sat in the chair talking for, you know, their candidate until everyone eventually left and then they declared victory. So.

01:54:36,546 --> 01:54:44,990
SPEAKER_0:  Basically, they came in, there was a certain set of rules, agreed upon rules, and they came in playing the power games, saying, Well, actually...

01:54:45,698 --> 01:54:50,078
SPEAKER_0:  If we do this, you won't have the power, we have the power to take it.

01:54:50,370 --> 01:54:51,614
SPEAKER_0:  if we just break all the rules.

01:54:52,002 --> 01:54:53,214
SPEAKER_0:  Right? And so they did and they won.

01:54:53,474 --> 01:55:08,254
SPEAKER_0:  And that became this hugely influential thing, which then they conquered California through again, these people were taken aback. These proper Republican candidates were appalled by the kind of like, the insults that were being hurled at them and the intimidation and the bullying. And eventually they ended up-

01:55:08,834 --> 01:55:10,654
SPEAKER_0:  and the National Convention, which was called like.

01:55:10,882 --> 01:55:12,894
SPEAKER_0:  the right wing Woodstock, it was like.

01:55:13,122 --> 01:55:20,158
SPEAKER_0:  You know, the Republican National Convention in 64 was just, again, it was jeering and they wouldn't let their moderates or their progressives even speak.

01:55:20,514 --> 01:55:28,958
SPEAKER_0:  and there was racism, you know, Jackie Robinson was there and he was a proud Republican and he said that like, he feels like he was a Jew in Hitler's Germany with the way that blacks were being treated there.

01:55:29,346 --> 01:55:31,518
SPEAKER_0:  and it was nasty, but what did they do? they had

01:55:31,938 --> 01:55:33,342
SPEAKER_0:  fiery, you know.

01:55:33,602 --> 01:55:36,222
SPEAKER_0:  plurality enough to win and they won.

01:55:36,642 --> 01:55:40,478
SPEAKER_0:  They ended up getting crushed in the general election, and they kind of faded away. But to me, I was like-

01:55:40,834 --> 01:55:43,166
SPEAKER_0:  That was an interesting story. I see it as-

01:55:43,490 --> 01:55:47,294
SPEAKER_0:  I have this character in the book called the Golem, which is a big, kind of a big dumb

01:55:47,970 --> 01:55:54,622
SPEAKER_0:  Powerful monster that's the you know the emergent property of like a political echo chambers like this big giant and stupid But it's powerful and scary.

01:55:55,234 --> 01:55:55,614
SPEAKER_0:  Um.

01:55:56,258 --> 01:55:58,366
SPEAKER_0:  And to me, I was like, a golem rose up.

01:55:58,818 --> 01:56:00,990
SPEAKER_0:  conquered the party for a second, knocked it on its ass.

01:56:01,218 --> 01:56:01,758
SPEAKER_0:  And then.

01:56:02,626 --> 01:56:03,134
SPEAKER_0:  And then...

01:56:04,034 --> 01:56:04,606
SPEAKER_0:  Faded away.

01:56:05,122 --> 01:56:11,838
SPEAKER_0:  And to me, when I look at the Trump revolution in a lot, and not just Trump, the last 20 years, I see that same lower right.

01:56:12,386 --> 01:56:13,726
SPEAKER_0:  that lower right monster.

01:56:14,178 --> 01:56:14,782
SPEAKER_0:  Kind of.

01:56:15,042 --> 01:56:15,582
SPEAKER_0:  Um...

01:56:17,314 --> 01:56:22,270
SPEAKER_0:  making another charge for it, but this time succeeding really taking over the party for a long period of time.

01:56:22,658 --> 01:56:23,358
SPEAKER_0:  I see.

01:56:23,618 --> 01:56:26,590
SPEAKER_0:  The same story, which is the power games are being played.

01:56:27,010 --> 01:56:32,830
SPEAKER_0:  in a situation when it had always been, the government relies on all these unwritten rules and norms to function?

01:56:33,186 --> 01:56:38,782
SPEAKER_0:  But for example, you have in 2016, Merrick Garland gets nominated by Obama.

01:56:39,490 --> 01:56:41,982
SPEAKER_0:  and the unwritten norm says that...

01:56:42,722 --> 01:56:46,590
SPEAKER_0:  when the president nominates a justice, then you pass them through unless there's some egregious thing.

01:56:46,818 --> 01:56:52,926
SPEAKER_0:  That's what has happened. But they said, actually, this is the last year of his presidency and the people should choose. I don't think we should set a new precedent where.

01:56:53,346 --> 01:56:58,174
SPEAKER_0:  The president can't nominate people, nominate a Supreme Court justice in the last year.

01:56:58,690 --> 01:57:03,806
SPEAKER_0:  So they pass it through and it ends up being Gorsuch. And so they lose that seat.

01:57:04,098 --> 01:57:05,950
SPEAKER_0:  Now three years later, it's Trump's last year.

01:57:06,434 --> 01:57:07,486
SPEAKER_0:  and it's another election year.

01:57:08,322 --> 01:57:09,246
SPEAKER_0:  and Ginsburg.

01:57:09,538 --> 01:57:10,046
SPEAKER_0:  dies.

01:57:10,274 --> 01:57:16,318
SPEAKER_0:  And what did they say? They say, oh, let's keep our precedent. They said, no, oh, actually we changed our mind. We're going to nominate Amy Barrett.

01:57:17,346 --> 01:57:43,806
SPEAKER_0:  So to me, that is classic power games, right? That there's no actual rule. And what you're doing is they had the, they did technically have the power to block the nomination then. And then they technically had the power to put someone in and they're pretending there's some principle to it, but they're just extra, they're going for a short-term edge at the expense of what is like the workings of the system in the long run. And then one of the Democrats have to do in that situation, cause both parties have been doing this, is they either can lose now all the time or they start playing the power games too.

01:57:44,162 --> 01:57:46,334
SPEAKER_0:  And now you have a prisoner's dilemma where it's like.

01:57:46,658 --> 01:57:50,078
SPEAKER_0:  both end up doing this thing and everyone ends up worse off.

01:57:50,690 --> 01:57:51,518
SPEAKER_0:  the debt ceiling.

01:57:51,778 --> 01:57:56,190
SPEAKER_0:  All these power plays that are being made with these holding the country hostage, this is Power Games.

01:57:56,642 --> 01:58:00,126
SPEAKER_0:  And to me, that's what Goldwater was doing in the 60s, but it was a healthier time in a way.

01:58:00,354 --> 01:58:02,270
SPEAKER_0:  because there was this plurality.

01:58:02,754 --> 01:58:05,406
SPEAKER_0:  within the parties reduced some of the national tribalism.

01:58:05,666 --> 01:58:10,590
SPEAKER_0:  And there wasn't as much of an appeal to that. But today, it's just like, do whatever you have to do to beat the enemies.

01:58:10,914 --> 01:58:13,054
SPEAKER_0:  And so I'm seeing a rise in power games.

01:58:13,506 --> 01:58:19,262
SPEAKER_0:  And I talk about the Republicans because they did a lot of these things first. They have been a little bit more egregious, but both parties have been doing it over the last

01:58:19,522 --> 01:58:20,647
SPEAKER_0:  2030 year.

01:58:20,647 --> 01:58:21,982
SPEAKER_1:  blame

01:58:22,274 --> 01:58:23,870
SPEAKER_1:  Or maybe there's a different term for it.

01:58:25,218 --> 01:58:26,206
SPEAKER_1:  at uh...

01:58:26,690 --> 01:58:33,278
SPEAKER_1:  the subsystems of this. So is it the media, is it the politicians like in the Senate and in Congress?

01:58:33,506 --> 01:58:35,582
SPEAKER_1:  Is it Trump, so the leadership?

01:58:36,066 --> 01:58:37,534
SPEAKER_1:  Is it?

01:58:38,338 --> 01:58:39,422
SPEAKER_1:  Or maybe it's us.

01:58:40,162 --> 01:58:41,022
SPEAKER_1:  human beings.

01:58:41,474 --> 01:58:46,174
SPEAKER_1:  Maybe social media versus mainstream media. Is there a sense of where?

01:58:46,594 --> 01:58:47,719
SPEAKER_1:  What is the cause of what is this?

01:58:47,719 --> 01:58:52,446
SPEAKER_0:  It's very complex. So Ezra Klein's a great book, while we're polarized, where he talks about a lot of this.

01:58:52,770 --> 01:58:54,430
SPEAKER_0:  There's some of these are, you know.

01:58:54,818 --> 01:58:55,582
SPEAKER_0:  It's really no one's fault.

01:58:56,002 --> 01:58:57,342
SPEAKER_0:  First of all, it's the environment.

01:58:57,762 --> 01:58:59,710
SPEAKER_0:  has changed in a bunch of ways you just mentioned?

01:59:00,066 --> 01:59:03,486
SPEAKER_0:  And what happens when you take human nature, which is a constant, and you put it into an environment?

01:59:04,322 --> 01:59:05,022
SPEAKER_0:  behavior comes out.

01:59:05,794 --> 01:59:08,062
SPEAKER_0:  The environment is the independent variable. When that changes...

01:59:08,514 --> 01:59:11,134
SPEAKER_0:  the dependent variable, the behavior, changes with it, right?

01:59:11,490 --> 01:59:14,494
SPEAKER_0:  And so the environment has changed in a lot of ways. So one major one.

01:59:15,266 --> 01:59:15,806
SPEAKER_0:  is.

01:59:16,930 --> 01:59:19,710
SPEAKER_0:  It used to for a long time actually. The death of my very own Jim McADAW thing was, did you know what this credits really looked like?

01:59:20,674 --> 01:59:21,150
SPEAKER_0:  the

01:59:21,410 --> 01:59:30,142
SPEAKER_0:  First it was the Republicans and then the Democrats just had a stranglehold on Congress. There was no, it was not even competitive. The Democrats for 40 years had.

01:59:30,466 --> 01:59:31,038
SPEAKER_0:  the majority.

01:59:31,938 --> 01:59:35,902
SPEAKER_0:  And so therefore, it actually is a decent environment to.

01:59:36,194 --> 01:59:36,798
SPEAKER_0:  compromise it.

01:59:37,154 --> 01:59:41,758
SPEAKER_0:  because now we can both, you know, what you want is Congress people thinking about their home district and-

01:59:41,986 --> 01:59:42,334
SPEAKER_0:  You know.

01:59:42,850 --> 01:59:50,014
SPEAKER_0:  voting yes on a national policy because we're gonna get a good deal on it back at home. That's actually healthy, as opposed to voting in lockstep together.

01:59:50,306 --> 01:59:51,006
SPEAKER_0:  because...

01:59:51,330 --> 01:59:55,358
SPEAKER_0:  This is what the red party is doing, regardless of what's good for my home district.

01:59:55,586 --> 01:59:57,054
SPEAKER_0:  You know, an example is Obamacare.

01:59:57,442 --> 02:00:00,798
SPEAKER_0:  there were certain Republican districts that would have actually officially been.

02:00:01,442 --> 02:00:05,054
SPEAKER_0:  benefited by Obamacare, but every Republican voted against it.

02:00:05,506 --> 02:00:07,358
SPEAKER_0:  And part of the reason is because there's no longer this.

02:00:07,682 --> 02:00:11,166
SPEAKER_0:  obvious majority. Every few years it switches. It's a 50-50 thing.

02:00:11,906 --> 02:00:12,670
SPEAKER_0:  And that's, you know.

02:00:12,994 --> 02:00:14,014
SPEAKER_0:  partially because...

02:00:14,530 --> 02:00:19,326
SPEAKER_0:  It's become so, we've been so subsumed with this one national divide of left versus right that.

02:00:19,682 --> 02:00:20,254
SPEAKER_0:  Um,

02:00:20,482 --> 02:00:21,598
SPEAKER_0:  That, that, that, that, that.

02:00:22,178 --> 02:00:25,182
SPEAKER_0:  People are not, people are whoever, you know, they're voting for the same party for president.

02:00:25,794 --> 02:00:34,686
SPEAKER_0:  all the way down the ticket now. And so you have this just kind of 50-50 color war and that's awful for compromise. So there's like 10 of these things, you know, that have redistricting.

02:00:34,946 --> 02:00:44,094
SPEAKER_0:  But also it is social media. It is, I call it hypercharged tribalism. In the 60s you had kind of distributed tribalism. You had some people that have worked up about the USSR.

02:00:44,898 --> 02:00:46,654
SPEAKER_0:  Right, they're national, that's what they care about.

02:00:47,138 --> 02:00:48,510
SPEAKER_0:  US versus foreign.

02:00:48,802 --> 02:00:53,982
SPEAKER_0:  You have some people that were saying left versus right, like they had today, and then other people that were saying that they were fighting within the party.

02:00:54,786 --> 02:01:05,758
SPEAKER_0:  But today you don't have that. You have ideological realignment, so you kind of got rid of a lot of the in-party fighting. And then there hasn't been that big of a foreign threat. Nothing like the USSR for a long time.

02:01:06,050 --> 02:01:12,670
SPEAKER_0:  So you kind of lost that and what's left is just this left versus right thing. And so that's kind of this hypercharged whirlpool that subsumes everything.

02:01:13,282 --> 02:01:14,270
SPEAKER_0:  And, um...

02:01:14,562 --> 02:01:17,982
SPEAKER_0:  And so, yeah, it's, I mean, people point to Newt Gingrich.

02:01:18,370 --> 02:01:26,078
SPEAKER_0:  And people like there's certain characters that enacted policies that stoked this kind of thing. But I think this is a much bigger kind of environmental shift.

02:01:26,338 --> 02:01:29,950
SPEAKER_1:  Well, that's going back to our questions about the role of individuals in human history.

02:01:30,210 --> 02:01:31,486
SPEAKER_1:  So the interesting.

02:01:31,810 --> 02:01:33,982
SPEAKER_1:  One of the many interesting questions here is about Trump.

02:01:34,466 --> 02:01:36,158
SPEAKER_1:  Is he a symptom or a cause?

02:01:37,026 --> 02:01:40,638
SPEAKER_1:  because he seems to be from the public narrative such a significant.

02:01:41,538 --> 02:01:42,398
SPEAKER_1:  catalysts.

02:01:42,914 --> 02:01:44,039
SPEAKER_1:  for some of the things we're seeing.

02:01:44,039 --> 02:01:47,966
SPEAKER_0:  This goes back to what we were talking about earlier, right? Like, is it the person or is it the times?

02:01:48,386 --> 02:01:50,526
SPEAKER_0:  I think he's a perfect example of it's a both.

02:01:50,786 --> 02:01:55,806
SPEAKER_0:  situation. I don't think that I don't think if you pluck Trump out of this situation I don't think if Trump was inevitable.

02:01:56,386 --> 02:01:58,046
SPEAKER_0:  but I think we were very vulnerable.

02:01:58,274 --> 02:01:59,102
SPEAKER_0:  to a demagogue?

02:01:59,842 --> 02:02:02,142
SPEAKER_0:  And if you hadn't been, Trump would have had no chance.

02:02:02,562 --> 02:02:05,566
SPEAKER_0:  And so why were we vulnerable to a demagogue?

02:02:05,794 --> 02:02:08,798
SPEAKER_0:  is because you have these.

02:02:09,538 --> 02:02:11,934
SPEAKER_0:  I mean, I think it's specifically on the right.

02:02:12,930 --> 02:02:16,030
SPEAKER_0:  If you actually look at the stats, it's pretty bad. Like, the people who-

02:02:16,290 --> 02:02:21,438
SPEAKER_0:  Because it's not just who voted for Trump. A lot of people just vote for the red, right? What's interesting is who voted for Obama.

02:02:21,890 --> 02:02:23,646
SPEAKER_0:  against Romney and then voted for Trump.

02:02:24,514 --> 02:02:26,526
SPEAKER_0:  Who, you know, these are not racists.

02:02:26,754 --> 02:02:30,494
SPEAKER_0:  These are not hardcore Republicans. They voted for Obama.

02:02:31,202 --> 02:02:32,830
SPEAKER_0:  And where did this switch come from?

02:02:33,186 --> 02:02:37,950
SPEAKER_0:  Places that had economic despair, where bridges were not working well. That's a signifier.

02:02:38,242 --> 02:02:40,830
SPEAKER_0:  You know, where paint's chipping in the schools.

02:02:41,154 --> 02:02:44,606
SPEAKER_0:  you know, these little things like this. So I think that, you know, you had this.

02:02:45,378 --> 02:02:48,830
SPEAKER_0:  a lot of these kind of rural towns, you have true despair. And then you also have.

02:02:49,282 --> 02:02:52,702
SPEAKER_0:  The number one indicator of voting for Trump was...

02:02:53,058 --> 02:02:53,950
SPEAKER_0:  distrust in media.

02:02:54,466 --> 02:02:59,582
SPEAKER_0:  and the media has become much less trustworthy.

02:03:00,386 --> 02:03:09,470
SPEAKER_0:  All these ingredients that actually make us very vulnerable to a demagogue and a demagogue is someone who takes advantage, right? There's someone who comes in and says, I can pull the right strings.

02:03:09,730 --> 02:03:14,494
SPEAKER_0:  and push all the right emotional buttons right now and get myself power.

02:03:14,946 --> 02:03:18,302
SPEAKER_0:  by taking advantage of the circumstances. And that is what Trump.

02:03:18,946 --> 02:03:19,518
SPEAKER_0:  Totally dead.

02:03:20,450 --> 02:03:24,446
SPEAKER_1:  It makes me wonder how easy it is for somebody who is a charismatic leader.

02:03:24,770 --> 02:03:26,590
SPEAKER_1:  to capitalize on.

02:03:27,586 --> 02:03:29,022
SPEAKER_1:  cultural resentment when

02:03:29,250 --> 02:03:30,430
SPEAKER_1:  when there's economic hardship.

02:03:31,202 --> 02:03:31,934
SPEAKER_1:  to channel that.

02:03:32,642 --> 02:03:37,918
SPEAKER_0:  So John Haight wrote a great article about like, basically, we like, truth is in an all time low right now.

02:03:38,210 --> 02:03:40,702
SPEAKER_0:  It's the media is not penalized for lying.

02:03:41,698 --> 02:03:42,078
SPEAKER_0:  Right?

02:03:42,594 --> 02:03:46,878
SPEAKER_0:  MSNBC, Fox News, these are not penalized for being inaccurate or penalized if they

02:03:47,138 --> 02:03:48,254
SPEAKER_0:  straight from the orthodoxy.

02:03:49,826 --> 02:03:52,414
SPEAKER_0:  on social media. It's not the truest tweets that go viral.

02:03:52,642 --> 02:03:54,366
SPEAKER_0:  Right? And so Trump-

02:03:56,034 --> 02:04:02,334
SPEAKER_0:  understood that better than anyone, right? He took advantage of it. He was living in the current world when everyone else was stuck in the past.

02:04:02,754 --> 02:04:04,254
SPEAKER_0:  and he saw that and he just.

02:04:04,962 --> 02:04:05,310
SPEAKER_0:  lies.

02:04:06,018 --> 02:04:07,454
SPEAKER_0:  He everything he said.

02:04:08,258 --> 02:04:10,526
SPEAKER_0:  you know, it doesn't, the truth was not relevant at all.

02:04:10,914 --> 02:04:18,366
SPEAKER_0:  It's just truly, it's not relevant to him in what he's talking about. He doesn't care and he knew that neither do a subset of the country. I was-

02:04:18,914 --> 02:04:21,598
SPEAKER_1:  thinking about this, just reading articles by journalists.

02:04:23,074 --> 02:04:24,222
SPEAKER_1:  Especially when you're not a-

02:04:25,026 --> 02:04:28,318
SPEAKER_1:  a famous journalist in yourself, but you're more like in your

02:04:29,154 --> 02:04:30,238
SPEAKER_1:  Times journalist.

02:04:30,466 --> 02:04:32,766
SPEAKER_1:  so the big famous things the institution you're part of

02:04:33,506 --> 02:04:37,598
SPEAKER_1:  that you can just lie, because you're not going to get punished for it.

02:04:38,050 --> 02:04:42,718
SPEAKER_1:  you're going to be rewarded for the popularity of an article. So if you write 10 articles,

02:04:44,066 --> 02:04:47,678
SPEAKER_1:  It's there's a huge incentive to just make stuff up you got to get clicks

02:04:47,906 --> 02:04:50,078
SPEAKER_1:  to get clicks. That's the first and foremost. And like.

02:04:50,338 --> 02:04:51,262
SPEAKER_1:  culturally.

02:04:51,970 --> 02:04:57,054
SPEAKER_1:  People attack their article to say the sound like one half the country will attack their article saying it's dishonest.

02:04:57,474 --> 02:04:58,430
SPEAKER_1:  but they'll kind of forget.

02:04:59,074 --> 02:04:59,806
SPEAKER_1:  Duh.

02:05:00,226 --> 02:05:02,334
SPEAKER_1:  You will not have a reputational hit.

02:05:03,010 --> 02:05:07,710
SPEAKER_1:  There won't be a memory like this person made up a lot of stuff in the past. No, they'll take one article at a time.

02:05:08,226 --> 02:05:10,558
SPEAKER_1:  and they'll attach the reputation hits.

02:05:10,914 --> 02:05:13,150
SPEAKER_1:  will be to New York Times, the institution. yeah.

02:05:13,730 --> 02:05:16,638
SPEAKER_1:  And so for the individual journalist, there's a huge incentive to make stuff up.

02:05:16,834 --> 02:05:17,918
SPEAKER_0:  Totally.

02:05:18,658 --> 02:05:20,606
SPEAKER_0:  And it's scary because...

02:05:21,250 --> 02:05:27,646
SPEAKER_0:  It's almost like you can't survive if you're just an old school, honest journalist who really works hard and tries to get it right and does it with nuance like.

02:05:28,098 --> 02:05:30,078
SPEAKER_0:  What you can be is you can be a big time.

02:05:30,306 --> 02:05:35,582
SPEAKER_0:  substack or big time podcaster. A lot of people do have a reputation for accuracy and rigor.

02:05:36,066 --> 02:05:37,342
SPEAKER_0:  and they have huge audiences.

02:05:37,826 --> 02:05:38,270
SPEAKER_0:  But.

02:05:38,850 --> 02:05:40,862
SPEAKER_0:  If you're working in a big company right now.

02:05:41,218 --> 02:05:41,758
SPEAKER_0:  Um...

02:05:42,466 --> 02:05:45,854
SPEAKER_0:  it's I mean especially I mean I like I I think that

02:05:46,082 --> 02:05:53,982
SPEAKER_0:  many of the big media brands are very much controlled by the left. And but I will say that the ones that are controlled by the right are even more egregious.

02:05:54,402 --> 02:05:59,582
SPEAKER_0:  not just in terms of accuracy, but also in terms of, you know, the New York Times for all of its criticisms.

02:06:00,802 --> 02:06:01,630
SPEAKER_0:  They have...

02:06:02,018 --> 02:06:03,102
SPEAKER_0:  A handful of...

02:06:03,490 --> 02:06:05,918
SPEAKER_0:  They relate every day here and there they put out a

02:06:06,210 --> 02:06:06,686
SPEAKER_0:  Pretty.

02:06:07,394 --> 02:06:13,086
SPEAKER_0:  you know, an article that strays from the, you know, Barry Weiss wrote there for a long time. And then you've got.

02:06:13,698 --> 02:06:19,326
SPEAKER_0:  They wrote an article criticizing free speech on campus stuff recently. And they have, you know, they have a couple...

02:06:19,714 --> 02:06:20,382
SPEAKER_0:  very, you know.

02:06:20,610 --> 02:06:23,454
SPEAKER_0:  left progressive friendly.

02:06:23,810 --> 02:06:27,198
SPEAKER_0:  conservatives, but they have conservatives that are operating the op-eds. Fox News.

02:06:27,618 --> 02:06:31,806
SPEAKER_0:  You're not seeing thoughtful, a bright bar, you're not seeing thoughtful.

02:06:32,290 --> 02:06:33,086
SPEAKER_0:  Progressives.

02:06:33,346 --> 02:06:34,206
SPEAKER_0:  writing there, right?

02:06:34,434 --> 02:06:36,606
SPEAKER_1:  There's some degree to which the New York Times

02:06:37,346 --> 02:06:38,718
SPEAKER_1:  I think still.

02:06:38,978 --> 02:06:41,246
SPEAKER_1:  incentivizing the values of the vertical.

02:06:42,050 --> 02:06:43,038
SPEAKER_1:  the high effort.

02:06:44,002 --> 02:06:47,518
SPEAKER_1:  you're allowed to have a conservative opinion.

02:06:47,874 --> 02:06:49,470
SPEAKER_1:  if you do a really damn good job.

02:06:49,698 --> 02:06:52,323
SPEAKER_1:  Like if it's a very thorough, in-depth...

02:06:52,323 --> 02:06:53,758
SPEAKER_0:  And if you...

02:06:54,242 --> 02:06:59,966
SPEAKER_0:  kind of pander to the progressive senses in all the right ways. You know, I always joke that, you know, a TED...

02:07:00,226 --> 02:07:04,702
SPEAKER_0:  They always have a couple, you know, token conservatives, but they get on stage and they're basically like, So...

02:07:05,058 --> 02:07:09,822
SPEAKER_0:  Totally, you're all, you know, the progressivisms are, it's right about all of this, but maybe.

02:07:10,050 --> 02:07:18,110
SPEAKER_0:  maybe libertarianism isn't all about, you know, it's just, so there is an element, but you know what? It's something, it's better than being a total tribal.

02:07:18,402 --> 02:07:27,454
SPEAKER_0:  I think you can see the New York Times tug of war, the internal tug of war, you can see it, because then they also have these awful instances, or the firing of James Bennett, which is a soul.

02:07:27,810 --> 02:07:29,502
SPEAKER_0:  other story, but like they have.

02:07:30,146 --> 02:07:32,126
SPEAKER_0:  Yeah, you can see it going both ways, but...

02:07:32,930 --> 02:07:44,286
SPEAKER_0:  The 60s, what did you have? You had ABC, NBC, CBS, you know, the 70s, you had these three news channels and they weren't always right and they definitely sometimes spun a narrative together maybe about the Vietnam or whatever, but.

02:07:45,986 --> 02:07:57,502
SPEAKER_0:  they, if one of them was just lying, they'd be embarrassed for it. They would be penalized. They'd be dinged and they'd be known as this is the trash one. And that would be terrible for their ratings because they weren't just catering to half the country. They're all catering to the whole country. So.

02:07:57,730 --> 02:08:01,790
SPEAKER_0:  both on the axis of accuracy and on the axis of neutrality. They had to be listened.

02:08:02,818 --> 02:08:03,422
SPEAKER_0:  you know.

02:08:03,906 --> 02:08:08,158
SPEAKER_0:  try to stay somewhere in the reasonable range and that's just gone.

02:08:08,834 --> 02:08:10,814
SPEAKER_1:  One of the things I'm really curious about.

02:08:11,138 --> 02:08:11,518
SPEAKER_1:  is.

02:08:12,386 --> 02:08:13,822
SPEAKER_1:  I think your book is incredible.

02:08:14,434 --> 02:08:16,510
SPEAKER_1:  I'm very curious to see how it's written about.

02:08:16,802 --> 02:08:17,470
SPEAKER_1:  by the press.

02:08:18,306 --> 02:08:19,166
SPEAKER_1:  because I could see.

02:08:19,490 --> 02:08:22,654
SPEAKER_1:  I get myself right with the help of ChachiPT of course.

02:08:23,010 --> 02:08:24,862
SPEAKER_1:  Click Bait Articles in either direction.

02:08:25,954 --> 02:08:29,246
SPEAKER_1:  It's easy to imagine. Your whole book is beautifully written.

02:08:29,570 --> 02:08:32,478
SPEAKER_1:  for clickbait articles. If any journalist out there need help.

02:08:32,706 --> 02:08:33,831
SPEAKER_1:  I can help you.

02:08:33,831 --> 02:08:34,302
SPEAKER_0:  Yeah.

02:08:34,466 --> 02:08:37,086
SPEAKER_1:  I can write out the most atrocious criticisms.

02:08:37,218 --> 02:08:40,958
SPEAKER_0:  Yeah, I'm ready. I'm braced.

02:08:41,378 --> 02:08:42,622
SPEAKER_1:  Umm...

02:08:42,914 --> 02:08:45,374
SPEAKER_1:  Yeah. So speaking of which.

02:08:45,890 --> 02:08:48,382
SPEAKER_1:  You write about social justice.

02:08:48,706 --> 02:08:52,734
SPEAKER_1:  You write about two kinds of social justice, liberal social justice and

02:08:53,250 --> 02:08:54,814
SPEAKER_1:  us as J.F.

02:08:55,042 --> 02:08:56,862
SPEAKER_1:  Social justice fundamentalism.

02:08:57,314 --> 02:08:57,982
SPEAKER_1:  What are those?

02:08:58,786 --> 02:09:05,054
SPEAKER_0:  Yeah, so like the term wokeness is so loaded with baggage, it's kind of like mocking and derogatory, and I was trying not to do that in this book.

02:09:05,506 --> 02:09:06,078
SPEAKER_0:  Um...

02:09:06,754 --> 02:09:08,958
SPEAKER_0:  If it's the terms loaded with baggage, you're already kind of.

02:09:09,186 --> 02:09:11,966
SPEAKER_0:  you're from the first minute you're already behind.

02:09:12,802 --> 02:09:13,342
SPEAKER_0:  Um...

02:09:13,730 --> 02:09:14,270
SPEAKER_0:  So.

02:09:16,226 --> 02:09:16,830
SPEAKER_0:  To me.

02:09:17,186 --> 02:09:21,150
SPEAKER_0:  Also, when people say, wokeness is bad.

02:09:21,698 --> 02:09:23,806
SPEAKER_0:  Social justice is bad, they're throwing the baby out of the bath.

02:09:24,226 --> 02:09:24,574
SPEAKER_0:  Um.

02:09:25,122 --> 02:09:25,662
SPEAKER_0:  because...

02:09:26,050 --> 02:09:26,462
SPEAKER_0:  the

02:09:26,914 --> 02:09:32,094
SPEAKER_0:  The proudest tradition in the US is liberal social justice. And what I mean by that, again, liberal meaning with-

02:09:32,386 --> 02:09:33,534
SPEAKER_0:  lowercase L.

02:09:34,434 --> 02:09:35,134
SPEAKER_0:  It is.

02:09:35,746 --> 02:09:41,918
SPEAKER_0:  it is intertwined with liberalism. So Martin Luther King, classic example, I have a dream speech, he says stuff like.

02:09:42,146 --> 02:09:43,006
SPEAKER_0:  this country.

02:09:43,426 --> 02:09:43,998
SPEAKER_0:  You know?

02:09:44,674 --> 02:09:47,870
SPEAKER_0:  is, you know, has made a promise.

02:09:48,898 --> 02:09:52,638
SPEAKER_0:  to all of its citizens and it has broken that promise to its black citizens.

02:09:52,930 --> 02:09:53,374
SPEAKER_0:  Right?

02:09:54,146 --> 02:09:54,782
SPEAKER_0:  In other words...

02:09:55,074 --> 02:09:58,878
SPEAKER_0:  Liberalism, the Constitution, the core ideals, those are great.

02:09:59,234 --> 02:10:01,918
SPEAKER_0:  We're not living up to them. We're failing on some of them.

02:10:02,850 --> 02:10:05,758
SPEAKER_0:  So civil disobedience, the goal of it wasn't to...

02:10:06,146 --> 02:10:15,166
SPEAKER_0:  hurt liberalism is to specifically break the laws that were already violating liberal that were the laws that were a violation of liberalism to expose that this is illiberal

02:10:15,650 --> 02:10:18,110
SPEAKER_0:  that the Constitution should not have people.

02:10:18,370 --> 02:10:20,350
SPEAKER_0:  of different skin color sitting in different parts of the bus.

02:10:20,834 --> 02:10:21,310
SPEAKER_0:  And so.

02:10:21,602 --> 02:10:22,110
SPEAKER_0:  It was.

02:10:22,530 --> 02:10:23,102
SPEAKER_0:  It was kind of a-

02:10:23,330 --> 02:10:27,454
SPEAKER_0:  It was really patriotic, you know, the civil rights movement. It was saying this is a beautiful, you know.

02:10:27,778 --> 02:10:30,782
SPEAKER_0:  We have a, liberalism is this beautiful thing we need to do better at it.

02:10:31,490 --> 02:10:34,910
SPEAKER_0:  So I call it liberal social justice And it used the tools of liberalism.

02:10:35,554 --> 02:10:36,926
SPEAKER_0:  to try too.

02:10:37,410 --> 02:10:37,886
SPEAKER_0:  Uh...

02:10:38,850 --> 02:10:43,326
SPEAKER_0:  to try to improve the flaws in that were going on. So free speech.

02:10:43,586 --> 02:10:46,910
SPEAKER_0:  You know, Mario Savio in the 60s was the, you know, he's a leftist.

02:10:47,394 --> 02:10:53,310
SPEAKER_0:  And what were the leftists doing in the 60s on Berkeley campus? They were saying we need more free speech.

02:10:54,018 --> 02:10:54,462
SPEAKER_0:  Um.

02:10:54,690 --> 02:10:59,518
SPEAKER_0:  because that's what liberal social justice was fighting for. But you can also go back to the 20s, women's suffrage.

02:10:59,810 --> 02:11:08,126
SPEAKER_0:  So the emancipation, the thing that America obviously has all of its, these are all ugly things that it had to get out of, but it got out of them.

02:11:08,450 --> 02:11:11,998
SPEAKER_0:  you know, one by one, and it's still getting out of them. And that's what's cool about America.

02:11:12,418 --> 02:11:16,254
SPEAKER_0:  And liberal social justice basically is the practice of saying, where are we not?

02:11:16,546 --> 02:11:19,998
SPEAKER_0:  being perfect liberals. And now let's fix that.

02:11:20,898 --> 02:11:23,198
SPEAKER_1:  So that's the idea of liberalism.

02:11:23,490 --> 02:11:27,390
SPEAKER_1:  permeates the history of the United States, 150 of whom granted more than 2,400 from those so o'er the world so

02:11:27,618 --> 02:11:33,502
SPEAKER_1:  many good images in this book, but one of them is highlighting the interplay of different ideas.

02:11:33,858 --> 02:11:36,350
SPEAKER_1:  over the past, let's say, 100 years.

02:11:36,834 --> 02:11:39,902
SPEAKER_1:  So liberalism is on one side, there's that thread.

02:11:40,226 --> 02:11:41,214
SPEAKER_1:  There's Marxism.

02:11:41,762 --> 02:11:43,870
SPEAKER_1:  on the other and then there's postmodernism.

02:11:45,154 --> 02:11:46,590
SPEAKER_1:  How do those interplay together?

02:11:47,042 --> 02:11:48,606
SPEAKER_0:  So it's interesting because...

02:11:49,314 --> 02:11:50,142
SPEAKER_0:  Marxism.

02:11:51,074 --> 02:11:56,254
SPEAKER_0:  is and all of its various descendants, obviously, there's a lot of things that are rooted in Marxism that aren't.

02:11:56,482 --> 02:11:57,790
SPEAKER_0:  you know, the same thing as...

02:11:58,146 --> 02:11:59,262
SPEAKER_0:  what Karl Marx preached.

02:12:00,034 --> 02:12:01,406
SPEAKER_0:  But what do they all have in common?

02:12:02,562 --> 02:12:04,126
SPEAKER_0:  They think liberalism.

02:12:04,738 --> 02:12:05,278
SPEAKER_0:  is back.

02:12:05,986 --> 02:12:07,934
SPEAKER_0:  Right? They actually think that.

02:12:08,642 --> 02:12:09,214
SPEAKER_0:  Um...

02:12:10,402 --> 02:12:12,030
SPEAKER_0:  that the opposite.

02:12:12,386 --> 02:12:15,198
SPEAKER_0:  of what Martin Luther King and other people.

02:12:15,650 --> 02:12:22,654
SPEAKER_0:  in the civil rights and other movements. They think the opposite. They think, he thinks, you know, liberalism is good, we need to preserve it. Liberalism is the problem.

02:12:23,202 --> 02:12:27,934
SPEAKER_0:  These other problems with racism and inequality that we're seeing, those are-

02:12:28,258 --> 02:12:44,862
SPEAKER_0:  inevitable results of liberalism. Liberalism is a rigged game, and it's just the power games in disguise. There is no liberal games. It's just the power games in disguise, and there's the upper people that oppress the lower people, and they convince the lower people, it's all about false consciousness, they convince the lower people.

02:12:45,090 --> 02:12:50,878
SPEAKER_0:  that everything is fair and that lower people vote against their own interests and they work to preserve the system that's oppressing them.

02:12:51,426 --> 02:12:57,022
SPEAKER_0:  And what do we need to do? We need to actually, it's much more revolutionary, we need to overthrow liberalism.

02:12:57,602 --> 02:12:58,398
SPEAKER_0:  Right?

02:12:58,690 --> 02:12:59,966
SPEAKER_0:  People think is, oh, you know, like.

02:13:00,226 --> 02:13:02,110
SPEAKER_0:  what we call a bookness is just.

02:13:02,626 --> 02:13:07,326
SPEAKER_0:  You know, a normal social justice activist activism, but it's like more extreme, right? It's this, no, no, it's the.

02:13:07,970 --> 02:13:10,142
SPEAKER_0:  Polar opposite, polar opposite. And so.

02:13:10,466 --> 02:13:10,910
SPEAKER_0:  Um.

02:13:11,202 --> 02:13:15,454
SPEAKER_0:  Now that's the Marxist threat. Now postmodernism is kind of, you know, this.

02:13:15,714 --> 02:13:21,694
SPEAKER_0:  that is super controversial and I don't think anyone calls themselves a postmodernist so take all this with a grain of salt in terms of the term but

02:13:21,922 --> 02:13:23,102
SPEAKER_0:  What's the definition of radical?

02:13:23,586 --> 02:13:24,798
SPEAKER_0:  The definition of radical to me is.

02:13:25,442 --> 02:13:25,886
SPEAKER_0:  How?

02:13:26,114 --> 02:13:26,494
SPEAKER_0:  Deep.

02:13:27,138 --> 02:13:27,774
SPEAKER_0:  you want.

02:13:28,002 --> 02:13:28,830
SPEAKER_0:  change to happen.

02:13:29,250 --> 02:13:29,630
SPEAKER_0:  So.

02:13:30,050 --> 02:13:32,158
SPEAKER_0:  So a liberal progressive.

02:13:32,834 --> 02:13:37,374
SPEAKER_0:  and a conservative progressive will disagree about policies the liberal progressive wants to

02:13:37,634 --> 02:13:38,142
SPEAKER_0:  You know?

02:13:38,562 --> 02:13:41,214
SPEAKER_0:  change a lot of policies and change, change, change, right?

02:13:41,634 --> 02:13:43,998
SPEAKER_0:  and the conservative is more wants to keep things the way they are.

02:13:44,994 --> 02:13:47,422
SPEAKER_0:  But they're both conservative when it comes to liberalism.

02:13:48,226 --> 02:13:50,046
SPEAKER_0:  um, beneath it, the liberal kind of

02:13:50,402 --> 02:13:54,558
SPEAKER_0:  foundation of the country. They both want to, they both are become conservatives about that.

02:13:55,394 --> 02:14:01,182
SPEAKER_0:  The Marxist is more radical because they want to go one notch deeper and actually overthrow that foundation.

02:14:01,570 --> 02:14:02,462
SPEAKER_0:  Now what's below?

02:14:03,106 --> 02:14:07,294
SPEAKER_0:  what's below liberalism is kind of the core tenets of modernity.

02:14:07,874 --> 02:14:09,886
SPEAKER_0:  this idea of reason and

02:14:10,370 --> 02:14:13,214
SPEAKER_0:  um... the notion that there is an objective truth and

02:14:13,762 --> 02:14:14,334
SPEAKER_0:  Um...

02:14:14,850 --> 02:14:27,486
SPEAKER_0:  science as the scientific method, right? These things are actually beneath, and even the Marxist, if you look at the Frankfurt School, you know, these post-Marxist thinkers and Marx himself, they were not anti-science. They believed in that bottom bottom.

02:14:28,194 --> 02:14:39,998
SPEAKER_0:  Foundation they were they were they were they were actually wanted to preserve modernity But they wanted to get rid of liberalism on top of it The postmodernist is even more radical because they want to actually go down to the bottom level and overthrow they think science itself

02:14:40,418 --> 02:14:43,198
SPEAKER_0:  is a tool of oppression. I think it's a tool.

02:14:43,458 --> 02:14:45,566
SPEAKER_0:  where oppression kind of flows through.

02:14:45,826 --> 02:14:46,974
SPEAKER_0:  You know, they think that the white.

02:14:47,394 --> 02:14:49,118
SPEAKER_0:  Western world has invented these.

02:14:49,570 --> 02:14:53,022
SPEAKER_0:  concepts like they claim that there's an objective truth and that there's

02:14:53,282 --> 02:15:00,478
SPEAKER_0:  you know, reason and science, and they think all of that is just one meta-narrative, and it goes a long way to serve the interests of the powerful.

02:15:00,898 --> 02:15:02,142
SPEAKER_1:  So in the sense that...

02:15:02,658 --> 02:15:10,846
SPEAKER_1:  It's almost caricatured, but that is to the core of their belief that math could be racist, for example. Oh yeah, absolutely. Not the education of math, but-

02:15:11,074 --> 02:15:11,966
SPEAKER_1:  literally

02:15:12,450 --> 02:15:13,022
SPEAKER_1:  M A C K

02:15:13,154 --> 02:15:24,798
SPEAKER_0:  The notion in math that there's a right answer and a wrong answer. That they believe is a meta-narrative that serves white supremacy. Or in the postmodernist might have said it serves just the powerful.

02:15:25,122 --> 02:15:26,014
SPEAKER_0:  or the wealthy.

02:15:26,786 --> 02:15:32,510
SPEAKER_0:  So what social justice fundamentalism is, is you take the Marxist thread that has been going on.

02:15:33,090 --> 02:15:34,462
SPEAKER_0:  in lots of countries.

02:15:34,754 --> 02:15:41,662
SPEAKER_0:  and whoever the upper and lower is, that's what they all have in common, but the upper and lower, you know, from Marx was...

02:15:41,890 --> 02:15:44,574
SPEAKER_0:  the ruling class and the oppressed class, it was economic.

02:15:45,026 --> 02:15:46,846
SPEAKER_0:  And then.

02:15:47,106 --> 02:15:48,542
SPEAKER_0:  But you come here and...

02:15:49,378 --> 02:15:55,198
SPEAKER_0:  The economic class doesn't, you know, doesn't resonate as much here as it did maybe in some of those other places, but what does resonate here?

02:15:55,650 --> 02:15:59,486
SPEAKER_0:  in the 60s and 70s is race and gender and these kind of.

02:15:59,714 --> 02:16:00,766
SPEAKER_0:  Social justice.

02:16:01,090 --> 02:16:06,974
SPEAKER_0:  disagreements and so what social justice fundamentalism is is this basically this this tried and true framework

02:16:07,490 --> 02:16:08,030
SPEAKER_0:  of.

02:16:08,898 --> 02:16:11,646
SPEAKER_0:  of this Marxist framework.

02:16:12,514 --> 02:16:16,574
SPEAKER_0:  kind of with a new skin on it, which is American social justice. And then is

02:16:16,962 --> 02:16:26,782
SPEAKER_0:  made even more radical with the infusion of postmodernism, where not just is liberalism bad, but actually, like you said, math can be racist. So it's this kind of like-

02:16:27,298 --> 02:16:30,334
SPEAKER_0:  Philosophical Frankenstein. This like stitched together of these.

02:16:30,722 --> 02:16:31,166
SPEAKER_0:  It

02:16:31,394 --> 02:16:38,590
SPEAKER_0:  And so again, it's called, they wear the same uniform as the liberal social justice. They say social justice, right? Racial equality.

02:16:39,394 --> 02:16:40,574
SPEAKER_0:  but it has nothing to do.

02:16:40,802 --> 02:16:44,177
SPEAKER_0:  with liberal social justice. It is directly opposed to liberal social.

02:16:44,177 --> 02:16:46,302
SPEAKER_1:  This is fascinating, the evolution.

02:16:47,106 --> 02:16:49,886
SPEAKER_1:  of ideas if we ignore the harm done by it.

02:16:50,498 --> 02:16:53,598
SPEAKER_1:  It's fascinating how humans get together and evolve these ideas.

02:16:53,890 --> 02:16:55,678
SPEAKER_1:  as you show Marxism.

02:16:56,098 --> 02:17:00,606
SPEAKER_1:  is the idea that society is a zero sum. I mean, I guess zero sum is a really important thing here.

02:17:01,090 --> 02:17:08,478
SPEAKER_1:  zero-sum struggle between the ruling class and the working class with power being exerted through politics and economics then you

02:17:08,770 --> 02:17:09,246
SPEAKER_1:  add.

02:17:10,050 --> 02:17:13,694
SPEAKER_1:  Critical theory, Marxism 2.0 on top of that, and you add to

02:17:13,922 --> 02:17:16,606
SPEAKER_1:  Politics and economics, you had culture and institutions.

02:17:16,962 --> 02:17:22,367
SPEAKER_1:  And then on top of that, for postmodernism, you add science, you have morality, basically anything else you can.

02:17:22,367 --> 02:17:36,798
SPEAKER_0:  to stitch together Frankenstein. And if you notice, which is not necessarily bad, but in this case, I think it's actually violating the Marxist tradition by being anti-science. And it's violating the postmodernism because what postmodernists were, they were radical skeptics.

02:17:37,026 --> 02:17:41,054
SPEAKER_0:  not just of, they were radical skeptics, not just of the way things were, but of their own beliefs.

02:17:41,634 --> 02:17:46,302
SPEAKER_0:  And social justice fundamentalism is suddenly not at all.

02:17:46,850 --> 02:17:53,342
SPEAKER_0:  self-critical. It says that we have the answers, which is the opposite of what postmodernists would ever say. No, you just have another meta-narrative.

02:17:53,666 --> 02:17:58,654
SPEAKER_0:  So, and it's also violating of course, the tradition of like liberal social justice in a million ways because it's anti-liberal.

02:17:59,138 --> 02:18:01,342
SPEAKER_0:  And so this Frankenstein comes together meanwhile.

02:18:02,210 --> 02:18:06,814
SPEAKER_0:  liberal social justice doesn't have a Frankenstein. It's very clear. It's very, it's a

02:18:07,074 --> 02:18:08,574
SPEAKER_0:  Crisp Ideology that says...

02:18:08,994 --> 02:18:13,566
SPEAKER_0:  We need, they're trying to make, we trying to get to a more perfect union. They're trying to, to, to.

02:18:14,498 --> 02:18:16,734
SPEAKER_0:  Keep the promises made in the Constitution.

02:18:17,314 --> 02:18:19,486
SPEAKER_0:  And that's what it's trying to do. And so it's.

02:18:19,714 --> 02:18:21,438
SPEAKER_0:  It's much simpler in a lot of ways.

02:18:21,954 --> 02:18:23,079
SPEAKER_0:  So you write that.

02:18:23,079 --> 02:18:26,622
SPEAKER_1:  My big problem with social justice fundamentalism isn't the ideology itself.

02:18:26,914 --> 02:18:32,862
SPEAKER_1:  It's what scholars and activists started to do sometimes around 2013 when they began to wield a

02:18:33,250 --> 02:18:39,838
SPEAKER_1:  Kajjo that's not supposed to have any place in the country like the US. It's the actions not the ideas.

02:18:40,002 --> 02:18:41,214
SPEAKER_0:  Well, to be clear.

02:18:41,474 --> 02:18:47,358
SPEAKER_0:  I don't like the ideology. I think it's a low rung ideology. I think it's morally inconsistent.

02:18:47,714 --> 02:18:51,902
SPEAKER_0:  based on, you know, it's flip flops on its morals, depending on the group.

02:18:52,546 --> 02:18:53,246
SPEAKER_0:  I think it's.

02:18:53,474 --> 02:18:55,390
SPEAKER_0:  echo chambery. I think it's

02:18:55,810 --> 02:18:59,486
SPEAKER_0:  I think it's full of inaccuracies and kind of.

02:18:59,842 --> 02:19:08,158
SPEAKER_0:  can't stand up to debate. So I think it's a low, but there's a ton of low-rank ideologies I don't like. I don't like a lot of religious doctrines. I don't like a lot of political doctrines, right?

02:19:08,898 --> 02:19:10,910
SPEAKER_0:  The US is a place inherently.

02:19:11,234 --> 02:19:15,870
SPEAKER_0:  that is a mishmash of a ton of ideologies. And I'm not gonna like two thirds of them at any given time.

02:19:16,354 --> 02:19:22,078
SPEAKER_0:  So my problem, the reason I'm writing about this is not because I'm like, by the way, this ideology is not something I like. That's not interesting.

02:19:23,266 --> 02:19:27,166
SPEAKER_0:  the reason that it must be written about right now, this particular ideology.

02:19:27,426 --> 02:19:28,414
SPEAKER_0:  is because.

02:19:29,058 --> 02:19:29,534
SPEAKER_0:  It's not.

02:19:29,794 --> 02:19:35,390
SPEAKER_0:  playing nicely with others. But if you want to be a hardcore evangelical...

02:19:35,938 --> 02:19:36,446
SPEAKER_0:  Christian.

02:19:36,770 --> 02:19:45,566
SPEAKER_0:  Go, in the US says, live and let live. Not only are you allowed to have an echo chamber of some kind, it's actively protected here. Live and let live, they can do what they want, you do what you want.

02:19:45,858 --> 02:19:49,214
SPEAKER_0:  Now if the Evangelical Christian started saying, by the way...

02:19:49,506 --> 02:19:55,230
SPEAKER_0:  anyone who says anything that conflicts with evangelical Christianity is going to be severely

02:19:55,554 --> 02:20:01,662
SPEAKER_0:  socially punished and they have the cultural power to do so, which they don't in this case. They might like to but they don't have the-

02:20:01,922 --> 02:20:04,414
SPEAKER_0:  but they're able to get anyone fired who they want.

02:20:04,802 --> 02:20:05,886
SPEAKER_0:  and they're able to actually.

02:20:06,146 --> 02:20:14,238
SPEAKER_0:  change the curriculum in all these schools to suddenly not conflict with, no more evolution in the textbooks because they don't want it. Now I would write a book about-

02:20:14,658 --> 02:20:17,022
SPEAKER_0:  Why about evangelical Christianity?

02:20:17,634 --> 02:20:19,806
SPEAKER_0:  because that's what every liberal.

02:20:20,066 --> 02:20:22,846
SPEAKER_0:  regardless of what you think of the actual horizontal beliefs.

02:20:23,650 --> 02:20:27,614
SPEAKER_0:  Doesn't matter what they believe, when they start violating Live and Let Live.

02:20:28,322 --> 02:20:29,630
SPEAKER_0:  and shutting down.

02:20:30,178 --> 02:20:30,622
SPEAKER_0:  Other.

02:20:31,010 --> 02:20:34,398
SPEAKER_0:  area, other segments of society and in kind of, it's almost like a

02:20:34,658 --> 02:20:39,358
SPEAKER_0:  you know, not to, you know, it's not the best analogy, but like, it's like a, an echo chamber is like a benign tumor.

02:20:40,066 --> 02:20:44,830
SPEAKER_0:  And what you have to watch out for is a tumor that starts to metastasize, starts to forcefully spread.

02:20:45,282 --> 02:20:46,814
SPEAKER_0:  and damage the tissue around.

02:20:47,522 --> 02:20:50,142
SPEAKER_0:  And that's what this particular ideology.

02:20:50,658 --> 02:20:51,390
SPEAKER_0:  has been doing.

02:20:51,842 --> 02:20:52,862
SPEAKER_1:  Do you worry about...

02:20:53,634 --> 02:20:54,014
SPEAKER_1:  You know.

02:20:54,722 --> 02:20:55,838
SPEAKER_1:  as an existential.

02:20:56,258 --> 02:20:56,958
SPEAKER_1:  Threat.

02:20:57,794 --> 02:20:58,334
SPEAKER_1:  too.

02:20:59,074 --> 02:21:02,110
SPEAKER_1:  to liberalism in the West, in the United States.

02:21:03,554 --> 02:21:06,430
SPEAKER_1:  Is it a problem or is it?

02:21:07,586 --> 02:21:11,006
SPEAKER_1:  the biggest problem that's threatening all of human civilization.

02:21:11,106 --> 02:21:15,486
SPEAKER_0:  I would not say it's the biggest problem. It might be.

02:21:15,842 --> 02:21:19,934
SPEAKER_0:  I wouldn't, if someone, if it turns out in 50 years someone says actually it was, I wouldn't be shocked.

02:21:20,802 --> 02:21:24,094
SPEAKER_0:  But I also, I wouldn't bet on that because there's a lot of problems.

02:21:24,578 --> 02:21:26,974
SPEAKER_1:  I'm a little sorry to interrupt. It is-

02:21:27,266 --> 02:21:29,438
SPEAKER_1:  popular to say that kind of thing though.

02:21:30,146 --> 02:21:34,462
SPEAKER_1:  and it's less popular to say the same thing about AI or nuclear weapons.

02:21:35,010 --> 02:21:41,726
SPEAKER_1:  which worries me that i'm more worried about nuclear weapons even still than i am about woke ism.

02:21:42,114 --> 02:21:53,822
SPEAKER_0:  So I've gotten, I've had probably a thousand arguments about this. That's one nice thing about spending six years procrastinating on getting a book done is you end up test battle testing your ideas a million times. So I've heard this one a lot, right? Which is.

02:21:55,074 --> 02:21:56,222
SPEAKER_0:  There's kind of three groups of.

02:21:56,514 --> 02:21:58,686
SPEAKER_0:  former Obama voters.

02:21:59,266 --> 02:22:00,190
SPEAKER_0:  is super woke now.

02:22:00,450 --> 02:22:03,742
SPEAKER_0:  Another one is super anti woke now and the third

02:22:04,002 --> 02:22:07,486
SPEAKER_0:  is what you just said, which is sure, wokeness is over the top, right?

02:22:07,906 --> 02:22:09,214
SPEAKER_0:  They're not, you're not woke, but.

02:22:10,050 --> 02:22:11,678
SPEAKER_0:  I think that the Andy Wope people are.

02:22:12,290 --> 02:22:15,070
SPEAKER_0:  totally lost their mind, and it's just not that big a deal, right?

02:22:15,970 --> 02:22:17,342
SPEAKER_0:  Now here's why I disagree with that.

02:22:18,370 --> 02:22:19,038
SPEAKER_0:  because...

02:22:19,554 --> 02:22:20,254
SPEAKER_0:  It's not...

02:22:20,738 --> 02:22:21,950
SPEAKER_0:  It's not wokeness itself.

02:22:22,754 --> 02:22:23,550
SPEAKER_0:  It's that.

02:22:25,186 --> 02:22:26,014
SPEAKER_0:  of Radical.

02:22:26,370 --> 02:22:26,878
SPEAKER_0:  Um...

02:22:27,362 --> 02:22:29,950
SPEAKER_0:  political movement, of which there will always be a lot in the country.

02:22:30,946 --> 02:22:35,326
SPEAKER_0:  has managed to do something that a radical movement not supposed to be able to do in the US.

02:22:35,906 --> 02:22:37,886
SPEAKER_0:  which is they've managed to.

02:22:40,610 --> 02:22:41,214
SPEAKER_0:  Hi Jack.

02:22:42,018 --> 02:22:44,478
SPEAKER_0:  institutions all across the country and hijack.

02:22:46,210 --> 02:22:49,022
SPEAKER_0:  medical journals and universities and.

02:22:49,730 --> 02:22:54,142
SPEAKER_0:  you know, the ACLU, you know, all the activist organizations and non-profits.

02:22:54,722 --> 02:22:59,358
SPEAKER_0:  and NGOs. Yeah, and many, and many tech companies.

02:22:59,810 --> 02:23:00,414
SPEAKER_0:  and

02:23:01,474 --> 02:23:13,278
SPEAKER_0:  But so it's not that I think this thing is so bad. It's a little like we said with Trump. It's that what I'm scared, the reason Trump scares me is not cause Trump's so bad. It's that because it shows, it reveals that we were vulnerable to a demagogue.

02:23:14,114 --> 02:23:19,646
SPEAKER_0:  candidate. And what wokeness reveals to me is that we are currently, and until something changes, will continue to be.

02:23:19,874 --> 02:23:20,766
SPEAKER_0:  Vulnerable.

02:23:21,282 --> 02:23:21,758
SPEAKER_0:  too.

02:23:22,274 --> 02:23:22,942
SPEAKER_0:  A.

02:23:23,650 --> 02:23:24,254
SPEAKER_0:  Um...

02:23:25,474 --> 02:23:30,718
SPEAKER_0:  a bully movement, a forcefully expansionist movement that wants to actually

02:23:31,010 --> 02:23:31,358
SPEAKER_0:  Um.

02:23:33,378 --> 02:23:35,006
SPEAKER_0:  destroy the workings.

02:23:35,362 --> 02:23:36,894
SPEAKER_0:  in their liberal gears.

02:23:37,122 --> 02:23:37,918
SPEAKER_0:  tear them apart.

02:23:38,306 --> 02:23:41,150
SPEAKER_0:  And so here's the way I view a liberal democracy is it is it.

02:23:41,474 --> 02:23:45,406
SPEAKER_0:  It is a bunch of these institutions that were trial and error crafted over, you know.

02:23:45,954 --> 02:23:46,782
SPEAKER_0:  hundreds of years.

02:23:47,746 --> 02:23:49,694
SPEAKER_0:  and they all rely on trust.

02:23:49,922 --> 02:23:50,686
SPEAKER_0:  public trust.

02:23:50,914 --> 02:23:54,622
SPEAKER_0:  and a certain kind of feeling of unity that actually is critical.

02:23:54,978 --> 02:23:56,478
SPEAKER_0:  to a liberal democracy's functioning.

02:23:57,186 --> 02:23:59,998
SPEAKER_0:  And what I see this thing is, is as a parasite.

02:24:00,514 --> 02:24:02,302
SPEAKER_0:  on that, that whose goal

02:24:02,594 --> 02:24:06,718
SPEAKER_0:  And I'm not saying each, by the way, each individual in this is, I don't think they're bad people.

02:24:07,138 --> 02:24:10,302
SPEAKER_0:  I think that it's the ideology itself has the property of.

02:24:10,658 --> 02:24:12,606
SPEAKER_0:  Its goal is to tear apart.

02:24:13,442 --> 02:24:17,662
SPEAKER_0:  pretty delicate workings of the liberal democracy and shred the critical lines of trust.

02:24:18,498 --> 02:24:19,038
SPEAKER_0:  and

02:24:19,714 --> 02:24:22,814
SPEAKER_0:  So you talk about AI and you talk about all these other big problems, nuclear, right?

02:24:23,266 --> 02:24:28,862
SPEAKER_0:  The reason I stop, I like writing about that stuff a lot more than I like writing about politics. This was a fun topic for me.

02:24:29,090 --> 02:24:30,750
SPEAKER_0:  is because I realized that like.

02:24:31,394 --> 02:24:35,166
SPEAKER_0:  All of those things, if they were gonna have a good future with those things and they're actually threats?

02:24:35,618 --> 02:24:38,590
SPEAKER_0:  Like I said, we need to have our wits about us and we need the liberal.

02:24:39,202 --> 02:24:40,574
SPEAKER_0:  you know, gears and...

02:24:40,802 --> 02:24:47,038
SPEAKER_0:  and levers working, we need the liberal machine working. And so with something's threatening to undermine that, it affects everything else.

02:24:48,450 --> 02:24:56,286
SPEAKER_1:  We need to have our scientific mind about us, about these foundational ideas. But I guess my sense of hope comes from...

02:24:56,802 --> 02:24:59,550
SPEAKER_1:  observing the immune system respond to wokeism.

02:25:00,642 --> 02:25:02,110
SPEAKER_1:  There seems to be a...

02:25:02,498 --> 02:25:04,894
SPEAKER_1:  Pro-liberalism immune system.

02:25:05,698 --> 02:25:09,054
SPEAKER_1:  And not only that, so like there's intellectuals, there's people that are.

02:25:09,538 --> 02:25:11,998
SPEAKER_1:  willing to do the fight, you talk about courage.

02:25:12,642 --> 02:25:16,126
SPEAKER_1:  being courageous and there is a hunger for that.

02:25:16,546 --> 02:25:19,358
SPEAKER_1:  such that those ideas can become viral and they take over.

02:25:19,682 --> 02:25:20,478
SPEAKER_1:  So I just.

02:25:20,802 --> 02:25:23,326
SPEAKER_1:  don't see a mechanism by which wokeism

02:25:24,290 --> 02:25:25,502
SPEAKER_1:  accelerates.

02:25:25,922 --> 02:25:26,910
SPEAKER_1:  like exponentially.

02:25:27,202 --> 02:25:27,934
SPEAKER_1:  and takes over.

02:25:28,226 --> 02:25:32,542
SPEAKER_1:  Like it's expand. It feels like as it expands the immune system responds.

02:25:33,058 --> 02:25:34,046
SPEAKER_1:  of the deep.

02:25:34,402 --> 02:25:37,214
SPEAKER_1:  the immune system of liberalism.

02:25:37,506 --> 02:25:38,782
SPEAKER_1:  Basically a country.

02:25:39,074 --> 02:25:43,614
SPEAKER_1:  and at least in the United States that still ultimately at the core of the individual values

02:25:43,874 --> 02:25:46,046
SPEAKER_1:  The freedom speech, just freedoms in general.

02:25:46,274 --> 02:25:47,550
SPEAKER_1:  the freedom of an individual.

02:25:48,098 --> 02:25:49,022
SPEAKER_1:  But that's the battle.

02:25:49,506 --> 02:25:53,118
SPEAKER_0:  Which is stronger. So to me it is like a virus and an immune system. Number one is immune system and a virus is when you are in a situation like if you have

02:25:53,730 --> 02:25:54,334
SPEAKER_0:  and

02:25:55,010 --> 02:25:59,742
SPEAKER_0:  I totally agree, I see the same story happening and I'm sitting here rooting for the immune system.

02:26:00,194 --> 02:26:02,590
SPEAKER_0:  Are you still worried? Well, here's the thing.

02:26:03,362 --> 02:26:06,270
SPEAKER_0:  a liberal democracy is always going to be vulnerable.

02:26:06,594 --> 02:26:07,038
SPEAKER_0:  too.

02:26:07,682 --> 02:26:09,982
SPEAKER_0:  a movement like this, right? And there will be more.

02:26:10,562 --> 02:26:13,758
SPEAKER_0:  because it's not a totalitarian dictatorship, because...

02:26:13,986 --> 02:26:18,878
SPEAKER_0:  If you can socially pressure people to not say what they're thinking, you can suddenly start to just take over.

02:26:19,202 --> 02:26:24,222
SPEAKER_0:  You can break the liberalism of the liberal democracy quite easily and suddenly a lot of things are illiberal.

02:26:25,282 --> 02:26:26,174
SPEAKER_0:  On the other hand...

02:26:27,714 --> 02:26:32,926
SPEAKER_0:  the same vulnerability, the same system that's vulnerable to that also is hard to truly conquer.

02:26:33,986 --> 02:26:35,326
SPEAKER_0:  Because now the Maoists...

02:26:36,322 --> 02:26:39,134
SPEAKER_0:  Right? Similar kind of vibe. They were saying that science is evil.

02:26:39,618 --> 02:26:44,222
SPEAKER_0:  and that the intellectuals are, it's all this big conspiracy.

02:26:46,722 --> 02:26:47,966
SPEAKER_0:  they could murder you.

02:26:49,122 --> 02:26:49,758
SPEAKER_0:  and

02:26:50,306 --> 02:26:52,030
SPEAKER_0:  They had the hard cudgel in their hand.

02:26:52,898 --> 02:26:54,302
SPEAKER_0:  Right, and the hard cut, Joel.

02:26:55,458 --> 02:26:56,254
SPEAKER_0:  is scary.

02:26:56,578 --> 02:26:57,342
SPEAKER_0:  and

02:26:57,986 --> 02:27:02,782
SPEAKER_0:  and you can conquer a country with a hard cudgel. But you can't use that in the US.

02:27:03,138 --> 02:27:04,542
SPEAKER_0:  So what they have is a soft cut jolt.

02:27:04,962 --> 02:27:06,142
SPEAKER_0:  which can have the same effect.

02:27:07,170 --> 02:27:09,886
SPEAKER_0:  Initially, you can scare people into shutting up.

02:27:10,146 --> 02:27:13,950
SPEAKER_0:  You can't maybe imprison them and murder them, but if you can socially ostracize them and get them fired,

02:27:14,402 --> 02:27:15,902
SPEAKER_0:  Basically, he's going to have the same effect.

02:27:16,354 --> 02:27:19,966
SPEAKER_0:  So the soft cudgel can have the same effect for a while, but the thing is...

02:27:20,898 --> 02:27:21,310
SPEAKER_0:  It's.

02:27:21,602 --> 02:27:22,910
SPEAKER_0:  It's a little bit of a house of cards.

02:27:23,138 --> 02:27:24,574
SPEAKER_0:  because it relies on fear.

02:27:25,250 --> 02:27:26,718
SPEAKER_0:  And as soon as...

02:27:27,362 --> 02:27:28,478
SPEAKER_0:  That fear goes away?

02:27:29,282 --> 02:27:30,366
SPEAKER_0:  the whole thing falls apart.

02:27:30,850 --> 02:27:33,246
SPEAKER_0:  Right? The soft culture requires...

02:27:33,570 --> 02:27:34,046
SPEAKER_0:  people to.

02:27:34,370 --> 02:27:41,246
SPEAKER_0:  be so scared of getting canceled or getting whatever. And as soon as some people start, I'm Toby Lutka of Shopify, I always like.

02:27:41,602 --> 02:27:44,734
SPEAKER_0:  Think about, you know, he just said, you know what, I'm not scared of this soft cudgel, and spoke up.

02:27:45,474 --> 02:27:49,854
SPEAKER_0:  and said, we're not political at this company and we're not a family, we're a team and we're gonna do this. You know what, like.

02:27:50,594 --> 02:27:51,719
SPEAKER_0:  They're thriving. Goodbye.

02:27:51,719 --> 02:27:55,857
SPEAKER_1:  this podcast. He seems like a fascinating human being. He's amazing. He spoke up.

02:27:55,857 --> 02:28:00,158
SPEAKER_0:  He's one of the smartest and kindest dudes, but he's also...

02:28:00,866 --> 02:28:03,742
SPEAKER_0:  He has courage at a time when it's hard, but here's the thing.

02:28:04,098 --> 02:28:08,382
SPEAKER_0:  is that it's different than that. You need so much less courage against a soft cudgel than you do.

02:28:09,026 --> 02:28:13,438
SPEAKER_0:  the Iranians throwing their hijabs into the fire. Those people's courage just-

02:28:13,794 --> 02:28:14,110
SPEAKER_0:  Just.

02:28:14,370 --> 02:28:17,790
SPEAKER_0:  blows away any courage we have here. Cause they might get executed.

02:28:18,882 --> 02:28:23,102
SPEAKER_0:  That's the thing is that you can actually have courage right now and it's so.

02:28:24,642 --> 02:28:25,534
SPEAKER_0:  ok

02:28:25,794 --> 02:28:27,669
SPEAKER_0:  Don't worry about it.

02:28:27,669 --> 02:28:28,510
SPEAKER_1:  I'm done.

02:28:29,058 --> 02:28:30,046
SPEAKER_1:  Oh man.

02:28:30,306 --> 02:28:35,582
SPEAKER_1:  The irony of that. And you talk about two things to fight this, two things, awareness and courage.

02:28:36,642 --> 02:28:37,758
SPEAKER_1:  was the awareness piece.

02:28:39,362 --> 02:28:41,022
SPEAKER_0:  The awareness piece is...

02:28:41,410 --> 02:28:42,014
SPEAKER_0:  Um...

02:28:43,234 --> 02:28:45,694
SPEAKER_0:  is under first just no understanding the stakes.

02:28:46,370 --> 02:28:56,574
SPEAKER_0:  Like getting our heads out of the sand and being like, look, technology's blowing up exponentially. Our society's trust is devolving, like we're kind of falling apart in some important ways. We're losing our grip.

02:28:56,802 --> 02:28:57,790
SPEAKER_0:  on some stability.

02:28:58,274 --> 02:28:59,134
SPEAKER_0:  at the worst time.

02:28:59,778 --> 02:29:05,662
SPEAKER_0:  That's the first point, just a big picture. And then also awareness of, I think, this vertical axis or whatever your version of it is.

02:29:05,890 --> 02:29:06,846
SPEAKER_0:  This concept of...

02:29:08,034 --> 02:29:10,526
SPEAKER_0:  How do I really form my beliefs? Where do they actually come from?

02:29:10,914 --> 02:29:14,782
SPEAKER_0:  Weird, you know, are they someone else's beliefs? Am I following a checklist?

02:29:16,802 --> 02:29:27,998
SPEAKER_0:  How about my values? You know, I used to identify with the blue party or the red party, but now they've changed and and I suddenly am okay with that. Is that because my values changed with it or am I actually anchored?

02:29:28,226 --> 02:29:29,406
SPEAKER_0:  to the party, not to any.

02:29:29,762 --> 02:29:30,270
SPEAKER_0:  principle.

02:29:30,530 --> 02:29:31,806
SPEAKER_0:  asking yourself these questions.

02:29:32,194 --> 02:29:32,638
SPEAKER_0:  Um.

02:29:33,378 --> 02:29:34,526
SPEAKER_0:  asking your, you know, look.

02:29:35,010 --> 02:29:43,998
SPEAKER_0:  looking for where do I feel disgusted by fellow human beings? Maybe I'm being a crazy tribal person without realizing it. How about the people around me? Am I being bullied by some echo chamber?

02:29:44,290 --> 02:29:45,214
SPEAKER_0:  without realizing it.

02:29:45,858 --> 02:29:47,454
SPEAKER_0:  Am I the bully somewhere?

02:29:47,714 --> 02:29:50,622
SPEAKER_0:  Right? So that's the first thing. I think just to kind of-

02:29:51,042 --> 02:29:52,030
SPEAKER_0:  Do a self audit.

02:29:52,930 --> 02:29:54,590
SPEAKER_0:  And...

02:29:55,010 --> 02:29:55,998
SPEAKER_0:  And I think that like.

02:29:56,674 --> 02:30:01,726
SPEAKER_0:  Just some awareness like that, just a self audit about these things can go a long way. But if you don't-

02:30:02,082 --> 02:30:04,542
SPEAKER_0:  If you keep it to yourself, it's almost useless.

02:30:05,634 --> 02:30:09,950
SPEAKER_0:  because if you don't have, awareness without courage does very little.

02:30:10,722 --> 02:30:11,102
SPEAKER_0:  So.

02:30:11,554 --> 02:30:16,958
SPEAKER_0:  Courage is when you take that awareness and you actually export it out into the world and it starts affecting other people.

02:30:17,282 --> 02:30:19,902
SPEAKER_0:  And so courage can happen on multiple levels. It can happen.

02:30:20,514 --> 02:30:22,846
SPEAKER_0:  by first of all, just stop saying stuff you don't believe.

02:30:23,138 --> 02:30:24,286
SPEAKER_0:  If you're being pressured by a-

02:30:24,546 --> 02:30:24,990
SPEAKER_0:  Kind of a.

02:30:25,314 --> 02:30:26,750
SPEAKER_0:  ideology or movement.

02:30:27,138 --> 02:30:29,150
SPEAKER_0:  to say stuff that you don't actually believe.

02:30:29,506 --> 02:30:34,014
SPEAKER_0:  Just stop, just, just, just stay in your ground and don't say anything. That's, that's courage. That's one first step.

02:30:34,946 --> 02:30:36,318
SPEAKER_0:  Start speaking out.

02:30:36,930 --> 02:30:39,006
SPEAKER_0:  in small groups starts, you know.

02:30:39,330 --> 02:30:43,870
SPEAKER_0:  actually speak your mind, see what happens. The sky doesn't usually fall. Actually, people usually respect you for it.

02:30:44,130 --> 02:30:48,254
SPEAKER_0:  You know, and it's not every group, but like you'd be surprised. And then eventually, you know, maybe.

02:30:49,314 --> 02:30:51,102
SPEAKER_0:  speaking out in bigger groups, start going public.

02:30:51,458 --> 02:30:55,518
SPEAKER_0:  you know, go public with it. But, and you don't need everyone doing this. Some people will lose their jobs for it.

02:30:55,778 --> 02:30:56,830
SPEAKER_0:  I'm not talking to those people.

02:30:57,250 --> 02:31:00,190
SPEAKER_0:  Most people won't lose their jobs, but they have the same fear.

02:31:01,026 --> 02:31:04,766
SPEAKER_0:  as if they would, right? And it's like, what, are you gonna get criticized or you can get a bunch of-

02:31:05,282 --> 02:31:08,478
SPEAKER_0:  people, you know, angry Twitter people will criticize you.

02:31:08,898 --> 02:31:10,142
SPEAKER_0:  Yeah, it's not pleasant.

02:31:10,370 --> 02:31:13,342
SPEAKER_0:  But actually that's a little bit like our primitive minds.

02:31:13,602 --> 02:31:14,494
SPEAKER_0:  that really

02:31:15,682 --> 02:31:16,734
SPEAKER_0:  when it was programmed.

02:31:17,186 --> 02:31:24,126
SPEAKER_0:  that kind of ostracism or criticism will leave you out of the tribe and you'll die. Today, it's kind of a delusional fear. It's not actually that scary.

02:31:24,674 --> 02:31:27,838
SPEAKER_0:  And the people who have realized that can exercise incredible leadership right now.

02:31:28,578 --> 02:31:31,518
SPEAKER_1:  So you have a really interesting description.

02:31:32,194 --> 02:31:33,246
SPEAKER_1:  of censorship.

02:31:33,666 --> 02:31:37,342
SPEAKER_1:  Also self-censorship also as you've been talking about

02:31:37,602 --> 02:31:39,230
SPEAKER_1:  Uh, who's King Mustache and-

02:31:39,682 --> 02:31:40,734
SPEAKER_1:  uh... this gap

02:31:41,090 --> 02:31:46,302
SPEAKER_1:  I think I hope you write even more, even more than you've read in the book about these ideas because it's so strong.

02:31:46,690 --> 02:31:47,262
SPEAKER_1:  this

02:31:47,586 --> 02:31:51,326
SPEAKER_1:  censorship gaps that are created between the dormant thought pile.

02:31:52,002 --> 02:31:53,182
SPEAKER_1:  And, uh...

02:31:53,474 --> 02:31:54,206
SPEAKER_1:  the kind of

02:31:54,466 --> 02:31:55,591
SPEAKER_1:  thing under the speech.

02:31:55,591 --> 02:31:56,030
SPEAKER_0:  Yeah.

02:31:56,642 --> 02:31:58,782
SPEAKER_0:  So first of all, so I like to think of.

02:31:59,682 --> 02:32:02,398
SPEAKER_0:  I think it's a useful tool, is this thing called a thought pod.

02:32:02,850 --> 02:32:07,134
SPEAKER_0:  which is if you have a, on any given issue, you have a horizontal spectrum.

02:32:07,746 --> 02:32:09,438
SPEAKER_0:  and just say I could take your brain out of your head.

02:32:09,762 --> 02:32:12,798
SPEAKER_0:  and I put it on the thought pile right where you happened to.

02:32:13,122 --> 02:32:14,462
SPEAKER_0:  believe about that issue.

02:32:14,818 --> 02:32:17,182
SPEAKER_0:  Now I did that for everyone in the community or in a society.

02:32:17,698 --> 02:32:21,342
SPEAKER_0:  and you're gonna end up with a big mushy pile that I think will often just form a bell curve.

02:32:21,602 --> 02:32:24,382
SPEAKER_0:  If it's really politicized, it might form like a camel with two humps.

02:32:24,610 --> 02:32:30,910
SPEAKER_0:  because it's like concentrated here, but for a typical issue, it'll just form a fear of AI. You're gonna have a bell curve, right? Things like this.

02:32:31,458 --> 02:32:32,286
SPEAKER_0:  That's the thought pot.

02:32:32,674 --> 02:32:34,430
SPEAKER_0:  Now the second thing is a line.

02:32:34,786 --> 02:32:37,310
SPEAKER_0:  that I call the speech curve, which is what people are saying.

02:32:37,698 --> 02:32:43,102
SPEAKER_0:  So the speech curve is high when not just a lot of people are saying it, but it's being said from the biggest platforms, being said in the.

02:32:43,586 --> 02:32:46,494
SPEAKER_0:  you know, in the New York Times and it's being said.

02:32:46,722 --> 02:32:50,590
SPEAKER_0:  by the president on the state of the union. Those things are the top of the speech curve.

02:32:51,394 --> 02:32:58,110
SPEAKER_0:  Now, and then, you know, when the speech occurs lower, it means it's being said either whispered in small groups or it's just not very many people are talking about.

02:32:58,498 --> 02:32:59,422
SPEAKER_0:  Now a healthy.

02:32:59,810 --> 02:33:03,198
SPEAKER_0:  when a free speech democracy is healthy on a certain topic?

02:33:04,354 --> 02:33:13,022
SPEAKER_0:  You've got the speech curve sitting right on top of the thought pile. They mirror each other, which is naturally what would happen. More people think something is going to be said more often and from higher platforms.

02:33:14,530 --> 02:33:22,366
SPEAKER_0:  What censorship does, and censorship can be from the government, so I use the tale of King Mustache. King Mustache is a little tiny tyrant.

02:33:22,882 --> 02:33:27,998
SPEAKER_0:  and he's very sensitive and people are making fun of his mustache and they're saying he's not a good king and he does not.

02:33:28,226 --> 02:33:31,230
SPEAKER_0:  So what does he do? He enacts a policy and he says...

02:33:31,810 --> 02:33:36,478
SPEAKER_0:  Anyone who has heard criti- anyone who has heard criticizing me or my mustache or my rule.

02:33:36,994 --> 02:33:37,950
SPEAKER_0:  will be put to death.

02:33:39,330 --> 02:33:41,822
SPEAKER_0:  and immediately at the town school because of his father.

02:33:42,082 --> 02:33:43,166
SPEAKER_0:  was very liberal.

02:33:43,458 --> 02:33:45,758
SPEAKER_0:  There was always free speech in his kingdom.

02:33:46,434 --> 02:33:51,838
SPEAKER_0:  But now King Mustad has just taken over and he's saying this is a new rules now. And so a few people yell out and they say that's not how we do things here.

02:33:52,866 --> 02:33:54,814
SPEAKER_0:  And that moment is what I call a moment of truth.

02:33:55,906 --> 02:34:01,694
SPEAKER_0:  Did the Kings guards stand with the principles of the kingdom and say, yeah, King Mustache, it's not what we do? Is

02:34:02,114 --> 02:34:03,678
SPEAKER_0:  he would kind of have to, he's not that he can do.

02:34:04,418 --> 02:34:12,574
SPEAKER_0:  or are they going to execute? So in this case, it's as if he laid down an electric fence over a part of the thought pile and said, no one's allowed to speak over here. A speech curve.

02:34:13,058 --> 02:34:16,062
SPEAKER_0:  Maybe people will think these things, but the speech curve cannot go over here.

02:34:16,930 --> 02:34:20,286
SPEAKER_0:  but the electric fence wasn't actually electrified until the Kings guards.

02:34:20,802 --> 02:34:24,446
SPEAKER_0:  in a moment of truth, get scared and say, okay, and they hang the five people who spoke out.

02:34:25,154 --> 02:34:26,046
SPEAKER_0:  So in that moment...

02:34:26,562 --> 02:34:31,230
SPEAKER_0:  That fence just became electric. And now no one criticizes King mustache anymore.

02:34:31,618 --> 02:34:35,582
SPEAKER_0:  So I use this as an allegory. Now, of course, he has a hard cudgel so he can execute people.

02:34:36,066 --> 02:34:37,726
SPEAKER_0:  But now when we look at the US.

02:34:38,338 --> 02:34:39,966
SPEAKER_0:  What you're seeing right now is a lot of pressure.

02:34:40,706 --> 02:34:44,830
SPEAKER_0:  which is very similar and electric fences being laid down saying no one can criticize these ideas.

02:34:45,474 --> 02:34:47,358
SPEAKER_0:  And if you do, you won't be executed, you'll be canceled.

02:34:47,682 --> 02:34:48,510
SPEAKER_0:  Jbi, jobb gy!

02:34:48,770 --> 02:34:49,406
SPEAKER_0:  You'll be fired.

02:34:50,210 --> 02:34:50,526
SPEAKER_0:  now.

02:34:50,786 --> 02:34:55,294
SPEAKER_0:  What is that fence electrified from there? No, they don't work at the company, they can't fire you.

02:34:56,002 --> 02:35:01,310
SPEAKER_0:  but they can start a Twitter mob when someone violates that speech curve, when someone violates that speech rule,

02:35:02,018 --> 02:35:02,590
SPEAKER_0:  And then.

02:35:03,522 --> 02:35:05,918
SPEAKER_0:  leadership at the company has the moment of truth.

02:35:06,754 --> 02:35:10,174
SPEAKER_0:  and what the leaders should do is stand up for.

02:35:10,690 --> 02:35:12,478
SPEAKER_0:  company's values, which is almost always.

02:35:12,866 --> 02:35:20,990
SPEAKER_0:  in favor of the employee and say, look, you know, even if they made a mistake, they make people make mistakes. We're not going to fire them. Or maybe that person actually said something that's reasonable and we should discuss it. But either way.

02:35:21,442 --> 02:35:22,238
SPEAKER_0:  We're not gonna fire them.

02:35:22,498 --> 02:35:34,718
SPEAKER_0:  And if they said no, what happens is the Twitter mob actually doesn't have, they can't execute you. They go away and the fence is proven to have no electricity. The problem with the past few years is what's happened again and again is the leader gets scared.

02:35:35,010 --> 02:35:37,278
SPEAKER_0:  and they get scared of the trademark when they fire them. Boom!

02:35:37,698 --> 02:35:38,974
SPEAKER_0:  That fence has electricity.

02:35:39,554 --> 02:35:41,406
SPEAKER_0:  And now, actually, if you cross that...

02:35:42,370 --> 02:35:46,142
SPEAKER_0:  It's not just a threat. Like you will have, you'll be out of a job.

02:35:46,626 --> 02:35:50,526
SPEAKER_0:  Like it's really bad, like you'll have a huge penalty. You might not be able to feed your kids.

02:35:51,170 --> 02:35:56,030
SPEAKER_0:  So that's an electric fence that goes up. Now what happens when an electric fence goes up and it's proven to actually be electrified?

02:35:56,290 --> 02:35:58,942
SPEAKER_0:  the speech curve morphs into a totally different position.

02:35:59,554 --> 02:36:05,470
SPEAKER_0:  And now these new people say, instead of having the kind of marketplace of ideas, you know, bit that turns into a kind of a natural bell curve.

02:36:05,730 --> 02:36:06,494
SPEAKER_0:  They say, no, no, no.

02:36:06,786 --> 02:36:10,014
SPEAKER_0:  These ideas are okay to say, not just okay, you'll be socially rewarded.

02:36:10,306 --> 02:36:13,694
SPEAKER_0:  and these ones don't. That's the rules of their own echo chamber that they're now applying to everyone.

02:36:13,954 --> 02:36:14,494
SPEAKER_0:  and it's working.

02:36:15,042 --> 02:36:24,126
SPEAKER_0:  And so the speech curve distorts. And so you end up with now instead of one region, which is a region of kind of active communal thinking, what people are thinking and saying, you now have three regions.

02:36:25,282 --> 02:36:34,878
SPEAKER_0:  you have a little active communal thinking, but mostly you now have this dormant thought pile, which is all these opinions that suddenly everyone's scared to say out loud. Everyone's thinking, but they're scared to say it out loud. Everyone's thinking, but no one's saying.

02:36:35,458 --> 02:36:37,470
SPEAKER_0:  And then you have this other region, which is this.

02:36:37,922 --> 02:36:41,982
SPEAKER_0:  the approved ideas of this now cultural kind of dictator.

02:36:42,978 --> 02:36:49,662
SPEAKER_0:  And those are being spoken from the largest platforms and they're being repeated by the president and they're being repeated all over the place.

02:36:49,922 --> 02:36:52,030
SPEAKER_0:  you know, even though people don't believe it.

02:36:52,546 --> 02:36:56,574
SPEAKER_0:  And that's this distortion. And what happens is the society becomes really stupid.

02:36:56,834 --> 02:37:00,190
SPEAKER_0:  because active communal thinking is the region where we can actually think together.

02:37:00,450 --> 02:37:03,134
SPEAKER_0:  and now no one can think together and it gets...

02:37:03,362 --> 02:37:05,863
SPEAKER_0:  siloed into small private conversations.

02:37:05,863 --> 02:37:15,070
SPEAKER_1:  It's really powerful what you said about institutions and so on. It's not trivial to, from a leadership position to be like, no, we, we defend the employee or defend the.

02:37:15,362 --> 02:37:18,718
SPEAKER_1:  uh... the at the employed the person with us on our

02:37:19,042 --> 02:37:19,710
SPEAKER_1:  I could win on-

02:37:20,258 --> 02:37:22,142
SPEAKER_1:  There's cause there's no actual

02:37:23,458 --> 02:37:30,110
SPEAKER_1:  uh... grounded to the any kind of violation we're hearing about some of the resist the mob it's ultimately to the leader i guess

02:37:30,530 --> 02:37:33,918
SPEAKER_1:  of a particular institution or a particular company. And it's difficult.

02:37:34,306 --> 02:37:39,582
SPEAKER_0:  Oh yeah, no, no, it's not. If it were easy, there wouldn't be all of these.

02:37:39,874 --> 02:37:42,206
SPEAKER_0:  And by the way, this is, that's the immune system failing.

02:37:42,722 --> 02:37:49,054
SPEAKER_0:  That's the liberal immune system of that company failing, but also then it's an example, which means that a lot of other, you know, it's failing to...

02:37:49,314 --> 02:37:50,430
SPEAKER_0:  kind of to the country.

02:37:50,850 --> 02:37:56,446
SPEAKER_0:  It's not easy, of course it's not, because we have primitive minds that are wired to care so much about what people think of us.

02:37:56,706 --> 02:38:02,462
SPEAKER_0:  And even if we're not gonna, you know, first of all, we're scared that it's gonna start A, because you know, what do moms do?

02:38:03,458 --> 02:38:06,878
SPEAKER_0:  They don't just say, I'm gonna criticize you. I'm gonna criticize anyone who still-

02:38:07,298 --> 02:38:07,902
SPEAKER_0:  buys your product.

02:38:08,546 --> 02:38:10,462
SPEAKER_0:  I'm gonna criticize anyone who goes on your podcast.

02:38:10,722 --> 02:38:11,486
SPEAKER_0:  So it's not just you.

02:38:11,938 --> 02:38:15,774
SPEAKER_0:  It's now suddenly, if Lex becomes tarnished enough.

02:38:16,130 --> 02:38:30,782
SPEAKER_0:  Now I go on the podcast and people are saying, oh, I'm not buying his book. He went on Lex Friedman. No, no thanks, right? And now I get, it's a call, I call it a smear web. Like you've been smeared and it's so, we're in such a bad time that it smear travels to me. And now, meanwhile, someone buys my book and tries to share it. Someone said you're buying that guy's book?

02:38:31,010 --> 02:38:32,510
SPEAKER_0:  He goes on hamburgers because he thinks he type a long note in his head and then doesn't need to. We gave up on film and sounded to quiet until we found the

02:38:32,834 --> 02:38:35,422
SPEAKER_0:  You see how this happens, right? So that hasn't happened in this case, but that-

02:38:35,682 --> 02:38:36,062
SPEAKER_0:  So.

02:38:36,610 --> 02:38:40,574
SPEAKER_0:  We are so wired, A, that is kind of bad, right? Like that is actually like...

02:38:40,866 --> 02:38:41,662
SPEAKER_0:  Bad for you, but.

02:38:41,954 --> 02:38:45,886
SPEAKER_0:  but we're wired to care about it so much because it meant life or death back in the day.

02:38:46,658 --> 02:38:52,853
SPEAKER_1:  Yeah. And luckily in this case, we're both, uh, we probably can smear each other in this.

02:38:52,853 --> 02:38:55,103
SPEAKER_0:  Yes. This is wonderful. I smear you all in the- Give a-

02:38:55,103 --> 02:38:57,534
SPEAKER_1:  given the nature of your book.

02:38:58,050 --> 02:38:58,718
SPEAKER_1:  Um...

02:38:59,810 --> 02:39:06,110
SPEAKER_1:  What do you think about freedom of speech as a term and as an idea as a way to resist the mechanism this mechanism?

02:39:06,754 --> 02:39:09,790
SPEAKER_1:  of dormant thought pile and artificially generated speech.

02:39:10,594 --> 02:39:13,969
SPEAKER_1:  this ideal of the freedom of speech and protecting speech and celebration.

02:39:13,969 --> 02:39:14,686
SPEAKER_0:  Yeah.

02:39:15,234 --> 02:39:16,510
SPEAKER_0:  Well, so this is kind of.

02:39:17,762 --> 02:39:19,646
SPEAKER_0:  the point I was talking about earlier about.

02:39:21,346 --> 02:39:22,238
SPEAKER_0:  King mustache.

02:39:23,202 --> 02:39:25,758
SPEAKER_0:  made a rule against for he's created a fish

02:39:25,986 --> 02:39:27,111
SPEAKER_0:  Can I just, I just...

02:39:27,111 --> 02:39:27,614
SPEAKER_1:  love.

02:39:27,874 --> 02:39:36,574
SPEAKER_1:  One of the amazing things about your book, as you get later and later in the book, you cover more and more difficult issues as a way to illustrate the importance of the vertical perspective.

02:39:37,090 --> 02:39:37,502
SPEAKER_1:  but...

02:39:37,730 --> 02:39:39,230
SPEAKER_1:  There's something about.

02:39:39,650 --> 02:39:41,374
SPEAKER_1:  using hilarious

02:39:41,602 --> 02:39:42,878
SPEAKER_1:  drawings throughout?

02:39:43,426 --> 02:39:44,382
SPEAKER_1:  that make it.

02:39:44,706 --> 02:39:50,462
SPEAKER_1:  much more fun and it takes you away from the personal somehow. You start thinking in the space of ideas.

02:39:50,946 --> 02:40:00,269
SPEAKER_1:  versus like outside of the tribal type of thinking. So it's a really brilliant, I mean, I would advise for any way to do, when they write controversial books, to have hilarious drawings.

02:40:00,269 --> 02:40:10,334
SPEAKER_0:  It's true, put the silly stick figure in your thing and it lightens, it does, it lightens the mood. It gets people's guard down a little bit. Yeah, yeah. And it works. It reminds people that we're all friends here, right?

02:40:10,850 --> 02:40:12,702
SPEAKER_0:  We're, you know, let's like, you know.

02:40:13,058 --> 02:40:18,622
SPEAKER_0:  laugh at ourselves, laugh at the fact that we're like in a culture war a little bit, and now we can talk about it, right, as opposed to like.

02:40:19,106 --> 02:40:20,830
SPEAKER_0:  getting religious about it, but.

02:40:21,346 --> 02:40:24,830
SPEAKER_0:  But basically, King Mustache had no First Amendment. He said, we-

02:40:25,378 --> 02:40:27,422
SPEAKER_0:  the government is censoring, right? which is

02:40:27,714 --> 02:40:30,558
SPEAKER_0:  Very common around the world, right? Government censor all them. The US?

02:40:31,010 --> 02:40:31,838
SPEAKER_0:  You know, again, there's some.

02:40:32,162 --> 02:40:33,022
SPEAKER_0:  You can argue this.

02:40:33,250 --> 02:40:35,550
SPEAKER_0:  controversial things recently, but basically the US...

02:40:36,034 --> 02:40:37,278
SPEAKER_0:  The First Amendment isn't the problem.

02:40:38,338 --> 02:40:40,670
SPEAKER_0:  Right? No one is being arrested.

02:40:40,930 --> 02:40:41,982
SPEAKER_0:  for saying the wrong thing.

02:40:42,498 --> 02:40:44,094
SPEAKER_0:  but this graph is still happening.

02:40:44,610 --> 02:40:45,150
SPEAKER_0:  And so.

02:40:46,338 --> 02:40:47,390
SPEAKER_0:  So freedom of speech.

02:40:47,938 --> 02:40:50,814
SPEAKER_0:  When people, what people like to say is.

02:40:51,170 --> 02:40:56,574
SPEAKER_0:  If someone's complaining about a cancel culture and saying, you know, this is, this is an empty free speech.

02:40:57,346 --> 02:41:01,502
SPEAKER_0:  People like to point out, no it's not. The government's not arresting you for anything. This is called like-

02:41:01,858 --> 02:41:02,238
SPEAKER_0:  You know?

02:41:02,530 --> 02:41:04,798
SPEAKER_0:  The free market, buddy. Like this is called.

02:41:05,506 --> 02:41:11,070
SPEAKER_0:  you're putting your ideas out and you're getting criticized and your precious marketplace of ideas, there it is, right? I've gotten this a lot.

02:41:12,002 --> 02:41:14,686
SPEAKER_0:  And this is not making a critical distinction between...

02:41:14,946 --> 02:41:16,702
SPEAKER_0:  cancel culture and criticism culture.

02:41:17,506 --> 02:41:17,886
SPEAKER_0:  Um.

02:41:19,234 --> 02:41:20,318
SPEAKER_0:  Criticism culture.

02:41:20,642 --> 02:41:22,654
SPEAKER_0:  is a little bit of this kind of.

02:41:23,234 --> 02:41:24,830
SPEAKER_0:  high-rank idea lab stuff we talked about.

02:41:25,122 --> 02:41:26,910
SPEAKER_0:  Criticism culture attacks the idea.

02:41:27,810 --> 02:41:29,182
SPEAKER_0:  and

02:41:29,602 --> 02:41:30,110
SPEAKER_0:  and

02:41:30,402 --> 02:41:31,454
SPEAKER_0:  encourages

02:41:31,778 --> 02:41:32,350
SPEAKER_0:  further.

02:41:33,282 --> 02:41:35,614
SPEAKER_0:  discussion, right? In live in this discussion.

02:41:36,258 --> 02:41:37,150
SPEAKER_0:  It makes everyone smarter.

02:41:38,370 --> 02:41:40,030
SPEAKER_0:  Cancel culture attacks the person.

02:41:40,610 --> 02:41:41,118
SPEAKER_0:  Very different.

02:41:41,506 --> 02:41:44,542
SPEAKER_0:  Can't criticism culture says here's why this idea is so bad. Let me tell you.

02:41:44,898 --> 02:41:49,438
SPEAKER_0:  Cancer culture says here's why this person is bad and no one should talk to them and they should be fired.

02:41:50,146 --> 02:41:57,822
SPEAKER_0:  And what does that do? It doesn't enliven the discussion. It makes everyone scared to talk and it's the opposite. It shuts down discussion. So you still have your First Amendment.

02:41:58,050 --> 02:42:03,166
SPEAKER_0:  But First Amendment plus cancel culture equals, you might as well be in King Must, you might as well have government censorship.

02:42:03,778 --> 02:42:04,126
SPEAKER_0:  Right?

02:42:04,802 --> 02:42:08,990
SPEAKER_0:  First Amendment plus criticism culture, great. Now you have this vibrant marketplace of ideas.

02:42:09,282 --> 02:42:09,726
SPEAKER_0:  So.

02:42:10,530 --> 02:42:11,102
SPEAKER_0:  There's a

02:42:11,458 --> 02:42:12,766
SPEAKER_0:  Very clear difference.

02:42:13,154 --> 02:42:23,006
SPEAKER_0:  And so when people criticize the cancel culture and then someone says, oh, see, you're so sensitive. Now, you're doing the cancel culture yourself. You're trying to punish this person for criticizing. No, no, no, no, no.

02:42:23,554 --> 02:42:31,614
SPEAKER_0:  Every good liberal, and I mean that in the lower case, which is that anyone who believes in liberal democracies, regardless of what they believe should stand up,

02:42:31,874 --> 02:42:32,446
SPEAKER_0:  and say no.

02:42:32,674 --> 02:42:36,670
SPEAKER_0:  cancel culture and say this is not okay regardless of what the actual topic is.

02:42:37,154 --> 02:42:38,974
SPEAKER_0:  and that makes them a good liberal.

02:42:39,234 --> 02:42:40,958
SPEAKER_0:  versus if they're trying to cancel a-

02:42:41,186 --> 02:42:47,134
SPEAKER_0:  someone who's just criticizing, they're doing the opposite. Now they're shutting, so it's the opposite thing, but it's very easy to get confused. You can see.

02:42:47,810 --> 02:42:54,942
SPEAKER_0:  People take advantage of the, and sometimes they just don't know it themselves. The lines here can be very confusing. The wording can be very confusing and be-

02:42:55,170 --> 02:42:59,806
SPEAKER_0:  without that wording, suddenly it looks like someone who's criticizing cancel culture is

02:43:00,450 --> 02:43:01,214
SPEAKER_0:  canceling but they're not.

02:43:02,210 --> 02:43:05,662
SPEAKER_1:  you uh... applied this thinking to universities in particular

02:43:06,082 --> 02:43:06,558
SPEAKER_1:  Um.

02:43:07,298 --> 02:43:08,830
SPEAKER_1:  that there's a great...

02:43:09,890 --> 02:43:14,430
SPEAKER_1:  yet another great image on the trade off between knowledge and conviction.

02:43:14,850 --> 02:43:19,614
SPEAKER_1:  And it's what's commonly, actually you can maybe explain to me the difference, but use.

02:43:19,938 --> 02:43:22,334
SPEAKER_1:  It's often referred to as the Dunning-Kruger effect.

02:43:22,818 --> 02:43:26,078
SPEAKER_1:  where you, uh, when you first learn of a thing, you have an extremely-

02:43:27,042 --> 02:43:28,126
SPEAKER_1:  confidence about.

02:43:28,482 --> 02:43:30,110
SPEAKER_1:  self-estimation of how

02:43:30,338 --> 02:43:31,550
SPEAKER_1:  Well, you understand that thing.

02:43:31,842 --> 02:43:33,717
SPEAKER_1:  You actually say that down in cougar means something.

02:43:33,717 --> 02:43:41,214
SPEAKER_0:  else. So yeah, it's everyone I post this everyone's like Dunning Kruger and it's and it's what everyone thinks Dunning Kruger is. Dunning Kruger is a little different. It's

02:43:41,570 --> 02:43:42,270
SPEAKER_0:  it's you have a.

02:43:42,690 --> 02:43:44,638
SPEAKER_0:  diagonal line like this one, right, which is.

02:43:45,026 --> 02:43:51,934
SPEAKER_0:  the place you are, it's the, I call it like the humility tightrope, it's the humility sweet spot. It's exactly the right level of humility based on what you know.

02:43:52,386 --> 02:44:03,710
SPEAKER_0:  If you're below it, you're insecure. You actually have too much humility. You don't have enough confidence because you know more than you're giving yourself credit for. And when you're above the line, you're in the arrogant zone, right? You need a dose of humility, right? You think you know more than you do.

02:44:04,258 --> 02:44:10,782
SPEAKER_0:  So y'all wanna stay on that tight rope. And Dunning-Kruger is basically a straight line that's just a, has a lower slope. So you start off.

02:44:11,042 --> 02:44:14,526
SPEAKER_0:  You still are getting more confident as you go along.

02:44:15,074 --> 02:44:15,582
SPEAKER_0:  But.

02:44:16,002 --> 02:44:16,702
SPEAKER_0:  You.

02:44:17,378 --> 02:44:28,863
SPEAKER_0:  start off above that line and as you learn more, you end up below the line later. So, but anyway. So this wavy thing. This wavy thing is a different phenomenon. And it's related, but.

02:44:28,863 --> 02:44:31,518
SPEAKER_1:  this idea so to people just listening

02:44:31,938 --> 02:44:32,542
SPEAKER_1:  Um...

02:44:32,994 --> 02:44:34,238
SPEAKER_1:  There's a child's hill.

02:44:34,786 --> 02:44:38,846
SPEAKER_1:  Pretty damn sure you know a whole lot about it that's in the beginning.

02:44:39,074 --> 02:44:40,990
SPEAKER_1:  And then there's an insecure canyon.

02:44:41,218 --> 02:44:42,078
SPEAKER_1:  CRASH DOWN!

02:44:42,594 --> 02:44:46,014
SPEAKER_1:  acknowledging that you don't know that much and then there's a growth mountain

02:44:46,754 --> 02:44:47,518
SPEAKER_1:  grown-up mount.

02:44:47,906 --> 02:44:48,574
SPEAKER_1:  grown up mouth.

02:44:49,314 --> 02:44:50,526
SPEAKER_1:  uh... where

02:44:50,882 --> 02:44:54,078
SPEAKER_1:  after you feel ashamed and embarrassed about not knowing that much

02:44:54,402 --> 02:45:02,302
SPEAKER_1:  you begin to realize that knowing how little you know is the first step in becoming someone who actually knows stuff and that's the grown up mom.

02:45:03,170 --> 02:45:04,926
SPEAKER_1:  and you climb and climb and climb.

02:45:05,346 --> 02:45:07,710
SPEAKER_1:  uh... you're saying that in universities work

02:45:08,002 --> 02:45:08,958
SPEAKER_1:  pinning people.

02:45:09,922 --> 02:45:11,047
SPEAKER_1:  at the top of the child.

02:45:11,047 --> 02:45:13,310
SPEAKER_0:  So for me, this is a very...

02:45:13,730 --> 02:45:18,814
SPEAKER_0:  I think of myself with this because I went to college, like a lot of 18 year olds and I was.

02:45:19,394 --> 02:45:20,158
SPEAKER_0:  Very cocky.

02:45:20,386 --> 02:45:21,854
SPEAKER_0:  I just thought I knew it.

02:45:22,082 --> 02:45:25,118
SPEAKER_0:  And when it came to politics, I was like bright blue.

02:45:25,410 --> 02:45:29,726
SPEAKER_0:  just because I grew up in a bright blue suburb and I wasn't thinking that hard about it and I thought that, you know.

02:45:29,954 --> 02:45:38,878
SPEAKER_0:  And what I did when I went to college is met a lot of smart conservatives and a lot of smart progressives. But I've met a lot of people who weren't just going down a checklist and they knew stuff.

02:45:39,682 --> 02:45:44,734
SPEAKER_0:  And it's suddenly I realized that a lot of these views I have are not based on knowledge.

02:45:45,986 --> 02:45:47,134
SPEAKER_0:  they're based on.

02:45:47,586 --> 02:45:50,366
SPEAKER_0:  other people's conviction. Everyone else thinks that's true, which now I think it's-

02:45:50,594 --> 02:45:50,942
SPEAKER_0:  Well.

02:45:51,170 --> 02:45:52,830
SPEAKER_0:  I'm actually like...

02:45:53,634 --> 02:45:59,710
SPEAKER_0:  I'm transferring someone else's conviction to me and who knows why they have conviction? They might have conviction or transferring from someone else.

02:46:00,066 --> 02:46:03,134
SPEAKER_0:  And I'm a smart dude, I thought. Why-why am I-

02:46:04,610 --> 02:46:05,470
SPEAKER_0:  Why am I like?

02:46:05,954 --> 02:46:07,838
SPEAKER_0:  giving away my own independent...

02:46:08,450 --> 02:46:08,862
SPEAKER_0:  You know.

02:46:09,154 --> 02:46:09,662
SPEAKER_0:  Learning.

02:46:10,434 --> 02:46:19,262
SPEAKER_0:  abilities here and just adopting other views. So anyway, it was this humbling experience. And it wasn't just about politics, by the way. It was that I had strong views about a lot of stuff and-

02:46:19,554 --> 02:46:22,302
SPEAKER_0:  I just, I got lucky, or not lucky, I sought out.

02:46:22,946 --> 02:46:26,686
SPEAKER_0:  the kind of people I sought out were the type that loved to disagree and they were.

02:46:27,010 --> 02:46:27,902
SPEAKER_0:  Man, they knew stuff.

02:46:29,122 --> 02:46:40,798
SPEAKER_0:  And so you're quickly in, you know, in again, an idea lab culture, it was an idea lab. And also, I also went to, I started getting in the habit. I started loving listening to people who disagreed with me because it was so exhilarating listening to a smart person. When I thought there was no.

02:46:41,154 --> 02:46:46,686
SPEAKER_0:  no credence to this other argument, right? this side of this debate is obviously wrong.

02:46:46,946 --> 02:46:51,902
SPEAKER_0:  I wanted to see an Intelligence Squared on that debate in between. I wanted to go see, how she got into Intelligence Squared in college.

02:46:52,226 --> 02:46:53,310
SPEAKER_0:  I wanted to see.

02:46:53,666 --> 02:46:54,942
SPEAKER_0:  A smart person.

02:46:55,234 --> 02:47:03,102
SPEAKER_0:  who disagrees with me talking became so fascinating to me. Right, it was the most interesting thing. That was a new thing. I didn't think I liked that. And so what did that do? Dat dat!

02:47:03,650 --> 02:47:12,990
SPEAKER_0:  shoved me down the humble tumble here, number three. It shoved me down where I started to, and then I went the other way, where I realized that I had been, a lot of my identity had been based on this.

02:47:13,378 --> 02:47:22,270
SPEAKER_0:  faux feeling of knowledge, this idea that I thought I knew everything. Now that I don't have that, I was like, I felt really like dumb, and I felt really almost like embarrassed of what I knew.

02:47:22,690 --> 02:47:28,542
SPEAKER_0:  And so that's where I call this insecure canyon. I think it's sometimes when you're so used to thinking, you know everything and then you realize you don't, it's like it's.

02:47:28,866 --> 02:47:29,342
SPEAKER_0:  And then.

02:47:29,570 --> 02:47:36,126
SPEAKER_0:  you start to realize that actually really awesome thinkers, they don't judge me for this, they totally respect if I say.

02:47:36,354 --> 02:47:40,670
SPEAKER_0:  I don't know anything about this. They say, oh cool, you should read this and this and this. They don't say, you don't know anything. They don't say that.

02:47:41,218 --> 02:47:42,078
SPEAKER_0:  Right? And so.

02:47:42,850 --> 02:47:50,238
SPEAKER_0:  And not that I'm, by the way, this is not to say I'm now on grownup mountain and you should all join me. I often find myself drifting up with like a helium balloon. Oh-

02:47:50,498 --> 02:47:54,334
SPEAKER_0:  I think I read about the new thing and suddenly I think I have, I think I, you know.

02:47:54,690 --> 02:47:59,934
SPEAKER_0:  I read three things about, you know, a new AI thing, and I'm like, I'll go do a talk on this. And I'm like, no, I won't.

02:48:00,610 --> 02:48:05,150
SPEAKER_0:  I'm gonna just be spouting out the opinion of the person I just read, so I have to remind myself.

02:48:05,570 --> 02:48:08,638
SPEAKER_0:  but it's useful. What the reason my problem with colleges today.

02:48:09,474 --> 02:48:10,014
SPEAKER_0:  is that.

02:48:10,754 --> 02:48:13,822
SPEAKER_0:  I graduated in 2004. This is a recent change.

02:48:14,594 --> 02:48:15,038
SPEAKER_0:  is that.

02:48:16,482 --> 02:48:20,190
SPEAKER_0:  All of those speakers I went who disagreed with me, a lot of them were conservative.

02:48:20,770 --> 02:48:23,454
SPEAKER_0:  So many of those speakers would not be allowed on campuses today.

02:48:23,746 --> 02:48:27,486
SPEAKER_0:  And so many of the discussions I had were in big groups or classrooms.

02:48:27,874 --> 02:48:29,470
SPEAKER_0:  And this is still, you know, this was a liberal.

02:48:29,762 --> 02:48:30,302
SPEAKER_0:  campus.

02:48:31,010 --> 02:48:32,830
SPEAKER_0:  so many of those disagreements.

02:48:33,122 --> 02:48:33,662
SPEAKER_0:  Um.

02:48:34,370 --> 02:48:40,638
SPEAKER_0:  They're not happening today and you I've interviewed a ton of college students. It's chilly. It is, you know people keep to themselves

02:48:41,154 --> 02:48:46,846
SPEAKER_0:  So what's happening is not only are people losing that push off Child's Hill, which was so valuable to me.

02:48:47,106 --> 02:48:48,510
SPEAKER_0:  so valuable to me as a thinker.

02:48:48,994 --> 02:48:50,942
SPEAKER_0:  It kind of started my life as a better thinker.

02:48:51,586 --> 02:49:01,694
SPEAKER_0:  They're losing that, but actually what college, a lot of the college classes and the vibe in colleges, a lot of what it is now saying that there is one right set of views and it's this kind of woke ideology.

02:49:02,050 --> 02:49:02,398
SPEAKER_0:  Um.

02:49:02,658 --> 02:49:09,406
SPEAKER_0:  and it's right, and anyone who disagrees with it is bad, and don't speak up unless you're gonna agree with it.

02:49:09,666 --> 02:49:10,014
SPEAKER_0:  It's.

02:49:10,338 --> 02:49:11,198
SPEAKER_0:  teaching people.

02:49:11,650 --> 02:49:14,686
SPEAKER_0:  that Child's Hills, that, you know, it's nailing people's feet to Child's Hills.

02:49:14,914 --> 02:49:16,734
SPEAKER_0:  It's teaching people that these are right.

02:49:16,962 --> 02:49:18,046
SPEAKER_0:  This user right and like.

02:49:18,338 --> 02:49:22,174
SPEAKER_0:  You don't have any, you're nothing to, you should feel a complete conviction about this.

02:49:24,226 --> 02:49:24,894
SPEAKER_1:  How do we fix it?

02:49:26,530 --> 02:49:29,982
SPEAKER_1:  Is it part of the administration? Is it part of the culture? Is it part of the...

02:49:30,434 --> 02:49:31,134
SPEAKER_1:  uh...

02:49:31,490 --> 02:49:36,318
SPEAKER_1:  is a part of like actually instilling in the individual like 18 year olds, the idea that this is

02:49:37,090 --> 02:49:41,502
SPEAKER_1:  The beautiful way to live is to embrace the disagreement and the growth from that.

02:49:41,762 --> 02:49:43,038
SPEAKER_0:  It's awareness and courage.

02:49:43,458 --> 02:49:47,838
SPEAKER_0:  It's the same thing. First of all, awareness is people need to see.

02:49:48,834 --> 02:49:49,630
SPEAKER_0:  What's happening here?

02:49:50,626 --> 02:49:56,702
SPEAKER_0:  Kids are getting, losing the, they're not going to college and becoming better, tougher, more robust thinkers. Kids are getting, losing the, they're not going to college

02:49:57,698 --> 02:50:02,910
SPEAKER_0:  They're actually going to college and becoming zealots. They're getting taught to be zealots and the website still advertises.

02:50:03,458 --> 02:50:20,830
SPEAKER_0:  you know, wide variety of, you know, the website is a bait and switch. You list all the universities, yeah, Harvard. It's a bait and switch. It's still saying, here you're coming here for a wide intellectual, basically they're advertising, this is an idea lab and you get there and it's like, actually it's an echo chamber that you're paying money for. So if people realize that, they start to get mad hopefully.

02:50:21,634 --> 02:50:22,142
SPEAKER_0:  And then.

02:50:22,370 --> 02:50:27,774
SPEAKER_0:  I mean, yes, brave students. There's been some very brave students who have started, you know,

02:50:28,258 --> 02:50:30,878
SPEAKER_0:  big think clubs and stuff like that where it's like we're gonna have

02:50:31,266 --> 02:50:31,614
SPEAKER_0:  You know?

02:50:32,386 --> 02:50:35,678
SPEAKER_0:  to present both sides of a debate here. And that takes courage, but also...

02:50:35,970 --> 02:50:36,350
SPEAKER_0:  Um.

02:50:36,578 --> 02:50:43,198
SPEAKER_0:  and leadership. It's like if you look at these colleges, it's specifically the leaders.

02:50:44,066 --> 02:50:45,694
SPEAKER_0:  Who shows strength?

02:50:46,626 --> 02:50:47,902
SPEAKER_0:  who get the best results.

02:50:48,834 --> 02:50:52,382
SPEAKER_0:  Remember, the cudgel is soft, so if a leader of one of these places.

02:50:52,610 --> 02:50:53,150
SPEAKER_0:  says.

02:50:53,410 --> 02:50:57,214
SPEAKER_0:  you know, the college presidents who have shown some strength.

02:50:57,954 --> 02:51:00,958
SPEAKER_0:  they actually don't get as much trouble. It's the ones who pander, the ones who...

02:51:01,506 --> 02:51:02,046
SPEAKER_0:  Um,

02:51:03,330 --> 02:51:03,710
SPEAKER_0:  Ah.

02:51:03,938 --> 02:51:06,110
SPEAKER_0:  in that moment of truth, they...

02:51:06,626 --> 02:51:08,382
SPEAKER_0:  they shrink away, then...

02:51:09,538 --> 02:51:11,230
SPEAKER_0:  they get a lot more trouble, the mob smells blood.

02:51:12,034 --> 02:51:12,958
SPEAKER_1:  For the listener...

02:51:13,250 --> 02:51:13,822
SPEAKER_1:  Uhhh

02:51:14,338 --> 02:51:18,782
SPEAKER_1:  the podcast favorite Liv Burry just entered and your friend just entered the room.

02:51:19,362 --> 02:51:20,926
SPEAKER_1:  Do you mind if she joins us?

02:51:21,154 --> 02:51:23,806
SPEAKER_1:  I think there's a story she has about you.

02:51:24,770 --> 02:51:25,406
SPEAKER_1:  So live.

02:51:26,114 --> 02:51:28,350
SPEAKER_1:  You mentioned something that there's a funny story about.

02:51:28,642 --> 02:51:30,462
SPEAKER_1:  We haven't talked at all about the actual.

02:51:30,914 --> 02:51:32,062
SPEAKER_1:  process of writing the book.

02:51:33,346 --> 02:51:35,262
SPEAKER_1:  Is there you guys made a bet of some kind?

02:51:36,098 --> 02:51:37,342
SPEAKER_1:  Yeah.

02:51:37,570 --> 02:51:39,445
SPEAKER_1:  Is this a true story? Is this a complete...

02:51:39,445 --> 02:51:41,150
SPEAKER_0:  false fabricates. It's true.

02:51:41,826 --> 02:51:42,430
SPEAKER_0:  Liviz.

02:51:43,298 --> 02:51:46,206
SPEAKER_0:  She's mean, which you, I didn't, I did not know mean live.

02:51:46,690 --> 02:51:47,262
SPEAKER_0:  She's like...

02:51:47,618 --> 02:51:52,766
SPEAKER_0:  She's like a bully, she's like scary. I have to have that screenshot. So Liv was FaceTiming me and she was like...

02:51:53,762 --> 02:51:58,590
SPEAKER_0:  She was like being intimidating. I took a screenshot and I made it my phone background. Every time I opened it I was like, ah.

02:51:58,914 --> 02:52:01,342
SPEAKER_0:  So to give the background of this, it's because...

02:52:01,602 --> 02:52:05,022
SPEAKER_0:  if you hadn't noticed. Tim started writing this book, how many years ago? Six?

02:52:05,250 --> 02:52:06,142
SPEAKER_0:  2016.

02:52:06,402 --> 02:52:07,806
SPEAKER_0:  Right. Mid 2016.

02:52:08,034 --> 02:52:14,174
SPEAKER_0:  Right, as sort of a response to like the Trump stuff and... Not even, yeah, it was just supposed to be a mini-post. I was like, oh, I'm so like...

02:52:15,106 --> 02:52:26,942
SPEAKER_0:  I was like, I'm looking at all these like future tech things and I feel this like uneasiness, like, ah, we're gonna like mess up all these things. Why? There's like some cloud over our society. Let me just write a mini post and I opened it up to WordPress to write a one day little essay.

02:52:27,842 --> 02:52:31,326
SPEAKER_0:  And things went on politics. It was gonna be on like-

02:52:31,618 --> 02:52:34,942
SPEAKER_0:  This feeling I had that...

02:52:35,202 --> 02:52:35,742
SPEAKER_0:  Um,

02:52:36,098 --> 02:52:36,990
SPEAKER_0:  We were.

02:52:37,218 --> 02:52:45,118
SPEAKER_0:  our tech was just growing and growing and we were becoming less wise. What's up with that? And I just wanted to write like, just like a little like little thousand word essay on like.

02:52:45,378 --> 02:52:48,990
SPEAKER_0:  Something I think we should pay attention to when that was the beginning of this six year nightmare.

02:52:49,826 --> 02:52:52,830
SPEAKER_1:  Did you anticipate the blog post would take a long while?

02:52:54,978 --> 02:53:01,246
SPEAKER_0:  I don't remember the process fully in terms of, I remember you saying, oh, I'm actually writing this, it's turning into a bigger thing. And I was like, hmm.

02:53:01,538 --> 02:53:05,054
SPEAKER_0:  You know, because the more we talked about it, I remember we were talking about it, I was like, oh, this goes deep.

02:53:05,378 --> 02:53:10,014
SPEAKER_0:  because I didn't really understand the full scope of the situation, like nowhere near it.

02:53:10,370 --> 02:53:19,934
SPEAKER_0:  You sort of explained it and I was like, okay, yeah, I see that. And then the more we dug into it, the sort of the deeper and deeper and deeper it went. But no, I did not anticipate it would be six years. Let's put it that way. And it's a tunicel peak.

02:53:20,066 --> 02:53:22,046
SPEAKER_1:  When was your Ted talk on procrastination?

02:53:22,306 --> 02:53:22,814
SPEAKER_0:  So that was.

02:53:23,458 --> 02:53:26,206
SPEAKER_0:  That was March of 2016 and I started this book.

02:53:26,530 --> 02:53:27,294
SPEAKER_0:  three months later.

02:53:27,682 --> 02:53:34,366
SPEAKER_0:  and fell into the biggest procrastination hole that I've ever fallen into. Oh wow. And he isn't lost on me.

02:53:34,850 --> 02:53:35,294
SPEAKER_0:  It's.

02:53:35,618 --> 02:53:42,942
SPEAKER_0:  I just like how much credit I have for that TED talk. I'm like, I am legit procrastinator. I'm not just saying it.

02:53:43,554 --> 02:53:44,638
SPEAKER_0:  But it wasn't just us. Sure.

02:53:44,930 --> 02:53:53,790
SPEAKER_0:  Because I mean, you did intend it to start out as a blog post, but then you're like, actually, this needs to be multiple. Actually, let's make it into a full series. You know what, I'll turn it into a book.

02:53:54,178 --> 02:53:59,294
SPEAKER_0:  And then, that's what... And what also, what Liv witnessed a few times, and my wife is witnessing...

02:53:59,586 --> 02:54:01,118
SPEAKER_0:  30 of these is like these.

02:54:01,762 --> 02:54:03,294
SPEAKER_0:  these 180 epiphanies.

02:54:03,906 --> 02:54:04,734
SPEAKER_0:  where I'll be like.

02:54:05,602 --> 02:54:13,726
SPEAKER_0:  I'll have a moment when I'm, and I don't know what, sometimes it's that there's a really good idea, sometimes it's like I'm just dreading having to finish this the way it is.

02:54:14,082 --> 02:54:18,366
SPEAKER_0:  And so this epiphanies where it's like, you know what, I need to start over from the beginning and just make this like a short, like.

02:54:18,786 --> 02:54:19,358
SPEAKER_0:  20.

02:54:19,586 --> 02:54:21,566
SPEAKER_0:  little blog post list and then I'll.

02:54:21,922 --> 02:54:24,190
SPEAKER_0:  And I was like, no, no, no, I have like a new epiphany.

02:54:24,578 --> 02:54:28,318
SPEAKER_0:  And it's these, and yeah, it's kind of like the crazy person a little bit.

02:54:28,642 --> 02:54:33,822
SPEAKER_0:  But anyway, can I tell the story of the bed? Go for it. All right, so things came to a head.

02:54:34,114 --> 02:54:35,550
SPEAKER_0:  when we were in...

02:54:35,842 --> 02:54:37,694
SPEAKER_0:  We're all on vacation in the Dominican Republic.

02:54:38,146 --> 02:54:39,998
SPEAKER_0:  Tim and his wife, me and ego.

02:54:40,770 --> 02:54:41,694
SPEAKER_0:  and...

02:54:42,082 --> 02:54:43,102
SPEAKER_0:  We were in the ocean.

02:54:43,426 --> 02:54:47,678
SPEAKER_0:  And I remember you'd been in the ocean for like an hour just bobbing in there becoming that object..

02:54:48,130 --> 02:54:51,358
SPEAKER_0:  And we got talking and we were talking about the book and...

02:54:52,770 --> 02:54:54,846
SPEAKER_0:  You know, you were expressing just like this.

02:54:55,586 --> 02:54:59,102
SPEAKER_0:  You know, just the horror of the situation, basically. Like, look, I just-

02:54:59,778 --> 02:55:02,814
SPEAKER_0:  I'm so close, but there's still this and then there's this and...

02:55:03,298 --> 02:55:03,806
SPEAKER_0:  Um.

02:55:04,322 --> 02:55:07,678
SPEAKER_0:  an idea popped into my head, which is that, you know, poker players often...

02:55:08,098 --> 02:55:12,062
SPEAKER_0:  We will set ourselves like negative bets.

02:55:12,290 --> 02:55:13,150
SPEAKER_0:  You know, like, uh.

02:55:13,698 --> 02:55:15,966
SPEAKER_0:  essentially if we don't get a job done, then we have to.

02:55:16,386 --> 02:55:20,830
SPEAKER_0:  do something we really don't want to do. So instead of having a carrot, like a really, really big stick.

02:55:21,314 --> 02:55:23,518
SPEAKER_0:  Um, so I had the idea to ask Tim. Okay.

02:55:26,082 --> 02:55:29,630
SPEAKER_0:  What is the worst, either organization or individual?

02:55:30,114 --> 02:55:33,502
SPEAKER_0:  uh, that you, if you had to, you know, that you would

02:55:33,730 --> 02:55:35,966
SPEAKER_0:  loath to give a large sum of money to.

02:55:36,418 --> 02:55:38,814
SPEAKER_0:  and he thought about it for a little while and he gave his answer.

02:55:40,098 --> 02:55:41,054
SPEAKER_0:  And I was like, all right.

02:55:41,794 --> 02:55:42,590
SPEAKER_0:  What's your net worth?

02:55:43,074 --> 02:55:43,998
SPEAKER_0:  He said, isn't that worth?

02:55:44,226 --> 02:55:44,606
SPEAKER_0:  Alright.

02:55:44,962 --> 02:55:47,774
SPEAKER_0:  10% of your net worth to that thing if you don't get.

02:55:48,322 --> 02:55:50,270
SPEAKER_0:  the draft, because oh, sorry, just before that I'd asked him.

02:55:50,946 --> 02:55:53,150
SPEAKER_0:  how long, like if you had a gun to your head.

02:55:53,410 --> 02:55:54,302
SPEAKER_0:  onto your wife's head.

02:55:55,074 --> 02:56:00,766
SPEAKER_0:  and you had to get the book into a state where you could send off a draft to your editor.

02:56:01,410 --> 02:56:05,534
SPEAKER_0:  how long?" And he's like, oh, I guess like I could get it like 95% good in a month. I was like, okay, great.

02:56:06,082 --> 02:56:07,070
SPEAKER_0:  in one month's time.

02:56:07,426 --> 02:56:08,350
SPEAKER_0:  if you do not.

02:56:08,802 --> 02:56:17,534
SPEAKER_0:  have that edit handed in. See how scared by this? They see this. Handed in. Really scary. 10% of your net worth is going to this thing that you really, really think is terrible.

02:56:17,954 --> 02:56:18,750
SPEAKER_0:  but you're forgetting the kicker.

02:56:20,226 --> 02:56:21,982
SPEAKER_0:  The kicker was that...

02:56:22,434 --> 02:56:22,814
SPEAKER_0:  You know.

02:56:23,266 --> 02:56:25,854
SPEAKER_0:  For Casper Nader's they self-defeat, that's what they do.

02:56:26,338 --> 02:56:26,878
SPEAKER_0:  and then lifts his.

02:56:27,394 --> 02:56:28,254
SPEAKER_0:  I'm gonna sweeten the deal.

02:56:29,922 --> 02:56:31,486
SPEAKER_0:  I am going to.

02:56:31,714 --> 02:56:33,278
SPEAKER_0:  basically match you.

02:56:34,178 --> 02:56:34,846
SPEAKER_0:  and

02:56:35,298 --> 02:56:37,854
SPEAKER_0:  I'm going to put in, I'm going to send.

02:56:38,178 --> 02:56:41,118
SPEAKER_0:  This is a huge amount of my own money there if you don't do it.

02:56:41,474 --> 02:56:42,910
SPEAKER_0:  So, and I can't, that's.

02:56:43,330 --> 02:56:44,455
SPEAKER_0:  That would be really bad enough.

02:56:44,455 --> 02:56:46,705
SPEAKER_1:  Not only are you screwing yourself, you're screwing a friend.

02:56:46,705 --> 02:56:49,182
SPEAKER_0:  And she was like, and as your friend.

02:56:49,474 --> 02:56:51,102
SPEAKER_0:  Because I'm your friend, I will.

02:56:51,490 --> 02:56:51,838
SPEAKER_0:  Send it.

02:56:52,322 --> 02:56:53,406
SPEAKER_0:  I will send the money.

02:56:55,682 --> 02:56:56,670
SPEAKER_0:  I mean, like...

02:56:56,898 --> 02:56:59,902
SPEAKER_0:  that, you know, like tyranny.

02:57:00,610 --> 02:57:01,438
SPEAKER_0:  and um...

02:57:03,394 --> 02:57:04,254
SPEAKER_0:  I got the draft in.

02:57:04,578 --> 02:57:12,254
SPEAKER_0:  Just! I know, well I was- Ego could have tested this. Actually, it was funny, because it was supposed to be by the summer solstice or whatever it was. A certain date!? Subscribe...

02:57:13,218 --> 02:57:16,926
SPEAKER_0:  And I got it in at four AM.

02:57:17,282 --> 02:57:29,950
SPEAKER_0:  like the next morning, but then and and and and they were both like, that doesn't count. I'm like, it does. It's still for me. It's the same day still. It's OK. Can you imagine how fucked in the head you have to be? Yeah, so like literally technically passed the deadline by four hours.

02:57:30,338 --> 02:57:33,790
SPEAKER_0:  I mean, for an obscene amount of money to a thing you loathe.

02:57:34,082 --> 02:57:38,654
SPEAKER_0:  That's how bad his sickness is. Because I knew the hard deadline. I knew that...

02:57:38,882 --> 02:57:43,294
SPEAKER_0:  there was no way she was going to actually send that money because it was 4am so I knew I actually had the whole

02:57:43,970 --> 02:57:46,846
SPEAKER_0:  I should actually punish you and just I should send like

02:57:47,170 --> 02:57:48,702
SPEAKER_0:  nominal amount to that thing.

02:57:48,962 --> 02:57:50,526
SPEAKER_0:  No thanks. No.

02:57:51,202 --> 02:57:58,078
SPEAKER_1:  But yeah. Is there some micro like lessons from that, from how to avoid procrastination writing a book that you've learned?

02:57:58,242 --> 02:58:05,566
SPEAKER_0:  Yes, well, I've learned a lot of things. I mean, like, first, don't take don't write like a dissertation about like proving some grand theory of society because that's

02:58:06,722 --> 02:58:15,646
SPEAKER_0:  really procrastinating. Like I would have been an awful PhD student for that reason. And so like, I'm gonna do another book and it's gonna be like a bunch of short chapters that are one-offs, because that's like.

02:58:15,874 --> 02:58:16,478
SPEAKER_0:  It just doesn't.

02:58:16,898 --> 02:58:21,023
SPEAKER_1:  feed into your book is like a giant like framework. There is grand theories all through you.

02:58:21,023 --> 02:58:22,718
SPEAKER_0:  I know and I learned not to do that again.

02:58:23,330 --> 02:58:37,918
SPEAKER_0:  I did it once, I don't wanna do it again. Oh, with the book was a mistake. So the book is a giant mistake. Yes, don't do another one of this. Look, some people should, it's just not for me. You just did it. I know, and it almost killed me. Okay, so that's the first one, but secondly, yeah, like.

02:58:38,242 --> 02:58:49,662
SPEAKER_0:  Basically, there's two ways to fix procrastination. One is you fix, it's like a picture you have a boat that's leaking and it's not working very well. You can fix it in two ways. You can get your hammer and nails out and your boards can actually fix the boat.

02:58:50,498 --> 02:58:51,198
SPEAKER_0:  or you can...

02:58:51,426 --> 02:58:54,878
SPEAKER_0:  Duct tape it for now to get yourself across the river, but it's not actually fixed.

02:58:55,234 --> 02:58:55,550
SPEAKER_0:  So.

02:58:56,034 --> 02:58:56,766
SPEAKER_0:  Ideally...

02:58:57,122 --> 02:58:59,262
SPEAKER_0:  down the road, I have repaired.

02:58:59,490 --> 02:59:06,430
SPEAKER_0:  whatever kind of bizarre mental illness that I have that makes me procrastinate, in a very, like, I just don't self-defeat in this way anymore.

02:59:06,754 --> 02:59:07,902
SPEAKER_0:  But in the meantime...

02:59:08,898 --> 02:59:19,838
SPEAKER_0:  I can duct tape the boat by bringing what I call the panic monster into the situation via things like this and this scary person and having external pressure to have external pressure of some kind is critical.

02:59:20,290 --> 02:59:20,926
SPEAKER_0:  For me.

02:59:21,218 --> 02:59:25,662
SPEAKER_0:  It's it's yes. I don't have the muscle to do the work. I need to do without external pressure

02:59:25,986 --> 02:59:26,590
SPEAKER_1:  By the way, Liv.

02:59:27,042 --> 02:59:29,278
SPEAKER_1:  would is there a possible future we write a book

02:59:29,538 --> 02:59:29,854
SPEAKER_1:  Ugh.

02:59:30,722 --> 02:59:38,597
SPEAKER_0:  And meanwhile, by the way, huge procrastinator. That's the funny thing about this. How long did your last video take? Oh my god.

02:59:38,597 --> 02:59:42,430
SPEAKER_1:  Is there advice that you give to live how to get the videos done faster?

02:59:42,562 --> 02:59:46,718
SPEAKER_0:  Well, it would be the same exact thing. I mean, actually, I can give good procrastination advice. panic monster

02:59:47,074 --> 02:59:51,966
SPEAKER_0:  Yeah, well we should do it together. It should be like we have this date, but you know it's

02:59:52,482 --> 02:59:59,902
SPEAKER_0:  It's um... We should actually just do another bet. I have to have my script done by this time. Yes. Yeah, so I gotta get the third part out. Because then you'll actually do it. And um,

03:00:00,290 --> 03:00:00,926
SPEAKER_0:  and

03:00:01,250 --> 03:00:06,206
SPEAKER_0:  And it's not the thing is the time in, but it's like, if you could take three weeks on a video and instead you take.

03:00:06,914 --> 03:00:07,518
SPEAKER_0:  10 weeks?

03:00:08,194 --> 03:00:23,838
SPEAKER_0:  It's not like, oh, well, I'm having more fun in those 10 weeks. The whole 10 weeks are bad. Yeah, it's torture. Bad. So you're just having a bad time and you're getting less work done and less work out. And it's not like you're enjoying your personal life. It's bad for your relationships. It's bad for your own. Well, you keep doing it anyway.

03:00:24,258 --> 03:00:25,502
SPEAKER_0:  Yeah, well, a lot of people.

03:00:25,826 --> 03:00:28,318
SPEAKER_0:  Why do people have troubles keeping a diet?

03:00:28,642 --> 03:00:29,767
SPEAKER_0:  Right? Yeah. Don't mind!

03:00:29,767 --> 03:00:32,017
SPEAKER_1:  Why'd you point at me?

03:00:32,017 --> 03:00:32,958
SPEAKER_0:  That was offensive.

03:00:33,442 --> 03:00:36,067
SPEAKER_0:  What's your procrastination weakness? Do you have one?

03:00:36,067 --> 03:00:38,686
SPEAKER_1:  Everything everything everything

03:00:39,042 --> 03:00:40,510
SPEAKER_1:  It's everything, preparing.

03:00:40,770 --> 03:00:42,302
SPEAKER_1:  for conversation. I had your book.

03:00:42,882 --> 03:00:43,678
SPEAKER_1:  Amazing book.

03:00:44,002 --> 03:00:45,758
SPEAKER_1:  I really enjoyed it. I started reading it.

03:00:46,658 --> 03:00:47,966
SPEAKER_1:  I was like, this is awesome.

03:00:48,418 --> 03:00:53,534
SPEAKER_1:  It's so awesome that I'm going to save it when I'm behind a computer and can take notes.

03:00:53,826 --> 03:00:54,910
SPEAKER_1:  Like good notes.

03:00:55,138 --> 03:00:58,014
SPEAKER_1:  Of course that resulted in like last minute, everything, everything.

03:00:58,370 --> 03:01:00,094
SPEAKER_1:  Everything I'm doing in my life.

03:01:00,418 --> 03:01:07,294
SPEAKER_0:  Not everyone's like that. People self-defeat in different ways. Some people don't have this particular problem. Adam Grant is a, he calls himself a pre-crastinator where-

03:01:07,522 --> 03:01:08,574
SPEAKER_0:  She gets an assignment.

03:01:09,154 --> 03:01:09,662
SPEAKER_0:  He will.

03:01:10,370 --> 03:01:16,702
SPEAKER_0:  go home and do it until it's done and handed it, which is also not necessarily good. You know, it's like you're rushing it either way, but it's better.

03:01:17,474 --> 03:01:20,478
SPEAKER_0:  Some people have the opposite thing where they will.

03:01:20,866 --> 03:01:22,174
SPEAKER_0:  De..de..de..de..de..

03:01:22,626 --> 03:01:31,075
SPEAKER_0:  the looming deadline makes them so anxious that they go and fix it, right? And the procrastinator, I think, has a similar anxiety, but they resolve it in a totally different way.

03:01:31,075 --> 03:01:32,575
SPEAKER_1:  Don't solve it, just live with the anxiety.

03:01:32,575 --> 03:01:34,846
SPEAKER_0:  Right. Right. They just live with the exact... Now, I think...

03:01:35,106 --> 03:01:39,966
SPEAKER_0:  There's an even bigger group of people. So there's these people, the Adam Grant, there's people like me.

03:01:40,290 --> 03:01:44,510
SPEAKER_0:  And then there's people who have a healthy relationship with deadlines, but they're still part of a bigger group of people.

03:01:45,186 --> 03:01:45,886
SPEAKER_0:  actually

03:01:46,178 --> 03:01:47,678
SPEAKER_0:  They, they, um...

03:01:48,802 --> 03:01:49,310
SPEAKER_0:  They.

03:01:49,666 --> 03:01:50,238
SPEAKER_0:  NEED

03:01:50,466 --> 03:01:51,166
SPEAKER_0:  deadline.

03:01:51,778 --> 03:01:55,230
SPEAKER_0:  there to do something. So they actually, they still are motivated.

03:01:55,874 --> 03:02:10,398
SPEAKER_0:  buy a deadline. And as soon as you have all the things in life that don't have a deadline, like working out and like working on that album you wanted to write, they don't do anything either. So there's actually like, that's why procrastination is a much bigger problem than people realize because it's not just the funny last second people. It's anyone who-

03:02:10,658 --> 03:02:13,150
SPEAKER_0:  actually can't get things done that don't have a double.

03:02:14,722 --> 03:02:15,774
SPEAKER_0:  You dedicate your book.

03:02:16,482 --> 03:02:22,398
SPEAKER_1:  quote, to Tannis, who never planned on being married to someone who would spend six years talking about his book.

03:02:22,690 --> 03:02:24,222
SPEAKER_1:  on politics, but here we are.

03:02:24,674 --> 03:02:27,710
SPEAKER_1:  What's the secret to a successful relationship with a procrastinator?

03:02:28,450 --> 03:02:29,566
SPEAKER_1:  That's maybe for both of you.

03:02:30,306 --> 03:02:31,646
SPEAKER_0:  Um, well, I think.

03:02:32,642 --> 03:02:35,267
SPEAKER_0:  The first most important thing. You already started with a-

03:02:35,267 --> 03:02:36,767
SPEAKER_1:  political answer I could tell okay

03:02:36,767 --> 03:02:40,734
SPEAKER_0:  No, the first and most important thing is because people who don't procrastinate

03:02:41,442 --> 03:02:42,078
SPEAKER_0:  If you're not...

03:02:42,370 --> 03:02:46,462
SPEAKER_0:  It's like you will, people, the instinct is to judge it as like a

03:02:47,586 --> 03:02:52,126
SPEAKER_0:  that either just think they're just being like a loser or they're taking it, they'll take it personally, you know.

03:02:52,354 --> 03:02:57,566
SPEAKER_0:  and instead to see this as like this is this is a some form of

03:02:58,210 --> 03:02:59,582
SPEAKER_0:  addiction or some form of

03:03:00,002 --> 03:03:01,310
SPEAKER_0:  um, ailment.

03:03:01,538 --> 03:03:08,926
SPEAKER_0:  you know, they're not just being a dick, right? Like they have a problem and some compassion, but then also maybe finding that line where you can.

03:03:09,186 --> 03:03:09,694
SPEAKER_0:  you know.

03:03:10,114 --> 03:03:15,902
SPEAKER_0:  maybe apply some tough love, some middle ground. On the other hand, you might say that, you know, you don't want the significant other.

03:03:16,386 --> 03:03:23,422
SPEAKER_0:  relationship where it's like they're the one nagging you. Maybe that's, you don't want them even being part of that. And I think maybe it's better to have a live do it instead.

03:03:23,682 --> 03:03:32,926
SPEAKER_0:  Right, having someone who can create the infrastructure where they aren't the direct stick. You need a bit of carrot and stick, right? Maybe they can be the person who keeps reminding them of the carrot.

03:03:34,018 --> 03:03:38,430
SPEAKER_0:  and then they set up the friend group to be the stick. And then that keeps your relationship. So yeah.

03:03:38,530 --> 03:03:39,966
SPEAKER_1:  in a good place. Security check'mon bro.

03:03:40,226 --> 03:03:40,574
SPEAKER_1:  Like.

03:03:40,866 --> 03:03:42,878
SPEAKER_1:  looming in the background, that's your friend, group.

03:03:43,554 --> 03:03:43,902
SPEAKER_1:  Okay.

03:03:44,290 --> 03:03:50,334
SPEAKER_1:  At the beginning of the conversation, we talked about how all of human history can be presented as a thousand page book.

03:03:51,170 --> 03:03:51,518
SPEAKER_1:  out

03:03:52,098 --> 03:03:55,742
SPEAKER_1:  What are you excited about for the 1000th?

03:03:57,186 --> 03:03:58,910
SPEAKER_1:  How do you say that? First page.

03:03:59,874 --> 03:04:01,566
SPEAKER_1:  uh... so the next two hundred fifty years

03:04:02,114 --> 03:04:04,414
SPEAKER_1:  What are you most excited about?

03:04:05,026 --> 03:04:07,102
SPEAKER_0:  I'm most excited about.

03:04:07,490 --> 03:04:07,998
SPEAKER_0:  Um...

03:04:08,770 --> 03:04:10,046
SPEAKER_0:  Have you read the Fable of the Dragon?

03:04:10,754 --> 03:04:11,070
SPEAKER_0:  Okay.

03:04:11,970 --> 03:04:19,998
SPEAKER_0:  It's an allegory for death and it's Nick Bostrom and he talks about the, he compares death to a dragon that eats.

03:04:20,290 --> 03:04:26,462
SPEAKER_0:  60 million people or whatever the number is every year. And you just every year we shepherd those people up and they feed them to the dragon and

03:04:26,754 --> 03:04:35,230
SPEAKER_0:  that there's a Stockholm syndrome when we say that's just a lot of man and that's what we have to do. And anyone who says maybe we should try to beat the dragon, they get called vain and narcissistic.

03:04:35,842 --> 03:04:37,054
SPEAKER_0:  But someone who...

03:04:37,826 --> 03:04:45,598
SPEAKER_0:  tries to someone who does chemo, no one calls them being a narcissistic. They say they're good, good for you, right? You're a hero, you're fighting the good fight.

03:04:46,018 --> 03:04:48,382
SPEAKER_0:  So I think there's some disconnect here, and I think that if we can.

03:04:49,442 --> 03:04:53,278
SPEAKER_0:  Get out of that Stockholm syndrome and realize that death is just the machine.

03:04:53,538 --> 03:04:55,422
SPEAKER_0:  the human physical machine.

03:04:55,778 --> 03:04:56,222
SPEAKER_0:  Failing.

03:04:56,450 --> 03:04:56,830
SPEAKER_0:  and that.

03:04:57,634 --> 03:04:58,398
SPEAKER_0:  There's no.

03:04:58,722 --> 03:05:02,110
SPEAKER_0:  The law of nature that says you can't, with enough technology...

03:05:02,754 --> 03:05:03,198
SPEAKER_0:  Um.

03:05:03,970 --> 03:05:09,118
SPEAKER_0:  repair the machine and keep it going until no one wants to live forever. People think they do.adership.io

03:05:09,346 --> 03:05:10,590
SPEAKER_0:  But until people are.

03:05:10,882 --> 03:05:11,294
SPEAKER_0:  Ready.

03:05:11,714 --> 03:05:16,766
SPEAKER_0:  And I think when we hit a world where we have enough tech that we can continue.

03:05:17,154 --> 03:05:20,542
SPEAKER_0:  to keep the human machine alive until the person says, I'm done, I'm ready.

03:05:20,962 --> 03:05:33,598
SPEAKER_0:  I think we will look back and we will think that anything before that time, that'll be the real ADBC. You know, we'll look back at BC before the big advancement and it'll seem so sad and so heartbreaking, barbaric and people will say, I can't believe.

03:05:34,274 --> 03:05:38,910
SPEAKER_0:  that humans like us had to live with that when they lost loved ones and they died before they were ready.

03:05:39,266 --> 03:05:42,174
SPEAKER_0:  I think that's the ultimate achievement, but we need to.

03:05:42,626 --> 03:05:43,006
SPEAKER_0:  Stop.

03:05:44,290 --> 03:05:45,822
SPEAKER_0:  criticizing and smearing people.

03:05:46,466 --> 03:05:47,591
SPEAKER_0:  who you talk about it.

03:05:47,591 --> 03:05:51,070
SPEAKER_1:  So you think that's actually doable in the next 250 years?

03:05:52,738 --> 03:05:57,502
SPEAKER_0:  A lot happens in 250 years, especially when technology really exponentially. Paulie Yeah!

03:05:58,626 --> 03:05:59,966
SPEAKER_1:  you think humans will be around?

03:06:00,514 --> 03:06:03,139
SPEAKER_1:  versus a a complete takes over where i mean it means

03:06:03,139 --> 03:06:11,838
SPEAKER_0:  I mean, look, the optimist in me and maybe the stupid kind of 20, 23 person in me says, yeah, of course we'll make it. We'll figure it out. Remember, next week, we will be rolling all the way into the heat in the Airbus!

03:06:12,578 --> 03:06:15,710
SPEAKER_0:  I mean, we are going into a-

03:06:16,162 --> 03:06:22,110
SPEAKER_0:  I have a friend who knows as much about the future as anyone I know. He's a big investor.

03:06:22,338 --> 03:06:26,014
SPEAKER_0:  future tech and he's really on the pulse with things and he just says the future is going to be weird.

03:06:26,434 --> 03:06:34,654
SPEAKER_0:  That's what he says, if you're just gonna be weird. And it's gonna be weird. Don't look at the last few decades of your life and apply that forward and say, that's just what life is like. No, no, no, it's gonna be weird and different.

03:06:35,458 --> 03:06:39,582
SPEAKER_1:  While some of my favorite things in this world are weird. And speaking of which.

03:06:39,938 --> 03:06:45,150
SPEAKER_1:  It's good to have this conversation. It's good to have you as friends. This was an incredible one. Thanks for coming back.

03:06:45,666 --> 03:06:48,222
SPEAKER_1:  And Liv, thanks for talking with me a bunch more times.

03:06:48,450 --> 03:06:49,575
SPEAKER_1:  This is awesome. Thanks.

03:06:49,575 --> 03:06:50,078
SPEAKER_0:  Thank you.

03:06:51,138 --> 03:06:53,246
SPEAKER_1:  Thanks for listening to this conversation with Tim Urban.

03:06:53,666 --> 03:06:56,862
SPEAKER_1:  To support this podcast, please check out our sponsors in the description.

03:06:57,378 --> 03:06:57,822
SPEAKER_1:  And now...

03:06:58,082 --> 03:07:00,286
SPEAKER_1:  Let me leave you with some words from Winston Churchill.

03:07:01,250 --> 03:07:02,622
SPEAKER_1:  When there's no enemy within.

03:07:03,042 --> 03:07:04,350
SPEAKER_1:  The enemy's outside.

03:07:04,802 --> 03:07:05,662
SPEAKER_1:  cannot hurt you.

03:07:06,754 --> 03:07:08,574
SPEAKER_1:  Thank you for listening and hope to see you.

03:07:08,930 --> 03:07:09,566
SPEAKER_1:  next time.
